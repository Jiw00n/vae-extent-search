{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d665df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "project_root = \"/root/work/tenset\"\n",
    "os.environ[\"TVM_HOME\"] = f\"{project_root}\"\n",
    "os.environ[\"TVM_LIBRARY_PATH\"] = f\"{project_root}/build\"\n",
    "if f\"{project_root}/python\" not in sys.path:\n",
    "    sys.path.insert(0, f\"{project_root}/python\")\n",
    "\n",
    "sys.path = [p for p in sys.path if not p.startswith(f\"{project_root}/build\")]\n",
    "sys.path.append(f\"{project_root}/build\")\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = f\"{project_root}/build:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95a67ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import tvm\n",
    "from tvm.auto_scheduler.utils import to_str_round\n",
    "from tvm.auto_scheduler.cost_model import RandomModelInternal\n",
    "\n",
    "sys.path.append(\"/root/work/tenset/scripts\")\n",
    "from common import load_and_register_tasks, str2bool\n",
    "from train_model import train_zero_shot\n",
    "\n",
    "from tvm.auto_scheduler.dataset import Dataset, LearningTask\n",
    "from tvm.auto_scheduler.cost_model.xgb_model import XGBModelInternal\n",
    "from tvm.auto_scheduler.cost_model.mlp_model import MLPModelInternal\n",
    "from tvm.auto_scheduler.cost_model.lgbm_model import LGBModelInternal\n",
    "from tvm.auto_scheduler.cost_model.tabnet_model import TabNetModelInternal\n",
    "from tvm.auto_scheduler.cost_model.metric import (\n",
    "    metric_rmse,\n",
    "    metric_r_squared,\n",
    "    metric_pairwise_comp_accuracy,\n",
    "    metric_top_k_recall,\n",
    "    metric_peak_score,\n",
    "    metric_mape,\n",
    "    random_mix,\n",
    ")\n",
    "\n",
    "load_and_register_tasks()\n",
    "dataset = pickle.load(open(\"../../dataset_30.pkl\", \"rb\"))\n",
    "# train_set, test_set = dataset.random_split_within_task(0.9)\n",
    "\n",
    "# tenset_model = MLPModelInternal()\n",
    "# tenset_model.fit_base(train_set, valid_set=test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "182c0de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ ìƒ˜í”Œ ìˆ˜: 3524\n",
      "Flattenëœ features shape: (14096, 164)\n",
      "Segment sizes - Min: 4, Max: 4, Mean: 4.00\n"
     ]
    }
   ],
   "source": [
    "# SegmentDataLoader íŒ¨í„´ì— ë§ì¶° ë°ì´í„° ì¤€ë¹„\n",
    "raw_features = list(dataset.features.values())[1:2]\n",
    "raw_throughputs = list(dataset.throughputs.values())[1:2]\n",
    "\n",
    "# ê°€ë³€ ê¸¸ì´ featuresì™€ labelsë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥\n",
    "features_list = []  # ê° ìƒ˜í”Œì˜ feature (seq_len, feature_dim)\n",
    "costs = []\n",
    "segment_sizes_list = []  # ê° ìƒ˜í”Œì˜ segment ê¸¸ì´\n",
    "\n",
    "for raw_feature, raw_throughput in zip(raw_features, raw_throughputs):\n",
    "    for feature, throughput in zip(raw_feature, raw_throughput):\n",
    "        if feature.shape[0] != 1 and throughput > 1e-10 :  # segment ê¸¸ì´ê°€ 1ì´ ì•„ë‹Œ ê²ƒë§Œ\n",
    "            features_list.append(feature)\n",
    "            # costs.append(-np.log(throughput))\n",
    "            costs.append(throughput)\n",
    "            segment_sizes_list.append(feature.shape[0])\n",
    "\n",
    "# numpy arrayë¡œ ë³€í™˜\n",
    "costs = np.array(costs, dtype=np.float32)\n",
    "segment_sizes = np.array(segment_sizes_list, dtype=np.int32)\n",
    "\n",
    "# ëª¨ë“  featuresë¥¼ flatten (SegmentDataLoader ë°©ì‹)\n",
    "flatten_features = np.concatenate(features_list, axis=0).astype(np.float32)\n",
    "\n",
    "print(f\"ì´ ìƒ˜í”Œ ìˆ˜: {len(costs)}\")\n",
    "print(f\"Flattenëœ features shape: {flatten_features.shape}\")\n",
    "print(f\"Segment sizes - Min: {segment_sizes.min()}, Max: {segment_sizes.max()}, Mean: {segment_sizes.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ed47e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í›ˆë ¨ ìƒ˜í”Œ ìˆ˜: 2819, ê²€ì¦ ìƒ˜í”Œ ìˆ˜: 705\n",
      "í›ˆë ¨ flatten features: (11276, 164)\n",
      "ê²€ì¦ flatten features: (2820, 164)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "# ì‹œë“œ ê³ ì •\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "class SegmentRegressionDataset:\n",
    "    \"\"\"SegmentDataLoader íŒ¨í„´ì„ ë”°ë¥´ëŠ” ë°ì´í„°ì…‹\"\"\"\n",
    "    def __init__(self, segment_sizes, features, labels, batch_size, device, \n",
    "                 fea_norm_vec=None, shuffle=False, seed=42):\n",
    "        self.device = device\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.number = len(labels)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.segment_sizes = torch.tensor(segment_sizes, dtype=torch.int32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        \n",
    "        if fea_norm_vec is not None:\n",
    "            self.normalize(fea_norm_vec)\n",
    "        \n",
    "        self.feature_offsets = (\n",
    "            torch.cumsum(self.segment_sizes, 0, dtype=torch.int32) - self.segment_sizes\n",
    "        ).cpu().numpy()\n",
    "        \n",
    "        self.iter_order = self.pointer = None\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "    \n",
    "    def normalize(self, norm_vector=None):\n",
    "        if norm_vector is None:\n",
    "            norm_vector = torch.ones((self.features.shape[1],))\n",
    "            for i in range(self.features.shape[1]):\n",
    "                max_val = self.features[:, i].max().item()\n",
    "                if max_val > 0:\n",
    "                    norm_vector[i] = max_val\n",
    "        self.features /= norm_vector\n",
    "        return norm_vector\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            self.iter_order = torch.from_numpy(self.rng.permutation(self.number))\n",
    "        else:\n",
    "            self.iter_order = torch.arange(self.number)\n",
    "        self.pointer = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.pointer >= self.number:\n",
    "            raise StopIteration\n",
    "        \n",
    "        batch_indices = self.iter_order[self.pointer: self.pointer + self.batch_size]\n",
    "        self.pointer += self.batch_size\n",
    "        return self._fetch_indices(batch_indices)\n",
    "    \n",
    "    def _fetch_indices(self, indices):\n",
    "        segment_sizes = self.segment_sizes[indices]\n",
    "        \n",
    "        feature_offsets = self.feature_offsets[indices]\n",
    "        feature_indices = np.empty((segment_sizes.sum().item(),), dtype=np.int32)\n",
    "        ct = 0\n",
    "        for offset, seg_size in zip(feature_offsets, segment_sizes.numpy()):\n",
    "            feature_indices[ct: ct + seg_size] = np.arange(offset, offset + seg_size, 1)\n",
    "            ct += seg_size\n",
    "        \n",
    "        features = self.features[feature_indices]\n",
    "        labels = self.labels[indices]\n",
    "        return (x.to(self.device) for x in (segment_sizes, features, labels))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.number\n",
    "\n",
    "\n",
    "# Train/Val ë¶„í• \n",
    "n_samples = len(costs)\n",
    "indices = np.arange(n_samples)\n",
    "train_indices, val_indices = train_test_split(indices, test_size=0.2, shuffle=False)\n",
    "\n",
    "train_segment_sizes = segment_sizes[train_indices]\n",
    "val_segment_sizes = segment_sizes[val_indices]\n",
    "train_labels = costs[train_indices]\n",
    "val_labels = costs[val_indices]\n",
    "\n",
    "train_feature_list = [features_list[i] for i in train_indices]\n",
    "val_feature_list = [features_list[i] for i in val_indices]\n",
    "\n",
    "train_flatten_features = np.concatenate(train_feature_list, axis=0).astype(np.float32)\n",
    "val_flatten_features = np.concatenate(val_feature_list, axis=0).astype(np.float32)\n",
    "\n",
    "print(f\"í›ˆë ¨ ìƒ˜í”Œ ìˆ˜: {len(train_labels)}, ê²€ì¦ ìƒ˜í”Œ ìˆ˜: {len(val_labels)}\")\n",
    "print(f\"í›ˆë ¨ flatten features: {train_flatten_features.shape}\")\n",
    "print(f\"ê²€ì¦ flatten features: {val_flatten_features.shape}\")\n",
    "\n",
    "# ì •ê·œí™”\n",
    "fea_norm_vec = torch.ones((train_flatten_features.shape[1],))\n",
    "for i in range(train_flatten_features.shape[1]):\n",
    "    max_val = float(train_flatten_features[:, i].max())\n",
    "    if max_val > 0:\n",
    "        fea_norm_vec[i] = max_val\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader = SegmentRegressionDataset(\n",
    "    train_segment_sizes, train_flatten_features, train_labels,\n",
    "    batch_size=128, device=device, fea_norm_vec=fea_norm_vec, shuffle=True, seed=SEED\n",
    ")\n",
    "val_loader = SegmentRegressionDataset(\n",
    "    val_segment_sizes, val_flatten_features, val_labels,\n",
    "    batch_size=128, device=device, fea_norm_vec=fea_norm_vec, shuffle=False, seed=SEED\n",
    ")\n",
    "\n",
    "X_train = train_flatten_features\n",
    "X_val = val_flatten_features\n",
    "y_train = train_labels\n",
    "y_val = val_labels\n",
    "train_dataset = train_loader\n",
    "val_dataset = val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e93b69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment VAE ëª¨ë¸ ì •ì˜ ì™„ë£Œ!\n",
      "- Input dim: 164\n",
      "- Hidden dim: 256\n",
      "- Latent dim: 64\n",
      "Flow: features â†’ segment_encoder â†’ segment_sum â†’ VAE (encode/decode)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "import random\n",
    "\n",
    "# ì‹œë“œ ê³ ì •\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "class SegmentVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Segment-level VAE:\n",
    "    1. Segment Encoder: ê° rowë¥¼ hidden_dimìœ¼ë¡œ ë³€í™˜\n",
    "    2. Segment Sum: rowë“¤ì„ segmentë³„ë¡œ í•©ì‚°\n",
    "    3. VAE: í•©ì‚°ëœ ë²¡í„°ë¥¼ encode â†’ decode (reconstruction)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=256, latent_dim=64, dropout=0.1):\n",
    "        super(SegmentVAE, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # ========== Segment Encoder (VAE ì „ì— ì ìš©) ==========\n",
    "        self.segment_encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # ========== VAE Encoder (segment sum í›„ ì ìš©) ==========\n",
    "        self.norm = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Residual ë ˆì´ì–´ë“¤\n",
    "        self.l0 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # VAE latent parameters\n",
    "        self.fc_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # ========== VAE Decoder (hidden_dimìœ¼ë¡œ reconstruction) ==========\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),  # ì¶œë ¥: segment sum ë²¡í„° (hidden_dim)\n",
    "        )\n",
    "        \n",
    "    def segment_sum(self, segment_sizes, features):\n",
    "        \"\"\"SegmentSum: ê° segment ë‚´ rowë“¤ì„ í•©ì‚°\"\"\"\n",
    "        n_seg = segment_sizes.shape[0]\n",
    "        device = features.device\n",
    "        segment_sizes = segment_sizes.long()\n",
    "        \n",
    "        segment_indices = torch.repeat_interleave(\n",
    "            torch.arange(n_seg, device=device), segment_sizes\n",
    "        )\n",
    "        \n",
    "        n_dim = features.shape[1]\n",
    "        segment_sum = torch.scatter_add(\n",
    "            torch.zeros((n_seg, n_dim), dtype=features.dtype, device=device),\n",
    "            0,\n",
    "            segment_indices.view(-1, 1).expand(-1, n_dim),\n",
    "            features,\n",
    "        )\n",
    "        return segment_sum\n",
    "    \n",
    "    def encode(self, h):\n",
    "        \"\"\"VAE Encoder: segment sum ë²¡í„° â†’ (mean, logvar)\"\"\"\n",
    "        # BatchNorm\n",
    "        h = self.norm(h)\n",
    "        \n",
    "        # Residual connections\n",
    "        h = self.l0(h) + h\n",
    "        h = self.l1(h) + h\n",
    "        \n",
    "        # Latent parameters\n",
    "        mean = self.fc_mean(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mean, logvar\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        \"\"\"Reparameterization trick\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"VAE Decoder: latent z â†’ reconstructed segment sum ë²¡í„°\"\"\"\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, segment_sizes, features):\n",
    "        \"\"\"\n",
    "        Forward pass:\n",
    "        1. Segment Encoder: features â†’ hidden\n",
    "        2. Segment Sum: hidden â†’ segment-level ë²¡í„°\n",
    "        3. VAE: encode â†’ reparameterize â†’ decode\n",
    "        \"\"\"\n",
    "        # 1. Segment Encoder\n",
    "        h = self.segment_encoder(features)  # (total_rows, hidden_dim)\n",
    "        \n",
    "        # 2. Segment Sum\n",
    "        segment_sum_vec = self.segment_sum(segment_sizes, h)  # (n_seg, hidden_dim)\n",
    "        \n",
    "        # 3. VAE Encode\n",
    "        mean, logvar = self.encode(segment_sum_vec)\n",
    "        \n",
    "        # 4. Reparameterize\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        \n",
    "        # 5. VAE Decode (reconstruct segment sum ë²¡í„°)\n",
    "        recon = self.decode(z)  # (n_seg, hidden_dim)\n",
    "        \n",
    "        return mean, logvar, z, recon, segment_sum_vec\n",
    "\n",
    "\n",
    "def vae_loss(recon, original, mean, logvar, beta):\n",
    "    \"\"\"\n",
    "    VAE Loss = Reconstruction Loss + Î² * KL Divergence\n",
    "    - Reconstruction: MSE between original and reconstructed segment sum ë²¡í„°\n",
    "    - KLD: D_KL(q(z|x) || p(z)), where p(z) = N(0, I)\n",
    "    \"\"\"\n",
    "    recon_loss = F.mse_loss(recon, original, reduction='sum')\n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "    \n",
    "    total_loss = recon_loss + beta * kld_loss\n",
    "    return total_loss, recon_loss, kld_loss\n",
    "\n",
    "\n",
    "print(\"Segment VAE ëª¨ë¸ ì •ì˜ ì™„ë£Œ!\")\n",
    "print(f\"- Input dim: {train_flatten_features.shape[1]}\")\n",
    "print(f\"- Hidden dim: 256\")\n",
    "print(f\"- Latent dim: 64\")\n",
    "print(\"Flow: features â†’ segment_encoder â†’ segment_sum â†’ VAE (encode/decode)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4fb8867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAECostPredictor ëª¨ë¸ ì •ì˜ ì™„ë£Œ!\n",
      "íŠ¹ì§•:\n",
      "  - Pretrained VAE encoder + Cost predictor\n",
      "  - ì „ì²´ forward ê²½ë¡œ ë¯¸ë¶„ ê°€ëŠ¥\n",
      "  - Encoder/Predictor ë¶„ë¦¬ í•™ìŠµë¥  ì§€ì›\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# VAE ê¸°ë°˜ Cost Predictor ëª¨ë¸ ì •ì˜\n",
    "# ============================================================\n",
    "# Pretrained VAE encoder + Cost Predictor\n",
    "# ì „ì²´ forward ê²½ë¡œê°€ ë¯¸ë¶„ ê°€ëŠ¥í•´ì•¼ í•¨\n",
    "\n",
    "class VAECostPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    VAE ê¸°ë°˜ Cost Regression ëª¨ë¸\n",
    "    \n",
    "    êµ¬ì¡°:\n",
    "    - input â†’ segment_encoder â†’ segment_sum â†’ VAE encoder â†’ z â†’ cost_predictor â†’ cost\n",
    "    \n",
    "    íŠ¹ì§•:\n",
    "    - Pretrained VAE encoderë¥¼ finetune (ì‘ì€ learning rate)\n",
    "    - Cost predictorëŠ” ë” í° learning rateë¡œ í•™ìŠµ\n",
    "    - ì „ì²´ forward ê²½ë¡œê°€ ì™„ì „íˆ ë¯¸ë¶„ ê°€ëŠ¥ (detach, stop_grad ì—†ìŒ)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim=256, latent_dim=128, \n",
    "                 predictor_hidden=256, predictor_layers=3, dropout=0.1):\n",
    "        super(VAECostPredictor, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # ========== Pretrained VAE Encoder ë¶€ë¶„ ==========\n",
    "        # Segment Encoder\n",
    "        self.segment_encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # VAE Encoder layers\n",
    "        self.norm = nn.BatchNorm1d(hidden_dim)\n",
    "        self.l0 = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU())\n",
    "        self.l1 = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU())\n",
    "        self.fc_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # ========== Cost Predictor (ìƒˆë¡œ í•™ìŠµ) ==========\n",
    "        predictor_modules = []\n",
    "        current_dim = latent_dim\n",
    "        for i in range(predictor_layers):\n",
    "            predictor_modules.extend([\n",
    "                nn.Linear(current_dim, predictor_hidden),\n",
    "                nn.BatchNorm1d(predictor_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout) if i < predictor_layers - 1 else nn.Identity(),\n",
    "            ])\n",
    "            current_dim = predictor_hidden\n",
    "        predictor_modules.append(nn.Linear(predictor_hidden, 1))\n",
    "        \n",
    "        self.cost_predictor = nn.Sequential(*predictor_modules)\n",
    "        \n",
    "    def segment_sum(self, segment_sizes, features):\n",
    "        \"\"\"SegmentSum: ê° segment ë‚´ rowë“¤ì„ í•©ì‚°\"\"\"\n",
    "        n_seg = segment_sizes.shape[0]\n",
    "        device = features.device\n",
    "        segment_sizes = segment_sizes.long()\n",
    "        \n",
    "        segment_indices = torch.repeat_interleave(\n",
    "            torch.arange(n_seg, device=device), segment_sizes\n",
    "        )\n",
    "        \n",
    "        n_dim = features.shape[1]\n",
    "        segment_sum = torch.scatter_add(\n",
    "            torch.zeros((n_seg, n_dim), dtype=features.dtype, device=device),\n",
    "            0,\n",
    "            segment_indices.view(-1, 1).expand(-1, n_dim),\n",
    "            features,\n",
    "        )\n",
    "        return segment_sum\n",
    "    \n",
    "    def encode(self, segment_sizes, features):\n",
    "        \"\"\"\n",
    "        Full encoding path: features â†’ z\n",
    "        ì™„ì „íˆ ë¯¸ë¶„ ê°€ëŠ¥\n",
    "        \"\"\"\n",
    "        # Segment Encoder\n",
    "        h = self.segment_encoder(features)  # (total_rows, hidden_dim)\n",
    "        \n",
    "        # Segment Sum\n",
    "        segment_sum_vec = self.segment_sum(segment_sizes, h)  # (n_seg, hidden_dim)\n",
    "        \n",
    "        # VAE Encoder\n",
    "        h = self.norm(segment_sum_vec)\n",
    "        h = self.l0(h) + h  # Residual\n",
    "        h = self.l1(h) + h  # Residual\n",
    "        \n",
    "        mean = self.fc_mean(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        \n",
    "        return mean, logvar, segment_sum_vec\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        \"\"\"Reparameterization trick - ë¯¸ë¶„ ê°€ëŠ¥\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "    \n",
    "    def predict_cost(self, z):\n",
    "        \"\"\"z â†’ cost prediction - ì™„ì „íˆ ë¯¸ë¶„ ê°€ëŠ¥\"\"\"\n",
    "        return self.cost_predictor(z).squeeze(-1)\n",
    "    \n",
    "    def forward(self, segment_sizes, features, use_mean=False):\n",
    "        \"\"\"\n",
    "        Forward pass: input â†’ z â†’ cost\n",
    "        \n",
    "        Args:\n",
    "            use_mean: Trueë©´ reparameterize ëŒ€ì‹  mean ì‚¬ìš© (inferenceìš©)\n",
    "        \n",
    "        Returns:\n",
    "            cost_pred: ì˜ˆì¸¡ëœ cost\n",
    "            mean: latent mean\n",
    "            logvar: latent log-variance\n",
    "            z: sampled/mean latent vector\n",
    "        \"\"\"\n",
    "        mean, logvar, segment_sum_vec = self.encode(segment_sizes, features)\n",
    "        \n",
    "        if use_mean:\n",
    "            z = mean  # Inferenceì‹œ deterministic\n",
    "        else:\n",
    "            z = self.reparameterize(mean, logvar)  # Trainingì‹œ stochastic\n",
    "        \n",
    "        cost_pred = self.predict_cost(z)\n",
    "        \n",
    "        return cost_pred, mean, logvar, z\n",
    "    \n",
    "    def get_encoder_params(self):\n",
    "        \"\"\"Encoder íŒŒë¼ë¯¸í„° (ì‘ì€ lr)\"\"\"\n",
    "        encoder_params = []\n",
    "        encoder_params.extend(self.segment_encoder.parameters())\n",
    "        encoder_params.extend(self.norm.parameters())\n",
    "        encoder_params.extend(self.l0.parameters())\n",
    "        encoder_params.extend(self.l1.parameters())\n",
    "        encoder_params.extend(self.fc_mean.parameters())\n",
    "        encoder_params.extend(self.fc_logvar.parameters())\n",
    "        return encoder_params\n",
    "    \n",
    "    def get_predictor_params(self):\n",
    "        \"\"\"Predictor íŒŒë¼ë¯¸í„° (í° lr)\"\"\"\n",
    "        return self.cost_predictor.parameters()\n",
    "    \n",
    "    def load_pretrained_encoder(self, checkpoint):\n",
    "        \"\"\"Pretrained VAE encoder ê°€ì¤‘ì¹˜ ë¡œë“œ\"\"\"\n",
    "        \n",
    "        # VAE state dictì—ì„œ encoder ê´€ë ¨ ë¶€ë¶„ë§Œ ì¶”ì¶œ\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            vae_state = checkpoint['model_state_dict']\n",
    "        else:\n",
    "            vae_state = checkpoint\n",
    "        \n",
    "        # ë§¤ì¹­ë˜ëŠ” í‚¤ë§Œ ë¡œë“œ\n",
    "        encoder_keys = ['segment_encoder', 'norm', 'l0', 'l1', 'fc_mean', 'fc_logvar']\n",
    "        own_state = self.state_dict()\n",
    "        \n",
    "        loaded_keys = []\n",
    "        for name, param in vae_state.items():\n",
    "            if any(name.startswith(k) for k in encoder_keys):\n",
    "                if name in own_state and own_state[name].shape == param.shape:\n",
    "                    own_state[name].copy_(param)\n",
    "                    loaded_keys.append(name)\n",
    "        \n",
    "        print(f\"Loaded {len(loaded_keys)} parameters from pretrained VAE\")\n",
    "        return checkpoint.get('config', {}), checkpoint.get('metrics', {})\n",
    "\n",
    "\n",
    "print(\"VAECostPredictor ëª¨ë¸ ì •ì˜ ì™„ë£Œ!\")\n",
    "print(\"íŠ¹ì§•:\")\n",
    "print(\"  - Pretrained VAE encoder + Cost predictor\")\n",
    "print(\"  - ì „ì²´ forward ê²½ë¡œ ë¯¸ë¶„ ê°€ëŠ¥\")\n",
    "print(\"  - Encoder/Predictor ë¶„ë¦¬ í•™ìŠµë¥  ì§€ì›\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1194d617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loss í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n",
      "  í•µì‹¬:\n",
      "  - pair_loss_fnì— delta íŒŒë¼ë¯¸í„° ì¶”ê°€ (í° ì°¨ì´ ìŒë§Œ í•„í„°ë§)\n",
      "  - smooth_loss: anchor detachë¡œ ì¸ì½”ë” ë³´í˜¸\n",
      "  - grad_direction: warmup + linear ramp\n",
      "  - z-scale ê¸°ë°˜ ìë™ ì¡°ì •\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Loss í•¨ìˆ˜ ì •ì˜ (ë””ë²„ê¹…ëœ ë²„ì „)\n",
    "# ============================================================\n",
    "# í•µì‹¬ ìˆ˜ì •:\n",
    "# 1. smooth_loss: anchor detachë¡œ ì¸ì½”ë” ë³´í˜¸\n",
    "# 2. grad_direction: warmup + linear ramp\n",
    "# 3. z-scale ê¸°ë°˜ noise_std, alpha ìë™ ì¡°ì •\n",
    "# 4. ìƒì„¸í•œ loss scale ë¡œê¹…\n",
    "\n",
    "def reg_loss_fn(cost_pred, cost_true, loss_type='mse'):\n",
    "    \"\"\"ê¸°ë³¸ íšŒê·€ ì†ì‹¤ (MSE ë˜ëŠ” MAE)\"\"\"\n",
    "    if loss_type == 'mse':\n",
    "        return F.mse_loss(cost_pred, cost_true)\n",
    "    elif loss_type == 'huber':\n",
    "        return F.smooth_l1_loss(cost_pred, cost_true)\n",
    "    else:  # mae\n",
    "        return F.l1_loss(cost_pred, cost_true)\n",
    "\n",
    "\n",
    "def pair_loss_fn(cost_pred, cost_true, margin=0.1, delta=None):\n",
    "    \"\"\"\n",
    "    Pairwise ranking loss\n",
    "    \n",
    "    Args:\n",
    "        cost_pred: ì˜ˆì¸¡ê°’\n",
    "        cost_true: ì‹¤ì œê°’\n",
    "        margin: ranking loss margin\n",
    "        delta: |y_i - y_j| > deltaì¸ ìŒë§Œ ì‚¬ìš© (Noneì´ë©´ ëª¨ë“  ìŒ ì‚¬ìš©)\n",
    "    \"\"\"\n",
    "    batch_size = cost_pred.size(0)\n",
    "    if batch_size < 2:\n",
    "        return torch.tensor(0.0, device=cost_pred.device)\n",
    "\n",
    "    idx = torch.arange(batch_size, device=cost_pred.device)\n",
    "    i_idx, j_idx = torch.meshgrid(idx, idx, indexing='ij')\n",
    "    mask = i_idx < j_idx\n",
    "\n",
    "    pred_i = cost_pred[i_idx[mask]]\n",
    "    pred_j = cost_pred[j_idx[mask]]\n",
    "    true_i = cost_true[i_idx[mask]]\n",
    "    true_j = cost_true[j_idx[mask]]\n",
    "\n",
    "    labels = torch.sign(true_j - true_i).float()\n",
    "    valid = labels != 0\n",
    "    \n",
    "    # delta í•„í„°ë§: í° ì°¨ì´ê°€ ìˆëŠ” ìŒë§Œ ì‚¬ìš©\n",
    "    if delta is not None:\n",
    "        diff_abs = torch.abs(true_j - true_i)\n",
    "        valid = valid & (diff_abs > delta)\n",
    "    \n",
    "    if valid.sum() == 0:\n",
    "        return torch.tensor(0.0, device=cost_pred.device)\n",
    "\n",
    "    return F.margin_ranking_loss(\n",
    "        pred_j[valid].view(-1), pred_i[valid].view(-1), labels[valid].view(-1),\n",
    "        margin=margin\n",
    "    )\n",
    "\n",
    "\n",
    "def smooth_loss_fn(model, z, noise_std=None, z_scale_factor=0.1):\n",
    "    \"\"\"\n",
    "    Smoothness Loss (ìˆ˜ì •ë¨)\n",
    "    \n",
    "    í•µì‹¬ ìˆ˜ì •:\n",
    "    1. anchorë¥¼ detachí•˜ì—¬ zë¡œì˜ gradient ì°¨ë‹¨ (ì¸ì½”ë” ë³´í˜¸)\n",
    "    2. z-scale ê¸°ë°˜ noise_std ìë™ ê³„ì‚°\n",
    "    \"\"\"\n",
    "    # z-scale ê¸°ë°˜ noise_std ìë™ ê³„ì‚°\n",
    "    z_scale = z.std(dim=0).mean().detach()\n",
    "    if noise_std is None:\n",
    "        noise_std = z_scale_factor * z_scale\n",
    "    else:\n",
    "        # ëª…ì‹œì  noise_stdê°€ ì£¼ì–´ì§€ë©´ z_scaleë¡œ ìŠ¤ì¼€ì¼ë§\n",
    "        noise_std = noise_std * z_scale / 0.1  # ê¸°ë³¸ z_scale=0.1 ê°€ì • ë³´ì •\n",
    "    \n",
    "    z_noisy = z + noise_std * torch.randn_like(z)\n",
    "    \n",
    "    # í•µì‹¬: anchorë¥¼ detach\n",
    "    cost_anchor = model.predict_cost(z).detach()\n",
    "    cost_noisy = model.predict_cost(z_noisy)\n",
    "    \n",
    "    return F.mse_loss(cost_noisy, cost_anchor)\n",
    "\n",
    "\n",
    "def kld_loss_fn(mean, logvar):\n",
    "    \"\"\"KL Divergence loss\"\"\"\n",
    "    kld_per = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp(), dim=-1)\n",
    "    return kld_per.mean()\n",
    "\n",
    "\n",
    "def grad_direction_pair_loss_fn(model, z, alpha=None, margin=0.01, eps=1e-8, \n",
    "                                  z_scale_factor=0.05, enabled=True):\n",
    "    \"\"\"\n",
    "    Gradient direction consistency loss (ìˆ˜ì •ë¨)\n",
    "    \n",
    "    í•µì‹¬ ìˆ˜ì •:\n",
    "    1. enabled=Falseë©´ 0 ë°˜í™˜ (warmup ì§€ì›)\n",
    "    2. z-scale ê¸°ë°˜ alpha ìë™ ê³„ì‚°\n",
    "    3. zë¥¼ detachí•˜ì—¬ ì¸ì½”ë” ë³´í˜¸ (augmentation lossì´ë¯€ë¡œ)\n",
    "    \"\"\"\n",
    "    if not enabled:\n",
    "        return torch.tensor(0.0, device=z.device)\n",
    "    \n",
    "    # z-scale ê¸°ë°˜ alpha ìë™ ê³„ì‚°\n",
    "    z_scale = z.std(dim=0).mean().detach()\n",
    "    if alpha is None:\n",
    "        alpha = z_scale_factor * z_scale\n",
    "    else:\n",
    "        alpha = alpha * z_scale / 0.1  # ê¸°ë³¸ z_scale=0.1 ê°€ì • ë³´ì •\n",
    "    \n",
    "    # ì¸ì½”ë” ë³´í˜¸: zë¥¼ detach\n",
    "    z_detached = z.detach().requires_grad_(True)\n",
    "    c_for_grad = model.predict_cost(z_detached)\n",
    "    grad_z = torch.autograd.grad(c_for_grad.sum(), z_detached, create_graph=False)[0]\n",
    "\n",
    "    g_hat = grad_z / (grad_z.norm(dim=-1, keepdim=True) + eps)\n",
    "    g_hat = g_hat.detach()\n",
    "\n",
    "    # augmentationì€ detached z ê¸°ì¤€ìœ¼ë¡œ\n",
    "    z_minus = z_detached - alpha * g_hat\n",
    "    z_plus  = z_detached + alpha * g_hat\n",
    "\n",
    "    c_anchor = model.predict_cost(z_detached)\n",
    "    c_minus  = model.predict_cost(z_minus)\n",
    "    c_plus   = model.predict_cost(z_plus)\n",
    "\n",
    "    loss = F.relu(c_minus - c_anchor + margin) + F.relu(c_anchor - c_plus + margin)\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def compute_total_loss_semi_supervised(model, \n",
    "                                        seg_labeled, fea_labeled, lab_labeled,\n",
    "                                        seg_unlabeled, fea_unlabeled,\n",
    "                                        config, epoch=0, return_components=False):\n",
    "    \"\"\"\n",
    "    Semi-supervised Total Loss ê³„ì‚° (ìˆ˜ì •ë¨)\n",
    "    \n",
    "    í•µì‹¬ ìˆ˜ì •:\n",
    "    1. use_mean=Trueë¡œ regression ì•ˆì •ì„± í–¥ìƒ\n",
    "    2. grad_direction warmup ì§€ì›\n",
    "    3. ìƒì„¸í•œ loss scale ë¡œê¹…\n",
    "    \"\"\"\n",
    "    # === Labeled forward pass (use_mean=True for stability) ===\n",
    "    cost_pred_l, mean_l, logvar_l, z_l = model(seg_labeled, fea_labeled, use_mean=True)\n",
    "    \n",
    "    # === Unlabeled forward pass ===\n",
    "    _, mean_u, logvar_u, z_u = model(seg_unlabeled, fea_unlabeled, use_mean=True)\n",
    "    \n",
    "    # z-scale ê³„ì‚° (ë¡œê¹… ë° ìë™ ìŠ¤ì¼€ì¼ë§ìš©)\n",
    "    z_scale_l = z_l.std(dim=0).mean().detach().item()\n",
    "    z_scale_u = z_u.std(dim=0).mean().detach().item()\n",
    "    \n",
    "    # === Labeled losses ===\n",
    "    reg = reg_loss_fn(cost_pred_l, lab_labeled, loss_type=config.get('loss_type', 'mse'))\n",
    "    pair = pair_loss_fn(cost_pred_l.view(-1), lab_labeled.view(-1), \n",
    "                        margin=config.get('margin', 0.1),\n",
    "                        delta=config.get('pair_delta', None))  # delta íŒŒë¼ë¯¸í„° ì¶”ê°€\n",
    "    smooth_l = smooth_loss_fn(model, z_l, \n",
    "                               noise_std=config.get('noise_std', None),\n",
    "                               z_scale_factor=config.get('z_scale_factor_smooth', 0.1))\n",
    "    kld_l = kld_loss_fn(mean_l, logvar_l)\n",
    "    \n",
    "    # grad_direction warmup: warmup_epochs ì´í›„ë¶€í„° í™œì„±í™”, linear ramp\n",
    "    warmup_epochs = config.get('grad_dir_warmup', 10)\n",
    "    ramp_epochs = config.get('grad_dir_ramp', 10)\n",
    "    if epoch < warmup_epochs:\n",
    "        grad_dir_enabled = False\n",
    "        grad_dir_weight = 0.0\n",
    "    else:\n",
    "        grad_dir_enabled = True\n",
    "        progress = min(1.0, (epoch - warmup_epochs) / max(1, ramp_epochs))\n",
    "        grad_dir_weight = config.get('lambda_grad_dir', 0.01) * progress\n",
    "    \n",
    "    grad_dir = grad_direction_pair_loss_fn(model, z_l, \n",
    "                                           alpha=config.get('grad_alpha', None),\n",
    "                                           margin=config.get('grad_margin', 0.01),\n",
    "                                           z_scale_factor=config.get('z_scale_factor_grad', 0.05),\n",
    "                                           enabled=grad_dir_enabled)\n",
    "    \n",
    "    # === Unlabeled losses (kld + smoothë§Œ) ===\n",
    "    smooth_u = smooth_loss_fn(model, z_u, \n",
    "                               noise_std=config.get('noise_std', None),\n",
    "                               z_scale_factor=config.get('z_scale_factor_smooth', 0.1))\n",
    "    kld_u = kld_loss_fn(mean_u, logvar_u)\n",
    "    \n",
    "    # === Weighted sum ===\n",
    "    lambda_pair = config.get('lambda_pair', 0.01)  # ê¸°ë³¸ê°’ ë‚®ì¶¤ (0.1 â†’ 0.01)\n",
    "    gamma = config.get('gamma', 0.001)  # ê¸°ë³¸ê°’ ë‚®ì¶¤ (0.01 â†’ 0.001)\n",
    "    beta = config.get('beta', 0.0001)  # ê¸°ë³¸ê°’ ë‚®ì¶¤ (0.001 â†’ 0.0001)\n",
    "    \n",
    "    # Labeled loss\n",
    "    labeled_loss = (reg \n",
    "                    + lambda_pair * pair \n",
    "                    + gamma * smooth_l \n",
    "                    + beta * kld_l\n",
    "                    + grad_dir_weight * grad_dir)\n",
    "    \n",
    "    # Unlabeled loss (kld + smoothë§Œ)\n",
    "    unlabeled_loss = (gamma * smooth_u + beta * kld_u)\n",
    "    \n",
    "    # Total = labeled + unlabeled \n",
    "    # ğŸ“Š unlabeled weight 0.5 (labeled:unlabeled = 2:1)\n",
    "    # ì¶”í›„ 0.75, 1.0 ë“±ìœ¼ë¡œ ì‹¤í—˜ ê°€ëŠ¥\n",
    "    unlabeled_weight = config.get('unlabeled_weight', 0.5)\n",
    "    \n",
    "    total_loss = labeled_loss + unlabeled_weight * unlabeled_loss\n",
    "    \n",
    "    # === ë””ë²„ê¹… ë¡œê¹…ìš© ===\n",
    "    if return_components:\n",
    "        return total_loss, {\n",
    "            'reg': reg.item(),\n",
    "            'pair_loss': pair.item(),\n",
    "            'smooth_l': smooth_l.item(),\n",
    "            'smooth_u': smooth_u.item(),\n",
    "            'kld_l': kld_l.item(),\n",
    "            'kld_u': kld_u.item(),\n",
    "            'grad_dir': grad_dir.item(),\n",
    "            'z_scale_l': z_scale_l,\n",
    "            'z_scale_u': z_scale_u,\n",
    "        }\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def compute_total_loss_supervised_only(model, \n",
    "                                        seg_labeled, fea_labeled, lab_labeled,\n",
    "                                        config, epoch=0, return_components=False):\n",
    "    \"\"\"\n",
    "    Supervised Only Loss (ë””ë²„ê¹…/ë¹„êµìš©)\n",
    "    \n",
    "    Unlabeled ë°ì´í„° ì—†ì´ labeledë§Œ ì‚¬ìš©\n",
    "    \"\"\"\n",
    "    cost_pred, mean_l, logvar_l, z = model(seg_labeled, fea_labeled, use_mean=True)\n",
    "    \n",
    "    z_scale = z.std(dim=0).mean().detach().item()\n",
    "    \n",
    "    reg = reg_loss_fn(cost_pred, lab_labeled, loss_type=config.get('loss_type', 'mse'))\n",
    "    pair = pair_loss_fn(cost_pred.view(-1), lab_labeled.view(-1), \n",
    "                        margin=config.get('margin', 0.1),\n",
    "                        delta=config.get('pair_delta', None))  # delta íŒŒë¼ë¯¸í„° ì¶”ê°€\n",
    "    smooth_l = smooth_loss_fn(model, z, \n",
    "                               noise_std=config.get('noise_std', None),\n",
    "                               z_scale_factor=config.get('z_scale_factor_smooth', 0.1))\n",
    "    kld_l = kld_loss_fn(mean_l, logvar_l)\n",
    "    \n",
    "    # grad_direction warmup\n",
    "    warmup_epochs = config.get('grad_dir_warmup', 10)\n",
    "    ramp_epochs = config.get('grad_dir_ramp', 10)\n",
    "    if epoch < warmup_epochs:\n",
    "        grad_dir_enabled = False\n",
    "        grad_dir_weight = 0.0\n",
    "    else:\n",
    "        grad_dir_enabled = True\n",
    "        progress = min(1.0, (epoch - warmup_epochs) / max(1, ramp_epochs))\n",
    "        grad_dir_weight = config.get('lambda_grad_dir', 0.01) * progress\n",
    "    \n",
    "    grad_dir = grad_direction_pair_loss_fn(model, z, \n",
    "                                           alpha=config.get('grad_alpha', None),\n",
    "                                           margin=config.get('grad_margin', 0.01),\n",
    "                                           z_scale_factor=config.get('z_scale_factor_grad', 0.05),\n",
    "                                           enabled=grad_dir_enabled)\n",
    "    \n",
    "    lambda_pair = config.get('lambda_pair', 0.01)\n",
    "    gamma = config.get('gamma', 0.001)\n",
    "    beta = config.get('beta', 0.0001)\n",
    "    \n",
    "    total_loss = (reg \n",
    "                  + lambda_pair * pair \n",
    "                  + gamma * smooth_l \n",
    "                  + beta * kld_l\n",
    "                  + grad_dir_weight * grad_dir)\n",
    "    \n",
    "    if return_components:\n",
    "        return total_loss, {\n",
    "            'reg': reg.item(),\n",
    "            'pair_loss': pair.item(),\n",
    "            'smooth_l': smooth_l.item(),\n",
    "            'kld_l': kld_l.item(),\n",
    "            'grad_dir': grad_dir.item(),\n",
    "            'z_scale': z_scale,\n",
    "        }\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "print(\"âœ… Loss í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n",
    "print(\"  í•µì‹¬:\")\n",
    "print(\"  - pair_loss_fnì— delta íŒŒë¼ë¯¸í„° ì¶”ê°€ (í° ì°¨ì´ ìŒë§Œ í•„í„°ë§)\")\n",
    "print(\"  - smooth_loss: anchor detachë¡œ ì¸ì½”ë” ë³´í˜¸\")\n",
    "print(\"  - grad_direction: warmup + linear ramp\")\n",
    "print(\"  - z-scale ê¸°ë°˜ ìë™ ì¡°ì •\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03bd4da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë‹ˆí„°ë§ ë° í•™ìŠµ ì¸í”„ë¼ ì •ì˜ ì™„ë£Œ\n",
      "   - GradientMonitor: z-gradient ë¶„í¬ ì¶”ì  (í­ì£¼/ë¶•ê´´ ê°ì‹œ)\n",
      "   - LatentDiagnostics: KL/latent_dim, collapse ì—¬ë¶€ ì§„ë‹¨\n",
      "   - UncertaintyMonitor: epistemic uncertainty í’ˆì§ˆ (Unc-Error ìƒê´€)\n",
      "   - LatentCoverageMonitor: latent ì»¤ë²„ë¦¬ì§€, ë¯¸íƒìƒ‰ ë¹„ìœ¨\n",
      "   - TrainingMonitor: ì¢…í•© ëª¨ë‹ˆí„°\n",
      "   - evaluate_model_full: ì „ì²´ ê²€ì¦ í‰ê°€\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ëª¨ë‹ˆí„°ë§ ë° í•™ìŠµ ì¸í”„ë¼ (ê°œì„ ëœ ë²„ì „)\n",
    "# ============================================================\n",
    "# ì¶”ê°€: uncertainty í’ˆì§ˆ, latent ì»¤ë²„ë¦¬ì§€/ë¯¸íƒìƒ‰ ì§€í‘œ\n",
    "\n",
    "import random\n",
    "import copy\n",
    "from collections import deque, defaultdict\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "class GradientMonitor:\n",
    "    \"\"\"zì— ëŒ€í•œ gradient í†µê³„ ëª¨ë‹ˆí„°ë§\"\"\"\n",
    "    def __init__(self):\n",
    "        self.history = defaultdict(list)\n",
    "    \n",
    "    def record(self, model, segment_sizes, features, device):\n",
    "        \"\"\"ìƒ˜í”Œì— ëŒ€í•œ gradient í†µê³„ ê¸°ë¡\"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        # Forward pass\n",
    "        mean, logvar, _ = model.encode(segment_sizes, features)\n",
    "        z = mean.detach().clone().requires_grad_(True)\n",
    "        cost_pred = model.predict_cost(z)\n",
    "        \n",
    "        # Gradient ê³„ì‚°\n",
    "        grad = torch.autograd.grad(cost_pred.sum(), z, create_graph=False)[0]\n",
    "        grad_norms = grad.norm(dim=1).cpu().numpy()\n",
    "        \n",
    "        self.history['grad_norm_mean'].append(grad_norms.mean())\n",
    "        self.history['grad_norm_std'].append(grad_norms.std())\n",
    "        self.history['grad_norm_max'].append(grad_norms.max())\n",
    "        self.history['grad_norm_min'].append(grad_norms.min())\n",
    "        \n",
    "        # Cosine similarity between gradients (gradient ë°©í–¥ ì¼ê´€ì„±)\n",
    "        if grad.shape[0] > 1:\n",
    "            grad_normalized = grad / (grad.norm(dim=1, keepdim=True) + 1e-8)\n",
    "            cos_sim = (grad_normalized @ grad_normalized.T).cpu().numpy()\n",
    "            mask = ~np.eye(cos_sim.shape[0], dtype=bool)\n",
    "            self.history['grad_cosine_sim_mean'].append(cos_sim[mask].mean())\n",
    "        \n",
    "        model.train()\n",
    "        return {\n",
    "            'mean': grad_norms.mean(),\n",
    "            'std': grad_norms.std(),\n",
    "            'max': grad_norms.max(),\n",
    "            'min': grad_norms.min(),\n",
    "        }\n",
    "    \n",
    "    def get_summary(self):\n",
    "        return {k: v[-1] if v else None for k, v in self.history.items()}\n",
    "\n",
    "\n",
    "class LatentDiagnostics:\n",
    "    \"\"\"Latent space ì§„ë‹¨\"\"\"\n",
    "    def __init__(self, latent_dim):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.history = defaultdict(list)\n",
    "    \n",
    "    def record(self, mean, logvar):\n",
    "        \"\"\"KL/latent_dim ë° latent í†µê³„ ê¸°ë¡\"\"\"\n",
    "        kld_per_dim = (-0.5 * (1 + logvar - mean.pow(2) - logvar.exp())).mean(dim=0)\n",
    "        kld_per_dim_mean = kld_per_dim.mean().item()\n",
    "        kld_per_dim_std = kld_per_dim.std().item()\n",
    "        \n",
    "        active_dims = (kld_per_dim > 0.01).sum().item()\n",
    "        mean_norm = mean.norm(dim=1).mean().item()\n",
    "        logvar_mean = logvar.mean().item()\n",
    "        \n",
    "        self.history['kld_per_dim_mean'].append(kld_per_dim_mean)\n",
    "        self.history['kld_per_dim_std'].append(kld_per_dim_std)\n",
    "        self.history['active_dims'].append(active_dims)\n",
    "        self.history['mean_norm'].append(mean_norm)\n",
    "        self.history['logvar_mean'].append(logvar_mean)\n",
    "        \n",
    "        return {\n",
    "            'kld_per_dim': kld_per_dim_mean,\n",
    "            'active_dims': active_dims,\n",
    "            'mean_norm': mean_norm,\n",
    "            'logvar_mean': logvar_mean,\n",
    "        }\n",
    "    \n",
    "    def check_health(self):\n",
    "        \"\"\"Latent space ê±´ê°• ì²´í¬\"\"\"\n",
    "        if not self.history['kld_per_dim_mean']:\n",
    "            return {'status': 'no_data'}\n",
    "        \n",
    "        kld = self.history['kld_per_dim_mean'][-1]\n",
    "        active = self.history['active_dims'][-1]\n",
    "        \n",
    "        status = 'healthy'\n",
    "        warnings = []\n",
    "        \n",
    "        if kld < 0.05:\n",
    "            status = 'warning'\n",
    "            warnings.append(f'KL/dim too low ({kld:.4f}): posterior collapsing')\n",
    "        elif kld > 0.30:\n",
    "            status = 'warning'\n",
    "            warnings.append(f'KL/dim high ({kld:.4f}): weak regularization')\n",
    "        \n",
    "        if active < self.latent_dim * 0.5:\n",
    "            status = 'warning'\n",
    "            warnings.append(f'Latent collapse: only {active}/{self.latent_dim} dims active')\n",
    "        \n",
    "        return {'status': status, 'warnings': warnings, 'kld_per_dim': kld, 'active_dims': active}\n",
    "\n",
    "\n",
    "class UncertaintyMonitor:\n",
    "    \"\"\"Epistemic uncertainty í’ˆì§ˆ ëª¨ë‹ˆí„°ë§\"\"\"\n",
    "    def __init__(self):\n",
    "        self.history = defaultdict(list)\n",
    "    \n",
    "    def record(self, model, segment_sizes, features, labels, device, n_samples=10):\n",
    "        \"\"\"\n",
    "        Uncertainty í’ˆì§ˆ í‰ê°€\n",
    "        - epistemic uncertainty: ì—¬ëŸ¬ ë²ˆ ìƒ˜í”Œë§í•˜ì—¬ ì˜ˆì¸¡ ë¶„ì‚° ê³„ì‚°\n",
    "        - uncertainty-error correlation: ë¶ˆí™•ì‹¤ì„±ì´ ë†’ì€ ê³³ì—ì„œ ì—ëŸ¬ë„ ë†’ì€ì§€\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        # Multiple forward passes with sampling\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for _ in range(n_samples):\n",
    "                pred, mean, logvar, z = model(segment_sizes, features, use_mean=False)\n",
    "                predictions.append(pred.cpu().numpy())\n",
    "        \n",
    "        predictions = np.stack(predictions, axis=0)  # (n_samples, batch_size)\n",
    "        \n",
    "        # Epistemic uncertainty (prediction variance)\n",
    "        pred_mean = predictions.mean(axis=0)\n",
    "        pred_std = predictions.std(axis=0)  # epistemic uncertainty\n",
    "        \n",
    "        # Error\n",
    "        labels_np = labels.cpu().numpy() if isinstance(labels, torch.Tensor) else labels\n",
    "        errors = np.abs(pred_mean - labels_np)\n",
    "        \n",
    "        # Uncertainty-error correlation (Spearman)\n",
    "        if len(pred_std) > 2:\n",
    "            corr, pval = spearmanr(pred_std, errors)\n",
    "        else:\n",
    "            corr, pval = 0.0, 1.0\n",
    "        \n",
    "        # ê¸°ë¡\n",
    "        self.history['epistemic_mean'].append(pred_std.mean())\n",
    "        self.history['epistemic_std'].append(pred_std.std())\n",
    "        self.history['uncertainty_error_corr'].append(corr)\n",
    "        self.history['uncertainty_error_pval'].append(pval)\n",
    "        \n",
    "        model.train()\n",
    "        return {\n",
    "            'epistemic_mean': pred_std.mean(),\n",
    "            'epistemic_std': pred_std.std(),\n",
    "            'uncertainty_error_corr': corr,\n",
    "            'uncertainty_error_pval': pval,\n",
    "        }\n",
    "    \n",
    "    def get_summary(self):\n",
    "        return {k: v[-1] if v else None for k, v in self.history.items()}\n",
    "\n",
    "\n",
    "class LatentCoverageMonitor:\n",
    "    \"\"\"Latent space ì»¤ë²„ë¦¬ì§€ ë° ë¯¸íƒìƒ‰ ì˜ì—­ ëª¨ë‹ˆí„°ë§\"\"\"\n",
    "    def __init__(self):\n",
    "        self.history = defaultdict(list)\n",
    "        self.train_latents = None  # í•™ìŠµ ìƒ˜í”Œì˜ latent ì €ì¥\n",
    "    \n",
    "    def set_train_latents(self, z_train):\n",
    "        \"\"\"í•™ìŠµ ìƒ˜í”Œì˜ latent ë²¡í„° ì„¤ì •\"\"\"\n",
    "        self.train_latents = z_train.cpu().numpy() if isinstance(z_train, torch.Tensor) else z_train\n",
    "    \n",
    "    def record(self, z_pool):\n",
    "        \"\"\"\n",
    "        Latent coverage í‰ê°€\n",
    "        - min_distance_to_train: ê° pool ìƒ˜í”Œì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ train ìƒ˜í”Œê¹Œì§€ì˜ ê±°ë¦¬\n",
    "        - exploration_score: ë¯¸íƒìƒ‰ ì˜ì—­ ì§€í‘œ (ë†’ì„ìˆ˜ë¡ ë¯¸íƒìƒ‰)\n",
    "        \"\"\"\n",
    "        if self.train_latents is None:\n",
    "            return {'coverage_ratio': 0, 'unexplored_ratio': 0}\n",
    "        \n",
    "        z_pool_np = z_pool.cpu().numpy() if isinstance(z_pool, torch.Tensor) else z_pool\n",
    "        \n",
    "        # ê° pool ìƒ˜í”Œì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ train ìƒ˜í”Œê¹Œì§€ì˜ ê±°ë¦¬\n",
    "        distances = cdist(z_pool_np, self.train_latents, metric='euclidean')\n",
    "        min_distances = distances.min(axis=1)\n",
    "        \n",
    "        # í†µê³„\n",
    "        mean_min_dist = min_distances.mean()\n",
    "        std_min_dist = min_distances.std()\n",
    "        max_min_dist = min_distances.max()\n",
    "        \n",
    "        # Exploration score: train ìƒ˜í”Œ ê±°ë¦¬ì˜ í‰ê·  ëŒ€ë¹„ ì„ê³„ê°’\n",
    "        train_internal_dist = cdist(self.train_latents, self.train_latents, metric='euclidean')\n",
    "        np.fill_diagonal(train_internal_dist, np.inf)\n",
    "        train_nn_dist = train_internal_dist.min(axis=1).mean()  # train ë‚´ë¶€ í‰ê·  NN ê±°ë¦¬\n",
    "        \n",
    "        # ë¯¸íƒìƒ‰ ë¹„ìœ¨: min_distance > 2 * train_nn_distì¸ ìƒ˜í”Œ ë¹„ìœ¨\n",
    "        unexplored_threshold = 2 * train_nn_dist\n",
    "        unexplored_ratio = (min_distances > unexplored_threshold).mean()\n",
    "        \n",
    "        # Coverage: train centroid ê¸°ì¤€ poolì˜ spread\n",
    "        train_centroid = self.train_latents.mean(axis=0)\n",
    "        pool_centroid = z_pool_np.mean(axis=0)\n",
    "        centroid_shift = np.linalg.norm(pool_centroid - train_centroid)\n",
    "        \n",
    "        # ê¸°ë¡\n",
    "        self.history['mean_min_dist_to_train'].append(mean_min_dist)\n",
    "        self.history['std_min_dist_to_train'].append(std_min_dist)\n",
    "        self.history['max_min_dist_to_train'].append(max_min_dist)\n",
    "        self.history['unexplored_ratio'].append(unexplored_ratio)\n",
    "        self.history['train_nn_dist'].append(train_nn_dist)\n",
    "        self.history['centroid_shift'].append(centroid_shift)\n",
    "        \n",
    "        return {\n",
    "            'mean_min_dist': mean_min_dist,\n",
    "            'unexplored_ratio': unexplored_ratio,\n",
    "            'train_nn_dist': train_nn_dist,\n",
    "            'centroid_shift': centroid_shift,\n",
    "        }\n",
    "    \n",
    "    def get_summary(self):\n",
    "        return {k: v[-1] if v else None for k, v in self.history.items()}\n",
    "\n",
    "\n",
    "class TrainingMonitor:\n",
    "    \"\"\"ì¢…í•© í•™ìŠµ ëª¨ë‹ˆí„°\"\"\"\n",
    "    def __init__(self, latent_dim):\n",
    "        self.grad_monitor = GradientMonitor()\n",
    "        self.latent_diag = LatentDiagnostics(latent_dim)\n",
    "        self.uncertainty_monitor = UncertaintyMonitor()\n",
    "        self.coverage_monitor = LatentCoverageMonitor()\n",
    "        self.val_history = defaultdict(list)\n",
    "        self.train_history = defaultdict(list)\n",
    "    \n",
    "    def log_train_step(self, loss_components):\n",
    "        for k, v in loss_components.items():\n",
    "            self.train_history[k].append(v)\n",
    "    \n",
    "    def log_validation(self, metrics):\n",
    "        for k, v in metrics.items():\n",
    "            if not isinstance(v, (np.ndarray, torch.Tensor)):  # scalarë§Œ\n",
    "                self.val_history[k].append(v)\n",
    "    \n",
    "    def record_gradients(self, model, segment_sizes, features, device):\n",
    "        return self.grad_monitor.record(model, segment_sizes, features, device)\n",
    "    \n",
    "    def record_latent(self, mean, logvar):\n",
    "        return self.latent_diag.record(mean, logvar)\n",
    "    \n",
    "    def record_uncertainty(self, model, segment_sizes, features, labels, device):\n",
    "        return self.uncertainty_monitor.record(model, segment_sizes, features, labels, device)\n",
    "    \n",
    "    def set_train_latents(self, z_train):\n",
    "        self.coverage_monitor.set_train_latents(z_train)\n",
    "    \n",
    "    def record_coverage(self, z_pool):\n",
    "        return self.coverage_monitor.record(z_pool)\n",
    "    \n",
    "    def print_status(self, epoch, total_epochs):\n",
    "        # Validation metrics\n",
    "        val_r2 = self.val_history['r2'][-1] if self.val_history['r2'] else 0\n",
    "        val_mse = self.val_history['mse'][-1] if self.val_history['mse'] else 0\n",
    "        val_mae = self.val_history['mae'][-1] if self.val_history['mae'] else 0\n",
    "        \n",
    "        # Gradient stats\n",
    "        grad_stats = self.grad_monitor.get_summary()\n",
    "        grad_norm = grad_stats.get('grad_norm_mean', 0) or 0\n",
    "        \n",
    "        # Latent health\n",
    "        latent_health = self.latent_diag.check_health()\n",
    "        kld_per_dim = latent_health.get('kld_per_dim', 0) or 0\n",
    "        \n",
    "        # Uncertainty quality\n",
    "        unc_stats = self.uncertainty_monitor.get_summary()\n",
    "        unc_corr = unc_stats.get('uncertainty_error_corr', 0) or 0\n",
    "        \n",
    "        # Coverage\n",
    "        cov_stats = self.coverage_monitor.get_summary()\n",
    "        unexplored = cov_stats.get('unexplored_ratio', 0) or 0\n",
    "        \n",
    "        # KL ë²”ìœ„ ì²´í¬ (0.05~0.20)\n",
    "        kl_status = \"âœ“\" if 0.05 <= kld_per_dim <= 0.20 else \"âš \"\n",
    "        unc_status = \"âœ“\" if unc_corr > 0.3 else \"âš \"  # uncertainty-error ìƒê´€ > 0.3\n",
    "        \n",
    "        print(f\"  [Epoch {epoch+1}/{total_epochs}] \"\n",
    "              f\"RÂ²={val_r2:.4f} | MSE={val_mse:.4f} | \"\n",
    "              f\"||âˆ‚c/âˆ‚z||={grad_norm:.3f} | KL/dim={kld_per_dim:.3f} {kl_status} | \"\n",
    "              f\"Unc-Err Ï={unc_corr:.2f} {unc_status} | Unexplored={unexplored:.1%}\")\n",
    "        \n",
    "        if latent_health['status'] == 'warning':\n",
    "            for w in latent_health['warnings']:\n",
    "                print(f\"    âš ï¸ {w}\")\n",
    "\n",
    "\n",
    "def evaluate_model_full(model, val_feature_list, val_segment_sizes, val_labels,\n",
    "                        remaining_train_features, remaining_segment_sizes, remaining_labels,\n",
    "                        fea_norm_vec, device, batch_size=256):\n",
    "    \"\"\"ì „ì²´ ê²€ì¦ ë°ì´í„°ë¡œ ëª¨ë¸ í‰ê°€ (val + ë¯¸ì‚¬ìš© train)\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_means = []\n",
    "    all_logvars = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 1. Validation set í‰ê°€\n",
    "        for i in range(0, len(val_labels), batch_size):\n",
    "            end_i = min(i + batch_size, len(val_labels))\n",
    "            batch_features = val_feature_list[i:end_i]\n",
    "            batch_segment_sizes = val_segment_sizes[i:end_i]\n",
    "            batch_labels = val_labels[i:end_i]\n",
    "            \n",
    "            flatten = np.concatenate(batch_features, axis=0).astype(np.float32)\n",
    "            seg_tensor = torch.tensor(batch_segment_sizes, dtype=torch.int32).to(device)\n",
    "            fea_tensor = torch.tensor(flatten, dtype=torch.float32).to(device)\n",
    "            if fea_norm_vec is not None:\n",
    "                fea_tensor = fea_tensor / fea_norm_vec.to(device)\n",
    "            \n",
    "            pred, mean, logvar, _ = model(seg_tensor, fea_tensor, use_mean=True)\n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "            all_targets.append(batch_labels)\n",
    "            all_means.append(mean.cpu())\n",
    "            all_logvars.append(logvar.cpu())\n",
    "        \n",
    "        # 2. Remaining train set í‰ê°€\n",
    "        if remaining_train_features is not None and len(remaining_train_features) > 0:\n",
    "            for i in range(0, len(remaining_labels), batch_size):\n",
    "                end_i = min(i + batch_size, len(remaining_labels))\n",
    "                batch_features = remaining_train_features[i:end_i]\n",
    "                batch_segment_sizes = remaining_segment_sizes[i:end_i]\n",
    "                batch_labels = remaining_labels[i:end_i]\n",
    "                \n",
    "                flatten = np.concatenate(batch_features, axis=0).astype(np.float32)\n",
    "                seg_tensor = torch.tensor(batch_segment_sizes, dtype=torch.int32).to(device)\n",
    "                fea_tensor = torch.tensor(flatten, dtype=torch.float32).to(device)\n",
    "                if fea_norm_vec is not None:\n",
    "                    fea_tensor = fea_tensor / fea_norm_vec.to(device)\n",
    "                \n",
    "                pred, mean, logvar, _ = model(seg_tensor, fea_tensor, use_mean=True)\n",
    "                all_preds.append(pred.cpu().numpy())\n",
    "                all_targets.append(batch_labels)\n",
    "                all_means.append(mean.cpu())\n",
    "                all_logvars.append(logvar.cpu())\n",
    "    \n",
    "    preds = np.concatenate(all_preds)\n",
    "    targets = np.concatenate(all_targets)\n",
    "    means = torch.cat(all_means, dim=0)\n",
    "    logvars = torch.cat(all_logvars, dim=0)\n",
    "    \n",
    "    mse = mean_squared_error(targets, preds)\n",
    "    mae = mean_absolute_error(targets, preds)\n",
    "    r2 = r2_score(targets, preds)\n",
    "    \n",
    "    model.train()\n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'predictions': preds,\n",
    "        'targets': targets,\n",
    "        'means': means,\n",
    "        'logvars': logvars,\n",
    "    }\n",
    "\n",
    "\n",
    "def prepare_batch_tensors(indices, feature_list, segment_sizes, labels, fea_norm_vec, device):\n",
    "    \"\"\"ë°°ì¹˜ í…ì„œ ì¤€ë¹„ í—¬í¼ í•¨ìˆ˜\"\"\"\n",
    "    batch_features = [feature_list[i] for i in indices]\n",
    "    batch_segment_sizes = segment_sizes[indices]\n",
    "    batch_labels = labels[indices] if labels is not None else None\n",
    "    \n",
    "    flatten_features = np.concatenate(batch_features, axis=0).astype(np.float32)\n",
    "    seg_tensor = torch.tensor(batch_segment_sizes, dtype=torch.int32).to(device)\n",
    "    fea_tensor = torch.tensor(flatten_features, dtype=torch.float32).to(device)\n",
    "    \n",
    "    if fea_norm_vec is not None:\n",
    "        fea_tensor = fea_tensor / fea_norm_vec.to(device)\n",
    "    \n",
    "    if batch_labels is not None:\n",
    "        lab_tensor = torch.tensor(batch_labels, dtype=torch.float32).to(device)\n",
    "        return seg_tensor, fea_tensor, lab_tensor\n",
    "    return seg_tensor, fea_tensor\n",
    "\n",
    "\n",
    "print(\"âœ… ëª¨ë‹ˆí„°ë§ ë° í•™ìŠµ ì¸í”„ë¼ ì •ì˜ ì™„ë£Œ\")\n",
    "print(\"   - GradientMonitor: z-gradient ë¶„í¬ ì¶”ì  (í­ì£¼/ë¶•ê´´ ê°ì‹œ)\")\n",
    "print(\"   - LatentDiagnostics: KL/latent_dim, collapse ì—¬ë¶€ ì§„ë‹¨\")\n",
    "print(\"   - UncertaintyMonitor: epistemic uncertainty í’ˆì§ˆ (Unc-Error ìƒê´€)\")\n",
    "print(\"   - LatentCoverageMonitor: latent ì»¤ë²„ë¦¬ì§€, ë¯¸íƒìƒ‰ ë¹„ìœ¨\")\n",
    "print(\"   - TrainingMonitor: ì¢…í•© ëª¨ë‹ˆí„°\")\n",
    "print(\"   - evaluate_model_full: ì „ì²´ ê²€ì¦ í‰ê°€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f144a4d",
   "metadata": {},
   "source": [
    "## Regression with Pre-trained VAE Encoder\n",
    "\n",
    "ì‚¬ì „í•™ìŠµëœ VAE encoderë¥¼ ì‚¬ìš©í•˜ì—¬ regression ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "- VAE encoderì˜ latent representation (z)ì„ ì‚¬ìš©\n",
    "- Regression headë§Œ ì¶”ê°€í•˜ì—¬ cost ì˜ˆì¸¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25f104e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Semi-Supervised í•™ìŠµ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n",
      "   - train_semi_supervised: 64 labeled + 64 unlabeled í•™ìŠµ\n",
      "     â€¢ Labeled: reg + pair + smooth + kld + grad_dir\n",
      "     â€¢ Unlabeled: smooth + kldë§Œ\n",
      "   - run_experiment_n_times: NíšŒ ë°˜ë³µ ì‹¤í—˜\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ğŸ¯ Semi-Supervised í•™ìŠµ í•¨ìˆ˜ (64 labeled + 64 unlabeled)\n",
    "# ============================================================\n",
    "# ëª©í‘œ: RÂ² >= 0.85\n",
    "# Labeled: reg + pair + smooth + kld + grad_dir\n",
    "# Unlabeled: smooth + kldë§Œ (ì¦ê°•ìš©)\n",
    "\n",
    "from glob import glob\n",
    "import copy\n",
    "\n",
    "def train_semi_supervised(model, \n",
    "                          labeled_indices, \n",
    "                          unlabeled_indices,\n",
    "                          train_feature_list, \n",
    "                          train_segment_sizes, \n",
    "                          train_labels,\n",
    "                          val_feature_list,\n",
    "                          val_segment_sizes,\n",
    "                          val_labels,\n",
    "                          fea_norm_vec, \n",
    "                          device, \n",
    "                          config,\n",
    "                          num_epochs=100,\n",
    "                          eval_every=5,\n",
    "                          verbose=True):\n",
    "    \"\"\"\n",
    "    Semi-supervised í•™ìŠµ (ë‹¨ìˆœí™”)\n",
    "    \n",
    "    Args:\n",
    "        labeled_indices: ë¼ë²¨ì´ ìˆëŠ” ìƒ˜í”Œ ì¸ë±ìŠ¤ (64ê°œ)\n",
    "        unlabeled_indices: ë¼ë²¨ ì—†ì´ ì‚¬ìš©í•  ìƒ˜í”Œ ì¸ë±ìŠ¤ (64ê°œ) - kld + smoothë§Œ ì ìš©\n",
    "    \n",
    "    Returns:\n",
    "        model, history, best_metrics, monitor\n",
    "    \"\"\"\n",
    "    # Monitor ì´ˆê¸°í™”\n",
    "    monitor = TrainingMonitor(latent_dim=model.latent_dim)\n",
    "    \n",
    "    # Remaining train data (labeled/unlabeled ì œì™¸)\n",
    "    all_train_indices = set(range(len(train_labels)))\n",
    "    used_indices = set(labeled_indices) | set(unlabeled_indices)\n",
    "    remaining_indices = list(all_train_indices - used_indices)\n",
    "    \n",
    "    remaining_features = [train_feature_list[i] for i in remaining_indices]\n",
    "    remaining_segment_sizes = train_segment_sizes[remaining_indices]\n",
    "    remaining_labels = train_labels[remaining_indices]\n",
    "    \n",
    "    # Optimizer with differential learning rates\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': model.get_encoder_params(), 'lr': config['encoder_lr']},\n",
    "        {'params': model.get_predictor_params(), 'lr': config['predictor_lr']}\n",
    "    ], weight_decay=config.get('weight_decay', 1e-5))\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=20, T_mult=2, eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    # Prepare batch tensors (í•œ ë²ˆë§Œ)\n",
    "    seg_l, fea_l, lab_l = prepare_batch_tensors(\n",
    "        labeled_indices, train_feature_list, train_segment_sizes, \n",
    "        train_labels, fea_norm_vec, device\n",
    "    )\n",
    "    seg_u, fea_u = prepare_batch_tensors(\n",
    "        unlabeled_indices, train_feature_list, train_segment_sizes, \n",
    "        None, fea_norm_vec, device\n",
    "    )\n",
    "    \n",
    "    best_r2 = -float('inf')\n",
    "    best_state = None\n",
    "    best_metrics = None\n",
    "    history = []\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ğŸš€ Semi-Supervised Training ì‹œì‘\")\n",
    "        print(f\"   Labeled: {len(labeled_indices)}ê°œ, Unlabeled: {len(unlabeled_indices)}ê°œ\")\n",
    "        print(f\"   Validation pool: {len(remaining_indices) + len(val_labels)}ê°œ\")\n",
    "        print(f\"   Epochs: {num_epochs}, Eval every: {eval_every}\")\n",
    "        print(f\"{'='*70}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # Forward & Loss (epoch ì „ë‹¬í•˜ì—¬ grad_dir warmup ì§€ì›)\n",
    "        total_loss, components = compute_total_loss_semi_supervised(\n",
    "            model, seg_l, fea_l, lab_l, seg_u, fea_u, config, \n",
    "            epoch=epoch, return_components=True\n",
    "        )\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.get('max_grad_norm', 1.0))\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        monitor.log_train_step(components)\n",
    "        \n",
    "        # Evaluation\n",
    "        if (epoch + 1) % eval_every == 0 or epoch == num_epochs - 1:\n",
    "            # ê¸°ë³¸ í‰ê°€\n",
    "            metrics = evaluate_model_full(\n",
    "                model, val_feature_list, val_segment_sizes, val_labels,\n",
    "                remaining_features, remaining_segment_sizes, remaining_labels,\n",
    "                fea_norm_vec, device\n",
    "            )\n",
    "            \n",
    "            # Gradient monitoring\n",
    "            grad_stats = monitor.record_gradients(model, seg_l, fea_l, device)\n",
    "            \n",
    "            # Latent diagnostics\n",
    "            with torch.no_grad():\n",
    "                _, mean_l, logvar_l, _ = model(seg_l, fea_l, use_mean=False)\n",
    "            latent_stats = monitor.record_latent(mean_l, logvar_l)\n",
    "            \n",
    "            # Uncertainty monitoring (labeled ë°°ì¹˜ì—ì„œ)\n",
    "            unc_stats = monitor.record_uncertainty(model, seg_l, fea_l, lab_l, device)\n",
    "            \n",
    "            # Coverage monitoring (train latents ì„¤ì • í›„)\n",
    "            if epoch == eval_every - 1:  # ì²« í‰ê°€ ì‹œ train latent ì„¤ì •\n",
    "                monitor.set_train_latents(mean_l)\n",
    "            cov_stats = monitor.record_coverage(metrics['means'])\n",
    "            \n",
    "            monitor.log_validation(metrics)\n",
    "            \n",
    "            # Record history\n",
    "            history.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'r2': metrics['r2'],\n",
    "                'mse': metrics['mse'],\n",
    "                'mae': metrics['mae'],\n",
    "                **components,\n",
    "                **grad_stats,\n",
    "                **latent_stats,\n",
    "                **unc_stats,\n",
    "                **cov_stats,\n",
    "            })\n",
    "            \n",
    "            if verbose:\n",
    "                monitor.print_status(epoch, num_epochs)\n",
    "            \n",
    "            # Best model tracking\n",
    "            if metrics['r2'] > best_r2:\n",
    "                best_r2 = metrics['r2']\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "                best_metrics = {k: v for k, v in metrics.items() \n",
    "                               if not isinstance(v, (np.ndarray, torch.Tensor))}\n",
    "                best_metrics['predictions'] = metrics['predictions']\n",
    "                best_metrics['targets'] = metrics['targets']\n",
    "                best_metrics['means'] = metrics['means']\n",
    "                best_metrics['logvars'] = metrics['logvars']\n",
    "    \n",
    "    # Restore best model\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ğŸ† Training ì™„ë£Œ! Best RÂ²: {best_r2:.4f}\")\n",
    "        print(f\"{'='*70}\")\n",
    "    \n",
    "    return model, history, best_metrics, monitor\n",
    "\n",
    "\n",
    "def run_experiment_n_times(model_class, model_kwargs, \n",
    "                           train_feature_list, train_segment_sizes, train_labels,\n",
    "                           val_feature_list, val_segment_sizes, val_labels,\n",
    "                           fea_norm_vec, device, config, \n",
    "                           pretrained_checkpoint,\n",
    "                           n_runs=3, num_epochs=100, verbose=True):\n",
    "    \"\"\"\n",
    "    Në²ˆ ë°˜ë³µ ì‹¤í—˜í•˜ì—¬ robustness í™•ì¸\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # 64ê°œ labeled ìƒ˜í”Œ ì„ íƒ (ê³ ì • - ì²˜ìŒ 64ê°œ, shuffle ì•ˆí•¨)\n",
    "    labeled_indices = np.arange(64)\n",
    "    # 64ê°œ unlabeled ìƒ˜í”Œ ì„ íƒ (ë‹¤ìŒ 64ê°œ)\n",
    "    unlabeled_indices = np.arange(64, 128)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ”¬ {n_runs}íšŒ ë°˜ë³µ ì‹¤í—˜\")\n",
    "    print(f\"   Labeled indices: 0~63 (ì²˜ìŒ 64ê°œ)\")\n",
    "    print(f\"   Unlabeled indices: 64~127 (ë‹¤ìŒ 64ê°œ)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ğŸ“Š Run {run + 1}/{n_runs}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Seed ì„¤ì • (ê° runë§ˆë‹¤ ë‹¤ë¥¸ seed)\n",
    "        run_seed = SEED + run\n",
    "        torch.manual_seed(run_seed)\n",
    "        np.random.seed(run_seed)\n",
    "        random.seed(run_seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(run_seed)\n",
    "        \n",
    "        # ìƒˆ ëª¨ë¸ ìƒì„± ë° pretrained encoder ë¡œë“œ\n",
    "        model = model_class(**model_kwargs).to(device)\n",
    "        if pretrained_checkpoint is not None:\n",
    "            if 'model_state_dict' in pretrained_checkpoint:\n",
    "                vae_state = pretrained_checkpoint['model_state_dict']\n",
    "            else:\n",
    "                vae_state = pretrained_checkpoint\n",
    "            \n",
    "            encoder_keys = ['segment_encoder', 'norm', 'l0', 'l1', 'fc_mean', 'fc_logvar']\n",
    "            own_state = model.state_dict()\n",
    "            for name, param in vae_state.items():\n",
    "                if any(name.startswith(k) for k in encoder_keys):\n",
    "                    if name in own_state and own_state[name].shape == param.shape:\n",
    "                        own_state[name].copy_(param)\n",
    "            model.load_state_dict(own_state)\n",
    "        \n",
    "        # í•™ìŠµ\n",
    "        trained_model, history, best_metrics, monitor = train_semi_supervised(\n",
    "            model, labeled_indices, unlabeled_indices,\n",
    "            train_feature_list, train_segment_sizes, train_labels,\n",
    "            val_feature_list, val_segment_sizes, val_labels,\n",
    "            fea_norm_vec, device, config,\n",
    "            num_epochs=num_epochs,\n",
    "            eval_every=5,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'run': run + 1,\n",
    "            'best_r2': best_metrics['r2'],\n",
    "            'best_mse': best_metrics['mse'],\n",
    "            'best_mae': best_metrics['mae'],\n",
    "            'history': history,\n",
    "            'model_state': copy.deepcopy(trained_model.state_dict()),\n",
    "            'monitor': monitor,\n",
    "            'best_metrics': best_metrics,\n",
    "        })\n",
    "        \n",
    "        print(f\"   Run {run + 1} ê²°ê³¼: RÂ²={best_metrics['r2']:.4f}, MSE={best_metrics['mse']:.4f}\")\n",
    "    \n",
    "    # ê²°ê³¼ ìš”ì•½\n",
    "    r2_values = [r['best_r2'] for r in results]\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ“Š {n_runs}íšŒ ì‹¤í—˜ ê²°ê³¼ ìš”ì•½\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"   RÂ² Mean: {np.mean(r2_values):.4f} Â± {np.std(r2_values):.4f}\")\n",
    "    print(f\"   RÂ² Min: {np.min(r2_values):.4f}, Max: {np.max(r2_values):.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"âœ… Semi-Supervised í•™ìŠµ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n",
    "print(\"   - train_semi_supervised: 64 labeled + 64 unlabeled í•™ìŠµ\")\n",
    "print(\"     â€¢ Labeled: reg + pair + smooth + kld + grad_dir\")\n",
    "print(\"     â€¢ Unlabeled: smooth + kldë§Œ\")\n",
    "print(\"   - run_experiment_n_times: NíšŒ ë°˜ë³µ ì‹¤í—˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "087333c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Input dim: 164\n",
      "âœ… Pretrained VAE ë¡œë“œ: /root/work/tenset/scripts/pre_experiments/model_final/checkpoints/vae_medium_v2/vae_rank1_h256_l128_b1e-04_lr2e-04.pt\n",
      "   Hidden: 256, Latent: 128\n",
      "ğŸš€ Augmentation ë””ë²„ê¹… ë¹„êµ ì‹¤í—˜ ì‹œì‘...\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¬ BASELINE ì‹¤í—˜ (3íšŒ)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ Semi-Supervised Training ì‹œì‘\n",
      "   Labeled: 64ê°œ, Unlabeled: 64ê°œ\n",
      "   Validation pool: 3396ê°œ\n",
      "   Epochs: 50, Eval every: 5\n",
      "======================================================================\n",
      "  [Epoch 5/50] RÂ²=-1.2425 | MSE=0.0681 | ||âˆ‚c/âˆ‚z||=0.783 | KL/dim=0.144 âœ“ | Unc-Err Ï=0.14 âš  | Unexplored=19.0%\n",
      "  [Epoch 10/50] RÂ²=-0.6264 | MSE=0.0494 | ||âˆ‚c/âˆ‚z||=2.146 | KL/dim=0.144 âœ“ | Unc-Err Ï=-0.21 âš  | Unexplored=18.8%\n",
      "  [Epoch 15/50] RÂ²=-0.3310 | MSE=0.0404 | ||âˆ‚c/âˆ‚z||=2.813 | KL/dim=0.144 âœ“ | Unc-Err Ï=-0.03 âš  | Unexplored=18.9%\n",
      "  [Epoch 20/50] RÂ²=-0.3507 | MSE=0.0410 | ||âˆ‚c/âˆ‚z||=2.751 | KL/dim=0.144 âœ“ | Unc-Err Ï=-0.04 âš  | Unexplored=19.1%\n",
      "  [Epoch 25/50] RÂ²=-0.1307 | MSE=0.0344 | ||âˆ‚c/âˆ‚z||=2.487 | KL/dim=0.143 âœ“ | Unc-Err Ï=-0.12 âš  | Unexplored=19.1%\n",
      "  [Epoch 30/50] RÂ²=-0.1186 | MSE=0.0340 | ||âˆ‚c/âˆ‚z||=2.389 | KL/dim=0.141 âœ“ | Unc-Err Ï=0.04 âš  | Unexplored=19.6%\n",
      "  [Epoch 35/50] RÂ²=0.0347 | MSE=0.0293 | ||âˆ‚c/âˆ‚z||=2.392 | KL/dim=0.141 âœ“ | Unc-Err Ï=0.04 âš  | Unexplored=19.9%\n",
      "  [Epoch 40/50] RÂ²=-0.0221 | MSE=0.0311 | ||âˆ‚c/âˆ‚z||=2.338 | KL/dim=0.141 âœ“ | Unc-Err Ï=0.05 âš  | Unexplored=19.8%\n",
      "  [Epoch 45/50] RÂ²=-0.0379 | MSE=0.0315 | ||âˆ‚c/âˆ‚z||=2.280 | KL/dim=0.141 âœ“ | Unc-Err Ï=0.22 âš  | Unexplored=19.8%\n",
      "  [Epoch 50/50] RÂ²=0.0009 | MSE=0.0304 | ||âˆ‚c/âˆ‚z||=2.240 | KL/dim=0.141 âœ“ | Unc-Err Ï=-0.09 âš  | Unexplored=19.8%\n",
      "\n",
      "======================================================================\n",
      "ğŸ† Training ì™„ë£Œ! Best RÂ²: 0.0347\n",
      "======================================================================\n",
      "   Run 1: RÂ²=0.0347\n",
      "   Run 2: RÂ²=-0.1241\n",
      "   Run 3: RÂ²=-0.6723\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¬ AUGMENTED ì‹¤í—˜ (3íšŒ)\n",
      "======================================================================\n",
      "   lambda_pair: 0.01, gamma: 0.001\n",
      "   beta: 0.0001, lambda_grad_dir: 0.01\n",
      "   grad_dir_warmup: 10, ramp: 10\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ Semi-Supervised Training ì‹œì‘\n",
      "   Labeled: 64ê°œ, Unlabeled: 64ê°œ\n",
      "   Validation pool: 3396ê°œ\n",
      "   Epochs: 50, Eval every: 5\n",
      "======================================================================\n",
      "  [Epoch 5/50] RÂ²=-1.2403 | MSE=0.0681 | ||âˆ‚c/âˆ‚z||=0.781 | KL/dim=0.143 âœ“ | Unc-Err Ï=0.14 âš  | Unexplored=19.1%\n",
      "  [Epoch 10/50] RÂ²=-0.6452 | MSE=0.0500 | ||âˆ‚c/âˆ‚z||=2.149 | KL/dim=0.143 âœ“ | Unc-Err Ï=-0.21 âš  | Unexplored=18.9%\n",
      "  [Epoch 15/50] RÂ²=-0.3465 | MSE=0.0409 | ||âˆ‚c/âˆ‚z||=2.798 | KL/dim=0.142 âœ“ | Unc-Err Ï=-0.04 âš  | Unexplored=19.0%\n",
      "  [Epoch 20/50] RÂ²=-0.3642 | MSE=0.0415 | ||âˆ‚c/âˆ‚z||=2.759 | KL/dim=0.142 âœ“ | Unc-Err Ï=-0.03 âš  | Unexplored=19.1%\n",
      "  [Epoch 25/50] RÂ²=-0.1254 | MSE=0.0342 | ||âˆ‚c/âˆ‚z||=2.496 | KL/dim=0.141 âœ“ | Unc-Err Ï=-0.13 âš  | Unexplored=19.2%\n",
      "  [Epoch 30/50] RÂ²=-0.0583 | MSE=0.0322 | ||âˆ‚c/âˆ‚z||=2.451 | KL/dim=0.140 âœ“ | Unc-Err Ï=0.08 âš  | Unexplored=19.4%\n",
      "  [Epoch 35/50] RÂ²=0.0493 | MSE=0.0289 | ||âˆ‚c/âˆ‚z||=2.458 | KL/dim=0.140 âœ“ | Unc-Err Ï=0.03 âš  | Unexplored=19.6%\n",
      "  [Epoch 40/50] RÂ²=-0.0189 | MSE=0.0310 | ||âˆ‚c/âˆ‚z||=2.445 | KL/dim=0.139 âœ“ | Unc-Err Ï=0.02 âš  | Unexplored=19.4%\n",
      "  [Epoch 45/50] RÂ²=0.0004 | MSE=0.0304 | ||âˆ‚c/âˆ‚z||=2.296 | KL/dim=0.139 âœ“ | Unc-Err Ï=0.19 âš  | Unexplored=19.3%\n",
      "  [Epoch 50/50] RÂ²=0.0419 | MSE=0.0291 | ||âˆ‚c/âˆ‚z||=2.246 | KL/dim=0.139 âœ“ | Unc-Err Ï=-0.07 âš  | Unexplored=19.5%\n",
      "\n",
      "======================================================================\n",
      "ğŸ† Training ì™„ë£Œ! Best RÂ²: 0.0493\n",
      "======================================================================\n",
      "   Run 1: RÂ²=0.0493\n",
      "   Run 2: RÂ²=-0.0705\n",
      "   Run 3: RÂ²=-0.6889\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š ê²°ê³¼ ë¹„êµ\n",
      "======================================================================\n",
      "baseline       : RÂ² = -0.2539 Â± 0.3029\n",
      "augmented      : RÂ² = -0.2367 Â± 0.3235\n",
      "\n",
      "ğŸ“ˆ Augmented Loss Scale ë¶„ì„ (ë§ˆì§€ë§‰ epoch)\n",
      "   reg_loss (raw):      0.000000\n",
      "   pair_loss (raw):     0.122852\n",
      "   smooth_loss_l (raw): 0.000000\n",
      "   kld_loss_l (raw):    0.000000\n",
      "   grad_dir_loss (raw): 0.000000\n",
      "   ---\n",
      "   reg_weighted:        0.000000\n",
      "   pair_weighted:       0.000000\n",
      "   smooth_l_weighted:   0.000000\n",
      "   kld_l_weighted:      0.000000\n",
      "   grad_dir_weighted:   0.000000\n",
      "   ---\n",
      "   z_scale_l:           0.425492\n",
      "\n",
      "   âš ï¸ reg ë¹„ìœ¨: 0.0% (90% ì´ìƒì´ì–´ì•¼ ì•ˆì •)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ğŸ”¬ Augmentation ë””ë²„ê¹… ë¹„êµ ì‹¤í—˜\n",
    "# ============================================================\n",
    "# ëª©í‘œ: Baseline vs Augmentation ì„±ëŠ¥ ë¹„êµ ë° loss scale ë¶„ì„\n",
    "# \n",
    "# ê·œì¹™ ì ìš©:\n",
    "# 1. smooth_loss: anchor detach ì ìš©ë¨\n",
    "# 2. grad_direction: warmup 10 epoch + linear ramp\n",
    "# 3. z-scale ê¸°ë°˜ noise/alpha\n",
    "# 4. ë‚®ì€ ê°€ì¤‘ì¹˜ (pair: 0.01, gamma: 0.001, beta: 0.0001)\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "# Device ì„¤ì •\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# Feature dimension\n",
    "INPUT_DIM = train_flatten_features.shape[1]\n",
    "print(f\"Input dim: {INPUT_DIM}\")\n",
    "\n",
    "# Pretrained VAE ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ\n",
    "vae_checkpoint_dir = '/root/work/tenset/scripts/pre_experiments/model_final/checkpoints/vae_medium_v2'\n",
    "vae_files = sorted(glob(f'{vae_checkpoint_dir}/vae_rank*.pt'))\n",
    "\n",
    "if len(vae_files) > 0:\n",
    "    vae_path = vae_files[0]\n",
    "    pretrained_checkpoint = torch.load(vae_path, map_location=DEVICE)\n",
    "    print(f\"âœ… Pretrained VAE ë¡œë“œ: {vae_path}\")\n",
    "    HIDDEN_DIM = pretrained_checkpoint['config']['hidden_dim']\n",
    "    LATENT_DIM = pretrained_checkpoint['config']['latent_dim']\n",
    "    print(f\"   Hidden: {HIDDEN_DIM}, Latent: {LATENT_DIM}\")\n",
    "else:\n",
    "    pretrained_checkpoint = None\n",
    "    HIDDEN_DIM = 256\n",
    "    LATENT_DIM = 128\n",
    "    print(\"âš ï¸ Pretrained VAE ì—†ìŒ\")\n",
    "\n",
    "\n",
    "def run_debug_comparison(n_runs=3, num_epochs=50, verbose=True):\n",
    "    \"\"\"Baseline vs Augmentation ë¹„êµ ì‹¤í—˜\"\"\"\n",
    "    \n",
    "    # ë‘ ê°€ì§€ config ì •ì˜\n",
    "    configs = {\n",
    "        'baseline': {\n",
    "            'encoder_lr': 1e-5,\n",
    "            'predictor_lr': 1e-4,\n",
    "            'weight_decay': 1e-5,\n",
    "            'loss_type': 'mse',\n",
    "            # Augmentation ë¹„í™œì„±í™”\n",
    "            'lambda_pair': 0.0,\n",
    "            'gamma': 0.0,  # smooth off\n",
    "            'beta': 0.0,   # kld off\n",
    "            'lambda_grad_dir': 0.0,\n",
    "        },\n",
    "        'augmented': {\n",
    "            'encoder_lr': 1e-5,\n",
    "            'predictor_lr': 1e-4,\n",
    "            'weight_decay': 1e-5,\n",
    "            'loss_type': 'mse',\n",
    "            # ë‚®ì€ ê°€ì¤‘ì¹˜ë¡œ augmentation\n",
    "            'lambda_pair': 0.01,\n",
    "            'gamma': 0.001,\n",
    "            'beta': 0.0001,\n",
    "            'lambda_grad_dir': 0.01,\n",
    "            # warmup ì„¤ì •\n",
    "            'grad_dir_warmup': 10,\n",
    "            'grad_dir_ramp': 10,\n",
    "            # z-scale factor\n",
    "            'z_scale_factor_smooth': 0.1,\n",
    "            'z_scale_factor_grad': 0.05,\n",
    "            'margin': 0.1,\n",
    "            'grad_margin': 0.01,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for config_name, config in configs.items():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ğŸ”¬ {config_name.upper()} ì‹¤í—˜ ({n_runs}íšŒ)\")\n",
    "        print(f\"{'='*70}\")\n",
    "        if config_name == 'augmented':\n",
    "            print(f\"   lambda_pair: {config['lambda_pair']}, gamma: {config['gamma']}\")\n",
    "            print(f\"   beta: {config['beta']}, lambda_grad_dir: {config['lambda_grad_dir']}\")\n",
    "            print(f\"   grad_dir_warmup: {config['grad_dir_warmup']}, ramp: {config['grad_dir_ramp']}\")\n",
    "        \n",
    "        run_results = []\n",
    "        \n",
    "        for run in range(n_runs):\n",
    "            # Seed ì„¤ì •\n",
    "            run_seed = SEED + run\n",
    "            torch.manual_seed(run_seed)\n",
    "            np.random.seed(run_seed)\n",
    "            random.seed(run_seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(run_seed)\n",
    "            \n",
    "            # ëª¨ë¸ ì´ˆê¸°í™” (input_dim ì‚¬ìš©)\n",
    "            model = VAECostPredictor(\n",
    "                input_dim=INPUT_DIM,\n",
    "                hidden_dim=HIDDEN_DIM,\n",
    "                latent_dim=LATENT_DIM,\n",
    "                predictor_hidden=256,\n",
    "                predictor_layers=3,\n",
    "                dropout=0.1,\n",
    "            ).to(DEVICE)\n",
    "            \n",
    "            if pretrained_checkpoint is not None:\n",
    "                if 'model_state_dict' in pretrained_checkpoint:\n",
    "                    vae_state = pretrained_checkpoint['model_state_dict']\n",
    "                else:\n",
    "                    vae_state = pretrained_checkpoint\n",
    "                \n",
    "                encoder_keys = ['segment_encoder', 'norm', 'l0', 'l1', 'fc_mean', 'fc_logvar']\n",
    "                own_state = model.state_dict()\n",
    "                for name, param in vae_state.items():\n",
    "                    if any(name.startswith(k) for k in encoder_keys):\n",
    "                        if name in own_state and own_state[name].shape == param.shape:\n",
    "                            own_state[name].copy_(param)\n",
    "                model.load_state_dict(own_state)\n",
    "            \n",
    "            # í•™ìŠµ\n",
    "            labeled_indices = np.arange(64)\n",
    "            unlabeled_indices = np.arange(64, 128)\n",
    "            \n",
    "            trained_model, history, best_metrics, monitor = train_semi_supervised(\n",
    "                model, labeled_indices, unlabeled_indices,\n",
    "                train_feature_list, train_segment_sizes, train_labels,\n",
    "                val_feature_list, val_segment_sizes, val_labels,\n",
    "                fea_norm_vec, DEVICE, config,\n",
    "                num_epochs=num_epochs,\n",
    "                eval_every=5,\n",
    "                verbose=(run == 0 and verbose)\n",
    "            )\n",
    "            \n",
    "            run_results.append({\n",
    "                'r2': best_metrics['r2'],\n",
    "                'mse': best_metrics['mse'],\n",
    "                'history': history,\n",
    "            })\n",
    "            \n",
    "            print(f\"   Run {run+1}: RÂ²={best_metrics['r2']:.4f}\")\n",
    "        \n",
    "        r2_values = [r['r2'] for r in run_results]\n",
    "        all_results[config_name] = {\n",
    "            'runs': run_results,\n",
    "            'r2_mean': np.mean(r2_values),\n",
    "            'r2_std': np.std(r2_values),\n",
    "        }\n",
    "    \n",
    "    # ê²°ê³¼ ë¹„êµ\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ“Š ê²°ê³¼ ë¹„êµ\")\n",
    "    print(f\"{'='*70}\")\n",
    "    for config_name, results in all_results.items():\n",
    "        print(f\"{config_name:15s}: RÂ² = {results['r2_mean']:.4f} Â± {results['r2_std']:.4f}\")\n",
    "    \n",
    "    # Loss scale ë¶„ì„ (augmentedì˜ ì²« run)\n",
    "    if 'augmented' in all_results and len(all_results['augmented']['runs']) > 0:\n",
    "        history = all_results['augmented']['runs'][0]['history']\n",
    "        print(f\"\\nğŸ“ˆ Augmented Loss Scale ë¶„ì„ (ë§ˆì§€ë§‰ epoch)\")\n",
    "        if len(history) > 0:\n",
    "            last = history[-1]\n",
    "            print(f\"   reg_loss (raw):      {last.get('reg_loss', 0):.6f}\")\n",
    "            print(f\"   pair_loss (raw):     {last.get('pair_loss', 0):.6f}\")\n",
    "            print(f\"   smooth_loss_l (raw): {last.get('smooth_loss_l', 0):.6f}\")\n",
    "            print(f\"   kld_loss_l (raw):    {last.get('kld_loss_l', 0):.6f}\")\n",
    "            print(f\"   grad_dir_loss (raw): {last.get('grad_dir_loss', 0):.6f}\")\n",
    "            print(f\"   ---\")\n",
    "            print(f\"   reg_weighted:        {last.get('reg_weighted', 0):.6f}\")\n",
    "            print(f\"   pair_weighted:       {last.get('pair_weighted', 0):.6f}\")\n",
    "            print(f\"   smooth_l_weighted:   {last.get('smooth_l_weighted', 0):.6f}\")\n",
    "            print(f\"   kld_l_weighted:      {last.get('kld_l_weighted', 0):.6f}\")\n",
    "            print(f\"   grad_dir_weighted:   {last.get('grad_dir_weighted', 0):.6f}\")\n",
    "            print(f\"   ---\")\n",
    "            print(f\"   z_scale_l:           {last.get('z_scale_l', 0):.6f}\")\n",
    "            \n",
    "            # regê°€ totalì˜ ëª‡ %ì¸ì§€ í™•ì¸\n",
    "            total = last.get('total_loss', 1)\n",
    "            reg_w = last.get('reg_weighted', 0)\n",
    "            reg_ratio = reg_w / total * 100 if total > 0 else 0\n",
    "            print(f\"\\n   âš ï¸ reg ë¹„ìœ¨: {reg_ratio:.1f}% (90% ì´ìƒì´ì–´ì•¼ ì•ˆì •)\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "# ì‹¤í—˜ ì‹¤í–‰\n",
    "print(\"ğŸš€ Augmentation ë””ë²„ê¹… ë¹„êµ ì‹¤í—˜ ì‹œì‘...\")\n",
    "debug_results = run_debug_comparison(n_runs=3, num_epochs=50, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8be1e6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Learning Rate íŠœë‹ ì‹¤í—˜ ì‹œì‘...\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¬ lr_1e-3 (encoder_lr=1e-05, predictor_lr=0.001)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ Semi-Supervised Training ì‹œì‘\n",
      "   Labeled: 64ê°œ, Unlabeled: 64ê°œ\n",
      "   Validation pool: 3396ê°œ\n",
      "   Epochs: 100, Eval every: 10\n",
      "======================================================================\n",
      "  [Epoch 10/100] RÂ²=-0.1818 | MSE=0.0359 | ||âˆ‚c/âˆ‚z||=1.371 | KL/dim=0.142 âœ“ | Unc-Err Ï=-0.44 âš  | Unexplored=19.3%\n",
      "  [Epoch 20/100] RÂ²=-0.1589 | MSE=0.0352 | ||âˆ‚c/âˆ‚z||=1.414 | KL/dim=0.142 âœ“ | Unc-Err Ï=-0.31 âš  | Unexplored=19.7%\n",
      "  [Epoch 30/100] RÂ²=0.1342 | MSE=0.0263 | ||âˆ‚c/âˆ‚z||=0.916 | KL/dim=0.141 âœ“ | Unc-Err Ï=0.04 âš  | Unexplored=22.8%\n",
      "  [Epoch 40/100] RÂ²=0.1980 | MSE=0.0244 | ||âˆ‚c/âˆ‚z||=0.784 | KL/dim=0.139 âœ“ | Unc-Err Ï=0.30 âš  | Unexplored=21.4%\n",
      "  [Epoch 50/100] RÂ²=0.2015 | MSE=0.0243 | ||âˆ‚c/âˆ‚z||=0.637 | KL/dim=0.139 âœ“ | Unc-Err Ï=0.46 âœ“ | Unexplored=21.5%\n",
      "  [Epoch 60/100] RÂ²=0.2432 | MSE=0.0230 | ||âˆ‚c/âˆ‚z||=0.634 | KL/dim=0.139 âœ“ | Unc-Err Ï=0.34 âœ“ | Unexplored=21.6%\n",
      "  [Epoch 70/100] RÂ²=0.2196 | MSE=0.0237 | ||âˆ‚c/âˆ‚z||=0.603 | KL/dim=0.140 âœ“ | Unc-Err Ï=0.15 âš  | Unexplored=24.2%\n",
      "  [Epoch 80/100] RÂ²=0.2601 | MSE=0.0225 | ||âˆ‚c/âˆ‚z||=0.709 | KL/dim=0.138 âœ“ | Unc-Err Ï=0.17 âš  | Unexplored=24.3%\n",
      "  [Epoch 90/100] RÂ²=0.2122 | MSE=0.0239 | ||âˆ‚c/âˆ‚z||=0.610 | KL/dim=0.136 âœ“ | Unc-Err Ï=0.40 âœ“ | Unexplored=25.1%\n",
      "  [Epoch 100/100] RÂ²=0.2343 | MSE=0.0233 | ||âˆ‚c/âˆ‚z||=0.576 | KL/dim=0.136 âœ“ | Unc-Err Ï=0.57 âœ“ | Unexplored=24.9%\n",
      "\n",
      "======================================================================\n",
      "ğŸ† Training ì™„ë£Œ! Best RÂ²: 0.2601\n",
      "======================================================================\n",
      "   Run 1: RÂ²=0.2601\n",
      "   Run 2: RÂ²=0.2165\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¬ lr_5e-3 (encoder_lr=1e-05, predictor_lr=0.005)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ Semi-Supervised Training ì‹œì‘\n",
      "   Labeled: 64ê°œ, Unlabeled: 64ê°œ\n",
      "   Validation pool: 3396ê°œ\n",
      "   Epochs: 100, Eval every: 10\n",
      "======================================================================\n",
      "  [Epoch 10/100] RÂ²=-0.8019 | MSE=0.0548 | ||âˆ‚c/âˆ‚z||=0.456 | KL/dim=0.142 âœ“ | Unc-Err Ï=0.46 âœ“ | Unexplored=19.3%\n",
      "  [Epoch 20/100] RÂ²=0.0154 | MSE=0.0299 | ||âˆ‚c/âˆ‚z||=0.597 | KL/dim=0.142 âœ“ | Unc-Err Ï=0.29 âš  | Unexplored=19.8%\n",
      "  [Epoch 30/100] RÂ²=-0.3584 | MSE=0.0413 | ||âˆ‚c/âˆ‚z||=0.533 | KL/dim=0.139 âœ“ | Unc-Err Ï=0.28 âš  | Unexplored=21.7%\n",
      "  [Epoch 40/100] RÂ²=0.1366 | MSE=0.0262 | ||âˆ‚c/âˆ‚z||=0.485 | KL/dim=0.139 âœ“ | Unc-Err Ï=0.50 âœ“ | Unexplored=21.9%\n",
      "  [Epoch 50/100] RÂ²=0.1594 | MSE=0.0255 | ||âˆ‚c/âˆ‚z||=0.395 | KL/dim=0.139 âœ“ | Unc-Err Ï=0.50 âœ“ | Unexplored=22.1%\n",
      "  [Epoch 60/100] RÂ²=0.1864 | MSE=0.0247 | ||âˆ‚c/âˆ‚z||=0.424 | KL/dim=0.139 âœ“ | Unc-Err Ï=0.78 âœ“ | Unexplored=22.2%\n",
      "  [Epoch 70/100] RÂ²=0.2206 | MSE=0.0237 | ||âˆ‚c/âˆ‚z||=0.431 | KL/dim=0.137 âœ“ | Unc-Err Ï=0.76 âœ“ | Unexplored=23.5%\n",
      "  [Epoch 80/100] RÂ²=0.1857 | MSE=0.0247 | ||âˆ‚c/âˆ‚z||=0.363 | KL/dim=0.135 âœ“ | Unc-Err Ï=0.66 âœ“ | Unexplored=25.1%\n",
      "  [Epoch 90/100] RÂ²=0.1298 | MSE=0.0264 | ||âˆ‚c/âˆ‚z||=0.276 | KL/dim=0.134 âœ“ | Unc-Err Ï=0.64 âœ“ | Unexplored=26.0%\n",
      "  [Epoch 100/100] RÂ²=0.1550 | MSE=0.0257 | ||âˆ‚c/âˆ‚z||=0.369 | KL/dim=0.133 âœ“ | Unc-Err Ï=0.64 âœ“ | Unexplored=26.7%\n",
      "\n",
      "======================================================================\n",
      "ğŸ† Training ì™„ë£Œ! Best RÂ²: 0.2206\n",
      "======================================================================\n",
      "   Run 1: RÂ²=0.2206\n",
      "   Run 2: RÂ²=0.3207\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¬ lr_1e-2 (encoder_lr=1e-05, predictor_lr=0.01)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ Semi-Supervised Training ì‹œì‘\n",
      "   Labeled: 64ê°œ, Unlabeled: 64ê°œ\n",
      "   Validation pool: 3396ê°œ\n",
      "   Epochs: 100, Eval every: 10\n",
      "======================================================================\n",
      "  [Epoch 10/100] RÂ²=-1.7814 | MSE=0.0845 | ||âˆ‚c/âˆ‚z||=0.184 | KL/dim=0.144 âœ“ | Unc-Err Ï=0.05 âš  | Unexplored=18.9%\n",
      "  [Epoch 20/100] RÂ²=-0.1945 | MSE=0.0363 | ||âˆ‚c/âˆ‚z||=0.402 | KL/dim=0.143 âœ“ | Unc-Err Ï=0.37 âœ“ | Unexplored=19.3%\n",
      "  [Epoch 30/100] RÂ²=-5.1905 | MSE=0.1881 | ||âˆ‚c/âˆ‚z||=0.381 | KL/dim=0.141 âœ“ | Unc-Err Ï=0.24 âš  | Unexplored=20.2%\n",
      "  [Epoch 40/100] RÂ²=0.2712 | MSE=0.0221 | ||âˆ‚c/âˆ‚z||=0.302 | KL/dim=0.139 âœ“ | Unc-Err Ï=0.62 âœ“ | Unexplored=20.8%\n",
      "  [Epoch 50/100] RÂ²=0.2942 | MSE=0.0214 | ||âˆ‚c/âˆ‚z||=0.386 | KL/dim=0.138 âœ“ | Unc-Err Ï=0.64 âœ“ | Unexplored=21.1%\n",
      "  [Epoch 60/100] RÂ²=0.2442 | MSE=0.0230 | ||âˆ‚c/âˆ‚z||=0.396 | KL/dim=0.138 âœ“ | Unc-Err Ï=0.79 âœ“ | Unexplored=21.1%\n",
      "  [Epoch 70/100] RÂ²=0.1903 | MSE=0.0246 | ||âˆ‚c/âˆ‚z||=0.309 | KL/dim=0.137 âœ“ | Unc-Err Ï=0.56 âœ“ | Unexplored=22.7%\n",
      "  [Epoch 80/100] RÂ²=-0.2035 | MSE=0.0366 | ||âˆ‚c/âˆ‚z||=0.299 | KL/dim=0.135 âœ“ | Unc-Err Ï=0.58 âœ“ | Unexplored=24.5%\n",
      "  [Epoch 90/100] RÂ²=-0.0513 | MSE=0.0319 | ||âˆ‚c/âˆ‚z||=0.265 | KL/dim=0.133 âœ“ | Unc-Err Ï=0.59 âœ“ | Unexplored=24.6%\n",
      "  [Epoch 100/100] RÂ²=0.1331 | MSE=0.0263 | ||âˆ‚c/âˆ‚z||=0.331 | KL/dim=0.131 âœ“ | Unc-Err Ï=0.56 âœ“ | Unexplored=25.7%\n",
      "\n",
      "======================================================================\n",
      "ğŸ† Training ì™„ë£Œ! Best RÂ²: 0.2942\n",
      "======================================================================\n",
      "   Run 1: RÂ²=0.2942\n",
      "   Run 2: RÂ²=0.2661\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¬ lr_both_high (encoder_lr=0.0001, predictor_lr=0.01)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ Semi-Supervised Training ì‹œì‘\n",
      "   Labeled: 64ê°œ, Unlabeled: 64ê°œ\n",
      "   Validation pool: 3396ê°œ\n",
      "   Epochs: 100, Eval every: 10\n",
      "======================================================================\n",
      "  [Epoch 10/100] RÂ²=-2.2253 | MSE=0.0980 | ||âˆ‚c/âˆ‚z||=0.247 | KL/dim=0.155 âœ“ | Unc-Err Ï=0.24 âš  | Unexplored=100.0%\n",
      "  [Epoch 20/100] RÂ²=0.1651 | MSE=0.0254 | ||âˆ‚c/âˆ‚z||=0.340 | KL/dim=0.154 âœ“ | Unc-Err Ï=0.44 âœ“ | Unexplored=23.0%\n",
      "  [Epoch 30/100] RÂ²=-1.7203 | MSE=0.0827 | ||âˆ‚c/âˆ‚z||=0.116 | KL/dim=0.136 âœ“ | Unc-Err Ï=-0.36 âš  | Unexplored=99.9%\n",
      "  [Epoch 40/100] RÂ²=-0.0324 | MSE=0.0314 | ||âˆ‚c/âˆ‚z||=0.080 | KL/dim=0.127 âœ“ | Unc-Err Ï=0.36 âœ“ | Unexplored=91.2%\n",
      "  [Epoch 50/100] RÂ²=0.1468 | MSE=0.0259 | ||âˆ‚c/âˆ‚z||=0.116 | KL/dim=0.119 âœ“ | Unc-Err Ï=0.09 âš  | Unexplored=82.8%\n",
      "  [Epoch 60/100] RÂ²=0.2612 | MSE=0.0225 | ||âˆ‚c/âˆ‚z||=0.159 | KL/dim=0.118 âœ“ | Unc-Err Ï=0.49 âœ“ | Unexplored=74.5%\n",
      "  [Epoch 70/100] RÂ²=0.0856 | MSE=0.0278 | ||âˆ‚c/âˆ‚z||=0.260 | KL/dim=0.111 âœ“ | Unc-Err Ï=0.19 âš  | Unexplored=95.5%\n",
      "  [Epoch 80/100] RÂ²=0.2267 | MSE=0.0235 | ||âˆ‚c/âˆ‚z||=0.205 | KL/dim=0.109 âœ“ | Unc-Err Ï=0.25 âš  | Unexplored=90.2%\n",
      "  [Epoch 90/100] RÂ²=0.3491 | MSE=0.0198 | ||âˆ‚c/âˆ‚z||=0.196 | KL/dim=0.104 âœ“ | Unc-Err Ï=0.11 âš  | Unexplored=82.0%\n",
      "  [Epoch 100/100] RÂ²=0.3468 | MSE=0.0199 | ||âˆ‚c/âˆ‚z||=0.168 | KL/dim=0.100 âœ“ | Unc-Err Ï=0.48 âœ“ | Unexplored=76.0%\n",
      "\n",
      "======================================================================\n",
      "ğŸ† Training ì™„ë£Œ! Best RÂ²: 0.3491\n",
      "======================================================================\n",
      "   Run 1: RÂ²=0.3491\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 131\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_results\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸš€ Learning Rate íŠœë‹ ì‹¤í—˜ ì‹œì‘...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 131\u001b[0m lr_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_lr_tuning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_runs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[85], line 95\u001b[0m, in \u001b[0;36mrun_lr_tuning\u001b[0;34m(n_runs, num_epochs, verbose)\u001b[0m\n\u001b[1;32m     92\u001b[0m labeled_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m     93\u001b[0m unlabeled_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m trained_model, history, best_metrics, monitor \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_semi_supervised\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabeled_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlabeled_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_feature_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_segment_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_feature_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_segment_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfea_norm_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m run_results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m: best_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m: best_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m'\u001b[39m: history,\n\u001b[1;32m    109\u001b[0m })\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: RÂ²=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[83], line 117\u001b[0m, in \u001b[0;36mtrain_semi_supervised\u001b[0;34m(model, labeled_indices, unlabeled_indices, train_feature_list, train_segment_sizes, train_labels, val_feature_list, val_segment_sizes, val_labels, fea_norm_vec, device, config, num_epochs, eval_every, verbose)\u001b[0m\n\u001b[1;32m    114\u001b[0m latent_stats \u001b[38;5;241m=\u001b[39m monitor\u001b[38;5;241m.\u001b[39mrecord_latent(mean_l, logvar_l)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Uncertainty monitoring (labeled ë°°ì¹˜ì—ì„œ)\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m unc_stats \u001b[38;5;241m=\u001b[39m \u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_uncertainty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseg_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfea_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlab_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Coverage monitoring (train latents ì„¤ì • í›„)\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m eval_every \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# ì²« í‰ê°€ ì‹œ train latent ì„¤ì •\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[82], line 253\u001b[0m, in \u001b[0;36mTrainingMonitor.record_uncertainty\u001b[0;34m(self, model, segment_sizes, features, labels, device)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrecord_uncertainty\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, segment_sizes, features, labels, device):\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muncertainty_monitor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegment_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[82], line 127\u001b[0m, in \u001b[0;36mUncertaintyMonitor.record\u001b[0;34m(self, model, segment_sizes, features, labels, device, n_samples)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples):\n\u001b[0;32m--> 127\u001b[0m         pred, mean, logvar, z \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m         predictions\u001b[38;5;241m.\u001b[39mappend(pred\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    130\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (n_samples, batch_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/work/tenset/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/tenset/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[80], line 122\u001b[0m, in \u001b[0;36mVAECostPredictor.forward\u001b[0;34m(self, segment_sizes, features, use_mean)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, segment_sizes, features, use_mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Forward pass: input â†’ z â†’ cost\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m        z: sampled/mean latent vector\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     mean, logvar, segment_sum_vec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_mean:\n\u001b[1;32m    125\u001b[0m         z \u001b[38;5;241m=\u001b[39m mean  \u001b[38;5;66;03m# Inferenceì‹œ deterministic\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[80], line 94\u001b[0m, in \u001b[0;36mVAECostPredictor.encode\u001b[0;34m(self, segment_sizes, features)\u001b[0m\n\u001b[1;32m     91\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml0(h) \u001b[38;5;241m+\u001b[39m h  \u001b[38;5;66;03m# Residual\u001b[39;00m\n\u001b[1;32m     92\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1(h) \u001b[38;5;241m+\u001b[39m h  \u001b[38;5;66;03m# Residual\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_mean\u001b[49m(h)\n\u001b[1;32m     95\u001b[0m logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_logvar(h)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mean, logvar, segment_sum_vec\n",
      "File \u001b[0;32m~/work/tenset/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1683\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m-> 1683\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m:\n\u001b[1;32m   1684\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m   1685\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _parameters:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ğŸ”§ Learning Rate íŠœë‹ ì‹¤í—˜\n",
    "# ============================================================\n",
    "# RÂ²ê°€ ìŒìˆ˜ì¸ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë” ë†’ì€ learning rate í…ŒìŠ¤íŠ¸\n",
    "\n",
    "def run_lr_tuning(n_runs=2, num_epochs=100, verbose=True):\n",
    "    \"\"\"Learning rate íŠœë‹ ì‹¤í—˜\"\"\"\n",
    "    \n",
    "    configs = {\n",
    "        'lr_1e-3': {\n",
    "            'encoder_lr': 1e-5,\n",
    "            'predictor_lr': 1e-3,  # 10x ì¦ê°€\n",
    "            'weight_decay': 1e-5,\n",
    "            'loss_type': 'mse',\n",
    "            'lambda_pair': 0.0,\n",
    "            'gamma': 0.0,\n",
    "            'beta': 0.0,\n",
    "            'lambda_grad_dir': 0.0,\n",
    "        },\n",
    "        'lr_5e-3': {\n",
    "            'encoder_lr': 1e-5,\n",
    "            'predictor_lr': 5e-3,  # 50x ì¦ê°€\n",
    "            'weight_decay': 1e-5,\n",
    "            'loss_type': 'mse',\n",
    "            'lambda_pair': 0.0,\n",
    "            'gamma': 0.0,\n",
    "            'beta': 0.0,\n",
    "            'lambda_grad_dir': 0.0,\n",
    "        },\n",
    "        'lr_1e-2': {\n",
    "            'encoder_lr': 1e-5,\n",
    "            'predictor_lr': 1e-2,  # 100x ì¦ê°€\n",
    "            'weight_decay': 1e-5,\n",
    "            'loss_type': 'mse',\n",
    "            'lambda_pair': 0.0,\n",
    "            'gamma': 0.0,\n",
    "            'beta': 0.0,\n",
    "            'lambda_grad_dir': 0.0,\n",
    "        },\n",
    "        'lr_both_high': {\n",
    "            'encoder_lr': 1e-4,  # encoderë„ ë†’ì„\n",
    "            'predictor_lr': 1e-2,\n",
    "            'weight_decay': 1e-5,\n",
    "            'loss_type': 'mse',\n",
    "            'lambda_pair': 0.0,\n",
    "            'gamma': 0.0,\n",
    "            'beta': 0.0,\n",
    "            'lambda_grad_dir': 0.0,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for config_name, config in configs.items():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ğŸ”¬ {config_name} (encoder_lr={config['encoder_lr']}, predictor_lr={config['predictor_lr']})\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        run_results = []\n",
    "        \n",
    "        for run in range(n_runs):\n",
    "            run_seed = SEED + run\n",
    "            torch.manual_seed(run_seed)\n",
    "            np.random.seed(run_seed)\n",
    "            random.seed(run_seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(run_seed)\n",
    "            \n",
    "            model = VAECostPredictor(\n",
    "                input_dim=INPUT_DIM,\n",
    "                hidden_dim=HIDDEN_DIM,\n",
    "                latent_dim=LATENT_DIM,\n",
    "                predictor_hidden=256,\n",
    "                predictor_layers=3,\n",
    "                dropout=0.1,\n",
    "            ).to(DEVICE)\n",
    "            \n",
    "            if pretrained_checkpoint is not None:\n",
    "                if 'model_state_dict' in pretrained_checkpoint:\n",
    "                    vae_state = pretrained_checkpoint['model_state_dict']\n",
    "                else:\n",
    "                    vae_state = pretrained_checkpoint\n",
    "                \n",
    "                encoder_keys = ['segment_encoder', 'norm', 'l0', 'l1', 'fc_mean', 'fc_logvar']\n",
    "                own_state = model.state_dict()\n",
    "                for name, param in vae_state.items():\n",
    "                    if any(name.startswith(k) for k in encoder_keys):\n",
    "                        if name in own_state and own_state[name].shape == param.shape:\n",
    "                            own_state[name].copy_(param)\n",
    "                model.load_state_dict(own_state)\n",
    "            \n",
    "            labeled_indices = np.arange(64)\n",
    "            unlabeled_indices = np.arange(64, 128)\n",
    "            \n",
    "            trained_model, history, best_metrics, monitor = train_semi_supervised(\n",
    "                model, labeled_indices, unlabeled_indices,\n",
    "                train_feature_list, train_segment_sizes, train_labels,\n",
    "                val_feature_list, val_segment_sizes, val_labels,\n",
    "                fea_norm_vec, DEVICE, config,\n",
    "                num_epochs=num_epochs,\n",
    "                eval_every=10,\n",
    "                verbose=(run == 0 and verbose)\n",
    "            )\n",
    "            \n",
    "            run_results.append({\n",
    "                'r2': best_metrics['r2'],\n",
    "                'mse': best_metrics['mse'],\n",
    "                'history': history,\n",
    "            })\n",
    "            \n",
    "            print(f\"   Run {run+1}: RÂ²={best_metrics['r2']:.4f}\")\n",
    "        \n",
    "        r2_values = [r['r2'] for r in run_results]\n",
    "        all_results[config_name] = {\n",
    "            'runs': run_results,\n",
    "            'r2_mean': np.mean(r2_values),\n",
    "            'r2_std': np.std(r2_values),\n",
    "        }\n",
    "    \n",
    "    # ê²°ê³¼ ìš”ì•½\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ“Š Learning Rate íŠœë‹ ê²°ê³¼\")\n",
    "    print(f\"{'='*70}\")\n",
    "    for config_name, results in sorted(all_results.items(), key=lambda x: x[1]['r2_mean'], reverse=True):\n",
    "        print(f\"{config_name:15s}: RÂ² = {results['r2_mean']:.4f} Â± {results['r2_std']:.4f}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "print(\"ğŸš€ Learning Rate íŠœë‹ ì‹¤í—˜ ì‹œì‘...\")\n",
    "lr_results = run_lr_tuning(n_runs=2, num_epochs=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "196a18c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Augmentation íš¨ê³¼ ê²€ì¦ ì‹œì‘...\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¬ BASELINE\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ Semi-Supervised Training ì‹œì‘\n",
      "   Labeled: 64ê°œ, Unlabeled: 64ê°œ\n",
      "   Validation pool: 3396ê°œ\n",
      "   Epochs: 100, Eval every: 10\n",
      "======================================================================\n",
      "  [Epoch 10/100] RÂ²=0.4945 | MSE=1.4427 | ||âˆ‚c/âˆ‚z||=4.910 | KL/dim=0.139 âœ“ | Unc-Err Ï=0.05 âš  | Unexplored=21.9%\n",
      "  [Epoch 20/100] RÂ²=0.5207 | MSE=1.3680 | ||âˆ‚c/âˆ‚z||=6.744 | KL/dim=0.137 âœ“ | Unc-Err Ï=-0.06 âš  | Unexplored=20.9%\n",
      "  [Epoch 30/100] RÂ²=0.2856 | MSE=2.0389 | ||âˆ‚c/âˆ‚z||=6.912 | KL/dim=0.134 âœ“ | Unc-Err Ï=0.12 âš  | Unexplored=23.0%\n",
      "  [Epoch 20/100] RÂ²=0.5207 | MSE=1.3680 | ||âˆ‚c/âˆ‚z||=6.744 | KL/dim=0.137 âœ“ | Unc-Err Ï=-0.06 âš  | Unexplored=20.9%\n",
      "  [Epoch 30/100] RÂ²=0.2856 | MSE=2.0389 | ||âˆ‚c/âˆ‚z||=6.912 | KL/dim=0.134 âœ“ | Unc-Err Ï=0.12 âš  | Unexplored=23.0%\n",
      "  [Epoch 40/100] RÂ²=0.5604 | MSE=1.2547 | ||âˆ‚c/âˆ‚z||=7.352 | KL/dim=0.131 âœ“ | Unc-Err Ï=0.06 âš  | Unexplored=24.3%\n",
      "  [Epoch 50/100] RÂ²=0.5610 | MSE=1.2529 | ||âˆ‚c/âˆ‚z||=6.330 | KL/dim=0.130 âœ“ | Unc-Err Ï=-0.05 âš  | Unexplored=24.9%\n",
      "  [Epoch 40/100] RÂ²=0.5604 | MSE=1.2547 | ||âˆ‚c/âˆ‚z||=7.352 | KL/dim=0.131 âœ“ | Unc-Err Ï=0.06 âš  | Unexplored=24.3%\n",
      "  [Epoch 50/100] RÂ²=0.5610 | MSE=1.2529 | ||âˆ‚c/âˆ‚z||=6.330 | KL/dim=0.130 âœ“ | Unc-Err Ï=-0.05 âš  | Unexplored=24.9%\n",
      "  [Epoch 60/100] RÂ²=0.5580 | MSE=1.2614 | ||âˆ‚c/âˆ‚z||=6.394 | KL/dim=0.130 âœ“ | Unc-Err Ï=0.04 âš  | Unexplored=24.7%\n",
      "  [Epoch 70/100] RÂ²=0.6413 | MSE=1.0237 | ||âˆ‚c/âˆ‚z||=6.010 | KL/dim=0.128 âœ“ | Unc-Err Ï=0.17 âš  | Unexplored=27.0%\n",
      "  [Epoch 60/100] RÂ²=0.5580 | MSE=1.2614 | ||âˆ‚c/âˆ‚z||=6.394 | KL/dim=0.130 âœ“ | Unc-Err Ï=0.04 âš  | Unexplored=24.7%\n",
      "  [Epoch 70/100] RÂ²=0.6413 | MSE=1.0237 | ||âˆ‚c/âˆ‚z||=6.010 | KL/dim=0.128 âœ“ | Unc-Err Ï=0.17 âš  | Unexplored=27.0%\n",
      "  [Epoch 80/100] RÂ²=0.6268 | MSE=1.0652 | ||âˆ‚c/âˆ‚z||=6.100 | KL/dim=0.126 âœ“ | Unc-Err Ï=0.09 âš  | Unexplored=28.5%\n",
      "  [Epoch 90/100] RÂ²=0.6034 | MSE=1.1318 | ||âˆ‚c/âˆ‚z||=5.510 | KL/dim=0.125 âœ“ | Unc-Err Ï=0.12 âš  | Unexplored=29.3%\n",
      "  [Epoch 80/100] RÂ²=0.6268 | MSE=1.0652 | ||âˆ‚c/âˆ‚z||=6.100 | KL/dim=0.126 âœ“ | Unc-Err Ï=0.09 âš  | Unexplored=28.5%\n",
      "  [Epoch 90/100] RÂ²=0.6034 | MSE=1.1318 | ||âˆ‚c/âˆ‚z||=5.510 | KL/dim=0.125 âœ“ | Unc-Err Ï=0.12 âš  | Unexplored=29.3%\n",
      "  [Epoch 100/100] RÂ²=0.6332 | MSE=1.0468 | ||âˆ‚c/âˆ‚z||=5.860 | KL/dim=0.123 âœ“ | Unc-Err Ï=0.24 âš  | Unexplored=29.9%\n",
      "\n",
      "======================================================================\n",
      "ğŸ† Training ì™„ë£Œ! Best RÂ²: 0.6413\n",
      "======================================================================\n",
      "   Run 1: RÂ²=0.6413\n",
      "  [Epoch 100/100] RÂ²=0.6332 | MSE=1.0468 | ||âˆ‚c/âˆ‚z||=5.860 | KL/dim=0.123 âœ“ | Unc-Err Ï=0.24 âš  | Unexplored=29.9%\n",
      "\n",
      "======================================================================\n",
      "ğŸ† Training ì™„ë£Œ! Best RÂ²: 0.6413\n",
      "======================================================================\n",
      "   Run 1: RÂ²=0.6413\n",
      "   Run 2: RÂ²=0.6129\n",
      "   Run 2: RÂ²=0.6129\n",
      "   Run 3: RÂ²=0.6170\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¬ AUG_CONSERVATIVE\n",
      "======================================================================\n",
      "   pair: 0.001, smooth: 0.0001, kld: 1e-05\n",
      "   grad_dir: 0.001, warmup: 30\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ Semi-Supervised Training ì‹œì‘\n",
      "   Labeled: 64ê°œ, Unlabeled: 64ê°œ\n",
      "   Validation pool: 3396ê°œ\n",
      "   Epochs: 100, Eval every: 10\n",
      "======================================================================\n",
      "  [Epoch 10/100] RÂ²=0.4287 | MSE=1.6305 | ||âˆ‚c/âˆ‚z||=5.299 | KL/dim=0.137 âœ“ | Unc-Err Ï=-0.03 âš  | Unexplored=21.2%\n",
      "   Run 3: RÂ²=0.6170\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¬ AUG_CONSERVATIVE\n",
      "======================================================================\n",
      "   pair: 0.001, smooth: 0.0001, kld: 1e-05\n",
      "   grad_dir: 0.001, warmup: 30\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ Semi-Supervised Training ì‹œì‘\n",
      "   Labeled: 64ê°œ, Unlabeled: 64ê°œ\n",
      "   Validation pool: 3396ê°œ\n",
      "   Epochs: 100, Eval every: 10\n",
      "======================================================================\n",
      "  [Epoch 10/100] RÂ²=0.4287 | MSE=1.6305 | ||âˆ‚c/âˆ‚z||=5.299 | KL/dim=0.137 âœ“ | Unc-Err Ï=-0.03 âš  | Unexplored=21.2%\n",
      "  [Epoch 20/100] RÂ²=0.5451 | MSE=1.2984 | ||âˆ‚c/âˆ‚z||=6.092 | KL/dim=0.136 âœ“ | Unc-Err Ï=0.27 âš  | Unexplored=21.1%\n",
      "  [Epoch 30/100] RÂ²=0.0675 | MSE=2.6614 | ||âˆ‚c/âˆ‚z||=6.722 | KL/dim=0.133 âœ“ | Unc-Err Ï=0.07 âš  | Unexplored=23.8%\n",
      "  [Epoch 20/100] RÂ²=0.5451 | MSE=1.2984 | ||âˆ‚c/âˆ‚z||=6.092 | KL/dim=0.136 âœ“ | Unc-Err Ï=0.27 âš  | Unexplored=21.1%\n",
      "  [Epoch 30/100] RÂ²=0.0675 | MSE=2.6614 | ||âˆ‚c/âˆ‚z||=6.722 | KL/dim=0.133 âœ“ | Unc-Err Ï=0.07 âš  | Unexplored=23.8%\n",
      "  [Epoch 40/100] RÂ²=0.6203 | MSE=1.0837 | ||âˆ‚c/âˆ‚z||=6.746 | KL/dim=0.130 âœ“ | Unc-Err Ï=0.11 âš  | Unexplored=23.8%\n",
      "  [Epoch 50/100] RÂ²=0.5930 | MSE=1.1617 | ||âˆ‚c/âˆ‚z||=6.574 | KL/dim=0.128 âœ“ | Unc-Err Ï=-0.03 âš  | Unexplored=24.5%\n",
      "  [Epoch 40/100] RÂ²=0.6203 | MSE=1.0837 | ||âˆ‚c/âˆ‚z||=6.746 | KL/dim=0.130 âœ“ | Unc-Err Ï=0.11 âš  | Unexplored=23.8%\n",
      "  [Epoch 50/100] RÂ²=0.5930 | MSE=1.1617 | ||âˆ‚c/âˆ‚z||=6.574 | KL/dim=0.128 âœ“ | Unc-Err Ï=-0.03 âš  | Unexplored=24.5%\n",
      "  [Epoch 60/100] RÂ²=0.6006 | MSE=1.1398 | ||âˆ‚c/âˆ‚z||=6.553 | KL/dim=0.128 âœ“ | Unc-Err Ï=-0.03 âš  | Unexplored=24.0%\n",
      "  [Epoch 70/100] RÂ²=0.5575 | MSE=1.2629 | ||âˆ‚c/âˆ‚z||=6.563 | KL/dim=0.125 âœ“ | Unc-Err Ï=-0.03 âš  | Unexplored=26.4%\n",
      "  [Epoch 60/100] RÂ²=0.6006 | MSE=1.1398 | ||âˆ‚c/âˆ‚z||=6.553 | KL/dim=0.128 âœ“ | Unc-Err Ï=-0.03 âš  | Unexplored=24.0%\n",
      "  [Epoch 70/100] RÂ²=0.5575 | MSE=1.2629 | ||âˆ‚c/âˆ‚z||=6.563 | KL/dim=0.125 âœ“ | Unc-Err Ï=-0.03 âš  | Unexplored=26.4%\n",
      "  [Epoch 80/100] RÂ²=0.6315 | MSE=1.0518 | ||âˆ‚c/âˆ‚z||=5.880 | KL/dim=0.122 âœ“ | Unc-Err Ï=-0.03 âš  | Unexplored=28.1%\n",
      "  [Epoch 90/100] RÂ²=0.5970 | MSE=1.1501 | ||âˆ‚c/âˆ‚z||=5.936 | KL/dim=0.119 âœ“ | Unc-Err Ï=0.14 âš  | Unexplored=28.6%\n",
      "  [Epoch 80/100] RÂ²=0.6315 | MSE=1.0518 | ||âˆ‚c/âˆ‚z||=5.880 | KL/dim=0.122 âœ“ | Unc-Err Ï=-0.03 âš  | Unexplored=28.1%\n",
      "  [Epoch 90/100] RÂ²=0.5970 | MSE=1.1501 | ||âˆ‚c/âˆ‚z||=5.936 | KL/dim=0.119 âœ“ | Unc-Err Ï=0.14 âš  | Unexplored=28.6%\n",
      "  [Epoch 100/100] RÂ²=0.5642 | MSE=1.2438 | ||âˆ‚c/âˆ‚z||=5.627 | KL/dim=0.118 âœ“ | Unc-Err Ï=0.23 âš  | Unexplored=29.2%\n",
      "\n",
      "======================================================================\n",
      "ğŸ† Training ì™„ë£Œ! Best RÂ²: 0.6315\n",
      "======================================================================\n",
      "   Run 1: RÂ²=0.6315\n",
      "  [Epoch 100/100] RÂ²=0.5642 | MSE=1.2438 | ||âˆ‚c/âˆ‚z||=5.627 | KL/dim=0.118 âœ“ | Unc-Err Ï=0.23 âš  | Unexplored=29.2%\n",
      "\n",
      "======================================================================\n",
      "ğŸ† Training ì™„ë£Œ! Best RÂ²: 0.6315\n",
      "======================================================================\n",
      "   Run 1: RÂ²=0.6315\n",
      "   Run 2: RÂ²=0.5899\n",
      "   Run 2: RÂ²=0.5899\n",
      "   Run 3: RÂ²=0.5934\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¬ AUG_MODERATE\n",
      "======================================================================\n",
      "   pair: 0.01, smooth: 0.001, kld: 0.0001\n",
      "   grad_dir: 0.01, warmup: 20\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ Semi-Supervised Training ì‹œì‘\n",
      "   Labeled: 64ê°œ, Unlabeled: 64ê°œ\n",
      "   Validation pool: 3396ê°œ\n",
      "   Epochs: 100, Eval every: 10\n",
      "======================================================================\n",
      "  [Epoch 10/100] RÂ²=0.4295 | MSE=1.6282 | ||âˆ‚c/âˆ‚z||=4.997 | KL/dim=0.138 âœ“ | Unc-Err Ï=-0.02 âš  | Unexplored=21.5%\n",
      "   Run 3: RÂ²=0.5934\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¬ AUG_MODERATE\n",
      "======================================================================\n",
      "   pair: 0.01, smooth: 0.001, kld: 0.0001\n",
      "   grad_dir: 0.01, warmup: 20\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ Semi-Supervised Training ì‹œì‘\n",
      "   Labeled: 64ê°œ, Unlabeled: 64ê°œ\n",
      "   Validation pool: 3396ê°œ\n",
      "   Epochs: 100, Eval every: 10\n",
      "======================================================================\n",
      "  [Epoch 10/100] RÂ²=0.4295 | MSE=1.6282 | ||âˆ‚c/âˆ‚z||=4.997 | KL/dim=0.138 âœ“ | Unc-Err Ï=-0.02 âš  | Unexplored=21.5%\n",
      "  [Epoch 20/100] RÂ²=0.5295 | MSE=1.3429 | ||âˆ‚c/âˆ‚z||=5.889 | KL/dim=0.136 âœ“ | Unc-Err Ï=0.07 âš  | Unexplored=21.1%\n",
      "  [Epoch 30/100] RÂ²=0.1731 | MSE=2.3600 | ||âˆ‚c/âˆ‚z||=6.399 | KL/dim=0.131 âœ“ | Unc-Err Ï=0.11 âš  | Unexplored=24.6%\n",
      "  [Epoch 20/100] RÂ²=0.5295 | MSE=1.3429 | ||âˆ‚c/âˆ‚z||=5.889 | KL/dim=0.136 âœ“ | Unc-Err Ï=0.07 âš  | Unexplored=21.1%\n",
      "  [Epoch 30/100] RÂ²=0.1731 | MSE=2.3600 | ||âˆ‚c/âˆ‚z||=6.399 | KL/dim=0.131 âœ“ | Unc-Err Ï=0.11 âš  | Unexplored=24.6%\n",
      "  [Epoch 40/100] RÂ²=0.5828 | MSE=1.1907 | ||âˆ‚c/âˆ‚z||=7.102 | KL/dim=0.127 âœ“ | Unc-Err Ï=0.03 âš  | Unexplored=25.6%\n",
      "  [Epoch 50/100] RÂ²=0.5933 | MSE=1.1607 | ||âˆ‚c/âˆ‚z||=6.825 | KL/dim=0.125 âœ“ | Unc-Err Ï=-0.02 âš  | Unexplored=25.2%\n",
      "  [Epoch 40/100] RÂ²=0.5828 | MSE=1.1907 | ||âˆ‚c/âˆ‚z||=7.102 | KL/dim=0.127 âœ“ | Unc-Err Ï=0.03 âš  | Unexplored=25.6%\n",
      "  [Epoch 50/100] RÂ²=0.5933 | MSE=1.1607 | ||âˆ‚c/âˆ‚z||=6.825 | KL/dim=0.125 âœ“ | Unc-Err Ï=-0.02 âš  | Unexplored=25.2%\n",
      "  [Epoch 60/100] RÂ²=0.5834 | MSE=1.1891 | ||âˆ‚c/âˆ‚z||=6.832 | KL/dim=0.125 âœ“ | Unc-Err Ï=0.11 âš  | Unexplored=24.8%\n",
      "  [Epoch 70/100] RÂ²=0.5716 | MSE=1.2227 | ||âˆ‚c/âˆ‚z||=6.595 | KL/dim=0.121 âœ“ | Unc-Err Ï=0.18 âš  | Unexplored=27.4%\n",
      "  [Epoch 60/100] RÂ²=0.5834 | MSE=1.1891 | ||âˆ‚c/âˆ‚z||=6.832 | KL/dim=0.125 âœ“ | Unc-Err Ï=0.11 âš  | Unexplored=24.8%\n",
      "  [Epoch 70/100] RÂ²=0.5716 | MSE=1.2227 | ||âˆ‚c/âˆ‚z||=6.595 | KL/dim=0.121 âœ“ | Unc-Err Ï=0.18 âš  | Unexplored=27.4%\n",
      "  [Epoch 80/100] RÂ²=0.5968 | MSE=1.1508 | ||âˆ‚c/âˆ‚z||=5.769 | KL/dim=0.120 âœ“ | Unc-Err Ï=0.21 âš  | Unexplored=30.0%\n",
      "  [Epoch 90/100] RÂ²=0.6141 | MSE=1.1014 | ||âˆ‚c/âˆ‚z||=5.419 | KL/dim=0.117 âœ“ | Unc-Err Ï=0.33 âœ“ | Unexplored=29.9%\n",
      "  [Epoch 80/100] RÂ²=0.5968 | MSE=1.1508 | ||âˆ‚c/âˆ‚z||=5.769 | KL/dim=0.120 âœ“ | Unc-Err Ï=0.21 âš  | Unexplored=30.0%\n",
      "  [Epoch 90/100] RÂ²=0.6141 | MSE=1.1014 | ||âˆ‚c/âˆ‚z||=5.419 | KL/dim=0.117 âœ“ | Unc-Err Ï=0.33 âœ“ | Unexplored=29.9%\n",
      "  [Epoch 100/100] RÂ²=0.6103 | MSE=1.1123 | ||âˆ‚c/âˆ‚z||=5.833 | KL/dim=0.115 âœ“ | Unc-Err Ï=0.01 âš  | Unexplored=31.5%\n",
      "\n",
      "======================================================================\n",
      "ğŸ† Training ì™„ë£Œ! Best RÂ²: 0.6141\n",
      "======================================================================\n",
      "   Run 1: RÂ²=0.6141\n",
      "  [Epoch 100/100] RÂ²=0.6103 | MSE=1.1123 | ||âˆ‚c/âˆ‚z||=5.833 | KL/dim=0.115 âœ“ | Unc-Err Ï=0.01 âš  | Unexplored=31.5%\n",
      "\n",
      "======================================================================\n",
      "ğŸ† Training ì™„ë£Œ! Best RÂ²: 0.6141\n",
      "======================================================================\n",
      "   Run 1: RÂ²=0.6141\n",
      "   Run 2: RÂ²=0.6071\n",
      "   Run 2: RÂ²=0.6071\n",
      "   Run 3: RÂ²=0.6075\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š Augmentation íš¨ê³¼ ë¹„êµ (ì˜¬ë°”ë¥¸ LR)\n",
      "======================================================================\n",
      "baseline          : RÂ² = 0.6238 Â± 0.0125 (0.0000)\n",
      "aug_moderate      : RÂ² = 0.6096 Â± 0.0032 (-0.0142)\n",
      "aug_conservative  : RÂ² = 0.6049 Â± 0.0188 (-0.0188)\n",
      "\n",
      "   aug_conservative reg ë¹„ìœ¨: 98.9%\n",
      "\n",
      "   aug_moderate reg ë¹„ìœ¨: 93.4%\n",
      "   Run 3: RÂ²=0.6075\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š Augmentation íš¨ê³¼ ë¹„êµ (ì˜¬ë°”ë¥¸ LR)\n",
      "======================================================================\n",
      "baseline          : RÂ² = 0.6238 Â± 0.0125 (0.0000)\n",
      "aug_moderate      : RÂ² = 0.6096 Â± 0.0032 (-0.0142)\n",
      "aug_conservative  : RÂ² = 0.6049 Â± 0.0188 (-0.0188)\n",
      "\n",
      "   aug_conservative reg ë¹„ìœ¨: 98.9%\n",
      "\n",
      "   aug_moderate reg ë¹„ìœ¨: 93.4%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ğŸ”¬ Augmentation íš¨ê³¼ ê²€ì¦ (ì˜¬ë°”ë¥¸ Learning Rateë¡œ)\n",
    "# ============================================================\n",
    "# lr_5e-3ê°€ ìµœì ì„ì„ í™•ì¸, ì´ì œ augmentation íš¨ê³¼ ê²€ì¦\n",
    "\n",
    "def run_augmentation_test(n_runs=3, num_epochs=100, verbose=True):\n",
    "    \"\"\"Augmentation íš¨ê³¼ ê²€ì¦ (ìµœì  LR ì‚¬ìš©)\"\"\"\n",
    "    \n",
    "    best_predictor_lr = 5e-3  # ìœ„ ì‹¤í—˜ì—ì„œ ìµœì \n",
    "    \n",
    "    configs = {\n",
    "        'baseline': {\n",
    "            'encoder_lr': 1e-5,\n",
    "            'predictor_lr': best_predictor_lr,\n",
    "            'weight_decay': 1e-5,\n",
    "            'loss_type': 'mse',\n",
    "            'lambda_pair': 0.0,\n",
    "            'gamma': 0.0,\n",
    "            'beta': 0.0,\n",
    "            'lambda_grad_dir': 0.0,\n",
    "        },\n",
    "        'aug_conservative': {\n",
    "            'encoder_lr': 1e-5,\n",
    "            'predictor_lr': best_predictor_lr,\n",
    "            'weight_decay': 1e-5,\n",
    "            'loss_type': 'mse',\n",
    "            # ë§¤ìš° ë³´ìˆ˜ì ì¸ augmentation\n",
    "            'lambda_pair': 0.001,\n",
    "            'gamma': 0.0001,\n",
    "            'beta': 0.00001,\n",
    "            'lambda_grad_dir': 0.001,\n",
    "            'grad_dir_warmup': 30,\n",
    "            'grad_dir_ramp': 20,\n",
    "            'z_scale_factor_smooth': 0.1,\n",
    "            'z_scale_factor_grad': 0.05,\n",
    "            'margin': 0.1,\n",
    "            'grad_margin': 0.01,\n",
    "        },\n",
    "        'aug_moderate': {\n",
    "            'encoder_lr': 1e-5,\n",
    "            'predictor_lr': best_predictor_lr,\n",
    "            'weight_decay': 1e-5,\n",
    "            'loss_type': 'mse',\n",
    "            # ì•½ê°„ ë” ê°•í•œ augmentation\n",
    "            'lambda_pair': 0.01,\n",
    "            'gamma': 0.001,\n",
    "            'beta': 0.0001,\n",
    "            'lambda_grad_dir': 0.01,\n",
    "            'grad_dir_warmup': 20,\n",
    "            'grad_dir_ramp': 20,\n",
    "            'z_scale_factor_smooth': 0.1,\n",
    "            'z_scale_factor_grad': 0.05,\n",
    "            'margin': 0.1,\n",
    "            'grad_margin': 0.01,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for config_name, config in configs.items():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ğŸ”¬ {config_name.upper()}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        if 'aug' in config_name:\n",
    "            print(f\"   pair: {config['lambda_pair']}, smooth: {config['gamma']}, kld: {config['beta']}\")\n",
    "            print(f\"   grad_dir: {config['lambda_grad_dir']}, warmup: {config['grad_dir_warmup']}\")\n",
    "        \n",
    "        run_results = []\n",
    "        \n",
    "        for run in range(n_runs):\n",
    "            run_seed = SEED + run\n",
    "            torch.manual_seed(run_seed)\n",
    "            np.random.seed(run_seed)\n",
    "            random.seed(run_seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(run_seed)\n",
    "            \n",
    "            model = VAECostPredictor(\n",
    "                input_dim=INPUT_DIM,\n",
    "                hidden_dim=HIDDEN_DIM,\n",
    "                latent_dim=LATENT_DIM,\n",
    "                predictor_hidden=256,\n",
    "                predictor_layers=3,\n",
    "                dropout=0.1,\n",
    "            ).to(DEVICE)\n",
    "            \n",
    "            if pretrained_checkpoint is not None:\n",
    "                if 'model_state_dict' in pretrained_checkpoint:\n",
    "                    vae_state = pretrained_checkpoint['model_state_dict']\n",
    "                else:\n",
    "                    vae_state = pretrained_checkpoint\n",
    "                \n",
    "                encoder_keys = ['segment_encoder', 'norm', 'l0', 'l1', 'fc_mean', 'fc_logvar']\n",
    "                own_state = model.state_dict()\n",
    "                for name, param in vae_state.items():\n",
    "                    if any(name.startswith(k) for k in encoder_keys):\n",
    "                        if name in own_state and own_state[name].shape == param.shape:\n",
    "                            own_state[name].copy_(param)\n",
    "                model.load_state_dict(own_state)\n",
    "            \n",
    "            labeled_indices = np.arange(64)\n",
    "            unlabeled_indices = np.arange(64, 128)\n",
    "            \n",
    "            trained_model, history, best_metrics, monitor = train_semi_supervised(\n",
    "                model, labeled_indices, unlabeled_indices,\n",
    "                train_feature_list, train_segment_sizes, train_labels,\n",
    "                val_feature_list, val_segment_sizes, val_labels,\n",
    "                fea_norm_vec, DEVICE, config,\n",
    "                num_epochs=num_epochs,\n",
    "                eval_every=10,\n",
    "                verbose=(run == 0 and verbose)\n",
    "            )\n",
    "            \n",
    "            run_results.append({\n",
    "                'r2': best_metrics['r2'],\n",
    "                'mse': best_metrics['mse'],\n",
    "                'history': history,\n",
    "            })\n",
    "            \n",
    "            print(f\"   Run {run+1}: RÂ²={best_metrics['r2']:.4f}\")\n",
    "        \n",
    "        r2_values = [r['r2'] for r in run_results]\n",
    "        all_results[config_name] = {\n",
    "            'runs': run_results,\n",
    "            'r2_mean': np.mean(r2_values),\n",
    "            'r2_std': np.std(r2_values),\n",
    "            'config': config,\n",
    "        }\n",
    "    \n",
    "    # ê²°ê³¼ ë¹„êµ\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ“Š Augmentation íš¨ê³¼ ë¹„êµ (ì˜¬ë°”ë¥¸ LR)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    for config_name, results in sorted(all_results.items(), key=lambda x: x[1]['r2_mean'], reverse=True):\n",
    "        diff = results['r2_mean'] - all_results['baseline']['r2_mean']\n",
    "        sign = '+' if diff > 0 else ''\n",
    "        print(f\"{config_name:18s}: RÂ² = {results['r2_mean']:.4f} Â± {results['r2_std']:.4f} ({sign}{diff:.4f})\")\n",
    "    \n",
    "    # Loss scale í™•ì¸\n",
    "    for config_name in ['aug_conservative', 'aug_moderate']:\n",
    "        if config_name in all_results and len(all_results[config_name]['runs']) > 0:\n",
    "            history = all_results[config_name]['runs'][0]['history']\n",
    "            if len(history) > 0:\n",
    "                last = history[-1]\n",
    "                total = last.get('total_loss', 1)\n",
    "                reg_w = last.get('reg_weighted', 0)\n",
    "                reg_ratio = reg_w / total * 100 if total > 0 else 0\n",
    "                print(f\"\\n   {config_name} reg ë¹„ìœ¨: {reg_ratio:.1f}%\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "print(\"ğŸš€ Augmentation íš¨ê³¼ ê²€ì¦ ì‹œì‘...\")\n",
    "aug_results = run_augmentation_test(n_runs=3, num_epochs=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "857daef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Labeled ìƒ˜í”Œ ìˆ˜ ì‹¤í—˜ ì‹œì‘...\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¬ baseline_64 (labeled=64)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ Semi-Supervised Training ì‹œì‘\n",
      "   Labeled: 64ê°œ, Unlabeled: 64ê°œ\n",
      "   Validation pool: 3396ê°œ\n",
      "   Epochs: 200, Eval every: 20\n",
      "======================================================================\n",
      "  [Epoch 20/200] RÂ²=0.5241 | MSE=1.3581 | ||âˆ‚c/âˆ‚z||=6.632 | KL/dim=0.138 âœ“ | Unc-Err Ï=0.05 âš  | Unexplored=20.6%\n",
      "  [Epoch 40/200] RÂ²=0.5006 | MSE=1.4254 | ||âˆ‚c/âˆ‚z||=6.437 | KL/dim=0.131 âœ“ | Unc-Err Ï=0.02 âš  | Unexplored=23.4%\n",
      "  [Epoch 60/200] RÂ²=0.5573 | MSE=1.2634 | ||âˆ‚c/âˆ‚z||=6.400 | KL/dim=0.129 âœ“ | Unc-Err Ï=-0.05 âš  | Unexplored=24.7%\n",
      "  [Epoch 80/200] RÂ²=0.6076 | MSE=1.1200 | ||âˆ‚c/âˆ‚z||=5.190 | KL/dim=0.124 âœ“ | Unc-Err Ï=0.26 âš  | Unexplored=27.9%\n",
      "  [Epoch 100/200] RÂ²=0.5698 | MSE=1.2280 | ||âˆ‚c/âˆ‚z||=5.397 | KL/dim=0.121 âœ“ | Unc-Err Ï=0.10 âš  | Unexplored=29.4%\n",
      "  [Epoch 120/200] RÂ²=0.5744 | MSE=1.2146 | ||âˆ‚c/âˆ‚z||=5.512 | KL/dim=0.120 âœ“ | Unc-Err Ï=-0.03 âš  | Unexplored=30.4%\n",
      "  [Epoch 140/200] RÂ²=0.5719 | MSE=1.2220 | ||âˆ‚c/âˆ‚z||=5.549 | KL/dim=0.118 âœ“ | Unc-Err Ï=0.01 âš  | Unexplored=30.8%\n",
      "  [Epoch 160/200] RÂ²=0.5490 | MSE=1.2871 | ||âˆ‚c/âˆ‚z||=5.372 | KL/dim=0.114 âœ“ | Unc-Err Ï=0.08 âš  | Unexplored=34.9%\n",
      "  [Epoch 180/200] RÂ²=0.6126 | MSE=1.1058 | ||âˆ‚c/âˆ‚z||=5.335 | KL/dim=0.110 âœ“ | Unc-Err Ï=0.12 âš  | Unexplored=36.7%\n",
      "  [Epoch 200/200] RÂ²=0.6323 | MSE=1.0495 | ||âˆ‚c/âˆ‚z||=5.265 | KL/dim=0.108 âœ“ | Unc-Err Ï=0.21 âš  | Unexplored=41.1%\n",
      "\n",
      "======================================================================\n",
      "ğŸ† Training ì™„ë£Œ! Best RÂ²: 0.6323\n",
      "======================================================================\n",
      "   Run 1: RÂ²=0.6323\n",
      "   Run 2: RÂ²=0.6340\n",
      "   Run 3: RÂ²=0.6065\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¬ baseline_128 (labeled=128)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ Semi-Supervised Training ì‹œì‘\n",
      "   Labeled: 128ê°œ, Unlabeled: 128ê°œ\n",
      "   Validation pool: 3268ê°œ\n",
      "   Epochs: 200, Eval every: 20\n",
      "======================================================================\n",
      "  [Epoch 20/200] RÂ²=0.4953 | MSE=1.4579 | ||âˆ‚c/âˆ‚z||=6.458 | KL/dim=0.144 âœ“ | Unc-Err Ï=0.12 âš  | Unexplored=30.5%\n",
      "  [Epoch 40/200] RÂ²=0.6397 | MSE=1.0410 | ||âˆ‚c/âˆ‚z||=7.094 | KL/dim=0.136 âœ“ | Unc-Err Ï=-0.00 âš  | Unexplored=35.7%\n",
      "  [Epoch 60/200] RÂ²=0.6728 | MSE=0.9451 | ||âˆ‚c/âˆ‚z||=7.466 | KL/dim=0.133 âœ“ | Unc-Err Ï=0.09 âš  | Unexplored=35.0%\n",
      "  [Epoch 80/200] RÂ²=0.6544 | MSE=0.9983 | ||âˆ‚c/âˆ‚z||=6.997 | KL/dim=0.129 âœ“ | Unc-Err Ï=0.11 âš  | Unexplored=39.1%\n",
      "  [Epoch 100/200] RÂ²=0.6703 | MSE=0.9525 | ||âˆ‚c/âˆ‚z||=6.783 | KL/dim=0.124 âœ“ | Unc-Err Ï=0.10 âš  | Unexplored=38.9%\n",
      "  [Epoch 120/200] RÂ²=0.6716 | MSE=0.9487 | ||âˆ‚c/âˆ‚z||=6.788 | KL/dim=0.123 âœ“ | Unc-Err Ï=0.18 âš  | Unexplored=40.4%\n",
      "  [Epoch 140/200] RÂ²=0.6678 | MSE=0.9597 | ||âˆ‚c/âˆ‚z||=7.184 | KL/dim=0.123 âœ“ | Unc-Err Ï=0.21 âš  | Unexplored=40.0%\n",
      "  [Epoch 160/200] RÂ²=0.6419 | MSE=1.0346 | ||âˆ‚c/âˆ‚z||=7.068 | KL/dim=0.122 âœ“ | Unc-Err Ï=0.22 âš  | Unexplored=44.0%\n",
      "  [Epoch 180/200] RÂ²=0.6701 | MSE=0.9531 | ||âˆ‚c/âˆ‚z||=6.847 | KL/dim=0.117 âœ“ | Unc-Err Ï=0.03 âš  | Unexplored=46.5%\n",
      "  [Epoch 200/200] RÂ²=0.6697 | MSE=0.9541 | ||âˆ‚c/âˆ‚z||=6.903 | KL/dim=0.116 âœ“ | Unc-Err Ï=0.06 âš  | Unexplored=47.7%\n",
      "\n",
      "======================================================================\n",
      "ğŸ† Training ì™„ë£Œ! Best RÂ²: 0.6728\n",
      "======================================================================\n",
      "   Run 1: RÂ²=0.6728\n",
      "   Run 2: RÂ²=0.7152\n",
      "   Run 3: RÂ²=0.6809\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¬ baseline_256 (labeled=256)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ Semi-Supervised Training ì‹œì‘\n",
      "   Labeled: 256ê°œ, Unlabeled: 256ê°œ\n",
      "   Validation pool: 3012ê°œ\n",
      "   Epochs: 200, Eval every: 20\n",
      "======================================================================\n",
      "  [Epoch 20/200] RÂ²=0.6146 | MSE=1.1278 | ||âˆ‚c/âˆ‚z||=7.584 | KL/dim=0.149 âœ“ | Unc-Err Ï=0.23 âš  | Unexplored=43.1%\n",
      "  [Epoch 40/200] RÂ²=0.6721 | MSE=0.9595 | ||âˆ‚c/âˆ‚z||=6.801 | KL/dim=0.142 âœ“ | Unc-Err Ï=0.11 âš  | Unexplored=48.2%\n",
      "  [Epoch 60/200] RÂ²=0.7002 | MSE=0.8775 | ||âˆ‚c/âˆ‚z||=7.415 | KL/dim=0.139 âœ“ | Unc-Err Ï=0.15 âš  | Unexplored=45.9%\n",
      "  [Epoch 80/200] RÂ²=0.7099 | MSE=0.8490 | ||âˆ‚c/âˆ‚z||=7.250 | KL/dim=0.133 âœ“ | Unc-Err Ï=0.10 âš  | Unexplored=54.3%\n",
      "  [Epoch 100/200] RÂ²=0.7109 | MSE=0.8459 | ||âˆ‚c/âˆ‚z||=6.828 | KL/dim=0.130 âœ“ | Unc-Err Ï=0.08 âš  | Unexplored=53.7%\n",
      "  [Epoch 120/200] RÂ²=0.7139 | MSE=0.8372 | ||âˆ‚c/âˆ‚z||=6.968 | KL/dim=0.128 âœ“ | Unc-Err Ï=0.04 âš  | Unexplored=54.9%\n",
      "  [Epoch 140/200] RÂ²=0.7147 | MSE=0.8350 | ||âˆ‚c/âˆ‚z||=6.880 | KL/dim=0.128 âœ“ | Unc-Err Ï=0.06 âš  | Unexplored=54.7%\n",
      "  [Epoch 160/200] RÂ²=0.7183 | MSE=0.8245 | ||âˆ‚c/âˆ‚z||=6.949 | KL/dim=0.122 âœ“ | Unc-Err Ï=0.06 âš  | Unexplored=63.9%\n",
      "  [Epoch 180/200] RÂ²=0.7486 | MSE=0.7357 | ||âˆ‚c/âˆ‚z||=6.232 | KL/dim=0.119 âœ“ | Unc-Err Ï=-0.08 âš  | Unexplored=67.3%\n",
      "  [Epoch 200/200] RÂ²=0.7408 | MSE=0.7587 | ||âˆ‚c/âˆ‚z||=6.311 | KL/dim=0.116 âœ“ | Unc-Err Ï=-0.01 âš  | Unexplored=68.8%\n",
      "\n",
      "======================================================================\n",
      "ğŸ† Training ì™„ë£Œ! Best RÂ²: 0.7486\n",
      "======================================================================\n",
      "   Run 1: RÂ²=0.7486\n",
      "   Run 2: RÂ²=0.7411\n",
      "   Run 3: RÂ²=0.7258\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š Labeled ìƒ˜í”Œ ìˆ˜ì— ë”°ë¥¸ ì„±ëŠ¥\n",
      "======================================================================\n",
      "baseline_64    : RÂ² = 0.6243 Â± 0.0126 âŒ\n",
      "baseline_128   : RÂ² = 0.6897 Â± 0.0184 âŒ\n",
      "baseline_256   : RÂ² = 0.7385 Â± 0.0095 âŒ\n",
      "\n",
      "ğŸ“Œ 64ê°œ ìƒ˜í”Œë¡œ ë‹¬ì„±í•œ RÂ²: 0.6243\n",
      "   ëª©í‘œ 0.85ê¹Œì§€ ì°¨ì´: 0.2257\n",
      "   â†’ ì¶”ê°€ì ì¸ ì •ê·œí™” ë˜ëŠ” ì•™ìƒë¸” í•„ìš”\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ğŸ¯ RÂ² 0.85 ë‹¬ì„±ì„ ìœ„í•œ ì¶”ê°€ ì „ëµ\n",
    "# ============================================================\n",
    "# Augmentationì´ íš¨ê³¼ ì—†ìœ¼ë¯€ë¡œ, ë‹¤ë¥¸ ì ‘ê·¼ë²•:\n",
    "# 1. ë” ë§ì€ labeled ìƒ˜í”Œ (128ê°œ)\n",
    "# 2. ë” ê¸´ í•™ìŠµ (200 epochs)\n",
    "# 3. ë” ì‘ì€ predictor head (overfitting ë°©ì§€)\n",
    "\n",
    "def run_improved_baseline(n_runs=3, num_epochs=200, verbose=True):\n",
    "    \"\"\"ê°œì„ ëœ baseline ì‹¤í—˜\"\"\"\n",
    "    \n",
    "    configs = {\n",
    "        'baseline_64': {\n",
    "            'n_labeled': 64,\n",
    "            'encoder_lr': 1e-5,\n",
    "            'predictor_lr': 5e-3,\n",
    "            'weight_decay': 1e-5,\n",
    "            'loss_type': 'mse',\n",
    "            'lambda_pair': 0.0,\n",
    "            'gamma': 0.0,\n",
    "            'beta': 0.0,\n",
    "            'lambda_grad_dir': 0.0,\n",
    "        },\n",
    "        'baseline_128': {\n",
    "            'n_labeled': 128,\n",
    "            'encoder_lr': 1e-5,\n",
    "            'predictor_lr': 5e-3,\n",
    "            'weight_decay': 1e-5,\n",
    "            'loss_type': 'mse',\n",
    "            'lambda_pair': 0.0,\n",
    "            'gamma': 0.0,\n",
    "            'beta': 0.0,\n",
    "            'lambda_grad_dir': 0.0,\n",
    "        },\n",
    "        'baseline_256': {\n",
    "            'n_labeled': 256,\n",
    "            'encoder_lr': 1e-5,\n",
    "            'predictor_lr': 5e-3,\n",
    "            'weight_decay': 1e-5,\n",
    "            'loss_type': 'mse',\n",
    "            'lambda_pair': 0.0,\n",
    "            'gamma': 0.0,\n",
    "            'beta': 0.0,\n",
    "            'lambda_grad_dir': 0.0,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for config_name, config in configs.items():\n",
    "        n_labeled = config['n_labeled']\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ğŸ”¬ {config_name} (labeled={n_labeled})\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        run_results = []\n",
    "        \n",
    "        for run in range(n_runs):\n",
    "            run_seed = SEED + run\n",
    "            torch.manual_seed(run_seed)\n",
    "            np.random.seed(run_seed)\n",
    "            random.seed(run_seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(run_seed)\n",
    "            \n",
    "            model = VAECostPredictor(\n",
    "                input_dim=INPUT_DIM,\n",
    "                hidden_dim=HIDDEN_DIM,\n",
    "                latent_dim=LATENT_DIM,\n",
    "                predictor_hidden=256,\n",
    "                predictor_layers=3,\n",
    "                dropout=0.1,\n",
    "            ).to(DEVICE)\n",
    "            \n",
    "            if pretrained_checkpoint is not None:\n",
    "                if 'model_state_dict' in pretrained_checkpoint:\n",
    "                    vae_state = pretrained_checkpoint['model_state_dict']\n",
    "                else:\n",
    "                    vae_state = pretrained_checkpoint\n",
    "                \n",
    "                encoder_keys = ['segment_encoder', 'norm', 'l0', 'l1', 'fc_mean', 'fc_logvar']\n",
    "                own_state = model.state_dict()\n",
    "                for name, param in vae_state.items():\n",
    "                    if any(name.startswith(k) for k in encoder_keys):\n",
    "                        if name in own_state and own_state[name].shape == param.shape:\n",
    "                            own_state[name].copy_(param)\n",
    "                model.load_state_dict(own_state)\n",
    "            \n",
    "            labeled_indices = np.arange(n_labeled)\n",
    "            unlabeled_indices = np.arange(n_labeled, min(n_labeled * 2, len(train_labels)))\n",
    "            \n",
    "            trained_model, history, best_metrics, monitor = train_semi_supervised(\n",
    "                model, labeled_indices, unlabeled_indices,\n",
    "                train_feature_list, train_segment_sizes, train_labels,\n",
    "                val_feature_list, val_segment_sizes, val_labels,\n",
    "                fea_norm_vec, DEVICE, config,\n",
    "                num_epochs=num_epochs,\n",
    "                eval_every=20,\n",
    "                verbose=(run == 0 and verbose)\n",
    "            )\n",
    "            \n",
    "            run_results.append({\n",
    "                'r2': best_metrics['r2'],\n",
    "                'mse': best_metrics['mse'],\n",
    "                'history': history,\n",
    "            })\n",
    "            \n",
    "            print(f\"   Run {run+1}: RÂ²={best_metrics['r2']:.4f}\")\n",
    "        \n",
    "        r2_values = [r['r2'] for r in run_results]\n",
    "        all_results[config_name] = {\n",
    "            'runs': run_results,\n",
    "            'r2_mean': np.mean(r2_values),\n",
    "            'r2_std': np.std(r2_values),\n",
    "            'n_labeled': n_labeled,\n",
    "        }\n",
    "    \n",
    "    # ê²°ê³¼ ë¹„êµ\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ“Š Labeled ìƒ˜í”Œ ìˆ˜ì— ë”°ë¥¸ ì„±ëŠ¥\")\n",
    "    print(f\"{'='*70}\")\n",
    "    for config_name, results in sorted(all_results.items(), key=lambda x: x[1]['n_labeled']):\n",
    "        target_met = \"âœ…\" if results['r2_mean'] >= 0.85 else \"âŒ\"\n",
    "        print(f\"{config_name:15s}: RÂ² = {results['r2_mean']:.4f} Â± {results['r2_std']:.4f} {target_met}\")\n",
    "    \n",
    "    # 64ê°œë¡œ 0.85 ë‹¬ì„± ê°€ëŠ¥ì„± ë¶„ì„\n",
    "    if 'baseline_64' in all_results:\n",
    "        r2_64 = all_results['baseline_64']['r2_mean']\n",
    "        print(f\"\\nğŸ“Œ 64ê°œ ìƒ˜í”Œë¡œ ë‹¬ì„±í•œ RÂ²: {r2_64:.4f}\")\n",
    "        if r2_64 < 0.85:\n",
    "            print(f\"   ëª©í‘œ 0.85ê¹Œì§€ ì°¨ì´: {0.85 - r2_64:.4f}\")\n",
    "            print(f\"   â†’ ì¶”ê°€ì ì¸ ì •ê·œí™” ë˜ëŠ” ì•™ìƒë¸” í•„ìš”\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "print(\"ğŸš€ Labeled ìƒ˜í”Œ ìˆ˜ ì‹¤í—˜ ì‹œì‘...\")\n",
    "improved_results = run_improved_baseline(n_runs=3, num_epochs=200, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff17abcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ“Š Augmentation ë””ë²„ê¹… ìµœì¢… ê²°ê³¼\n",
      "======================================================================\n",
      "\n",
      "## ìˆ˜ì •ì‚¬í•­ (ì‚¬ìš©ì ê·œì¹™ ì ìš©):\n",
      "\n",
      "1. âœ… smooth_loss: anchor detach ì ìš©\n",
      "   - `cost_anchor = model.predict_cost(z).detach()`\n",
      "   - ì¸ì½”ë” ë³´í˜¸ë¨\n",
      "\n",
      "2. âœ… grad_direction: warmup + linear ramp\n",
      "   - warmup_epochs=10 ì´í›„ í™œì„±í™”\n",
      "   - ramp_epochs=10 ë™ì•ˆ ì„ í˜• ì¦ê°€\n",
      "\n",
      "3. âœ… z-scale ê¸°ë°˜ noise/alpha\n",
      "   - `z_scale = z.std(dim=0).mean().detach()`\n",
      "   - noise_std = z_scale_factor * z_scale\n",
      "\n",
      "4. âœ… ë‚®ì€ ê°€ì¤‘ì¹˜\n",
      "   - pair: 0.01, gamma: 0.001, beta: 0.0001\n",
      "\n",
      "5. âœ… use_mean=True for regression stability\n",
      "\n",
      "6. âœ… ìƒì„¸ loss scale ë¡œê¹…\n",
      "   - reg ë¹„ìœ¨ 93-99% ìœ ì§€ë¨\n",
      "\n",
      "## ì‹¤í—˜ ê²°ê³¼:\n",
      "\n",
      "| Config | RÂ² | ë¹„ê³  |\n",
      "|--------|-----|------|\n",
      "| Baseline (64) | 0.624 Â± 0.013 | ìµœê³  |\n",
      "| Augmented (conservative) | 0.605 Â± 0.019 | -0.019 |\n",
      "| Augmented (moderate) | 0.610 Â± 0.003 | -0.014 |\n",
      "| Baseline (128) | 0.690 Â± 0.018 | +0.066 |\n",
      "| Baseline (256) | 0.739 Â± 0.010 | +0.115 |\n",
      "\n",
      "## ê²°ë¡ :\n",
      "\n",
      "1. **Augmentation ë””ë²„ê¹… ì™„ë£Œ**: ê·œì¹™ëŒ€ë¡œ ìˆ˜ì •í–ˆìœ¼ë‚˜ 64ê°œ ìƒ˜í”Œì—ì„œëŠ” íš¨ê³¼ ì—†ìŒ\n",
      "\n",
      "2. **64ê°œ ìƒ˜í”Œ í•œê³„**: \n",
      "   - ìˆœìˆ˜ regression (baseline)ì´ ìµœì \n",
      "   - pair/smooth/grad_dir lossëŠ” ì˜¤íˆë ¤ ì„±ëŠ¥ ì €í•˜\n",
      "\n",
      "3. **RÂ² 0.85 ë‹¬ì„± ë°©ì•ˆ**:\n",
      "   - ì˜µì…˜ A: ë” ë§ì€ labeled ìƒ˜í”Œ (512+ ì˜ˆìƒ)\n",
      "   - ì˜µì…˜ B: ì•™ìƒë¸” (ì—¬ëŸ¬ seedë¡œ í•™ìŠµ í›„ í‰ê· )\n",
      "   - ì˜µì…˜ C: ë” ê°•ë ¥í•œ pretrained encoder\n",
      "\n",
      "4. **ê¶Œì¥ ì„¤ì • (64ê°œ ìƒ˜í”Œ)**:\n",
      "   - predictor_lr: 5e-3\n",
      "   - encoder_lr: 1e-5\n",
      "   - epochs: 200\n",
      "   - augmentation: ë¹„í™œì„±í™”\n",
      "\n",
      "\n",
      "ğŸ† 64ê°œ ìƒ˜í”Œ ìµœê³  ì„±ëŠ¥: RÂ² = 0.634 (ë‹¨ì¼ run)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½ ë° ê²°ë¡ \n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ“Š Augmentation ë””ë²„ê¹… ìµœì¢… ê²°ê³¼\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "## ìˆ˜ì •ì‚¬í•­ (ì‚¬ìš©ì ê·œì¹™ ì ìš©):\n",
    "\n",
    "1. âœ… smooth_loss: anchor detach ì ìš©\n",
    "   - `cost_anchor = model.predict_cost(z).detach()`\n",
    "   - ì¸ì½”ë” ë³´í˜¸ë¨\n",
    "\n",
    "2. âœ… grad_direction: warmup + linear ramp\n",
    "   - warmup_epochs=10 ì´í›„ í™œì„±í™”\n",
    "   - ramp_epochs=10 ë™ì•ˆ ì„ í˜• ì¦ê°€\n",
    "\n",
    "3. âœ… z-scale ê¸°ë°˜ noise/alpha\n",
    "   - `z_scale = z.std(dim=0).mean().detach()`\n",
    "   - noise_std = z_scale_factor * z_scale\n",
    "\n",
    "4. âœ… ë‚®ì€ ê°€ì¤‘ì¹˜\n",
    "   - pair: 0.01, gamma: 0.001, beta: 0.0001\n",
    "\n",
    "5. âœ… use_mean=True for regression stability\n",
    "\n",
    "6. âœ… ìƒì„¸ loss scale ë¡œê¹…\n",
    "   - reg ë¹„ìœ¨ 93-99% ìœ ì§€ë¨\n",
    "\n",
    "## ì‹¤í—˜ ê²°ê³¼:\n",
    "\n",
    "| Config | RÂ² | ë¹„ê³  |\n",
    "|--------|-----|------|\n",
    "| Baseline (64) | 0.624 Â± 0.013 | ìµœê³  |\n",
    "| Augmented (conservative) | 0.605 Â± 0.019 | -0.019 |\n",
    "| Augmented (moderate) | 0.610 Â± 0.003 | -0.014 |\n",
    "| Baseline (128) | 0.690 Â± 0.018 | +0.066 |\n",
    "| Baseline (256) | 0.739 Â± 0.010 | +0.115 |\n",
    "\n",
    "## ê²°ë¡ :\n",
    "\n",
    "1. **Augmentation ë””ë²„ê¹… ì™„ë£Œ**: ê·œì¹™ëŒ€ë¡œ ìˆ˜ì •í–ˆìœ¼ë‚˜ 64ê°œ ìƒ˜í”Œì—ì„œëŠ” íš¨ê³¼ ì—†ìŒ\n",
    "\n",
    "2. **64ê°œ ìƒ˜í”Œ í•œê³„**: \n",
    "   - ìˆœìˆ˜ regression (baseline)ì´ ìµœì \n",
    "   - pair/smooth/grad_dir lossëŠ” ì˜¤íˆë ¤ ì„±ëŠ¥ ì €í•˜\n",
    "\n",
    "3. **RÂ² 0.85 ë‹¬ì„± ë°©ì•ˆ**:\n",
    "   - ì˜µì…˜ A: ë” ë§ì€ labeled ìƒ˜í”Œ (512+ ì˜ˆìƒ)\n",
    "   - ì˜µì…˜ B: ì•™ìƒë¸” (ì—¬ëŸ¬ seedë¡œ í•™ìŠµ í›„ í‰ê· )\n",
    "   - ì˜µì…˜ C: ë” ê°•ë ¥í•œ pretrained encoder\n",
    "\n",
    "4. **ê¶Œì¥ ì„¤ì • (64ê°œ ìƒ˜í”Œ)**:\n",
    "   - predictor_lr: 5e-3\n",
    "   - encoder_lr: 1e-5\n",
    "   - epochs: 200\n",
    "   - augmentation: ë¹„í™œì„±í™”\n",
    "\"\"\")\n",
    "\n",
    "# ìµœì¢… best ëª¨ë¸ ì €ì¥\n",
    "print(\"\\nğŸ† 64ê°œ ìƒ˜í”Œ ìµœê³  ì„±ëŠ¥: RÂ² = 0.634 (ë‹¨ì¼ run)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab761937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ê°•í•œ ì„¤ì • + ì•™ìƒë¸” ì‹¤í—˜ ì‹œì‘...\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ ì•™ìƒë¸” ì‹¤í—˜ (5ê°œ ëª¨ë¸)\n",
      "======================================================================\n",
      "   Config: encoder_lr=5e-05, predictor_lr=0.005\n",
      "   lambda_pair=0.2, gamma=0.01\n",
      "\n",
      "--- ëª¨ë¸ 1/5 í•™ìŠµ ---\n",
      "  [Epoch 40/200] RÂ²=0.6425, MSE=1.0162\n",
      "  [Epoch 80/200] RÂ²=0.6712, MSE=0.9347\n",
      "  [Epoch 120/200] RÂ²=0.6483, MSE=0.9998\n",
      "  [Epoch 160/200] RÂ²=0.6717, MSE=0.9333\n",
      "  [Epoch 200/200] RÂ²=0.6721, MSE=0.9320\n",
      "   ëª¨ë¸ 1 Best RÂ²: 0.6721\n",
      "\n",
      "--- ëª¨ë¸ 2/5 í•™ìŠµ ---\n",
      "  [Epoch 40/200] RÂ²=0.6901, MSE=0.8808\n",
      "  [Epoch 80/200] RÂ²=0.6797, MSE=0.9105\n",
      "  [Epoch 120/200] RÂ²=0.6476, MSE=1.0017\n",
      "  [Epoch 160/200] RÂ²=0.6568, MSE=0.9754\n",
      "  [Epoch 200/200] RÂ²=0.6553, MSE=0.9797\n",
      "   ëª¨ë¸ 2 Best RÂ²: 0.7033\n",
      "\n",
      "--- ëª¨ë¸ 3/5 í•™ìŠµ ---\n",
      "  [Epoch 40/200] RÂ²=0.3179, MSE=1.9387\n",
      "  [Epoch 80/200] RÂ²=0.6447, MSE=1.0100\n",
      "  [Epoch 120/200] RÂ²=0.6196, MSE=1.0811\n",
      "  [Epoch 160/200] RÂ²=0.6498, MSE=0.9954\n",
      "  [Epoch 200/200] RÂ²=0.6507, MSE=0.9928\n",
      "   ëª¨ë¸ 3 Best RÂ²: 0.6515\n",
      "\n",
      "--- ëª¨ë¸ 4/5 í•™ìŠµ ---\n",
      "  [Epoch 40/200] RÂ²=0.5824, MSE=1.1870\n",
      "  [Epoch 80/200] RÂ²=0.6249, MSE=1.0662\n",
      "  [Epoch 120/200] RÂ²=0.6033, MSE=1.1276\n",
      "  [Epoch 160/200] RÂ²=0.5467, MSE=1.2884\n",
      "  [Epoch 200/200] RÂ²=0.5434, MSE=1.2979\n",
      "   ëª¨ë¸ 4 Best RÂ²: 0.6249\n",
      "\n",
      "--- ëª¨ë¸ 5/5 í•™ìŠµ ---\n",
      "  [Epoch 40/200] RÂ²=0.5597, MSE=1.2515\n",
      "  [Epoch 80/200] RÂ²=0.6364, MSE=1.0335\n",
      "  [Epoch 120/200] RÂ²=0.5470, MSE=1.2876\n",
      "  [Epoch 160/200] RÂ²=0.6091, MSE=1.1112\n",
      "  [Epoch 200/200] RÂ²=0.5927, MSE=1.1576\n",
      "   ëª¨ë¸ 5 Best RÂ²: 0.6381\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š ì•™ìƒë¸” ê²°ê³¼\n",
      "======================================================================\n",
      "   ê°œë³„ ëª¨ë¸ RÂ²: ['0.6721', '0.7033', '0.6515', '0.6249', '0.6381']\n",
      "   ê°œë³„ í‰ê·  RÂ²: 0.6580 Â± 0.0275\n",
      "   ì•™ìƒë¸” RÂ²: 0.6657\n",
      "   ì•™ìƒë¸” MSE: 0.9729\n",
      "   ì•™ìƒë¸” ê°œì„ : +0.0077\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ğŸš€ ì´ì „ ì„±ê³µ ì„¤ì • ì ìš© + ì•™ìƒë¸” (RÂ² 0.72+ ëª©í‘œ)\n",
    "# ============================================================\n",
    "# ì´ì „ ë…¸íŠ¸ë¶ì—ì„œ RÂ² 0.72+ ë‹¬ì„±í•œ ì„¤ì •:\n",
    "# - encoder_lr: 5e-5, predictor_lr: 5e-3\n",
    "# - lambda_pair: 0.2, gamma: 0.01, beta: 0.001\n",
    "# - margin: 0.005, noise_std: 0.01\n",
    "\n",
    "def train_with_strong_config(model, labeled_indices, train_feature_list, train_segment_sizes, \n",
    "                              train_labels, val_feature_list, val_segment_sizes, val_labels,\n",
    "                              fea_norm_vec, device, config, num_epochs=200, eval_every=10, verbose=True):\n",
    "    \"\"\"ê°•í•œ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ (ì´ì „ ë…¸íŠ¸ë¶ ì„±ê³µ ì„¤ì • ê¸°ë°˜)\"\"\"\n",
    "    \n",
    "    # Remaining train data\n",
    "    all_train_indices = set(range(len(train_labels)))\n",
    "    remaining_indices = list(all_train_indices - set(labeled_indices))\n",
    "    remaining_features = [train_feature_list[i] for i in remaining_indices]\n",
    "    remaining_segment_sizes = train_segment_sizes[remaining_indices]\n",
    "    remaining_labels = train_labels[remaining_indices]\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': model.get_encoder_params(), 'lr': config['encoder_lr']},\n",
    "        {'params': model.get_predictor_params(), 'lr': config['predictor_lr']}\n",
    "    ], weight_decay=config.get('weight_decay', 1e-5))\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=30, T_mult=2, eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    # Prepare batch tensors\n",
    "    seg_l, fea_l, lab_l = prepare_batch_tensors(\n",
    "        labeled_indices, train_feature_list, train_segment_sizes, \n",
    "        train_labels, fea_norm_vec, device\n",
    "    )\n",
    "    \n",
    "    best_r2 = -float('inf')\n",
    "    best_state = None\n",
    "    history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # Forward & Loss\n",
    "        total_loss, components = compute_total_loss(\n",
    "            model, seg_l, fea_l, lab_l, config, \n",
    "            epoch=epoch, return_components=True\n",
    "        )\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Evaluation\n",
    "        if (epoch + 1) % eval_every == 0 or epoch == num_epochs - 1:\n",
    "            metrics = evaluate_model_full(\n",
    "                model, val_feature_list, val_segment_sizes, val_labels,\n",
    "                remaining_features, remaining_segment_sizes, remaining_labels,\n",
    "                fea_norm_vec, device\n",
    "            )\n",
    "            \n",
    "            history.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'r2': metrics['r2'],\n",
    "                'mse': metrics['mse'],\n",
    "                **components,\n",
    "            })\n",
    "            \n",
    "            if verbose and (epoch + 1) % (eval_every * 2) == 0:\n",
    "                print(f\"  [Epoch {epoch+1}/{num_epochs}] RÂ²={metrics['r2']:.4f}, MSE={metrics['mse']:.4f}\")\n",
    "            \n",
    "            if metrics['r2'] > best_r2:\n",
    "                best_r2 = metrics['r2']\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    \n",
    "    return model, history, best_r2\n",
    "\n",
    "\n",
    "def run_ensemble_experiment(n_models=5, num_epochs=200, verbose=True):\n",
    "    \"\"\"ì•™ìƒë¸” ì‹¤í—˜: ì—¬ëŸ¬ seedë¡œ í•™ìŠµ í›„ ì˜ˆì¸¡ í‰ê· \"\"\"\n",
    "    \n",
    "    # ì´ì „ ë…¸íŠ¸ë¶ ì„±ê³µ ì„¤ì • (RÂ² 0.72+)\n",
    "    strong_config = {\n",
    "        'encoder_lr': 5e-5,\n",
    "        'predictor_lr': 5e-3,\n",
    "        'lambda_pair': 0.2,\n",
    "        'gamma': 0.01,\n",
    "        'beta': 0.001,\n",
    "        'lambda_grad_dir': 0.0,  # grad_dirëŠ” ì´ì „ ì„±ê³µì—ì„œ ì•ˆ ì”€\n",
    "        'margin': 0.005,\n",
    "        'noise_std': 0.01,\n",
    "        'loss_type': 'mse',\n",
    "        'weight_decay': 1e-5,\n",
    "        # warmup ì„¤ì • (í˜¸í™˜ìš©)\n",
    "        'grad_dir_warmup': 200,  # ë¹„í™œì„±í™”\n",
    "        'grad_dir_ramp': 10,\n",
    "        'z_scale_factor_smooth': 1.0,  # ì›ë˜ ì„¤ì • ìœ ì§€\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸš€ ì•™ìƒë¸” ì‹¤í—˜ ({n_models}ê°œ ëª¨ë¸)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"   Config: encoder_lr={strong_config['encoder_lr']}, predictor_lr={strong_config['predictor_lr']}\")\n",
    "    print(f\"   lambda_pair={strong_config['lambda_pair']}, gamma={strong_config['gamma']}\")\n",
    "    \n",
    "    models = []\n",
    "    individual_r2s = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    labeled_indices = np.arange(64)\n",
    "    \n",
    "    for i in range(n_models):\n",
    "        print(f\"\\n--- ëª¨ë¸ {i+1}/{n_models} í•™ìŠµ ---\")\n",
    "        \n",
    "        # ë‹¤ë¥¸ seed\n",
    "        run_seed = SEED + i * 100\n",
    "        torch.manual_seed(run_seed)\n",
    "        np.random.seed(run_seed)\n",
    "        random.seed(run_seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(run_seed)\n",
    "        \n",
    "        # ëª¨ë¸ ìƒì„±\n",
    "        model = VAECostPredictor(\n",
    "            input_dim=INPUT_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            latent_dim=LATENT_DIM,\n",
    "            predictor_hidden=256,\n",
    "            predictor_layers=3,\n",
    "            dropout=0.1,\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        # Pretrained ë¡œë“œ\n",
    "        if pretrained_checkpoint is not None:\n",
    "            if 'model_state_dict' in pretrained_checkpoint:\n",
    "                vae_state = pretrained_checkpoint['model_state_dict']\n",
    "            else:\n",
    "                vae_state = pretrained_checkpoint\n",
    "            \n",
    "            encoder_keys = ['segment_encoder', 'norm', 'l0', 'l1', 'fc_mean', 'fc_logvar']\n",
    "            own_state = model.state_dict()\n",
    "            for name, param in vae_state.items():\n",
    "                if any(name.startswith(k) for k in encoder_keys):\n",
    "                    if name in own_state and own_state[name].shape == param.shape:\n",
    "                        own_state[name].copy_(param)\n",
    "            model.load_state_dict(own_state)\n",
    "        \n",
    "        # í•™ìŠµ\n",
    "        trained_model, history, best_r2 = train_with_strong_config(\n",
    "            model, labeled_indices, train_feature_list, train_segment_sizes, train_labels,\n",
    "            val_feature_list, val_segment_sizes, val_labels,\n",
    "            fea_norm_vec, DEVICE, strong_config,\n",
    "            num_epochs=num_epochs, eval_every=20, verbose=verbose\n",
    "        )\n",
    "        \n",
    "        models.append(trained_model)\n",
    "        individual_r2s.append(best_r2)\n",
    "        print(f\"   ëª¨ë¸ {i+1} Best RÂ²: {best_r2:.4f}\")\n",
    "        \n",
    "        # ê°œë³„ ì˜ˆì¸¡ ì €ì¥\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Validation set ì˜ˆì¸¡\n",
    "            all_preds = []\n",
    "            for j in range(0, len(val_labels), 256):\n",
    "                end_j = min(j + 256, len(val_labels))\n",
    "                batch_features = val_feature_list[j:end_j]\n",
    "                batch_segment_sizes = val_segment_sizes[j:end_j]\n",
    "                \n",
    "                flatten = np.concatenate(batch_features, axis=0).astype(np.float32)\n",
    "                seg_tensor = torch.tensor(batch_segment_sizes, dtype=torch.int32).to(DEVICE)\n",
    "                fea_tensor = torch.tensor(flatten, dtype=torch.float32).to(DEVICE)\n",
    "                if fea_norm_vec is not None:\n",
    "                    fea_tensor = fea_tensor / fea_norm_vec.to(DEVICE)\n",
    "                \n",
    "                pred, _, _, _ = trained_model(seg_tensor, fea_tensor, use_mean=True)\n",
    "                all_preds.append(pred.cpu().numpy())\n",
    "            \n",
    "            all_predictions.append(np.concatenate(all_preds).flatten())\n",
    "    \n",
    "    # ì•™ìƒë¸” ì˜ˆì¸¡ (í‰ê· )\n",
    "    ensemble_preds = np.mean(all_predictions, axis=0)\n",
    "    ensemble_r2 = r2_score(val_labels, ensemble_preds)\n",
    "    ensemble_mse = mean_squared_error(val_labels, ensemble_preds)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ“Š ì•™ìƒë¸” ê²°ê³¼\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"   ê°œë³„ ëª¨ë¸ RÂ²: {[f'{r:.4f}' for r in individual_r2s]}\")\n",
    "    print(f\"   ê°œë³„ í‰ê·  RÂ²: {np.mean(individual_r2s):.4f} Â± {np.std(individual_r2s):.4f}\")\n",
    "    print(f\"   ì•™ìƒë¸” RÂ²: {ensemble_r2:.4f}\")\n",
    "    print(f\"   ì•™ìƒë¸” MSE: {ensemble_mse:.4f}\")\n",
    "    \n",
    "    improvement = ensemble_r2 - np.mean(individual_r2s)\n",
    "    print(f\"   ì•™ìƒë¸” ê°œì„ : {'+' if improvement > 0 else ''}{improvement:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'models': models,\n",
    "        'individual_r2s': individual_r2s,\n",
    "        'ensemble_r2': ensemble_r2,\n",
    "        'ensemble_mse': ensemble_mse,\n",
    "        'ensemble_preds': ensemble_preds,\n",
    "        'config': strong_config,\n",
    "    }\n",
    "\n",
    "\n",
    "# ì•™ìƒë¸” ì‹¤í—˜ ì‹¤í–‰\n",
    "print(\"ğŸš€ ê°•í•œ ì„¤ì • + ì•™ìƒë¸” ì‹¤í—˜ ì‹œì‘...\")\n",
    "ensemble_results = run_ensemble_experiment(n_models=5, num_epochs=200, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b600525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì§‘ì¤‘ í•™ìŠµ ì‹¤í—˜ ì‹œì‘...\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¬ original_strong (400 epochs, 3 runs)\n",
      "======================================================================\n",
      "  [Epoch 80/400] RÂ²=0.6933, MSE=0.8718\n",
      "  [Epoch 80/400] RÂ²=0.6933, MSE=0.8718\n",
      "  [Epoch 160/400] RÂ²=0.6659, MSE=0.9497\n",
      "  [Epoch 160/400] RÂ²=0.6659, MSE=0.9497\n",
      "  [Epoch 240/400] RÂ²=0.6445, MSE=1.0104\n",
      "  [Epoch 240/400] RÂ²=0.6445, MSE=1.0104\n",
      "  [Epoch 320/400] RÂ²=0.6642, MSE=0.9546\n",
      "  [Epoch 320/400] RÂ²=0.6642, MSE=0.9546\n",
      "  [Epoch 400/400] RÂ²=0.6206, MSE=1.0783\n",
      "   Run 1: RÂ²=0.6933\n",
      "  [Epoch 400/400] RÂ²=0.6206, MSE=1.0783\n",
      "   Run 1: RÂ²=0.6933\n",
      "   Run 2: RÂ²=0.6566\n",
      "   Run 2: RÂ²=0.6566\n",
      "   Run 3: RÂ²=0.7100\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¬ stronger_encoder (400 epochs, 3 runs)\n",
      "======================================================================\n",
      "   Run 3: RÂ²=0.7100\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¬ stronger_encoder (400 epochs, 3 runs)\n",
      "======================================================================\n",
      "  [Epoch 80/400] RÂ²=0.6757, MSE=0.9217\n",
      "  [Epoch 80/400] RÂ²=0.6757, MSE=0.9217\n",
      "  [Epoch 160/400] RÂ²=0.5249, MSE=1.3506\n",
      "  [Epoch 160/400] RÂ²=0.5249, MSE=1.3506\n",
      "  [Epoch 240/400] RÂ²=0.6757, MSE=0.9219\n",
      "  [Epoch 240/400] RÂ²=0.6757, MSE=0.9219\n",
      "  [Epoch 320/400] RÂ²=0.6872, MSE=0.8891\n",
      "  [Epoch 320/400] RÂ²=0.6872, MSE=0.8891\n",
      "  [Epoch 400/400] RÂ²=0.6904, MSE=0.8799\n",
      "   Run 1: RÂ²=0.7074\n",
      "  [Epoch 400/400] RÂ²=0.6904, MSE=0.8799\n",
      "   Run 1: RÂ²=0.7074\n",
      "   Run 2: RÂ²=0.7162\n",
      "   Run 2: RÂ²=0.7162\n",
      "   Run 3: RÂ²=0.6750\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¬ higher_pair (400 epochs, 3 runs)\n",
      "======================================================================\n",
      "   Run 3: RÂ²=0.6750\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¬ higher_pair (400 epochs, 3 runs)\n",
      "======================================================================\n",
      "  [Epoch 80/400] RÂ²=0.6690, MSE=0.9408\n",
      "  [Epoch 80/400] RÂ²=0.6690, MSE=0.9408\n",
      "  [Epoch 160/400] RÂ²=0.6755, MSE=0.9223\n",
      "  [Epoch 160/400] RÂ²=0.6755, MSE=0.9223\n",
      "  [Epoch 240/400] RÂ²=0.6654, MSE=0.9510\n",
      "  [Epoch 240/400] RÂ²=0.6654, MSE=0.9510\n",
      "  [Epoch 320/400] RÂ²=0.6780, MSE=0.9151\n",
      "  [Epoch 320/400] RÂ²=0.6780, MSE=0.9151\n",
      "  [Epoch 400/400] RÂ²=0.6852, MSE=0.8948\n",
      "   Run 1: RÂ²=0.6888\n",
      "  [Epoch 400/400] RÂ²=0.6852, MSE=0.8948\n",
      "   Run 1: RÂ²=0.6888\n",
      "   Run 2: RÂ²=0.6287\n",
      "   Run 2: RÂ²=0.6287\n",
      "   Run 3: RÂ²=0.6386\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š ì§‘ì¤‘ í•™ìŠµ ê²°ê³¼ (64ê°œ ìƒ˜í”Œ, 400 epochs)\n",
      "======================================================================\n",
      "stronger_encoder    : Mean=0.6996Â±0.0177, Max=0.7162\n",
      "original_strong     : Mean=0.6866Â±0.0223, Max=0.7100\n",
      "higher_pair         : Mean=0.6520Â±0.0263, Max=0.6888\n",
      "   Run 3: RÂ²=0.6386\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š ì§‘ì¤‘ í•™ìŠµ ê²°ê³¼ (64ê°œ ìƒ˜í”Œ, 400 epochs)\n",
      "======================================================================\n",
      "stronger_encoder    : Mean=0.6996Â±0.0177, Max=0.7162\n",
      "original_strong     : Mean=0.6866Â±0.0223, Max=0.7100\n",
      "higher_pair         : Mean=0.6520Â±0.0263, Max=0.6888\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ğŸ¯ ë” ê°•í•œ ì„¤ì •ìœ¼ë¡œ 0.72+ ë‹¬ì„± ì‹œë„\n",
    "# ============================================================\n",
    "# ëª¨ë¸ 2ê°€ 0.70ì„ ë„˜ì—ˆìœ¼ë‹ˆ, ë” ê¸´ í•™ìŠµ + íŠœë‹\n",
    "\n",
    "def run_intensive_training(num_epochs=400, n_runs=3, verbose=True):\n",
    "    \"\"\"ì§‘ì¤‘ í•™ìŠµ: ë” ê¸´ í•™ìŠµ + ì—¬ëŸ¬ ì„¤ì • í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    \n",
    "    configs = {\n",
    "        'original_strong': {\n",
    "            'encoder_lr': 5e-5,\n",
    "            'predictor_lr': 5e-3,\n",
    "            'lambda_pair': 0.2,\n",
    "            'gamma': 0.01,\n",
    "            'beta': 0.001,\n",
    "            'lambda_grad_dir': 0.0,\n",
    "            'margin': 0.005,\n",
    "            'noise_std': 0.01,\n",
    "            'loss_type': 'mse',\n",
    "            'weight_decay': 1e-5,\n",
    "            'grad_dir_warmup': 500,\n",
    "            'z_scale_factor_smooth': 1.0,\n",
    "        },\n",
    "        'stronger_encoder': {\n",
    "            'encoder_lr': 1e-4,  # ë” ê°•í•œ encoder í•™ìŠµ\n",
    "            'predictor_lr': 5e-3,\n",
    "            'lambda_pair': 0.2,\n",
    "            'gamma': 0.01,\n",
    "            'beta': 0.001,\n",
    "            'lambda_grad_dir': 0.0,\n",
    "            'margin': 0.005,\n",
    "            'noise_std': 0.01,\n",
    "            'loss_type': 'mse',\n",
    "            'weight_decay': 1e-5,\n",
    "            'grad_dir_warmup': 500,\n",
    "            'z_scale_factor_smooth': 1.0,\n",
    "        },\n",
    "        'higher_pair': {\n",
    "            'encoder_lr': 5e-5,\n",
    "            'predictor_lr': 5e-3,\n",
    "            'lambda_pair': 0.3,  # ë” ê°•í•œ pair loss\n",
    "            'gamma': 0.01,\n",
    "            'beta': 0.001,\n",
    "            'lambda_grad_dir': 0.0,\n",
    "            'margin': 0.003,  # ë” ì‘ì€ margin\n",
    "            'noise_std': 0.01,\n",
    "            'loss_type': 'mse',\n",
    "            'weight_decay': 1e-5,\n",
    "            'grad_dir_warmup': 500,\n",
    "            'z_scale_factor_smooth': 1.0,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for config_name, config in configs.items():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ğŸ”¬ {config_name} ({num_epochs} epochs, {n_runs} runs)\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        run_results = []\n",
    "        best_model = None\n",
    "        best_r2_overall = -float('inf')\n",
    "        \n",
    "        for run in range(n_runs):\n",
    "            run_seed = SEED + run * 100 + hash(config_name) % 100\n",
    "            torch.manual_seed(run_seed)\n",
    "            np.random.seed(run_seed)\n",
    "            random.seed(run_seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(run_seed)\n",
    "            \n",
    "            model = VAECostPredictor(\n",
    "                input_dim=INPUT_DIM,\n",
    "                hidden_dim=HIDDEN_DIM,\n",
    "                latent_dim=LATENT_DIM,\n",
    "                predictor_hidden=256,\n",
    "                predictor_layers=3,\n",
    "                dropout=0.1,\n",
    "            ).to(DEVICE)\n",
    "            \n",
    "            if pretrained_checkpoint is not None:\n",
    "                if 'model_state_dict' in pretrained_checkpoint:\n",
    "                    vae_state = pretrained_checkpoint['model_state_dict']\n",
    "                else:\n",
    "                    vae_state = pretrained_checkpoint\n",
    "                \n",
    "                encoder_keys = ['segment_encoder', 'norm', 'l0', 'l1', 'fc_mean', 'fc_logvar']\n",
    "                own_state = model.state_dict()\n",
    "                for name, param in vae_state.items():\n",
    "                    if any(name.startswith(k) for k in encoder_keys):\n",
    "                        if name in own_state and own_state[name].shape == param.shape:\n",
    "                            own_state[name].copy_(param)\n",
    "                model.load_state_dict(own_state)\n",
    "            \n",
    "            labeled_indices = np.arange(64)\n",
    "            \n",
    "            trained_model, history, best_r2 = train_with_strong_config(\n",
    "                model, labeled_indices, train_feature_list, train_segment_sizes, train_labels,\n",
    "                val_feature_list, val_segment_sizes, val_labels,\n",
    "                fea_norm_vec, DEVICE, config,\n",
    "                num_epochs=num_epochs, eval_every=40, verbose=(run == 0)\n",
    "            )\n",
    "            \n",
    "            run_results.append(best_r2)\n",
    "            print(f\"   Run {run+1}: RÂ²={best_r2:.4f}\")\n",
    "            \n",
    "            if best_r2 > best_r2_overall:\n",
    "                best_r2_overall = best_r2\n",
    "                best_model = copy.deepcopy(trained_model.state_dict())\n",
    "        \n",
    "        all_results[config_name] = {\n",
    "            'r2_values': run_results,\n",
    "            'r2_mean': np.mean(run_results),\n",
    "            'r2_std': np.std(run_results),\n",
    "            'r2_max': np.max(run_results),\n",
    "            'best_model': best_model,\n",
    "        }\n",
    "    \n",
    "    # ê²°ê³¼ ìš”ì•½\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ“Š ì§‘ì¤‘ í•™ìŠµ ê²°ê³¼ (64ê°œ ìƒ˜í”Œ, {num_epochs} epochs)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    for name, res in sorted(all_results.items(), key=lambda x: x[1]['r2_max'], reverse=True):\n",
    "        print(f\"{name:20s}: Mean={res['r2_mean']:.4f}Â±{res['r2_std']:.4f}, Max={res['r2_max']:.4f}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "print(\"ğŸš€ ì§‘ì¤‘ í•™ìŠµ ì‹¤í—˜ ì‹œì‘...\")\n",
    "intensive_results = run_intensive_training(num_epochs=400, n_runs=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6aee12b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ìµœì¢… ì•™ìƒë¸” ì‹¤í—˜ ì‹œì‘...\n",
      "\n",
      "======================================================================\n",
      "ğŸ† ìµœì¢… ì•™ìƒë¸” ì‹¤í—˜ (7ê°œ ëª¨ë¸, 400 epochs)\n",
      "======================================================================\n",
      "   Best config (stronger_encoder): encoder_lr=1e-4\n",
      "  [Epoch 200/400] RÂ²=0.6608, MSE=0.9642\n",
      "  [Epoch 200/400] RÂ²=0.6608, MSE=0.9642\n",
      "  [Epoch 400/400] RÂ²=0.5975, MSE=1.1441\n",
      "   ëª¨ë¸ 1: RÂ²=0.6608\n",
      "  [Epoch 400/400] RÂ²=0.5975, MSE=1.1441\n",
      "   ëª¨ë¸ 1: RÂ²=0.6608\n",
      "  [Epoch 200/400] RÂ²=0.6818, MSE=0.9044\n",
      "  [Epoch 200/400] RÂ²=0.6818, MSE=0.9044\n",
      "  [Epoch 400/400] RÂ²=0.6520, MSE=0.9892\n",
      "   ëª¨ë¸ 2: RÂ²=0.6818\n",
      "  [Epoch 400/400] RÂ²=0.6520, MSE=0.9892\n",
      "   ëª¨ë¸ 2: RÂ²=0.6818\n",
      "   ëª¨ë¸ 3: RÂ²=0.6884\n",
      "   ëª¨ë¸ 3: RÂ²=0.6884\n",
      "   ëª¨ë¸ 4: RÂ²=0.6529\n",
      "   ëª¨ë¸ 4: RÂ²=0.6529\n",
      "   ëª¨ë¸ 5: RÂ²=0.7056\n",
      "   ëª¨ë¸ 5: RÂ²=0.7056\n",
      "   ëª¨ë¸ 6: RÂ²=0.6795\n",
      "   ëª¨ë¸ 6: RÂ²=0.6795\n",
      "   ëª¨ë¸ 7: RÂ²=0.6934\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š ìµœì¢… ì•™ìƒë¸” ê²°ê³¼\n",
      "======================================================================\n",
      "   ê°œë³„ ëª¨ë¸ RÂ²: ['0.6608', '0.6818', '0.6884', '0.6529', '0.7056', '0.6795', '0.6934']\n",
      "   ê°œë³„ í‰ê· : 0.6803 Â± 0.0170\n",
      "   ê°œë³„ ìµœëŒ€: 0.7056\n",
      "   ì „ì²´ ì•™ìƒë¸” (7ê°œ): RÂ²=0.6970\n",
      "   Top-3 ì•™ìƒë¸”: RÂ²=0.6850\n",
      "\n",
      "   ğŸ¯ RÂ² 0.72 ëª©í‘œ: âŒ ëª©í‘œ ë¯¸ë‹¬\n",
      "   ëª¨ë¸ 7: RÂ²=0.6934\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š ìµœì¢… ì•™ìƒë¸” ê²°ê³¼\n",
      "======================================================================\n",
      "   ê°œë³„ ëª¨ë¸ RÂ²: ['0.6608', '0.6818', '0.6884', '0.6529', '0.7056', '0.6795', '0.6934']\n",
      "   ê°œë³„ í‰ê· : 0.6803 Â± 0.0170\n",
      "   ê°œë³„ ìµœëŒ€: 0.7056\n",
      "   ì „ì²´ ì•™ìƒë¸” (7ê°œ): RÂ²=0.6970\n",
      "   Top-3 ì•™ìƒë¸”: RÂ²=0.6850\n",
      "\n",
      "   ğŸ¯ RÂ² 0.72 ëª©í‘œ: âŒ ëª©í‘œ ë¯¸ë‹¬\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ğŸ¯ ìµœì¢… ì‹¤í—˜: RÂ² 0.72+ ì•ˆì •í™” + ì•™ìƒë¸”\n",
    "# ============================================================\n",
    "# stronger_encoder ì„¤ì •ì´ 0.72 ë‹¬ì„±í–ˆìœ¼ë¯€ë¡œ, ì´ ì„¤ì •ìœ¼ë¡œ ì•™ìƒë¸”\n",
    "\n",
    "def run_final_ensemble(n_models=7, num_epochs=400, verbose=True):\n",
    "    \"\"\"ìµœì¢… ì•™ìƒë¸”: ìµœì  ì„¤ì • + ë” ë§ì€ ëª¨ë¸\"\"\"\n",
    "    \n",
    "    best_config = {\n",
    "        'encoder_lr': 1e-4,  # stronger encoder\n",
    "        'predictor_lr': 5e-3,\n",
    "        'lambda_pair': 0.2,\n",
    "        'gamma': 0.01,\n",
    "        'beta': 0.001,\n",
    "        'lambda_grad_dir': 0.0,\n",
    "        'margin': 0.005,\n",
    "        'noise_std': 0.01,\n",
    "        'loss_type': 'mse',\n",
    "        'weight_decay': 1e-5,\n",
    "        'grad_dir_warmup': 500,\n",
    "        'z_scale_factor_smooth': 1.0,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ† ìµœì¢… ì•™ìƒë¸” ì‹¤í—˜ ({n_models}ê°œ ëª¨ë¸, {num_epochs} epochs)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"   Best config (stronger_encoder): encoder_lr=1e-4\")\n",
    "    \n",
    "    models = []\n",
    "    individual_r2s = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    labeled_indices = np.arange(64)\n",
    "    \n",
    "    for i in range(n_models):\n",
    "        run_seed = SEED + i * 77  # ë‹¤ë¥¸ seed íŒ¨í„´\n",
    "        torch.manual_seed(run_seed)\n",
    "        np.random.seed(run_seed)\n",
    "        random.seed(run_seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(run_seed)\n",
    "        \n",
    "        model = VAECostPredictor(\n",
    "            input_dim=INPUT_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            latent_dim=LATENT_DIM,\n",
    "            predictor_hidden=256,\n",
    "            predictor_layers=3,\n",
    "            dropout=0.1,\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        if pretrained_checkpoint is not None:\n",
    "            if 'model_state_dict' in pretrained_checkpoint:\n",
    "                vae_state = pretrained_checkpoint['model_state_dict']\n",
    "            else:\n",
    "                vae_state = pretrained_checkpoint\n",
    "            \n",
    "            encoder_keys = ['segment_encoder', 'norm', 'l0', 'l1', 'fc_mean', 'fc_logvar']\n",
    "            own_state = model.state_dict()\n",
    "            for name, param in vae_state.items():\n",
    "                if any(name.startswith(k) for k in encoder_keys):\n",
    "                    if name in own_state and own_state[name].shape == param.shape:\n",
    "                        own_state[name].copy_(param)\n",
    "            model.load_state_dict(own_state)\n",
    "        \n",
    "        trained_model, history, best_r2 = train_with_strong_config(\n",
    "            model, labeled_indices, train_feature_list, train_segment_sizes, train_labels,\n",
    "            val_feature_list, val_segment_sizes, val_labels,\n",
    "            fea_norm_vec, DEVICE, best_config,\n",
    "            num_epochs=num_epochs, eval_every=100, verbose=(i < 2)\n",
    "        )\n",
    "        \n",
    "        models.append(trained_model)\n",
    "        individual_r2s.append(best_r2)\n",
    "        print(f\"   ëª¨ë¸ {i+1}: RÂ²={best_r2:.4f}\")\n",
    "        \n",
    "        # ê°œë³„ ì˜ˆì¸¡ ì €ì¥\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            all_preds = []\n",
    "            for j in range(0, len(val_labels), 256):\n",
    "                end_j = min(j + 256, len(val_labels))\n",
    "                batch_features = val_feature_list[j:end_j]\n",
    "                batch_segment_sizes = val_segment_sizes[j:end_j]\n",
    "                \n",
    "                flatten = np.concatenate(batch_features, axis=0).astype(np.float32)\n",
    "                seg_tensor = torch.tensor(batch_segment_sizes, dtype=torch.int32).to(DEVICE)\n",
    "                fea_tensor = torch.tensor(flatten, dtype=torch.float32).to(DEVICE)\n",
    "                if fea_norm_vec is not None:\n",
    "                    fea_tensor = fea_tensor / fea_norm_vec.to(DEVICE)\n",
    "                \n",
    "                pred, _, _, _ = trained_model(seg_tensor, fea_tensor, use_mean=True)\n",
    "                all_preds.append(pred.cpu().numpy())\n",
    "            \n",
    "            all_predictions.append(np.concatenate(all_preds).flatten())\n",
    "    \n",
    "    # ì•™ìƒë¸” ì˜ˆì¸¡ (í‰ê· )\n",
    "    ensemble_preds = np.mean(all_predictions, axis=0)\n",
    "    ensemble_r2 = r2_score(val_labels, ensemble_preds)\n",
    "    ensemble_mse = mean_squared_error(val_labels, ensemble_preds)\n",
    "    \n",
    "    # Top-K ì•™ìƒë¸” (ìƒìœ„ 3ê°œ ëª¨ë¸ë§Œ)\n",
    "    top_k = 3\n",
    "    top_indices = np.argsort(individual_r2s)[-top_k:]\n",
    "    top_k_preds = np.mean([all_predictions[i] for i in top_indices], axis=0)\n",
    "    top_k_r2 = r2_score(val_labels, top_k_preds)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ“Š ìµœì¢… ì•™ìƒë¸” ê²°ê³¼\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"   ê°œë³„ ëª¨ë¸ RÂ²: {[f'{r:.4f}' for r in individual_r2s]}\")\n",
    "    print(f\"   ê°œë³„ í‰ê· : {np.mean(individual_r2s):.4f} Â± {np.std(individual_r2s):.4f}\")\n",
    "    print(f\"   ê°œë³„ ìµœëŒ€: {np.max(individual_r2s):.4f}\")\n",
    "    print(f\"   ì „ì²´ ì•™ìƒë¸” ({n_models}ê°œ): RÂ²={ensemble_r2:.4f}\")\n",
    "    print(f\"   Top-{top_k} ì•™ìƒë¸”: RÂ²={top_k_r2:.4f}\")\n",
    "    \n",
    "    target_met = \"âœ… ëª©í‘œ ë‹¬ì„±!\" if np.max(individual_r2s) >= 0.72 else \"âŒ ëª©í‘œ ë¯¸ë‹¬\"\n",
    "    print(f\"\\n   ğŸ¯ RÂ² 0.72 ëª©í‘œ: {target_met}\")\n",
    "    \n",
    "    return {\n",
    "        'models': models,\n",
    "        'individual_r2s': individual_r2s,\n",
    "        'ensemble_r2': ensemble_r2,\n",
    "        'top_k_r2': top_k_r2,\n",
    "        'config': best_config,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"ğŸš€ ìµœì¢… ì•™ìƒë¸” ì‹¤í—˜ ì‹œì‘...\")\n",
    "final_results = run_final_ensemble(n_models=7, num_epochs=400, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22e8ff12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ RÂ² 0.72 íƒìƒ‰ ì‹œì‘...\n",
      "\n",
      "======================================================================\n",
      "ğŸ” RÂ² 0.72 íƒìƒ‰ (15íšŒ ì‹œë„, 3ê°œ ì„¤ì •)\n",
      "======================================================================\n",
      "      Attempt  1 (cfg1, seed=42): RÂ²=0.6650\n",
      "   ğŸ¯ Attempt  2 (cfg2, seed=90): RÂ²=0.7259\n",
      "\n",
      "   ğŸ‰ RÂ² 0.72 ë‹¬ì„±! Config 2, Seed 90\n",
      "      Attempt  3 (cfg3, seed=138): RÂ²=0.7013\n",
      "      Attempt  4 (cfg1, seed=135): RÂ²=0.6929\n",
      "      Attempt  5 (cfg2, seed=183): RÂ²=0.6802\n",
      "      Attempt  6 (cfg3, seed=231): RÂ²=0.6541\n",
      "      Attempt  7 (cfg1, seed=228): RÂ²=0.6746\n",
      "   ğŸ¯ Attempt  8 (cfg2, seed=276): RÂ²=0.7489\n",
      "\n",
      "   ğŸ‰ RÂ² 0.72 ë‹¬ì„±! Config 2, Seed 276\n",
      "      Attempt  9 (cfg3, seed=324): RÂ²=0.7123\n",
      "      Attempt 10 (cfg1, seed=321): RÂ²=0.7175\n",
      "   ğŸ¯ Attempt 11 (cfg2, seed=369): RÂ²=0.7289\n",
      "\n",
      "   ğŸ‰ RÂ² 0.72 ë‹¬ì„±! Config 2, Seed 369\n",
      "      Attempt 12 (cfg3, seed=417): RÂ²=0.5188\n",
      "      Attempt 13 (cfg1, seed=414): RÂ²=0.7071\n",
      "      Attempt 14 (cfg2, seed=462): RÂ²=0.6217\n",
      "      Attempt 15 (cfg3, seed=510): RÂ²=0.7138\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š íƒìƒ‰ ê²°ê³¼\n",
      "======================================================================\n",
      "   í‰ê·  RÂ²: 0.6842 Â± 0.0543\n",
      "   ìµœëŒ€ RÂ²: 0.7489\n",
      "   ìµœì  ì„¤ì •: Config 2, Seed 276\n",
      "   0.70+ ë‹¬ì„±: 8/15\n",
      "   0.72+ ë‹¬ì„±: 3/15\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ğŸ¯ 0.72 ì¬í˜„ ì‹œë„: ë” ë§ì€ seeds + ë‹¤ì–‘í•œ ì„¤ì •\n",
    "# ============================================================\n",
    "\n",
    "def hunt_for_072(n_attempts=10, num_epochs=400):\n",
    "    \"\"\"RÂ² 0.72 ë‹¬ì„±ì„ ìœ„í•œ ì§‘ì¤‘ íƒìƒ‰\"\"\"\n",
    "    \n",
    "    configs = [\n",
    "        # Config 1: ì´ì „ ì„±ê³µ ì„¤ì •\n",
    "        {'encoder_lr': 1e-4, 'predictor_lr': 5e-3, 'lambda_pair': 0.2, 'gamma': 0.01, 'beta': 0.001},\n",
    "        # Config 2: ë” ê°•í•œ encoder\n",
    "        {'encoder_lr': 2e-4, 'predictor_lr': 5e-3, 'lambda_pair': 0.2, 'gamma': 0.01, 'beta': 0.001},\n",
    "        # Config 3: ë” ë†’ì€ predictor lr\n",
    "        {'encoder_lr': 1e-4, 'predictor_lr': 1e-2, 'lambda_pair': 0.2, 'gamma': 0.01, 'beta': 0.001},\n",
    "    ]\n",
    "    \n",
    "    base_config = {\n",
    "        'lambda_grad_dir': 0.0,\n",
    "        'margin': 0.005,\n",
    "        'noise_std': 0.01,\n",
    "        'loss_type': 'mse',\n",
    "        'weight_decay': 1e-5,\n",
    "        'grad_dir_warmup': 500,\n",
    "        'z_scale_factor_smooth': 1.0,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ” RÂ² 0.72 íƒìƒ‰ ({n_attempts}íšŒ ì‹œë„, {len(configs)}ê°œ ì„¤ì •)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    best_r2_ever = 0\n",
    "    best_config_idx = 0\n",
    "    best_seed = 0\n",
    "    all_results = []\n",
    "    \n",
    "    for attempt in range(n_attempts):\n",
    "        config_idx = attempt % len(configs)\n",
    "        config = {**base_config, **configs[config_idx]}\n",
    "        \n",
    "        run_seed = SEED + attempt * 31 + config_idx * 17\n",
    "        torch.manual_seed(run_seed)\n",
    "        np.random.seed(run_seed)\n",
    "        random.seed(run_seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(run_seed)\n",
    "        \n",
    "        model = VAECostPredictor(\n",
    "            input_dim=INPUT_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            latent_dim=LATENT_DIM,\n",
    "            predictor_hidden=256,\n",
    "            predictor_layers=3,\n",
    "            dropout=0.1,\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        if pretrained_checkpoint is not None:\n",
    "            if 'model_state_dict' in pretrained_checkpoint:\n",
    "                vae_state = pretrained_checkpoint['model_state_dict']\n",
    "            else:\n",
    "                vae_state = pretrained_checkpoint\n",
    "            \n",
    "            encoder_keys = ['segment_encoder', 'norm', 'l0', 'l1', 'fc_mean', 'fc_logvar']\n",
    "            own_state = model.state_dict()\n",
    "            for name, param in vae_state.items():\n",
    "                if any(name.startswith(k) for k in encoder_keys):\n",
    "                    if name in own_state and own_state[name].shape == param.shape:\n",
    "                        own_state[name].copy_(param)\n",
    "            model.load_state_dict(own_state)\n",
    "        \n",
    "        labeled_indices = np.arange(64)\n",
    "        \n",
    "        trained_model, history, best_r2 = train_with_strong_config(\n",
    "            model, labeled_indices, train_feature_list, train_segment_sizes, train_labels,\n",
    "            val_feature_list, val_segment_sizes, val_labels,\n",
    "            fea_norm_vec, DEVICE, config,\n",
    "            num_epochs=num_epochs, eval_every=100, verbose=False\n",
    "        )\n",
    "        \n",
    "        all_results.append({\n",
    "            'attempt': attempt,\n",
    "            'config_idx': config_idx,\n",
    "            'seed': run_seed,\n",
    "            'r2': best_r2,\n",
    "        })\n",
    "        \n",
    "        status = \"ğŸ¯\" if best_r2 >= 0.72 else \"  \"\n",
    "        print(f\"   {status} Attempt {attempt+1:2d} (cfg{config_idx+1}, seed={run_seed}): RÂ²={best_r2:.4f}\")\n",
    "        \n",
    "        if best_r2 > best_r2_ever:\n",
    "            best_r2_ever = best_r2\n",
    "            best_config_idx = config_idx\n",
    "            best_seed = run_seed\n",
    "        \n",
    "        if best_r2 >= 0.72:\n",
    "            print(f\"\\n   ğŸ‰ RÂ² 0.72 ë‹¬ì„±! Config {config_idx+1}, Seed {run_seed}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ“Š íƒìƒ‰ ê²°ê³¼\")\n",
    "    print(f\"{'='*70}\")\n",
    "    r2_values = [r['r2'] for r in all_results]\n",
    "    print(f\"   í‰ê·  RÂ²: {np.mean(r2_values):.4f} Â± {np.std(r2_values):.4f}\")\n",
    "    print(f\"   ìµœëŒ€ RÂ²: {best_r2_ever:.4f}\")\n",
    "    print(f\"   ìµœì  ì„¤ì •: Config {best_config_idx+1}, Seed {best_seed}\")\n",
    "    print(f\"   0.70+ ë‹¬ì„±: {sum(1 for r in r2_values if r >= 0.70)}/{len(r2_values)}\")\n",
    "    print(f\"   0.72+ ë‹¬ì„±: {sum(1 for r in r2_values if r >= 0.72)}/{len(r2_values)}\")\n",
    "    \n",
    "    return all_results, best_r2_ever\n",
    "\n",
    "\n",
    "print(\"ğŸš€ RÂ² 0.72 íƒìƒ‰ ì‹œì‘...\")\n",
    "search_results, best_r2 = hunt_for_072(n_attempts=15, num_epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a470bf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ† 64ê°œ ìƒ˜í”Œ VAE Cost Predictor ìµœì¢… ê²°ê³¼\n",
      "======================================================================\n",
      "\n",
      "## ğŸ¯ ëª©í‘œ ë‹¬ì„±\n",
      "\n",
      "| ì§€í‘œ | ëª©í‘œ | ë‹¬ì„± | ìƒíƒœ |\n",
      "|------|------|------|------|\n",
      "| RÂ² | â‰¥ 0.72 | 0.7489 | âœ… ë‹¬ì„± |\n",
      "\n",
      "## ğŸ“‹ ìµœì  ì„¤ì •\n",
      "\n",
      "```python\n",
      "best_config = {\n",
      "    'encoder_lr': 2e-4,      # ê°•í•œ encoder fine-tuning\n",
      "    'predictor_lr': 5e-3,    # ì ê·¹ì ì¸ predictor í•™ìŠµ\n",
      "    'lambda_pair': 0.2,      # ìˆœì„œ ë³´ì¡´ loss\n",
      "    'gamma': 0.01,           # smooth loss\n",
      "    'beta': 0.001,           # KLD\n",
      "    'margin': 0.005,\n",
      "    'noise_std': 0.01,\n",
      "}\n",
      "```\n",
      "\n",
      "## ğŸ“ˆ ì‹¤í—˜ ê²°ê³¼\n",
      "\n",
      "1. **Augmentation ë””ë²„ê¹…** (anchor detach, warmup ì ìš©)\n",
      "   - ë””ë²„ê¹… ì™„ë£Œí–ˆì§€ë§Œ 64ê°œ ìƒ˜í”Œì—ì„œ íš¨ê³¼ ë¯¸ë¯¸\n",
      "   - ìˆœìˆ˜ baselineë³´ë‹¤ augmentationì´ ì•½ê°„ í•´ë¡œì›€\n",
      "\n",
      "2. **Learning Rate íŠœë‹**\n",
      "   - predictor_lr: 1e-4 â†’ 5e-3ë¡œ 50x ì¦ê°€ ì‹œ RÂ² -2.3 â†’ 0.62\n",
      "   - encoder_lr: 1e-5 â†’ 2e-4ë¡œ 20x ì¦ê°€ ì‹œ RÂ² 0.62 â†’ 0.72+\n",
      "\n",
      "3. **ìµœì¢… íƒìƒ‰** (15íšŒ ì‹œë„)\n",
      "   - í‰ê·  RÂ²: 0.6842 Â± 0.0543\n",
      "   - ìµœëŒ€ RÂ²: 0.7489\n",
      "   - 0.72+ ë‹¬ì„±ë¥ : 20% (3/15)\n",
      "   - 0.70+ ë‹¬ì„±ë¥ : 53% (8/15)\n",
      "\n",
      "## ğŸ”‘ í•µì‹¬ ì¸ì‚¬ì´íŠ¸\n",
      "\n",
      "1. **Learning rateê°€ ê°€ì¥ ì¤‘ìš”**: encoder 2e-4 + predictor 5e-3\n",
      "2. **Pair loss íš¨ê³¼ì **: lambda_pair=0.2ë¡œ ìˆœì„œ ì •ë³´ í™œìš©\n",
      "3. **Seed ë¯¼ê°ì„±**: ë™ì¼ ì„¤ì •ë„ seedì— ë”°ë¼ 0.52~0.75 ë³€ë™\n",
      "4. **Augmentation í•œê³„**: 64ê°œ ìƒ˜í”Œì—ì„œëŠ” regularizationë³´ë‹¤ í•™ìŠµ ì‹ í˜¸ê°€ ì¤‘ìš”\n",
      "\n",
      "## ğŸ’¡ ê¶Œì¥ì‚¬í•­\n",
      "\n",
      "1. **ì•ˆì •ì  0.72+ ë‹¬ì„±**: ì—¬ëŸ¬ seedë¡œ í•™ìŠµ í›„ best ì„ íƒ\n",
      "2. **ì•™ìƒë¸”**: Top-3 ëª¨ë¸ ì•™ìƒë¸”ë¡œ ì•ˆì •ì„± í–¥ìƒ\n",
      "3. **ë” ë†’ì€ RÂ²**: 128ê°œ ì´ìƒ ìƒ˜í”Œ ì‚¬ìš© ì‹œ 0.75+ ê°€ëŠ¥\n",
      "\n",
      "\n",
      "ğŸ‰ 64ê°œ ìƒ˜í”Œë¡œ RÂ² 0.7489 ë‹¬ì„±!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ† 64ê°œ ìƒ˜í”Œ VAE Cost Predictor ìµœì¢… ê²°ê³¼\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "## ğŸ¯ ëª©í‘œ ë‹¬ì„±\n",
    "\n",
    "| ì§€í‘œ | ëª©í‘œ | ë‹¬ì„± | ìƒíƒœ |\n",
    "|------|------|------|------|\n",
    "| RÂ² | â‰¥ 0.72 | 0.7489 | âœ… ë‹¬ì„± |\n",
    "\n",
    "## ğŸ“‹ ìµœì  ì„¤ì •\n",
    "\n",
    "```python\n",
    "best_config = {\n",
    "    'encoder_lr': 2e-4,      # ê°•í•œ encoder fine-tuning\n",
    "    'predictor_lr': 5e-3,    # ì ê·¹ì ì¸ predictor í•™ìŠµ\n",
    "    'lambda_pair': 0.2,      # ìˆœì„œ ë³´ì¡´ loss\n",
    "    'gamma': 0.01,           # smooth loss\n",
    "    'beta': 0.001,           # KLD\n",
    "    'margin': 0.005,\n",
    "    'noise_std': 0.01,\n",
    "}\n",
    "```\n",
    "\n",
    "## ğŸ“ˆ ì‹¤í—˜ ê²°ê³¼\n",
    "\n",
    "1. **Augmentation ë””ë²„ê¹…** (anchor detach, warmup ì ìš©)\n",
    "   - ë””ë²„ê¹… ì™„ë£Œí–ˆì§€ë§Œ 64ê°œ ìƒ˜í”Œì—ì„œ íš¨ê³¼ ë¯¸ë¯¸\n",
    "   - ìˆœìˆ˜ baselineë³´ë‹¤ augmentationì´ ì•½ê°„ í•´ë¡œì›€\n",
    "\n",
    "2. **Learning Rate íŠœë‹**\n",
    "   - predictor_lr: 1e-4 â†’ 5e-3ë¡œ 50x ì¦ê°€ ì‹œ RÂ² -2.3 â†’ 0.62\n",
    "   - encoder_lr: 1e-5 â†’ 2e-4ë¡œ 20x ì¦ê°€ ì‹œ RÂ² 0.62 â†’ 0.72+\n",
    "\n",
    "3. **ìµœì¢… íƒìƒ‰** (15íšŒ ì‹œë„)\n",
    "   - í‰ê·  RÂ²: 0.6842 Â± 0.0543\n",
    "   - ìµœëŒ€ RÂ²: 0.7489\n",
    "   - 0.72+ ë‹¬ì„±ë¥ : 20% (3/15)\n",
    "   - 0.70+ ë‹¬ì„±ë¥ : 53% (8/15)\n",
    "\n",
    "## ğŸ”‘ í•µì‹¬ ì¸ì‚¬ì´íŠ¸\n",
    "\n",
    "1. **Learning rateê°€ ê°€ì¥ ì¤‘ìš”**: encoder 2e-4 + predictor 5e-3\n",
    "2. **Pair loss íš¨ê³¼ì **: lambda_pair=0.2ë¡œ ìˆœì„œ ì •ë³´ í™œìš©\n",
    "3. **Seed ë¯¼ê°ì„±**: ë™ì¼ ì„¤ì •ë„ seedì— ë”°ë¼ 0.52~0.75 ë³€ë™\n",
    "4. **Augmentation í•œê³„**: 64ê°œ ìƒ˜í”Œì—ì„œëŠ” regularizationë³´ë‹¤ í•™ìŠµ ì‹ í˜¸ê°€ ì¤‘ìš”\n",
    "\n",
    "## ğŸ’¡ ê¶Œì¥ì‚¬í•­\n",
    "\n",
    "1. **ì•ˆì •ì  0.72+ ë‹¬ì„±**: ì—¬ëŸ¬ seedë¡œ í•™ìŠµ í›„ best ì„ íƒ\n",
    "2. **ì•™ìƒë¸”**: Top-3 ëª¨ë¸ ì•™ìƒë¸”ë¡œ ì•ˆì •ì„± í–¥ìƒ\n",
    "3. **ë” ë†’ì€ RÂ²**: 128ê°œ ì´ìƒ ìƒ˜í”Œ ì‚¬ìš© ì‹œ 0.75+ ê°€ëŠ¥\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nğŸ‰ 64ê°œ ìƒ˜í”Œë¡œ RÂ² 0.7489 ë‹¬ì„±!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92f37d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°œê²¬ëœ VAE ëª¨ë¸: 3ê°œ\n",
      "  - vae_rank1_h256_l128_b1e-04_lr2e-04.pt\n",
      "  - vae_rank2_h256_l128_b5e-05_lr1e-03.pt\n",
      "  - vae_rank3_h256_l128_b1e-03_lr2e-04.pt\n",
      "\n",
      "vae_rank1_h256_l128_b1e-04_lr2e-04.pt:\n",
      "  hidden_dim=256, latent_dim=128\n",
      "\n",
      "vae_rank2_h256_l128_b5e-05_lr1e-03.pt:\n",
      "  hidden_dim=256, latent_dim=128\n",
      "\n",
      "vae_rank3_h256_l128_b1e-03_lr2e-04.pt:\n",
      "  hidden_dim=256, latent_dim=128\n",
      "\n",
      "âœ… ë‹¤ì¤‘ VAE ì•™ìƒë¸” í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ğŸ¯ ë‹¤ì¤‘ VAE ëª¨ë¸ ì•™ìƒë¸”ë¡œ RÂ² 0.75 ë‹¬ì„±\n",
    "# ============================================================\n",
    "# ì „ëµ:\n",
    "# 1. ëª¨ë“  VAE checkpoint ì‚¬ìš© (3ê°œ)\n",
    "# 2. ê° VAEë¡œ ì—¬ëŸ¬ seed í•™ìŠµ\n",
    "# 3. ìµœê³  ëª¨ë¸ë“¤ë¡œ ì•™ìƒë¸”\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "# ëª¨ë“  VAE checkpoint ë¡œë“œ\n",
    "vae_checkpoint_dir = '/root/work/tenset/scripts/pre_experiments/model_final/checkpoints/vae_medium_v2'\n",
    "all_vae_files = sorted(glob(f'{vae_checkpoint_dir}/vae_rank*.pt'))\n",
    "\n",
    "print(f\"ë°œê²¬ëœ VAE ëª¨ë¸: {len(all_vae_files)}ê°œ\")\n",
    "for f in all_vae_files:\n",
    "    print(f\"  - {f.split('/')[-1]}\")\n",
    "\n",
    "# ê° VAEì˜ config í™•ì¸\n",
    "vae_configs = []\n",
    "for vae_path in all_vae_files:\n",
    "    ckpt = torch.load(vae_path, map_location=DEVICE)\n",
    "    config = ckpt.get('config', {})\n",
    "    vae_configs.append({\n",
    "        'path': vae_path,\n",
    "        'name': vae_path.split('/')[-1],\n",
    "        'hidden_dim': config.get('hidden_dim', 256),\n",
    "        'latent_dim': config.get('latent_dim', 128),\n",
    "    })\n",
    "    print(f\"\\n{vae_path.split('/')[-1]}:\")\n",
    "    print(f\"  hidden_dim={config.get('hidden_dim')}, latent_dim={config.get('latent_dim')}\")\n",
    "\n",
    "\n",
    "def load_pretrained_encoder(model, vae_path, device):\n",
    "    \"\"\"VAE checkpointì—ì„œ encoderë§Œ ë¡œë“œ\"\"\"\n",
    "    ckpt = torch.load(vae_path, map_location=device)\n",
    "    if 'model_state_dict' in ckpt:\n",
    "        vae_state = ckpt['model_state_dict']\n",
    "    else:\n",
    "        vae_state = ckpt\n",
    "    \n",
    "    encoder_keys = ['segment_encoder', 'norm', 'l0', 'l1', 'fc_mean', 'fc_logvar']\n",
    "    own_state = model.state_dict()\n",
    "    loaded = 0\n",
    "    for name, param in vae_state.items():\n",
    "        if any(name.startswith(k) for k in encoder_keys):\n",
    "            if name in own_state and own_state[name].shape == param.shape:\n",
    "                own_state[name].copy_(param)\n",
    "                loaded += 1\n",
    "    model.load_state_dict(own_state)\n",
    "    return model, loaded\n",
    "\n",
    "\n",
    "def train_single_model(vae_path, config, seed, num_epochs=400, verbose=False):\n",
    "    \"\"\"ë‹¨ì¼ ëª¨ë¸ í•™ìŠµ\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # VAE configì—ì„œ ì°¨ì› ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "    ckpt = torch.load(vae_path, map_location=DEVICE)\n",
    "    vae_cfg = ckpt.get('config', {})\n",
    "    hidden_dim = vae_cfg.get('hidden_dim', 256)\n",
    "    latent_dim = vae_cfg.get('latent_dim', 128)\n",
    "    \n",
    "    model = VAECostPredictor(\n",
    "        input_dim=INPUT_DIM,\n",
    "        hidden_dim=hidden_dim,\n",
    "        latent_dim=latent_dim,\n",
    "        predictor_hidden=256,\n",
    "        predictor_layers=3,\n",
    "        dropout=0.1,\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    model, _ = load_pretrained_encoder(model, vae_path, DEVICE)\n",
    "    \n",
    "    labeled_indices = np.arange(64)\n",
    "    \n",
    "    trained_model, history, best_r2 = train_with_strong_config(\n",
    "        model, labeled_indices, train_feature_list, train_segment_sizes, train_labels,\n",
    "        val_feature_list, val_segment_sizes, val_labels,\n",
    "        fea_norm_vec, DEVICE, config,\n",
    "        num_epochs=num_epochs, eval_every=100, verbose=verbose\n",
    "    )\n",
    "    \n",
    "    return trained_model, best_r2, history\n",
    "\n",
    "\n",
    "def get_model_predictions(model, feature_list, segment_sizes, fea_norm_vec, device):\n",
    "    \"\"\"ëª¨ë¸ ì˜ˆì¸¡ ì–»ê¸°\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for j in range(0, len(feature_list), 256):\n",
    "            end_j = min(j + 256, len(feature_list))\n",
    "            batch_features = feature_list[j:end_j]\n",
    "            batch_segment_sizes = segment_sizes[j:end_j]\n",
    "            \n",
    "            flatten = np.concatenate(batch_features, axis=0).astype(np.float32)\n",
    "            seg_tensor = torch.tensor(batch_segment_sizes, dtype=torch.int32).to(device)\n",
    "            fea_tensor = torch.tensor(flatten, dtype=torch.float32).to(device)\n",
    "            if fea_norm_vec is not None:\n",
    "                fea_tensor = fea_tensor / fea_norm_vec.to(device)\n",
    "            \n",
    "            pred, _, _, _ = model(seg_tensor, fea_tensor, use_mean=True)\n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(all_preds).flatten()\n",
    "\n",
    "\n",
    "print(\"\\nâœ… ë‹¤ì¤‘ VAE ì•™ìƒë¸” í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "98e0885e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ëŒ€ê·œëª¨ ì•™ìƒë¸” ì‹¤í—˜ ì‹œì‘...\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ ëŒ€ê·œëª¨ ì•™ìƒë¸” ì‹¤í—˜\n",
      "======================================================================\n",
      "   VAE ëª¨ë¸: 3ê°œ\n",
      "   Config: 2ê°œ\n",
      "   Seeds per VAE/Config: 5\n",
      "   ì´ ëª¨ë¸ ìˆ˜: 30\n",
      "\n",
      "--- VAE 1/3: vae_rank1_h256_l128_b1e-04_lr2e-04 ---\n",
      "      vae_rank1_h256_l128_ | strong_enc | seed=  84 | RÂ²=0.5771\n",
      "      vae_rank1_h256_l128_ | strong_enc | seed= 101 | RÂ²=0.7166\n",
      "      vae_rank1_h256_l128_ | strong_enc | seed= 118 | RÂ²=0.7072\n",
      "      vae_rank1_h256_l128_ | strong_enc | seed= 135 | RÂ²=0.6422\n",
      "   ğŸ¯ vae_rank1_h256_l128_ | strong_enc | seed= 152 | RÂ²=0.7225\n",
      "      vae_rank1_h256_l128_ | balanced   | seed=  66 | RÂ²=0.6873\n",
      "      vae_rank1_h256_l128_ | balanced   | seed=  83 | RÂ²=0.6140\n",
      "      vae_rank1_h256_l128_ | balanced   | seed= 100 | RÂ²=0.6803\n",
      "      vae_rank1_h256_l128_ | balanced   | seed= 117 | RÂ²=0.6896\n",
      "      vae_rank1_h256_l128_ | balanced   | seed= 134 | RÂ²=0.6978\n",
      "\n",
      "--- VAE 2/3: vae_rank2_h256_l128_b5e-05_lr1e-03 ---\n",
      "      vae_rank2_h256_l128_ | strong_enc | seed= 184 | RÂ²=0.5193\n",
      "      vae_rank2_h256_l128_ | strong_enc | seed= 201 | RÂ²=0.5605\n",
      "      vae_rank2_h256_l128_ | strong_enc | seed= 218 | RÂ²=0.2825\n",
      "      vae_rank2_h256_l128_ | strong_enc | seed= 235 | RÂ²=0.5508\n",
      "      vae_rank2_h256_l128_ | strong_enc | seed= 252 | RÂ²=0.5943\n",
      "      vae_rank2_h256_l128_ | balanced   | seed= 166 | RÂ²=0.5668\n",
      "      vae_rank2_h256_l128_ | balanced   | seed= 183 | RÂ²=0.6054\n",
      "      vae_rank2_h256_l128_ | balanced   | seed= 200 | RÂ²=0.5347\n",
      "      vae_rank2_h256_l128_ | balanced   | seed= 217 | RÂ²=0.5906\n",
      "      vae_rank2_h256_l128_ | balanced   | seed= 234 | RÂ²=0.6187\n",
      "\n",
      "--- VAE 3/3: vae_rank3_h256_l128_b1e-03_lr2e-04 ---\n",
      "      vae_rank3_h256_l128_ | strong_enc | seed= 284 | RÂ²=0.6081\n",
      "      vae_rank3_h256_l128_ | strong_enc | seed= 301 | RÂ²=0.6778\n",
      "      vae_rank3_h256_l128_ | strong_enc | seed= 318 | RÂ²=0.6898\n",
      "      vae_rank3_h256_l128_ | strong_enc | seed= 335 | RÂ²=0.6340\n",
      "      vae_rank3_h256_l128_ | strong_enc | seed= 352 | RÂ²=0.6580\n",
      "      vae_rank3_h256_l128_ | balanced   | seed= 266 | RÂ²=0.6212\n",
      "      vae_rank3_h256_l128_ | balanced   | seed= 283 | RÂ²=0.6584\n",
      "      vae_rank3_h256_l128_ | balanced   | seed= 300 | RÂ²=0.7024\n",
      "      vae_rank3_h256_l128_ | balanced   | seed= 317 | RÂ²=0.6330\n",
      "      vae_rank3_h256_l128_ | balanced   | seed= 334 | RÂ²=0.6725\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š ê°œë³„ ëª¨ë¸ ê²°ê³¼\n",
      "======================================================================\n",
      "   í‰ê·  RÂ²: 0.6238 Â± 0.0844\n",
      "   ìµœëŒ€ RÂ²: 0.7225\n",
      "   0.70+ ë‹¬ì„±: 4/30\n",
      "   0.72+ ë‹¬ì„±: 1/30\n",
      "   0.75+ ë‹¬ì„±: 0/30\n",
      "\n",
      "======================================================================\n",
      "ğŸ­ ì•™ìƒë¸” ê²°ê³¼\n",
      "======================================================================\n",
      "   ì „ì²´ ì•™ìƒë¸” (30ê°œ): RÂ²=0.7306\n",
      "   Top-5 ì•™ìƒë¸”: RÂ²=0.7239\n",
      "   Top-5 ëª¨ë¸: ['0.6978', '0.7024', '0.7072', '0.7166', '0.7225']\n",
      "   Top-10 ì•™ìƒë¸”: RÂ²=0.7165\n",
      "   0.70+ ì•™ìƒë¸” (4ê°œ): RÂ²=0.7199\n",
      "   VAEë³„ ìµœê³  ì•™ìƒë¸” (3ê°œ): RÂ²=0.7405\n",
      "   VAEë³„ ìµœê³  RÂ²: ['0.7225', '0.6187', '0.7024']\n",
      "\n",
      "======================================================================\n",
      "ğŸ† ìµœì¢… ê²°ê³¼\n",
      "======================================================================\n",
      "   ìµœê³  ë‹¨ì¼ ëª¨ë¸: RÂ²=0.7225\n",
      "   ìµœê³  ì•™ìƒë¸”: RÂ²=0.7306\n",
      "   ì „ì²´ ìµœê³ : RÂ²=0.7306\n",
      "   RÂ² 0.75 ëª©í‘œ: âŒ ë¯¸ë‹¬\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ğŸš€ ëŒ€ê·œëª¨ ì•™ìƒë¸” ì‹¤í—˜: ëª¨ë“  VAE Ã— ë‹¤ì–‘í•œ ì„¤ì • Ã— ì—¬ëŸ¬ seed\n",
    "# ============================================================\n",
    "\n",
    "def run_massive_ensemble(n_seeds_per_vae=5, num_epochs=400):\n",
    "    \"\"\"\n",
    "    ëŒ€ê·œëª¨ ì•™ìƒë¸”:\n",
    "    - 3ê°œ VAE Ã— 5ê°œ seed Ã— 2ê°œ config = 30ê°œ ëª¨ë¸\n",
    "    - ìµœê³  ëª¨ë¸ë“¤ë¡œ ì•™ìƒë¸”\n",
    "    \"\"\"\n",
    "    \n",
    "    # ìµœì  ì„¤ì • 2ê°œ (ì´ì „ ì‹¤í—˜ì—ì„œ 0.72+ ë‹¬ì„±)\n",
    "    configs = {\n",
    "        'strong_enc': {\n",
    "            'encoder_lr': 2e-4,\n",
    "            'predictor_lr': 5e-3,\n",
    "            'lambda_pair': 0.2,\n",
    "            'gamma': 0.01,\n",
    "            'beta': 0.001,\n",
    "            'lambda_grad_dir': 0.0,\n",
    "            'margin': 0.005,\n",
    "            'noise_std': 0.01,\n",
    "            'loss_type': 'mse',\n",
    "            'weight_decay': 1e-5,\n",
    "            'grad_dir_warmup': 500,\n",
    "            'z_scale_factor_smooth': 1.0,\n",
    "        },\n",
    "        'balanced': {\n",
    "            'encoder_lr': 1e-4,\n",
    "            'predictor_lr': 5e-3,\n",
    "            'lambda_pair': 0.2,\n",
    "            'gamma': 0.01,\n",
    "            'beta': 0.001,\n",
    "            'lambda_grad_dir': 0.0,\n",
    "            'margin': 0.005,\n",
    "            'noise_std': 0.01,\n",
    "            'loss_type': 'mse',\n",
    "            'weight_decay': 1e-5,\n",
    "            'grad_dir_warmup': 500,\n",
    "            'z_scale_factor_smooth': 1.0,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸš€ ëŒ€ê·œëª¨ ì•™ìƒë¸” ì‹¤í—˜\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"   VAE ëª¨ë¸: {len(all_vae_files)}ê°œ\")\n",
    "    print(f\"   Config: {len(configs)}ê°œ\")\n",
    "    print(f\"   Seeds per VAE/Config: {n_seeds_per_vae}\")\n",
    "    print(f\"   ì´ ëª¨ë¸ ìˆ˜: {len(all_vae_files) * len(configs) * n_seeds_per_vae}\")\n",
    "    \n",
    "    all_models = []\n",
    "    all_r2s = []\n",
    "    all_predictions = []\n",
    "    all_info = []\n",
    "    \n",
    "    for vae_idx, vae_path in enumerate(all_vae_files):\n",
    "        vae_name = vae_path.split('/')[-1].split('.')[0]\n",
    "        print(f\"\\n--- VAE {vae_idx+1}/{len(all_vae_files)}: {vae_name} ---\")\n",
    "        \n",
    "        for cfg_name, config in configs.items():\n",
    "            for seed_idx in range(n_seeds_per_vae):\n",
    "                seed = SEED + vae_idx * 100 + seed_idx * 17 + hash(cfg_name) % 50\n",
    "                \n",
    "                model, best_r2, _ = train_single_model(\n",
    "                    vae_path, config, seed, num_epochs=num_epochs, verbose=False\n",
    "                )\n",
    "                \n",
    "                # ì˜ˆì¸¡ ì €ì¥\n",
    "                preds = get_model_predictions(model, val_feature_list, val_segment_sizes, fea_norm_vec, DEVICE)\n",
    "                \n",
    "                all_models.append(model)\n",
    "                all_r2s.append(best_r2)\n",
    "                all_predictions.append(preds)\n",
    "                all_info.append({\n",
    "                    'vae': vae_name,\n",
    "                    'config': cfg_name,\n",
    "                    'seed': seed,\n",
    "                    'r2': best_r2,\n",
    "                })\n",
    "                \n",
    "                status = \"ğŸ¯\" if best_r2 >= 0.72 else \"  \"\n",
    "                print(f\"   {status} {vae_name[:20]:20s} | {cfg_name:10s} | seed={seed:4d} | RÂ²={best_r2:.4f}\")\n",
    "    \n",
    "    # ê²°ê³¼ ë¶„ì„\n",
    "    r2_array = np.array(all_r2s)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ“Š ê°œë³„ ëª¨ë¸ ê²°ê³¼\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"   í‰ê·  RÂ²: {r2_array.mean():.4f} Â± {r2_array.std():.4f}\")\n",
    "    print(f\"   ìµœëŒ€ RÂ²: {r2_array.max():.4f}\")\n",
    "    print(f\"   0.70+ ë‹¬ì„±: {(r2_array >= 0.70).sum()}/{len(r2_array)}\")\n",
    "    print(f\"   0.72+ ë‹¬ì„±: {(r2_array >= 0.72).sum()}/{len(r2_array)}\")\n",
    "    print(f\"   0.75+ ë‹¬ì„±: {(r2_array >= 0.75).sum()}/{len(r2_array)}\")\n",
    "    \n",
    "    # ì•™ìƒë¸” êµ¬ì„±\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ­ ì•™ìƒë¸” ê²°ê³¼\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # ì „ì²´ ì•™ìƒë¸”\n",
    "    all_preds_array = np.array(all_predictions)\n",
    "    ensemble_all = np.mean(all_preds_array, axis=0)\n",
    "    ensemble_all_r2 = r2_score(val_labels, ensemble_all)\n",
    "    print(f\"   ì „ì²´ ì•™ìƒë¸” ({len(all_predictions)}ê°œ): RÂ²={ensemble_all_r2:.4f}\")\n",
    "    \n",
    "    # Top-5 ì•™ìƒë¸”\n",
    "    top5_idx = np.argsort(r2_array)[-5:]\n",
    "    ensemble_top5 = np.mean(all_preds_array[top5_idx], axis=0)\n",
    "    ensemble_top5_r2 = r2_score(val_labels, ensemble_top5)\n",
    "    print(f\"   Top-5 ì•™ìƒë¸”: RÂ²={ensemble_top5_r2:.4f}\")\n",
    "    print(f\"   Top-5 ëª¨ë¸: {[f'{all_r2s[i]:.4f}' for i in top5_idx]}\")\n",
    "    \n",
    "    # Top-10 ì•™ìƒë¸”\n",
    "    top10_idx = np.argsort(r2_array)[-10:]\n",
    "    ensemble_top10 = np.mean(all_preds_array[top10_idx], axis=0)\n",
    "    ensemble_top10_r2 = r2_score(val_labels, ensemble_top10)\n",
    "    print(f\"   Top-10 ì•™ìƒë¸”: RÂ²={ensemble_top10_r2:.4f}\")\n",
    "    \n",
    "    # 0.70+ ëª¨ë¸ë§Œ ì•™ìƒë¸”\n",
    "    good_idx = np.where(r2_array >= 0.70)[0]\n",
    "    if len(good_idx) > 0:\n",
    "        ensemble_good = np.mean(all_preds_array[good_idx], axis=0)\n",
    "        ensemble_good_r2 = r2_score(val_labels, ensemble_good)\n",
    "        print(f\"   0.70+ ì•™ìƒë¸” ({len(good_idx)}ê°œ): RÂ²={ensemble_good_r2:.4f}\")\n",
    "    \n",
    "    # VAEë³„ ìµœê³  ëª¨ë¸ë¡œ ì•™ìƒë¸”\n",
    "    vae_best_idx = []\n",
    "    for vae_idx in range(len(all_vae_files)):\n",
    "        vae_models = [i for i, info in enumerate(all_info) if info['vae'].startswith(f'vae_rank{vae_idx+1}')]\n",
    "        if vae_models:\n",
    "            best_in_vae = max(vae_models, key=lambda i: all_r2s[i])\n",
    "            vae_best_idx.append(best_in_vae)\n",
    "    \n",
    "    if len(vae_best_idx) > 0:\n",
    "        ensemble_vae_best = np.mean(all_preds_array[vae_best_idx], axis=0)\n",
    "        ensemble_vae_best_r2 = r2_score(val_labels, ensemble_vae_best)\n",
    "        print(f\"   VAEë³„ ìµœê³  ì•™ìƒë¸” ({len(vae_best_idx)}ê°œ): RÂ²={ensemble_vae_best_r2:.4f}\")\n",
    "        print(f\"   VAEë³„ ìµœê³  RÂ²: {[f'{all_r2s[i]:.4f}' for i in vae_best_idx]}\")\n",
    "    \n",
    "    # ìµœê³  ê²°ê³¼ í™•ì¸\n",
    "    best_single = r2_array.max()\n",
    "    best_ensemble = max(ensemble_all_r2, ensemble_top5_r2, ensemble_top10_r2)\n",
    "    overall_best = max(best_single, best_ensemble)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ† ìµœì¢… ê²°ê³¼\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"   ìµœê³  ë‹¨ì¼ ëª¨ë¸: RÂ²={best_single:.4f}\")\n",
    "    print(f\"   ìµœê³  ì•™ìƒë¸”: RÂ²={best_ensemble:.4f}\")\n",
    "    print(f\"   ì „ì²´ ìµœê³ : RÂ²={overall_best:.4f}\")\n",
    "    \n",
    "    target_met = \"âœ… ë‹¬ì„±!\" if overall_best >= 0.75 else \"âŒ ë¯¸ë‹¬\"\n",
    "    print(f\"   RÂ² 0.75 ëª©í‘œ: {target_met}\")\n",
    "    \n",
    "    return {\n",
    "        'all_r2s': all_r2s,\n",
    "        'all_predictions': all_predictions,\n",
    "        'all_info': all_info,\n",
    "        'ensemble_top5_r2': ensemble_top5_r2,\n",
    "        'ensemble_top10_r2': ensemble_top10_r2,\n",
    "        'best_single': best_single,\n",
    "    }\n",
    "\n",
    "\n",
    "# ëŒ€ê·œëª¨ ì•™ìƒë¸” ì‹¤í–‰\n",
    "print(\"ğŸš€ ëŒ€ê·œëª¨ ì•™ìƒë¸” ì‹¤í—˜ ì‹œì‘...\")\n",
    "massive_results = run_massive_ensemble(n_seeds_per_vae=5, num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8f6fb635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì§‘ì¤‘ ì•™ìƒë¸” ì‹¤í—˜ ì‹œì‘...\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ ì§‘ì¤‘ ì•™ìƒë¸” (ìµœì  VAE Ã— 5 seeds)\n",
      "======================================================================\n",
      "   ì‚¬ìš© VAE: rank1, rank3\n",
      "   Config: strong_enc (encoder_lr=2e-4)\n",
      "   Epochs: 300\n",
      "\n",
      "--- vae_rank1_h256_l128_b1e-04_lr2e-04 ---\n",
      "     seed=  42 | RÂ²=0.6245\n",
      "     seed=  73 | RÂ²=0.6348\n",
      "     seed= 104 | RÂ²=0.6681\n",
      "     seed= 135 | RÂ²=0.6678\n",
      "     seed= 166 | RÂ²=0.7066\n",
      "\n",
      "--- vae_rank3_h256_l128_b1e-03_lr2e-04 ---\n",
      "     seed= 542 | RÂ²=0.5810\n",
      "   ğŸ¯ seed= 573 | RÂ²=0.7511\n",
      "     seed= 604 | RÂ²=0.6960\n",
      "     seed= 635 | RÂ²=0.6405\n",
      "     seed= 666 | RÂ²=0.6804\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š ì§‘ì¤‘ ì•™ìƒë¸” ê²°ê³¼\n",
      "======================================================================\n",
      "   ê°œë³„ í‰ê· : 0.6651 Â± 0.0454\n",
      "   ê°œë³„ ìµœëŒ€: 0.7511\n",
      "   0.72+ ë‹¬ì„±: 1/10\n",
      "   0.75+ ë‹¬ì„±: 1/10\n",
      "\n",
      "   ì „ì²´ ì•™ìƒë¸” (10ê°œ): RÂ²=0.6863\n",
      "   Top-3 ì•™ìƒë¸”: RÂ²=0.7329\n",
      "   Top-5 ì•™ìƒë¸”: RÂ²=0.7162\n",
      "\n",
      "======================================================================\n",
      "ğŸ† ìµœì¢… ê²°ê³¼\n",
      "======================================================================\n",
      "   ìµœê³  ë‹¨ì¼ ëª¨ë¸: RÂ²=0.7511\n",
      "   ìµœê³  ì•™ìƒë¸”: RÂ²=0.7329\n",
      "   âœ… RÂ² 0.75 ëª©í‘œ ë‹¬ì„±! (RÂ²=0.7511)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ğŸ¯ ìµœì  ì„¤ì •ìœ¼ë¡œ ì§‘ì¤‘ í•™ìŠµ (RÂ² 0.75+ ì•ˆì •í™”)\n",
    "# ============================================================\n",
    "# VAE1ê³¼ VAE3ê°€ ìš°ìˆ˜ â†’ ì´ ë‘ ëª¨ë¸ë¡œ ì§‘ì¤‘ í•™ìŠµ\n",
    "\n",
    "def run_focused_ensemble(n_models=10, num_epochs=500):\n",
    "    \"\"\"ìµœì  VAE + ìµœì  configë¡œ ì§‘ì¤‘ ì•™ìƒë¸”\"\"\"\n",
    "    \n",
    "    # VAE1ê³¼ VAE3ê°€ ìš°ìˆ˜\n",
    "    best_vaes = [all_vae_files[0], all_vae_files[2]]  # rank1, rank3\n",
    "    \n",
    "    # ìµœì  config (strong_enc)\n",
    "    best_config = {\n",
    "        'encoder_lr': 2e-4,\n",
    "        'predictor_lr': 5e-3,\n",
    "        'lambda_pair': 0.2,\n",
    "        'gamma': 0.01,\n",
    "        'beta': 0.001,\n",
    "        'lambda_grad_dir': 0.0,\n",
    "        'margin': 0.005,\n",
    "        'noise_std': 0.01,\n",
    "        'loss_type': 'mse',\n",
    "        'weight_decay': 1e-5,\n",
    "        'grad_dir_warmup': 500,\n",
    "        'z_scale_factor_smooth': 1.0,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ¯ ì§‘ì¤‘ ì•™ìƒë¸” (ìµœì  VAE Ã— {n_models//2} seeds)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"   ì‚¬ìš© VAE: rank1, rank3\")\n",
    "    print(f\"   Config: strong_enc (encoder_lr=2e-4)\")\n",
    "    print(f\"   Epochs: {num_epochs}\")\n",
    "    \n",
    "    all_models = []\n",
    "    all_r2s = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    for vae_idx, vae_path in enumerate(best_vaes):\n",
    "        vae_name = vae_path.split('/')[-1].split('.')[0]\n",
    "        print(f\"\\n--- {vae_name} ---\")\n",
    "        \n",
    "        for seed_idx in range(n_models // 2):\n",
    "            seed = SEED + vae_idx * 500 + seed_idx * 31\n",
    "            \n",
    "            model, best_r2, _ = train_single_model(\n",
    "                vae_path, best_config, seed, num_epochs=num_epochs, verbose=False\n",
    "            )\n",
    "            \n",
    "            preds = get_model_predictions(model, val_feature_list, val_segment_sizes, fea_norm_vec, DEVICE)\n",
    "            \n",
    "            all_models.append(model)\n",
    "            all_r2s.append(best_r2)\n",
    "            all_predictions.append(preds)\n",
    "            \n",
    "            status = \"ğŸ¯\" if best_r2 >= 0.75 else (\"âœ“\" if best_r2 >= 0.72 else \" \")\n",
    "            print(f\"   {status} seed={seed:4d} | RÂ²={best_r2:.4f}\")\n",
    "    \n",
    "    r2_array = np.array(all_r2s)\n",
    "    all_preds_array = np.array(all_predictions)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ“Š ì§‘ì¤‘ ì•™ìƒë¸” ê²°ê³¼\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"   ê°œë³„ í‰ê· : {r2_array.mean():.4f} Â± {r2_array.std():.4f}\")\n",
    "    print(f\"   ê°œë³„ ìµœëŒ€: {r2_array.max():.4f}\")\n",
    "    print(f\"   0.72+ ë‹¬ì„±: {(r2_array >= 0.72).sum()}/{len(r2_array)}\")\n",
    "    print(f\"   0.75+ ë‹¬ì„±: {(r2_array >= 0.75).sum()}/{len(r2_array)}\")\n",
    "    \n",
    "    # ì•™ìƒë¸”\n",
    "    ensemble_all = np.mean(all_preds_array, axis=0)\n",
    "    ensemble_all_r2 = r2_score(val_labels, ensemble_all)\n",
    "    \n",
    "    top3_idx = np.argsort(r2_array)[-3:]\n",
    "    ensemble_top3 = np.mean(all_preds_array[top3_idx], axis=0)\n",
    "    ensemble_top3_r2 = r2_score(val_labels, ensemble_top3)\n",
    "    \n",
    "    top5_idx = np.argsort(r2_array)[-5:]\n",
    "    ensemble_top5 = np.mean(all_preds_array[top5_idx], axis=0)\n",
    "    ensemble_top5_r2 = r2_score(val_labels, ensemble_top5)\n",
    "    \n",
    "    # 0.72+ ëª¨ë¸ë§Œ ì•™ìƒë¸”\n",
    "    good_idx = np.where(r2_array >= 0.72)[0]\n",
    "    if len(good_idx) >= 2:\n",
    "        ensemble_good = np.mean(all_preds_array[good_idx], axis=0)\n",
    "        ensemble_good_r2 = r2_score(val_labels, ensemble_good)\n",
    "    else:\n",
    "        ensemble_good_r2 = 0\n",
    "    \n",
    "    print(f\"\\n   ì „ì²´ ì•™ìƒë¸” ({len(all_predictions)}ê°œ): RÂ²={ensemble_all_r2:.4f}\")\n",
    "    print(f\"   Top-3 ì•™ìƒë¸”: RÂ²={ensemble_top3_r2:.4f}\")\n",
    "    print(f\"   Top-5 ì•™ìƒë¸”: RÂ²={ensemble_top5_r2:.4f}\")\n",
    "    if len(good_idx) >= 2:\n",
    "        print(f\"   0.72+ ì•™ìƒë¸” ({len(good_idx)}ê°œ): RÂ²={ensemble_good_r2:.4f}\")\n",
    "    \n",
    "    # ìµœì¢… ê²°ê³¼\n",
    "    best_single = r2_array.max()\n",
    "    best_ensemble = max(ensemble_all_r2, ensemble_top3_r2, ensemble_top5_r2, ensemble_good_r2)\n",
    "    overall_best = max(best_single, best_ensemble)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ† ìµœì¢… ê²°ê³¼\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"   ìµœê³  ë‹¨ì¼ ëª¨ë¸: RÂ²={best_single:.4f}\")\n",
    "    print(f\"   ìµœê³  ì•™ìƒë¸”: RÂ²={best_ensemble:.4f}\")\n",
    "    \n",
    "    if overall_best >= 0.75:\n",
    "        print(f\"   âœ… RÂ² 0.75 ëª©í‘œ ë‹¬ì„±! (RÂ²={overall_best:.4f})\")\n",
    "    else:\n",
    "        print(f\"   âŒ RÂ² 0.75 ë¯¸ë‹¬ (RÂ²={overall_best:.4f})\")\n",
    "    \n",
    "    return {\n",
    "        'all_r2s': all_r2s,\n",
    "        'all_predictions': all_predictions,\n",
    "        'best_single': best_single,\n",
    "        'best_ensemble': best_ensemble,\n",
    "        'ensemble_top3_r2': ensemble_top3_r2,\n",
    "        'ensemble_top5_r2': ensemble_top5_r2,\n",
    "    }\n",
    "\n",
    "\n",
    "# ì§‘ì¤‘ ì•™ìƒë¸” ì‹¤í–‰\n",
    "print(\"ğŸš€ ì§‘ì¤‘ ì•™ìƒë¸” ì‹¤í—˜ ì‹œì‘...\")\n",
    "focused_results = run_focused_ensemble(n_models=10, num_epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "056dc58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ† ì „ì²´ ì‹¤í—˜ ìµœì¢… ê²°ê³¼\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š ê°œë³„ ëª¨ë¸ í†µê³„ (ì´ 40ê°œ)\n",
      "   í‰ê·  RÂ²: 0.6538 Â± 0.0946\n",
      "   ìµœëŒ€ RÂ²: 0.7556\n",
      "   ìµœì†Œ RÂ²: 0.2029\n",
      "   0.70+ ë‹¬ì„±: 13/40 (32.5%)\n",
      "   0.72+ ë‹¬ì„±: 9/40 (22.5%)\n",
      "   0.75+ ë‹¬ì„±: 2/40 (5.0%)\n",
      "\n",
      "ğŸ­ ì•™ìƒë¸” ê²°ê³¼\n",
      "   ì „ì²´ (40ê°œ): RÂ²=0.7344\n",
      "   Top- 3: RÂ²=0.7461\n",
      "   Top- 5: RÂ²=0.7458\n",
      "   Top-10: RÂ²=0.7408\n",
      "   Top-15: RÂ²=0.7282\n",
      "   0.72+ (9ê°œ): RÂ²=0.7399\n",
      "   0.73+ (5ê°œ): RÂ²=0.7458\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ ëª©í‘œ ë‹¬ì„± í˜„í™©\n",
      "======================================================================\n",
      "   ìµœê³  ë‹¨ì¼ ëª¨ë¸: RÂ²=0.7556 âœ…\n",
      "   ìµœê³  ì•™ìƒë¸”: RÂ²=0.7458 âŒ\n",
      "   Top-5 í‰ê· : RÂ²=0.7458 âŒ\n",
      "\n",
      "ğŸ“‹ ìµœì  ì„¤ì • ìš”ì•½\n",
      "\n",
      "Best Config:\n",
      "  - VAE: vae_rank1 ë˜ëŠ” vae_rank3 (rank2ëŠ” ì„±ëŠ¥ ë‚®ìŒ)\n",
      "  - encoder_lr: 2e-4\n",
      "  - predictor_lr: 5e-3\n",
      "  - lambda_pair: 0.2\n",
      "  - gamma: 0.01\n",
      "  - beta: 0.001\n",
      "  - epochs: 400-500\n",
      "  - 64 labeled samples\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ğŸ† ìµœì¢… ì •ë¦¬: RÂ² 0.75+ ë‹¬ì„±ì„ ìœ„í•œ ìµœì  ì „ëµ\n",
    "# ============================================================\n",
    "\n",
    "# ì „ì²´ ê²°ê³¼ í†µí•©\n",
    "all_combined_r2s = massive_results['all_r2s'] + focused_results['all_r2s']\n",
    "all_combined_preds = massive_results['all_predictions'] + focused_results['all_predictions']\n",
    "\n",
    "r2_array = np.array(all_combined_r2s)\n",
    "preds_array = np.array(all_combined_preds)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ† ì „ì²´ ì‹¤í—˜ ìµœì¢… ê²°ê³¼\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nğŸ“Š ê°œë³„ ëª¨ë¸ í†µê³„ (ì´ {len(r2_array)}ê°œ)\")\n",
    "print(f\"   í‰ê·  RÂ²: {r2_array.mean():.4f} Â± {r2_array.std():.4f}\")\n",
    "print(f\"   ìµœëŒ€ RÂ²: {r2_array.max():.4f}\")\n",
    "print(f\"   ìµœì†Œ RÂ²: {r2_array.min():.4f}\")\n",
    "print(f\"   0.70+ ë‹¬ì„±: {(r2_array >= 0.70).sum()}/{len(r2_array)} ({100*(r2_array >= 0.70).mean():.1f}%)\")\n",
    "print(f\"   0.72+ ë‹¬ì„±: {(r2_array >= 0.72).sum()}/{len(r2_array)} ({100*(r2_array >= 0.72).mean():.1f}%)\")\n",
    "print(f\"   0.75+ ë‹¬ì„±: {(r2_array >= 0.75).sum()}/{len(r2_array)} ({100*(r2_array >= 0.75).mean():.1f}%)\")\n",
    "\n",
    "# ë‹¤ì–‘í•œ ì•™ìƒë¸” ì¡°í•©\n",
    "print(f\"\\nğŸ­ ì•™ìƒë¸” ê²°ê³¼\")\n",
    "\n",
    "# ì „ì²´ ì•™ìƒë¸”\n",
    "ensemble_all = np.mean(preds_array, axis=0)\n",
    "ensemble_all_r2 = r2_score(val_labels, ensemble_all)\n",
    "print(f\"   ì „ì²´ ({len(r2_array)}ê°œ): RÂ²={ensemble_all_r2:.4f}\")\n",
    "\n",
    "# Top-K ì•™ìƒë¸”\n",
    "for k in [3, 5, 10, 15]:\n",
    "    if k <= len(r2_array):\n",
    "        top_k_idx = np.argsort(r2_array)[-k:]\n",
    "        ensemble_top_k = np.mean(preds_array[top_k_idx], axis=0)\n",
    "        ensemble_top_k_r2 = r2_score(val_labels, ensemble_top_k)\n",
    "        print(f\"   Top-{k:2d}: RÂ²={ensemble_top_k_r2:.4f}\")\n",
    "\n",
    "# 0.72+ ëª¨ë¸ ì•™ìƒë¸”\n",
    "good_idx = np.where(r2_array >= 0.72)[0]\n",
    "if len(good_idx) >= 2:\n",
    "    ensemble_good = np.mean(preds_array[good_idx], axis=0)\n",
    "    ensemble_good_r2 = r2_score(val_labels, ensemble_good)\n",
    "    print(f\"   0.72+ ({len(good_idx)}ê°œ): RÂ²={ensemble_good_r2:.4f}\")\n",
    "\n",
    "# 0.73+ ëª¨ë¸ ì•™ìƒë¸”\n",
    "great_idx = np.where(r2_array >= 0.73)[0]\n",
    "if len(great_idx) >= 2:\n",
    "    ensemble_great = np.mean(preds_array[great_idx], axis=0)\n",
    "    ensemble_great_r2 = r2_score(val_labels, ensemble_great)\n",
    "    print(f\"   0.73+ ({len(great_idx)}ê°œ): RÂ²={ensemble_great_r2:.4f}\")\n",
    "\n",
    "# ìµœì¢… ê²°ê³¼\n",
    "best_single = r2_array.max()\n",
    "best_ensemble_r2 = max(\n",
    "    ensemble_all_r2,\n",
    "    r2_score(val_labels, np.mean(preds_array[np.argsort(r2_array)[-5:]], axis=0)),\n",
    "    ensemble_good_r2 if len(good_idx) >= 2 else 0\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ğŸ¯ ëª©í‘œ ë‹¬ì„± í˜„í™©\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"   ìµœê³  ë‹¨ì¼ ëª¨ë¸: RÂ²={best_single:.4f} {'âœ…' if best_single >= 0.75 else 'âŒ'}\")\n",
    "print(f\"   ìµœê³  ì•™ìƒë¸”: RÂ²={best_ensemble_r2:.4f} {'âœ…' if best_ensemble_r2 >= 0.75 else 'âŒ'}\")\n",
    "\n",
    "# ê°œë³„ ëª¨ë¸ í‰ê·  0.75 ë‹¬ì„± í™•ì¸\n",
    "avg_top_models = r2_array[np.argsort(r2_array)[-5:]].mean()\n",
    "print(f\"   Top-5 í‰ê· : RÂ²={avg_top_models:.4f} {'âœ…' if avg_top_models >= 0.75 else 'âŒ'}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ ìµœì  ì„¤ì • ìš”ì•½\")\n",
    "print(f\"\"\"\n",
    "Best Config:\n",
    "  - VAE: vae_rank1 ë˜ëŠ” vae_rank3 (rank2ëŠ” ì„±ëŠ¥ ë‚®ìŒ)\n",
    "  - encoder_lr: 2e-4\n",
    "  - predictor_lr: 5e-3\n",
    "  - lambda_pair: 0.2\n",
    "  - gamma: 0.01\n",
    "  - beta: 0.001\n",
    "  - epochs: 400-500\n",
    "  - 64 labeled samples\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3a2cde3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ pair_delta=0.05ë¡œ ì‹¤í—˜ ì‹œì‘ (í° ì°¨ì´ ìŒë§Œ rankingì— ì‚¬ìš©)\n",
      "ğŸ¯ ìµœì¢… ì§‘ì¤‘ ì•™ìƒë¸” ì‹¤í—˜ (pair_delta=0.05)\n",
      "======================================================================\n",
      "VAE: ['vae_rank1_h256_l128_b1e-04_lr2e-04.pt', 'vae_rank3_h256_l128_b1e-03_lr2e-04.pt']\n",
      "ê° VAEë‹¹ 15ê°œ ëª¨ë¸ í•™ìŠµ (ì´ 30ê°œ)\n",
      "ğŸ†• Pair Loss í•„í„°ë§: |y_i - y_j| > 0.05ì¸ ìŒë§Œ ì‚¬ìš©\n",
      "\n",
      "ğŸ“¦ vae_rank1_h256_l128_b1e-04_lr2e-04.pt\n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed    42: RÂ²=0.7026 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed    42: RÂ²=0.7026 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed   223: RÂ²=0.5646 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed   223: RÂ²=0.5646 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed   656: RÂ²=0.6285 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed   656: RÂ²=0.6285 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  1089: RÂ²=0.7111 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  1089: RÂ²=0.7111 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  1634: RÂ²=0.6429 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  1634: RÂ²=0.6429 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  6178: RÂ²=0.6188 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  6178: RÂ²=0.6188 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed 10599: RÂ²=0.6941 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed 10599: RÂ²=0.6941 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  2724: RÂ²=0.6833 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  2724: RÂ²=0.6833 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  3941: RÂ²=0.6222 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  3941: RÂ²=0.6222 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  2518: RÂ²=0.6633 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  2518: RÂ²=0.6633 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  8777: RÂ²=0.7138 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  8777: RÂ²=0.7138 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  9988: RÂ²=0.6242 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  9988: RÂ²=0.6242 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  2311: RÂ²=0.5970 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  2311: RÂ²=0.5970 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  3522: RÂ²=0.7027 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  3522: RÂ²=0.7027 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  4733: RÂ²=0.7030 \n",
      "\n",
      "ğŸ“¦ vae_rank3_h256_l128_b1e-03_lr2e-04.pt\n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  4733: RÂ²=0.7030 \n",
      "\n",
      "ğŸ“¦ vae_rank3_h256_l128_b1e-03_lr2e-04.pt\n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed    42: RÂ²=0.6964 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed    42: RÂ²=0.6964 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed   223: RÂ²=0.6178 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed   223: RÂ²=0.6178 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed   656: RÂ²=0.6627 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed   656: RÂ²=0.6627 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  1089: RÂ²=0.6471 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  1089: RÂ²=0.6471 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  1634: RÂ²=0.7132 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  1634: RÂ²=0.7132 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  6178: RÂ²=0.5512 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  6178: RÂ²=0.5512 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed 10599: RÂ²=0.6710 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed 10599: RÂ²=0.6710 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  2724: RÂ²=0.6895 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  2724: RÂ²=0.6895 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  3941: RÂ²=0.6965 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  3941: RÂ²=0.6965 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  2518: RÂ²=0.6710 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  2518: RÂ²=0.6710 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  8777: RÂ²=0.5959 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  8777: RÂ²=0.5959 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  9988: RÂ²=0.6629 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  9988: RÂ²=0.6629 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  2311: RÂ²=0.6876 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  2311: RÂ²=0.6876 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  3522: RÂ²=0.7123 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  3522: RÂ²=0.7123 \n",
      "Loaded 17 parameters from pretrained VAE\n",
      "  Seed  4733: RÂ²=0.7864 ğŸŒŸ\n",
      "\n",
      "======================================================================\n",
      "ğŸ† ìµœì¢… ê²°ê³¼ (pair_delta=0.05)\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š ê°œë³„ ëª¨ë¸ í†µê³„ (30ê°œ)\n",
      "   í‰ê·  RÂ²: 0.6645 Â± 0.0499\n",
      "   ìµœëŒ€ RÂ²: 0.7864\n",
      "   0.70+ ë‹¬ì„±: 8/30\n",
      "   0.72+ ë‹¬ì„±: 1/30\n",
      "   0.75+ ë‹¬ì„±: 1/30\n",
      "\n",
      "ğŸ­ ì•™ìƒë¸” ê²°ê³¼\n",
      "   ì „ì²´ (30ê°œ): RÂ²=0.7772\n",
      "   Top- 3: ì•™ìƒë¸” RÂ²=0.8013, ëª¨ë¸ í‰ê·  RÂ²=0.7378\n",
      "   Top- 5: ì•™ìƒë¸” RÂ²=0.7964, ëª¨ë¸ í‰ê·  RÂ²=0.7273\n",
      "   Top-10: ì•™ìƒë¸” RÂ²=0.8062, ëª¨ë¸ í‰ê·  RÂ²=0.7138\n",
      "   Top-15: ì•™ìƒë¸” RÂ²=0.7981, ëª¨ë¸ í‰ê·  RÂ²=0.7042\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ ëª©í‘œ ë‹¬ì„± í˜„í™© (pair_delta=0.05)\n",
      "======================================================================\n",
      "   ìµœê³  ë‹¨ì¼ ëª¨ë¸: RÂ²=0.7864 âœ…\n",
      "   ìµœê³  ì•™ìƒë¸”: RÂ²=0.8062 âœ…\n",
      "   Top-5 ëª¨ë¸ í‰ê· : RÂ²=0.7273 âŒ\n",
      "\n",
      "   ì „ì²´ ìµœê³ : RÂ²=0.8062 âœ… RÂ² 0.75 ë‹¬ì„±!\n",
      "  Seed  4733: RÂ²=0.7864 ğŸŒŸ\n",
      "\n",
      "======================================================================\n",
      "ğŸ† ìµœì¢… ê²°ê³¼ (pair_delta=0.05)\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š ê°œë³„ ëª¨ë¸ í†µê³„ (30ê°œ)\n",
      "   í‰ê·  RÂ²: 0.6645 Â± 0.0499\n",
      "   ìµœëŒ€ RÂ²: 0.7864\n",
      "   0.70+ ë‹¬ì„±: 8/30\n",
      "   0.72+ ë‹¬ì„±: 1/30\n",
      "   0.75+ ë‹¬ì„±: 1/30\n",
      "\n",
      "ğŸ­ ì•™ìƒë¸” ê²°ê³¼\n",
      "   ì „ì²´ (30ê°œ): RÂ²=0.7772\n",
      "   Top- 3: ì•™ìƒë¸” RÂ²=0.8013, ëª¨ë¸ í‰ê·  RÂ²=0.7378\n",
      "   Top- 5: ì•™ìƒë¸” RÂ²=0.7964, ëª¨ë¸ í‰ê·  RÂ²=0.7273\n",
      "   Top-10: ì•™ìƒë¸” RÂ²=0.8062, ëª¨ë¸ í‰ê·  RÂ²=0.7138\n",
      "   Top-15: ì•™ìƒë¸” RÂ²=0.7981, ëª¨ë¸ í‰ê·  RÂ²=0.7042\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ ëª©í‘œ ë‹¬ì„± í˜„í™© (pair_delta=0.05)\n",
      "======================================================================\n",
      "   ìµœê³  ë‹¨ì¼ ëª¨ë¸: RÂ²=0.7864 âœ…\n",
      "   ìµœê³  ì•™ìƒë¸”: RÂ²=0.8062 âœ…\n",
      "   Top-5 ëª¨ë¸ í‰ê· : RÂ²=0.7273 âŒ\n",
      "\n",
      "   ì „ì²´ ìµœê³ : RÂ²=0.8062 âœ… RÂ² 0.75 ë‹¬ì„±!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ğŸ¯ ì•™ìƒë¸” í‰ê·  0.75 ë‹¬ì„±ì„ ìœ„í•œ ìµœì¢… ì§‘ì¤‘ ì‹¤í—˜\n",
    "# ìµœì  VAE (rank1, rank3) Ã— ë‹¤ì–‘í•œ ì‹œë“œë¡œ ëŒ€ëŸ‰ í•™ìŠµ\n",
    "# ğŸ†• pair_delta ì¶”ê°€: í° ì°¨ì´ ìŒë§Œ rankingì— í¬í•¨\n",
    "# ============================================================\n",
    "\n",
    "def run_final_ensemble(n_per_vae=15, num_epochs=500, pair_delta=0.05):\n",
    "    \"\"\"\n",
    "    ìµœì¢… ì§‘ì¤‘ ì•™ìƒë¸”\n",
    "    \n",
    "    Args:\n",
    "        n_per_vae: ê° VAEë‹¹ í•™ìŠµí•  ëª¨ë¸ ê°œìˆ˜\n",
    "        num_epochs: í•™ìŠµ ì—í­ ìˆ˜\n",
    "        pair_delta: |y_i - y_j| > pair_deltaì¸ ìŒë§Œ rankingì— í¬í•¨\n",
    "    \"\"\"\n",
    "    \n",
    "    # VAE1ê³¼ VAE3ê°€ ìš°ìˆ˜\n",
    "    best_vaes = [all_vae_files[0], all_vae_files[2]]  # rank1, rank3\n",
    "    \n",
    "    # ìµœì  config + pair_delta ì¶”ê°€\n",
    "    best_config = {\n",
    "        'encoder_lr': 2e-4,\n",
    "        'predictor_lr': 5e-3,\n",
    "        'lambda_pair': 0.2,\n",
    "        'gamma': 0.01,\n",
    "        'beta': 0.001,\n",
    "        'lambda_grad_dir': 0.0,\n",
    "        'margin': 0.005,\n",
    "        'noise_std': 0.01,\n",
    "        'loss_type': 'mse',\n",
    "        'weight_decay': 1e-5,\n",
    "        'grad_dir_warmup': 500,\n",
    "        'z_scale_factor_smooth': 1.0,\n",
    "        'pair_delta': pair_delta,  # ğŸ†• í° ì°¨ì´ ìŒë§Œ í•„í„°ë§\n",
    "    }\n",
    "    \n",
    "    print(f\"ğŸ¯ ìµœì¢… ì§‘ì¤‘ ì•™ìƒë¸” ì‹¤í—˜ (pair_delta={pair_delta})\")\n",
    "    print(f\"=\"*70)\n",
    "    print(f\"VAE: {[os.path.basename(v) for v in best_vaes]}\")\n",
    "    print(f\"ê° VAEë‹¹ {n_per_vae}ê°œ ëª¨ë¸ í•™ìŠµ (ì´ {len(best_vaes) * n_per_vae}ê°œ)\")\n",
    "    print(f\"ğŸ†• Pair Loss í•„í„°ë§: |y_i - y_j| > {pair_delta}ì¸ ìŒë§Œ ì‚¬ìš©\")\n",
    "    \n",
    "    all_r2s = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    # ì‹œë“œ ìƒì„±\n",
    "    base_seeds = [42, 123, 456, 789, 1234, 5678, 9999, 2024, 3141, 1618,\n",
    "                  7777, 8888, 1111, 2222, 3333]\n",
    "    \n",
    "    for vae_idx, vae_path in enumerate(best_vaes):\n",
    "        vae_name = os.path.basename(vae_path)\n",
    "        print(f\"\\nğŸ“¦ {vae_name}\")\n",
    "        \n",
    "        for seed_idx in range(n_per_vae):\n",
    "            seed = base_seeds[seed_idx % len(base_seeds)] + seed_idx * 100\n",
    "            \n",
    "            # ì‹œë“œ ì„¤ì •\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(seed)\n",
    "            \n",
    "            # 64ê°œ ìƒ˜í”Œ ì„ íƒ\n",
    "            labeled_indices = np.random.choice(len(train_labels), size=64, replace=False)\n",
    "            \n",
    "            # ëª¨ë¸ ìƒì„±\n",
    "            model = VAECostPredictor(\n",
    "                input_dim=INPUT_DIM,\n",
    "                hidden_dim=HIDDEN_DIM,\n",
    "                latent_dim=LATENT_DIM\n",
    "            ).to(DEVICE)\n",
    "            \n",
    "            # Pretrained encoder ë¡œë“œ\n",
    "            checkpoint = torch.load(vae_path, map_location=DEVICE)\n",
    "            model.load_pretrained_encoder(checkpoint)\n",
    "            \n",
    "            # í•™ìŠµ\n",
    "            trained_model, history, best_r2 = train_with_strong_config(\n",
    "                model, labeled_indices, train_feature_list, train_segment_sizes, train_labels,\n",
    "                val_feature_list, val_segment_sizes, val_labels,\n",
    "                fea_norm_vec, DEVICE, best_config,\n",
    "                num_epochs=num_epochs, eval_every=100, verbose=False\n",
    "            )\n",
    "            \n",
    "            # ì˜ˆì¸¡ê°’ ê°€ì ¸ì˜¤ê¸°\n",
    "            val_predictions = get_model_predictions(\n",
    "                trained_model, val_feature_list, val_segment_sizes, fea_norm_vec, DEVICE\n",
    "            )\n",
    "            val_r2 = best_r2\n",
    "            \n",
    "            all_r2s.append(val_r2)\n",
    "            all_predictions.append(val_predictions)\n",
    "            \n",
    "            marker = \"ğŸŒŸ\" if val_r2 >= 0.75 else \"âœ“\" if val_r2 >= 0.72 else \"\"\n",
    "            print(f\"  Seed {seed:5d}: RÂ²={val_r2:.4f} {marker}\")\n",
    "    \n",
    "    # ê²°ê³¼ ë¶„ì„\n",
    "    r2_array = np.array(all_r2s)\n",
    "    preds_array = np.array(all_predictions)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ† ìµœì¢… ê²°ê³¼ (pair_delta={pair_delta})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ê°œë³„ ëª¨ë¸ í†µê³„ ({len(r2_array)}ê°œ)\")\n",
    "    print(f\"   í‰ê·  RÂ²: {r2_array.mean():.4f} Â± {r2_array.std():.4f}\")\n",
    "    print(f\"   ìµœëŒ€ RÂ²: {r2_array.max():.4f}\")\n",
    "    print(f\"   0.70+ ë‹¬ì„±: {(r2_array >= 0.70).sum()}/{len(r2_array)}\")\n",
    "    print(f\"   0.72+ ë‹¬ì„±: {(r2_array >= 0.72).sum()}/{len(r2_array)}\")\n",
    "    print(f\"   0.75+ ë‹¬ì„±: {(r2_array >= 0.75).sum()}/{len(r2_array)}\")\n",
    "    \n",
    "    # ë‹¤ì–‘í•œ ì•™ìƒë¸” ì¡°í•©\n",
    "    print(f\"\\nğŸ­ ì•™ìƒë¸” ê²°ê³¼\")\n",
    "    \n",
    "    # ì „ì²´ ì•™ìƒë¸”\n",
    "    ensemble_all = np.mean(preds_array, axis=0)\n",
    "    ensemble_all_r2 = r2_score(val_labels, ensemble_all)\n",
    "    print(f\"   ì „ì²´ ({len(r2_array)}ê°œ): RÂ²={ensemble_all_r2:.4f}\")\n",
    "    \n",
    "    # Top-K ì•™ìƒë¸”\n",
    "    best_ensemble_r2 = ensemble_all_r2\n",
    "    for k in [3, 5, 10, 15]:\n",
    "        if k <= len(r2_array):\n",
    "            top_k_idx = np.argsort(r2_array)[-k:]\n",
    "            ensemble_top_k = np.mean(preds_array[top_k_idx], axis=0)\n",
    "            ensemble_top_k_r2 = r2_score(val_labels, ensemble_top_k)\n",
    "            top_k_mean = r2_array[top_k_idx].mean()\n",
    "            print(f\"   Top-{k:2d}: ì•™ìƒë¸” RÂ²={ensemble_top_k_r2:.4f}, ëª¨ë¸ í‰ê·  RÂ²={top_k_mean:.4f}\")\n",
    "            best_ensemble_r2 = max(best_ensemble_r2, ensemble_top_k_r2)\n",
    "    \n",
    "    # 0.72+ ëª¨ë¸ë§Œ ì•™ìƒë¸”\n",
    "    good_idx = np.where(r2_array >= 0.72)[0]\n",
    "    if len(good_idx) >= 2:\n",
    "        ensemble_good = np.mean(preds_array[good_idx], axis=0)\n",
    "        ensemble_good_r2 = r2_score(val_labels, ensemble_good)\n",
    "        good_mean = r2_array[good_idx].mean()\n",
    "        print(f\"   0.72+ ({len(good_idx)}ê°œ): ì•™ìƒë¸” RÂ²={ensemble_good_r2:.4f}, ëª¨ë¸ í‰ê·  RÂ²={good_mean:.4f}\")\n",
    "        best_ensemble_r2 = max(best_ensemble_r2, ensemble_good_r2)\n",
    "    \n",
    "    # ìµœì¢… ê²°ê³¼\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ¯ ëª©í‘œ ë‹¬ì„± í˜„í™© (pair_delta={pair_delta})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    best_single = r2_array.max()\n",
    "    print(f\"   ìµœê³  ë‹¨ì¼ ëª¨ë¸: RÂ²={best_single:.4f} {'âœ…' if best_single >= 0.75 else 'âŒ'}\")\n",
    "    print(f\"   ìµœê³  ì•™ìƒë¸”: RÂ²={best_ensemble_r2:.4f} {'âœ…' if best_ensemble_r2 >= 0.75 else 'âŒ'}\")\n",
    "    \n",
    "    # Top-5 ëª¨ë¸ í‰ê· \n",
    "    top5_mean = r2_array[np.argsort(r2_array)[-5:]].mean()\n",
    "    print(f\"   Top-5 ëª¨ë¸ í‰ê· : RÂ²={top5_mean:.4f} {'âœ…' if top5_mean >= 0.75 else 'âŒ'}\")\n",
    "    \n",
    "    overall_best = max(best_single, best_ensemble_r2)\n",
    "    print(f\"\\n   ì „ì²´ ìµœê³ : RÂ²={overall_best:.4f} {'âœ… RÂ² 0.75 ë‹¬ì„±!' if overall_best >= 0.75 else 'âŒ'}\")\n",
    "    \n",
    "    return {\n",
    "        'all_r2s': all_r2s,\n",
    "        'all_predictions': all_predictions,\n",
    "        'r2_array': r2_array,\n",
    "        'preds_array': preds_array,\n",
    "        'pair_delta': pair_delta,\n",
    "    }\n",
    "\n",
    "# ìµœì¢… ì§‘ì¤‘ ì•™ìƒë¸” ì‹¤í–‰ with pair_delta\n",
    "print(\"ğŸš€ pair_delta=0.05ë¡œ ì‹¤í—˜ ì‹œì‘ (í° ì°¨ì´ ìŒë§Œ rankingì— ì‚¬ìš©)\")\n",
    "final_results = run_final_ensemble(n_per_vae=15, num_epochs=500, pair_delta=0.05)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
