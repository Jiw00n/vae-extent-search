{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92c93423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "project_root = \"/root/work/tenset\"\n",
    "os.environ[\"TVM_HOME\"] = f\"{project_root}\"\n",
    "os.environ[\"TVM_LIBRARY_PATH\"] = f\"{project_root}/build\"\n",
    "if f\"{project_root}/python\" not in sys.path:\n",
    "    sys.path.insert(0, f\"{project_root}/python\")\n",
    "\n",
    "sys.path = [p for p in sys.path if not p.startswith(f\"{project_root}/build\")]\n",
    "sys.path.append(f\"{project_root}/build\")\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = f\"{project_root}/build:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17700bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/root/work/tenset/scripts\")\n",
    "\n",
    "from tvm import auto_scheduler\n",
    "from print_programs import return_program\n",
    "from tvm.auto_scheduler.feature import get_per_store_features_from_file\n",
    "from make_dataset import load_and_register_tasks\n",
    "import numpy as np\n",
    "\n",
    "json_file = \"/root/work/tenset/dataset/measure_records_tenset/k80/([9f4c6b76f51d20e5c3bfb1817edd446e,1,64,64,64,1,1,64,256,1,1,1,256,1,64,64,256],cuda).json\"\n",
    "\n",
    "tasks = load_and_register_tasks()\n",
    "inputs, results = auto_scheduler.RecordReader(json_file).read_lines()\n",
    "raw_features, raw_normalized_throughputs, task_ids, min_latency = get_per_store_features_from_file(json_file, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbbc971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# records = {\n",
    "#     \"schedules\": [],\n",
    "#     \"cost_mean\": [],\n",
    "#     \"feature\" : []\n",
    "# }\n",
    "\n",
    "# for i in range(len(inputs)):\n",
    "#     state, cost = return_program(inputs[i], results[i])\n",
    "#     # break\n",
    "#     if state is not None:\n",
    "#         cost_mean = np.mean([x.value for x in cost])\n",
    "#         feature = raw_features[i]\n",
    "#         records[\"feature\"].append(feature)\n",
    "#         records[\"schedules\"].append(state)\n",
    "#         records[\"cost_mean\"].append(cost_mean)\n",
    "\n",
    "\n",
    "# features = np.array(records[\"feature\"], dtype=np.float32)\n",
    "# costs = np.array(records[\"cost_mean\"], dtype=np.float32) * 1000\n",
    "\n",
    "# features = features.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2de41380",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "costs = []\n",
    "for feature, throughput in zip(raw_features, raw_normalized_throughputs):\n",
    "    if throughput > 1.0e-10:\n",
    "        features.append(feature)\n",
    "        costs.append(-np.log(throughput))\n",
    "\n",
    "\n",
    "features = np.array(features, dtype=np.float32)\n",
    "costs = np.array(costs, dtype=np.float32)\n",
    "\n",
    "# features = features.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f8d47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        h = self.encoder(x)\n",
    "        # torch N*4,D -> N,4,D\n",
    "        h = h.view(-1, 4, h.size(-1))\n",
    "        # 평균값 계산 N,4,D -> N,D\n",
    "        h_mean = torch.mean(h, dim=1)\n",
    "        return self.net(h_mean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68068ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "def pairwise_ranking_loss(cost_pred, cost_true, margin=0.1):\n",
    "    \"\"\"\n",
    "    Pairwise ranking loss: 실제 cost 순서를 예측이 유지하도록.\n",
    "    cost_true[i] < cost_true[j] 이면 cost_pred[i] < cost_pred[j] + margin\n",
    "    \"\"\"\n",
    "    batch_size = cost_pred.size(0)\n",
    "    if batch_size < 2:\n",
    "        return torch.tensor(0.0, device=cost_pred.device)\n",
    "    \n",
    "    # 모든 쌍에 대해 ranking loss 계산\n",
    "    idx = torch.arange(batch_size, device=cost_pred.device)\n",
    "    i_idx, j_idx = torch.meshgrid(idx, idx, indexing='ij')\n",
    "    mask = i_idx < j_idx  # upper triangular only\n",
    "    \n",
    "    pred_i = cost_pred[i_idx[mask]]\n",
    "    pred_j = cost_pred[j_idx[mask]]\n",
    "    true_i = cost_true[i_idx[mask]]\n",
    "    true_j = cost_true[j_idx[mask]]\n",
    "    \n",
    "    # label: 1 if true_i < true_j, -1 otherwise\n",
    "    labels = torch.sign(true_j - true_i).float()\n",
    "    \n",
    "    # Margin ranking loss\n",
    "    loss = F.margin_ranking_loss(pred_j.view(-1), pred_i.view(-1), labels.view(-1), margin=margin)\n",
    "    return loss\n",
    "\n",
    "def reg_loss_fn(cost_pred, cost_true, loss_type='mse'):\n",
    "    \"\"\"\n",
    "    기본 회귀 손실 (MSE 또는 MAE)\n",
    "    \"\"\"\n",
    "    if loss_type == 'mse':\n",
    "        return F.mse_loss(cost_pred, cost_true)\n",
    "    else:  # mae\n",
    "        return F.l1_loss(cost_pred, cost_true)\n",
    "    \n",
    "def total_loss_fn(cost_pred, cost_true, alpha=0.5, margin=0.1):\n",
    "    \"\"\"\n",
    "    회귀 손실과 쌍별 순위 손실의 가중 합\n",
    "    \"\"\"\n",
    "    reg_loss = reg_loss_fn(cost_pred, cost_true, loss_type='mse')\n",
    "    rank_loss = pairwise_ranking_loss(cost_pred, cost_true, margin=margin)\n",
    "    return alpha * reg_loss + rank_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4513a557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_accuracy(cost_pred, labels):\n",
    "    \"\"\"\n",
    "    cost_pred, labels: (B,) 텐서\n",
    "    \"\"\"\n",
    "    n_samples = min(1000, len(cost_pred))\n",
    "    sample_indices = np.random.choice(len(cost_pred), n_samples, replace=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i in range(n_samples):\n",
    "            for j in range(i + 1, n_samples):\n",
    "                idx_i = sample_indices[i]\n",
    "                idx_j = sample_indices[j]\n",
    "                pred_diff = cost_pred[idx_i] - cost_pred[idx_j]\n",
    "                true_diff = labels[idx_i] - labels[idx_j]\n",
    "                if (pred_diff * true_diff) > 0:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "        accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edd9f0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class NpzRegressionDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        # y shape이 (N,)이면 (N,1)로 바꿔주는 게 편할 때가 많음\n",
    "        if self.y.ndim == 1:\n",
    "            self.y = self.y.unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_size = 64\n",
    "random_indices = np.random.permutation(len(features))[:train_size]\n",
    "features_64 = features[random_indices]\n",
    "costs_64 = costs[random_indices]\n",
    "X_train = features_64\n",
    "y_train = costs_64\n",
    "X_val = features[~np.isin(np.arange(len(features)), random_indices)]\n",
    "y_val = costs[~np.isin(np.arange(len(costs)), random_indices)]\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     features, costs, test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "X_train_flat = X_train.reshape(X_train.shape[0] * X_train.shape[1], X_train.shape[2])\n",
    "X_val_flat = X_val.reshape(X_val.shape[0] * X_val.shape[1], X_val.shape[2])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_flat_scaled = scaler.fit_transform(X_train_flat)\n",
    "X_val_flat_scaled = scaler.transform(X_val_flat)\n",
    "X_train_scaled = X_train_flat_scaled.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "X_val_scaled = X_val_flat_scaled.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2])\n",
    "\n",
    "train_dataset = NpzRegressionDataset(X_train_scaled, y_train)\n",
    "val_dataset   = NpzRegressionDataset(X_val_scaled,   y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f12b45b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_629101/247624797.py:33: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(cost_pred, cost_true)\n",
      "/tmp/ipykernel_629101/247624797.py:33: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(cost_pred, cost_true)\n",
      "/tmp/ipykernel_629101/247624797.py:33: UserWarning: Using a target size (torch.Size([116])) that is different to the input size (torch.Size([116, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(cost_pred, cost_true)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/200] train_loss=0.1069 val_loss=0.1823\n",
      "Epoch [100/200] train_loss=0.0962 val_loss=0.1771\n",
      "Epoch [150/200] train_loss=0.0959 val_loss=0.1759\n",
      "Epoch [200/200] train_loss=0.0959 val_loss=0.1758\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "input_dim = X_train.shape[-1]\n",
    "model = MLPRegressor(input_dim, hidden_dim=256)\n",
    "\n",
    "\n",
    "# criterion = nn.MarginRankingLoss(margin=0.01)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # --- train ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        yb = yb.squeeze()\n",
    "        # flatten input N,4,D -> N*4,D\n",
    "        xb = xb.view(xb.size(0) * xb.size(1), xb.size(2))\n",
    "\n",
    "        pred = model(xb)\n",
    "        loss = total_loss_fn(pred, yb, alpha=0.01, margin=0.01)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    train_loss /= len(train_dataset)\n",
    "\n",
    "    # --- validation ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            yb = yb.squeeze()\n",
    "            xb = xb.view(xb.size(0) * xb.size(1), xb.size(2))\n",
    "\n",
    "            pred = model(xb)\n",
    "            loss = total_loss_fn(pred, yb, alpha=0.01, margin=0.01)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    val_loss /= len(val_dataset)\n",
    "\n",
    "    if (epoch) % 50 == 0:\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] \"\n",
    "              f\"train_loss={train_loss:.4f} val_loss={val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd98eb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Pairwise Ranking Accuracy: 0.4006\n"
     ]
    }
   ],
   "source": [
    "rank_inputs = torch.from_numpy(features).float().to(device)\n",
    "rank_inputs = rank_inputs.view(rank_inputs.size(0) * rank_inputs.size(1), rank_inputs.size(2))\n",
    "rank_labels = torch.from_numpy(costs).float().to(device)\n",
    "pred = model(rank_inputs)\n",
    "rank_r2 = pair_accuracy(pred, rank_labels)\n",
    "print(f\"Final Pairwise Ranking Accuracy: {rank_r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
