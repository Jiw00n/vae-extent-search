{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d665df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "\n",
    "project_root = \"/root/work/tenset\"\n",
    "os.environ[\"TVM_HOME\"] = f\"{project_root}\"\n",
    "os.environ[\"TVM_LIBRARY_PATH\"] = f\"{project_root}/build\"\n",
    "if f\"{project_root}/python\" not in sys.path:\n",
    "    sys.path.insert(0, f\"{project_root}/python\")\n",
    "    \n",
    "\n",
    "sys.path = [p for p in sys.path if not p.startswith(f\"{project_root}/build\")]\n",
    "sys.path.append(f\"{project_root}/build\")\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = f\"{project_root}/build:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb3f4c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[auto_scheduler.SearchTask(0x1c4dd300),\n",
       " auto_scheduler.SearchTask(0x1c4ea560),\n",
       " auto_scheduler.SearchTask(0x1c4f4b50),\n",
       " auto_scheduler.SearchTask(0x1c4c0060),\n",
       " auto_scheduler.SearchTask(0x1c4cda60),\n",
       " auto_scheduler.SearchTask(0x1c4e64f0),\n",
       " auto_scheduler.SearchTask(0x1c5107a0),\n",
       " auto_scheduler.SearchTask(0x1c50fac0),\n",
       " auto_scheduler.SearchTask(0x1c4ce690),\n",
       " auto_scheduler.SearchTask(0x1c4cf4f0),\n",
       " auto_scheduler.SearchTask(0x1c5232d0),\n",
       " auto_scheduler.SearchTask(0x1c54aea0),\n",
       " auto_scheduler.SearchTask(0x1c5395f0),\n",
       " auto_scheduler.SearchTask(0x1c56c1b0),\n",
       " auto_scheduler.SearchTask(0x1c540a50),\n",
       " auto_scheduler.SearchTask(0x1c55e070),\n",
       " auto_scheduler.SearchTask(0x1c4c1550),\n",
       " auto_scheduler.SearchTask(0x1c58c850),\n",
       " auto_scheduler.SearchTask(0x1c595ec0),\n",
       " auto_scheduler.SearchTask(0x1c59fc80),\n",
       " auto_scheduler.SearchTask(0x1c5aa390),\n",
       " auto_scheduler.SearchTask(0x1c591d40),\n",
       " auto_scheduler.SearchTask(0x1c577000),\n",
       " auto_scheduler.SearchTask(0x1c5240c0),\n",
       " auto_scheduler.SearchTask(0x1c5cef40),\n",
       " auto_scheduler.SearchTask(0x1c5dbb10),\n",
       " auto_scheduler.SearchTask(0x1c5bdf40),\n",
       " auto_scheduler.SearchTask(0x1c5e5f40),\n",
       " auto_scheduler.SearchTask(0x1c5d7e40),\n",
       " auto_scheduler.SearchTask(0x1c5e1b20),\n",
       " auto_scheduler.SearchTask(0x1c603a20),\n",
       " auto_scheduler.SearchTask(0x1c5f5b50),\n",
       " auto_scheduler.SearchTask(0x1c613a10),\n",
       " auto_scheduler.SearchTask(0x1c61d6a0),\n",
       " auto_scheduler.SearchTask(0x1c5be870),\n",
       " auto_scheduler.SearchTask(0x1c522f50),\n",
       " auto_scheduler.SearchTask(0x1c5f9aa0),\n",
       " auto_scheduler.SearchTask(0x1c62b3e0),\n",
       " auto_scheduler.SearchTask(0x1c634de0),\n",
       " auto_scheduler.SearchTask(0x1c654b30),\n",
       " auto_scheduler.SearchTask(0x1c63fb00),\n",
       " auto_scheduler.SearchTask(0x1c681b50),\n",
       " auto_scheduler.SearchTask(0x1c5062f0),\n",
       " auto_scheduler.SearchTask(0x1c656330),\n",
       " auto_scheduler.SearchTask(0x1c671d30),\n",
       " auto_scheduler.SearchTask(0x1c6496a0),\n",
       " auto_scheduler.SearchTask(0x1c6a7e40),\n",
       " auto_scheduler.SearchTask(0x1c6c0200),\n",
       " auto_scheduler.SearchTask(0x1c6b5ee0),\n",
       " auto_scheduler.SearchTask(0x1c6d3b40),\n",
       " auto_scheduler.SearchTask(0x1c5598e0),\n",
       " auto_scheduler.SearchTask(0x1c6dd1d0),\n",
       " auto_scheduler.SearchTask(0x1c6ca600),\n",
       " auto_scheduler.SearchTask(0x1c6fba00),\n",
       " auto_scheduler.SearchTask(0x1c705c20),\n",
       " auto_scheduler.SearchTask(0x1c6dd7d0),\n",
       " auto_scheduler.SearchTask(0x1c70b450),\n",
       " auto_scheduler.SearchTask(0x1c7056f0),\n",
       " auto_scheduler.SearchTask(0x1c6603a0),\n",
       " auto_scheduler.SearchTask(0x1c6f2550),\n",
       " auto_scheduler.SearchTask(0x1c741800),\n",
       " auto_scheduler.SearchTask(0x1c73d1a0),\n",
       " auto_scheduler.SearchTask(0x1c741da0),\n",
       " auto_scheduler.SearchTask(0x1c604720),\n",
       " auto_scheduler.SearchTask(0x1c722e00),\n",
       " auto_scheduler.SearchTask(0x1c719ef0),\n",
       " auto_scheduler.SearchTask(0x1c7697e0),\n",
       " auto_scheduler.SearchTask(0x1c69ed60),\n",
       " auto_scheduler.SearchTask(0x1c6bfc00),\n",
       " auto_scheduler.SearchTask(0x1c69e400),\n",
       " auto_scheduler.SearchTask(0x1c69e050),\n",
       " auto_scheduler.SearchTask(0x1c79e120),\n",
       " auto_scheduler.SearchTask(0x1c6a2270),\n",
       " auto_scheduler.SearchTask(0x1c69dae0),\n",
       " auto_scheduler.SearchTask(0x1c7d6c20),\n",
       " auto_scheduler.SearchTask(0x1c740a50),\n",
       " auto_scheduler.SearchTask(0x1c7d2ab0),\n",
       " auto_scheduler.SearchTask(0x1c78bed0),\n",
       " auto_scheduler.SearchTask(0x1c7849f0),\n",
       " auto_scheduler.SearchTask(0x1c7e6940),\n",
       " auto_scheduler.SearchTask(0x1c7b6a20),\n",
       " auto_scheduler.SearchTask(0x1c7a8af0),\n",
       " auto_scheduler.SearchTask(0x1c8385b0),\n",
       " auto_scheduler.SearchTask(0x1c782330),\n",
       " auto_scheduler.SearchTask(0x1c833600),\n",
       " auto_scheduler.SearchTask(0x1c871500),\n",
       " auto_scheduler.SearchTask(0x1c86d6d0),\n",
       " auto_scheduler.SearchTask(0x1c89eb60),\n",
       " auto_scheduler.SearchTask(0x1c899e50),\n",
       " auto_scheduler.SearchTask(0x1c899860),\n",
       " auto_scheduler.SearchTask(0x1c854fe0),\n",
       " auto_scheduler.SearchTask(0x1c89bf70),\n",
       " auto_scheduler.SearchTask(0x1c8f2190),\n",
       " auto_scheduler.SearchTask(0x1c87cee0),\n",
       " auto_scheduler.SearchTask(0x1c8fb4d0),\n",
       " auto_scheduler.SearchTask(0x1c8a11d0),\n",
       " auto_scheduler.SearchTask(0x1c8b8be0),\n",
       " auto_scheduler.SearchTask(0x1c8efdb0),\n",
       " auto_scheduler.SearchTask(0x1c925930),\n",
       " auto_scheduler.SearchTask(0x1c932410),\n",
       " auto_scheduler.SearchTask(0x1c927240),\n",
       " auto_scheduler.SearchTask(0x1c922a10),\n",
       " auto_scheduler.SearchTask(0x1c953740),\n",
       " auto_scheduler.SearchTask(0x1c90e270),\n",
       " auto_scheduler.SearchTask(0x1c92f4b0),\n",
       " auto_scheduler.SearchTask(0x1c91b830),\n",
       " auto_scheduler.SearchTask(0x1c92e110),\n",
       " auto_scheduler.SearchTask(0x1c921a80),\n",
       " auto_scheduler.SearchTask(0x1c910af0),\n",
       " auto_scheduler.SearchTask(0x1c93a690),\n",
       " auto_scheduler.SearchTask(0x1c942fa0),\n",
       " auto_scheduler.SearchTask(0x1c9744b0),\n",
       " auto_scheduler.SearchTask(0x1c93f370),\n",
       " auto_scheduler.SearchTask(0x1c94af50),\n",
       " auto_scheduler.SearchTask(0x1c9bd0a0),\n",
       " auto_scheduler.SearchTask(0x1c947fb0),\n",
       " auto_scheduler.SearchTask(0x1c99c8d0),\n",
       " auto_scheduler.SearchTask(0x1c9d3150),\n",
       " auto_scheduler.SearchTask(0x1c949e50),\n",
       " auto_scheduler.SearchTask(0x1c9905e0),\n",
       " auto_scheduler.SearchTask(0x1c9d27b0),\n",
       " auto_scheduler.SearchTask(0x1ca03680),\n",
       " auto_scheduler.SearchTask(0x1ca297b0),\n",
       " auto_scheduler.SearchTask(0x1ca2cad0),\n",
       " auto_scheduler.SearchTask(0x1c9d1080),\n",
       " auto_scheduler.SearchTask(0x1c9b06f0),\n",
       " auto_scheduler.SearchTask(0x1ca3e380),\n",
       " auto_scheduler.SearchTask(0x1ca34e80),\n",
       " auto_scheduler.SearchTask(0x1c9e7ee0),\n",
       " auto_scheduler.SearchTask(0x1ca21510),\n",
       " auto_scheduler.SearchTask(0x1c9c35d0),\n",
       " auto_scheduler.SearchTask(0x1ca564a0),\n",
       " auto_scheduler.SearchTask(0x1ca4da20),\n",
       " auto_scheduler.SearchTask(0x1ca66d40),\n",
       " auto_scheduler.SearchTask(0x1ca75660),\n",
       " auto_scheduler.SearchTask(0x1ca640e0),\n",
       " auto_scheduler.SearchTask(0x1ca5c440),\n",
       " auto_scheduler.SearchTask(0x1ca8c690),\n",
       " auto_scheduler.SearchTask(0x1caa98f0),\n",
       " auto_scheduler.SearchTask(0x1ca894a0),\n",
       " auto_scheduler.SearchTask(0x1ca8f150),\n",
       " auto_scheduler.SearchTask(0x1cabe840),\n",
       " auto_scheduler.SearchTask(0x1cab0ad0),\n",
       " auto_scheduler.SearchTask(0x1cabad00),\n",
       " auto_scheduler.SearchTask(0x1ca92270),\n",
       " auto_scheduler.SearchTask(0x1ca91a90),\n",
       " auto_scheduler.SearchTask(0x1c7b8eb0),\n",
       " auto_scheduler.SearchTask(0x1cac3310),\n",
       " auto_scheduler.SearchTask(0x1cb1a940),\n",
       " auto_scheduler.SearchTask(0x1cb29ad0),\n",
       " auto_scheduler.SearchTask(0x1cb298b0),\n",
       " auto_scheduler.SearchTask(0x1cb5ab70),\n",
       " auto_scheduler.SearchTask(0x1cafcfc0),\n",
       " auto_scheduler.SearchTask(0x1cac7330),\n",
       " auto_scheduler.SearchTask(0x1c9c3030),\n",
       " auto_scheduler.SearchTask(0x1cb22a10),\n",
       " auto_scheduler.SearchTask(0x1cb3ca90),\n",
       " auto_scheduler.SearchTask(0x1cb80da0),\n",
       " auto_scheduler.SearchTask(0x1cb8ce10),\n",
       " auto_scheduler.SearchTask(0x1cacc610),\n",
       " auto_scheduler.SearchTask(0x1cb90820),\n",
       " auto_scheduler.SearchTask(0x1cb83ce0),\n",
       " auto_scheduler.SearchTask(0x1cbadad0),\n",
       " auto_scheduler.SearchTask(0x1cbb4750),\n",
       " auto_scheduler.SearchTask(0x1cb63180),\n",
       " auto_scheduler.SearchTask(0x1cbb42f0),\n",
       " auto_scheduler.SearchTask(0x1cbd4160),\n",
       " auto_scheduler.SearchTask(0x1cbdf990),\n",
       " auto_scheduler.SearchTask(0x1cbf76d0),\n",
       " auto_scheduler.SearchTask(0x1cbc83a0),\n",
       " auto_scheduler.SearchTask(0x1cbb7bc0),\n",
       " auto_scheduler.SearchTask(0x1cbf56c0),\n",
       " auto_scheduler.SearchTask(0x1cb65810),\n",
       " auto_scheduler.SearchTask(0x1cc2cb80),\n",
       " auto_scheduler.SearchTask(0x1cc17cb0),\n",
       " auto_scheduler.SearchTask(0x1cc288c0),\n",
       " auto_scheduler.SearchTask(0x1cc4a5e0),\n",
       " auto_scheduler.SearchTask(0x1cc0cdb0),\n",
       " auto_scheduler.SearchTask(0x1cc5b070),\n",
       " auto_scheduler.SearchTask(0x1cc64100),\n",
       " auto_scheduler.SearchTask(0x1cc1cab0),\n",
       " auto_scheduler.SearchTask(0x1cc4d390),\n",
       " auto_scheduler.SearchTask(0x1cc7fc20),\n",
       " auto_scheduler.SearchTask(0x1cc9bd10),\n",
       " auto_scheduler.SearchTask(0x1cc07eb0),\n",
       " auto_scheduler.SearchTask(0x1cc9e0f0),\n",
       " auto_scheduler.SearchTask(0x1ccdf720),\n",
       " auto_scheduler.SearchTask(0x1ccc89a0),\n",
       " auto_scheduler.SearchTask(0x1ccc4730),\n",
       " auto_scheduler.SearchTask(0x1cceea60),\n",
       " auto_scheduler.SearchTask(0x1ccd8b10),\n",
       " auto_scheduler.SearchTask(0x1cd0a500),\n",
       " auto_scheduler.SearchTask(0x1ccef060),\n",
       " auto_scheduler.SearchTask(0x1cd470c0),\n",
       " auto_scheduler.SearchTask(0x1cd44580),\n",
       " auto_scheduler.SearchTask(0x1cd6f0d0),\n",
       " auto_scheduler.SearchTask(0x1cd26280),\n",
       " auto_scheduler.SearchTask(0x1cd02e50),\n",
       " auto_scheduler.SearchTask(0x1cd74e30),\n",
       " auto_scheduler.SearchTask(0x1cd89a30),\n",
       " auto_scheduler.SearchTask(0x1cd43430),\n",
       " auto_scheduler.SearchTask(0x1cd9d080),\n",
       " auto_scheduler.SearchTask(0x1cdaa720),\n",
       " auto_scheduler.SearchTask(0x1cd67bf0),\n",
       " auto_scheduler.SearchTask(0x1cce7ea0),\n",
       " auto_scheduler.SearchTask(0x1cdad350),\n",
       " auto_scheduler.SearchTask(0x1ce029e0),\n",
       " auto_scheduler.SearchTask(0x1ce03650),\n",
       " auto_scheduler.SearchTask(0x1cd660d0),\n",
       " auto_scheduler.SearchTask(0x1cdd65d0),\n",
       " auto_scheduler.SearchTask(0x1cd847d0),\n",
       " auto_scheduler.SearchTask(0x1ce26da0),\n",
       " auto_scheduler.SearchTask(0x1ce12ba0),\n",
       " auto_scheduler.SearchTask(0x1ce6e270),\n",
       " auto_scheduler.SearchTask(0x1ce742b0),\n",
       " auto_scheduler.SearchTask(0x1ce68e30),\n",
       " auto_scheduler.SearchTask(0x1ce29e10),\n",
       " auto_scheduler.SearchTask(0x1ce286b0),\n",
       " auto_scheduler.SearchTask(0x1ce51db0),\n",
       " auto_scheduler.SearchTask(0x1ce0d4a0),\n",
       " auto_scheduler.SearchTask(0x1ceca600),\n",
       " auto_scheduler.SearchTask(0x1ceae150),\n",
       " auto_scheduler.SearchTask(0x1ceb68a0),\n",
       " auto_scheduler.SearchTask(0x1cecf8a0),\n",
       " auto_scheduler.SearchTask(0x1ced1ee0),\n",
       " auto_scheduler.SearchTask(0x1cef01a0),\n",
       " auto_scheduler.SearchTask(0x1ced4e50),\n",
       " auto_scheduler.SearchTask(0x1ceb9c90),\n",
       " auto_scheduler.SearchTask(0x1cef1df0),\n",
       " auto_scheduler.SearchTask(0x1cea1d20),\n",
       " auto_scheduler.SearchTask(0x1cf20890),\n",
       " auto_scheduler.SearchTask(0x1cf2f960),\n",
       " auto_scheduler.SearchTask(0x1cf2f740),\n",
       " auto_scheduler.SearchTask(0x1cf1c620),\n",
       " auto_scheduler.SearchTask(0x1cf34240),\n",
       " auto_scheduler.SearchTask(0x1cf91fc0),\n",
       " auto_scheduler.SearchTask(0x1cf56d30),\n",
       " auto_scheduler.SearchTask(0x1cf76490),\n",
       " auto_scheduler.SearchTask(0x1cfadb30),\n",
       " auto_scheduler.SearchTask(0x1cf38230),\n",
       " auto_scheduler.SearchTask(0x1cf81af0),\n",
       " auto_scheduler.SearchTask(0x1cfbf470),\n",
       " auto_scheduler.SearchTask(0x1cfbc2e0),\n",
       " auto_scheduler.SearchTask(0x1cfb9dd0),\n",
       " auto_scheduler.SearchTask(0x1cfed3d0),\n",
       " auto_scheduler.SearchTask(0x1cff6b90),\n",
       " auto_scheduler.SearchTask(0x1cffe030),\n",
       " auto_scheduler.SearchTask(0x1d024380),\n",
       " auto_scheduler.SearchTask(0x1d029210),\n",
       " auto_scheduler.SearchTask(0x1cff9f50),\n",
       " auto_scheduler.SearchTask(0x1cff3f50),\n",
       " auto_scheduler.SearchTask(0x1cfcc5b0),\n",
       " auto_scheduler.SearchTask(0x1d017460),\n",
       " auto_scheduler.SearchTask(0x1d067910),\n",
       " auto_scheduler.SearchTask(0x1d075d70),\n",
       " auto_scheduler.SearchTask(0x1d053590),\n",
       " auto_scheduler.SearchTask(0x1d055c70),\n",
       " auto_scheduler.SearchTask(0x1d0a7230),\n",
       " auto_scheduler.SearchTask(0x1cfe0a30),\n",
       " auto_scheduler.SearchTask(0x1cfdcc80),\n",
       " auto_scheduler.SearchTask(0x1d0f27f0),\n",
       " auto_scheduler.SearchTask(0x1d0d9880),\n",
       " auto_scheduler.SearchTask(0x1d0c6fa0),\n",
       " auto_scheduler.SearchTask(0x1d119c60),\n",
       " auto_scheduler.SearchTask(0x1d0e5e80),\n",
       " auto_scheduler.SearchTask(0x1d123340),\n",
       " auto_scheduler.SearchTask(0x1d08e980),\n",
       " auto_scheduler.SearchTask(0x1d08e620),\n",
       " auto_scheduler.SearchTask(0x1d095900),\n",
       " auto_scheduler.SearchTask(0x1d08fed0),\n",
       " auto_scheduler.SearchTask(0x1d14f120),\n",
       " auto_scheduler.SearchTask(0x1d1528a0),\n",
       " auto_scheduler.SearchTask(0x1d18c240),\n",
       " auto_scheduler.SearchTask(0x1d18afd0),\n",
       " auto_scheduler.SearchTask(0x1d135e50),\n",
       " auto_scheduler.SearchTask(0x1d176fc0),\n",
       " auto_scheduler.SearchTask(0x1d1af840),\n",
       " auto_scheduler.SearchTask(0x1d1c9920),\n",
       " auto_scheduler.SearchTask(0x1d1d59b0),\n",
       " auto_scheduler.SearchTask(0x1d1e5660),\n",
       " auto_scheduler.SearchTask(0x1d1e46a0),\n",
       " auto_scheduler.SearchTask(0x1cfceee0),\n",
       " auto_scheduler.SearchTask(0x1d1aba00),\n",
       " auto_scheduler.SearchTask(0x1d20be90),\n",
       " auto_scheduler.SearchTask(0x1d2be790),\n",
       " auto_scheduler.SearchTask(0x1d2d81a0),\n",
       " auto_scheduler.SearchTask(0x1d2d13b0),\n",
       " auto_scheduler.SearchTask(0x1d215080),\n",
       " auto_scheduler.SearchTask(0x1d1beb50),\n",
       " auto_scheduler.SearchTask(0x1d28e1f0),\n",
       " auto_scheduler.SearchTask(0x1d297620),\n",
       " auto_scheduler.SearchTask(0x1d296660),\n",
       " auto_scheduler.SearchTask(0x1d2ae400),\n",
       " auto_scheduler.SearchTask(0x1d2ada70),\n",
       " auto_scheduler.SearchTask(0x1d316400),\n",
       " auto_scheduler.SearchTask(0x1d3a6b60),\n",
       " auto_scheduler.SearchTask(0x1d3ade70),\n",
       " auto_scheduler.SearchTask(0x1d225370),\n",
       " auto_scheduler.SearchTask(0x1d3adc50),\n",
       " auto_scheduler.SearchTask(0x1d312080),\n",
       " auto_scheduler.SearchTask(0x1d357410),\n",
       " auto_scheduler.SearchTask(0x1d372200),\n",
       " auto_scheduler.SearchTask(0x1d31fdb0),\n",
       " auto_scheduler.SearchTask(0x1d328850),\n",
       " auto_scheduler.SearchTask(0x1d3cbfb0),\n",
       " auto_scheduler.SearchTask(0x1d3d4540),\n",
       " auto_scheduler.SearchTask(0x1d3b9190),\n",
       " auto_scheduler.SearchTask(0x1d391de0),\n",
       " auto_scheduler.SearchTask(0x1d3fa170),\n",
       " auto_scheduler.SearchTask(0x1d365200),\n",
       " auto_scheduler.SearchTask(0x1d411bb0),\n",
       " auto_scheduler.SearchTask(0x1d364dd0),\n",
       " auto_scheduler.SearchTask(0x1d4151e0),\n",
       " auto_scheduler.SearchTask(0x1d363be0),\n",
       " auto_scheduler.SearchTask(0x1d43aca0),\n",
       " auto_scheduler.SearchTask(0x1d3deca0),\n",
       " auto_scheduler.SearchTask(0x1d475210),\n",
       " auto_scheduler.SearchTask(0x1d49c1d0),\n",
       " auto_scheduler.SearchTask(0x1d470460),\n",
       " auto_scheduler.SearchTask(0x1d458780),\n",
       " auto_scheduler.SearchTask(0x1d488020),\n",
       " auto_scheduler.SearchTask(0x1d4c7a50),\n",
       " auto_scheduler.SearchTask(0x1d4819a0),\n",
       " auto_scheduler.SearchTask(0x1d4cccd0),\n",
       " auto_scheduler.SearchTask(0x1d4e4f40),\n",
       " auto_scheduler.SearchTask(0x1d485190),\n",
       " auto_scheduler.SearchTask(0x1d4b8140),\n",
       " auto_scheduler.SearchTask(0x1d50bae0),\n",
       " auto_scheduler.SearchTask(0x1d516090),\n",
       " auto_scheduler.SearchTask(0x1d511150),\n",
       " auto_scheduler.SearchTask(0x1d506d30),\n",
       " auto_scheduler.SearchTask(0x1d54dbf0),\n",
       " auto_scheduler.SearchTask(0x1d56b650),\n",
       " auto_scheduler.SearchTask(0x1d54bc20),\n",
       " auto_scheduler.SearchTask(0x1d58a920),\n",
       " auto_scheduler.SearchTask(0x1d585030),\n",
       " auto_scheduler.SearchTask(0x1d55f580),\n",
       " auto_scheduler.SearchTask(0x1d52a870),\n",
       " auto_scheduler.SearchTask(0x1d5c3860),\n",
       " auto_scheduler.SearchTask(0x1d5b33a0),\n",
       " auto_scheduler.SearchTask(0x1d5d69c0),\n",
       " auto_scheduler.SearchTask(0x1d6058d0),\n",
       " auto_scheduler.SearchTask(0x1d5bdf60),\n",
       " auto_scheduler.SearchTask(0x1d62e6f0),\n",
       " auto_scheduler.SearchTask(0x1d52f5d0),\n",
       " auto_scheduler.SearchTask(0x1d639630),\n",
       " auto_scheduler.SearchTask(0x1d63ea60),\n",
       " auto_scheduler.SearchTask(0x1d6222e0),\n",
       " auto_scheduler.SearchTask(0x1d64bd70),\n",
       " auto_scheduler.SearchTask(0x1cbf3a80),\n",
       " auto_scheduler.SearchTask(0x1d66cac0),\n",
       " auto_scheduler.SearchTask(0x1d5ecca0),\n",
       " auto_scheduler.SearchTask(0x1d691490),\n",
       " auto_scheduler.SearchTask(0x1d695c80),\n",
       " auto_scheduler.SearchTask(0x1d6b3510),\n",
       " auto_scheduler.SearchTask(0x1d6b7e60),\n",
       " auto_scheduler.SearchTask(0x1d6d1db0),\n",
       " auto_scheduler.SearchTask(0x1d68c190),\n",
       " auto_scheduler.SearchTask(0x1d6bd0f0),\n",
       " auto_scheduler.SearchTask(0x1d762a90),\n",
       " auto_scheduler.SearchTask(0x1d6ed2a0),\n",
       " auto_scheduler.SearchTask(0x1d6fc210),\n",
       " auto_scheduler.SearchTask(0x1d6fbff0),\n",
       " auto_scheduler.SearchTask(0x1d7005f0),\n",
       " auto_scheduler.SearchTask(0x1d771540),\n",
       " auto_scheduler.SearchTask(0x1d7556b0),\n",
       " auto_scheduler.SearchTask(0x1d79bf50),\n",
       " auto_scheduler.SearchTask(0x1d7c1200),\n",
       " auto_scheduler.SearchTask(0x1d79ebc0),\n",
       " auto_scheduler.SearchTask(0x1d742880),\n",
       " auto_scheduler.SearchTask(0x1d7da210),\n",
       " auto_scheduler.SearchTask(0x1d7efd70),\n",
       " auto_scheduler.SearchTask(0x1d7e34a0),\n",
       " auto_scheduler.SearchTask(0x1d7b9970),\n",
       " auto_scheduler.SearchTask(0x1d834ea0),\n",
       " auto_scheduler.SearchTask(0x1d83d130),\n",
       " auto_scheduler.SearchTask(0x1d8aa360),\n",
       " auto_scheduler.SearchTask(0x1d8b28f0),\n",
       " auto_scheduler.SearchTask(0x1d8b5130),\n",
       " auto_scheduler.SearchTask(0x1d8089d0),\n",
       " auto_scheduler.SearchTask(0x1d8a36a0),\n",
       " auto_scheduler.SearchTask(0x1d88f960),\n",
       " auto_scheduler.SearchTask(0x1d887970),\n",
       " auto_scheduler.SearchTask(0x1d8531c0),\n",
       " auto_scheduler.SearchTask(0x1d8e4fe0),\n",
       " auto_scheduler.SearchTask(0x1d893ed0),\n",
       " auto_scheduler.SearchTask(0x1d9016c0),\n",
       " auto_scheduler.SearchTask(0x1d8d11e0),\n",
       " auto_scheduler.SearchTask(0x1d87cbb0),\n",
       " auto_scheduler.SearchTask(0x1d8de340),\n",
       " auto_scheduler.SearchTask(0x1d92f800),\n",
       " auto_scheduler.SearchTask(0x1d943bc0),\n",
       " auto_scheduler.SearchTask(0x1d9393d0),\n",
       " auto_scheduler.SearchTask(0x1d93a600),\n",
       " auto_scheduler.SearchTask(0x1d93e0b0),\n",
       " auto_scheduler.SearchTask(0x1d91bd00),\n",
       " auto_scheduler.SearchTask(0x1d97c530),\n",
       " auto_scheduler.SearchTask(0x1d9870b0),\n",
       " auto_scheduler.SearchTask(0x1d988000),\n",
       " auto_scheduler.SearchTask(0x1d9a19b0),\n",
       " auto_scheduler.SearchTask(0x1d9af980),\n",
       " auto_scheduler.SearchTask(0x1d9b2ed0),\n",
       " auto_scheduler.SearchTask(0x1d978a80),\n",
       " auto_scheduler.SearchTask(0x1d9b3d40),\n",
       " auto_scheduler.SearchTask(0x1d9f4480),\n",
       " auto_scheduler.SearchTask(0x1d9d88a0),\n",
       " auto_scheduler.SearchTask(0x1d9efcb0),\n",
       " auto_scheduler.SearchTask(0x1d9ce380),\n",
       " auto_scheduler.SearchTask(0x1da3d510),\n",
       " auto_scheduler.SearchTask(0x1da37670),\n",
       " auto_scheduler.SearchTask(0x1d9c7420),\n",
       " auto_scheduler.SearchTask(0x1da2af60),\n",
       " auto_scheduler.SearchTask(0x1da741d0),\n",
       " auto_scheduler.SearchTask(0x1da90e90),\n",
       " auto_scheduler.SearchTask(0x1da9ae50),\n",
       " auto_scheduler.SearchTask(0x1da51940),\n",
       " auto_scheduler.SearchTask(0x1da4f750),\n",
       " auto_scheduler.SearchTask(0x1da977e0),\n",
       " auto_scheduler.SearchTask(0x1dad85d0),\n",
       " auto_scheduler.SearchTask(0x1da6fe30),\n",
       " auto_scheduler.SearchTask(0x1daf0fe0),\n",
       " auto_scheduler.SearchTask(0x1daf0650),\n",
       " auto_scheduler.SearchTask(0x1daea780),\n",
       " auto_scheduler.SearchTask(0x1dae40d0),\n",
       " auto_scheduler.SearchTask(0x1db36d30),\n",
       " auto_scheduler.SearchTask(0x1db5a0f0),\n",
       " auto_scheduler.SearchTask(0x1db56ea0),\n",
       " auto_scheduler.SearchTask(0x1db15d90),\n",
       " auto_scheduler.SearchTask(0x1db7b750),\n",
       " auto_scheduler.SearchTask(0x1db6e150),\n",
       " auto_scheduler.SearchTask(0x1db8fa80),\n",
       " auto_scheduler.SearchTask(0x1dbb6610),\n",
       " auto_scheduler.SearchTask(0x1dbdb990),\n",
       " auto_scheduler.SearchTask(0x1dbca020),\n",
       " auto_scheduler.SearchTask(0x1dbb4b70),\n",
       " auto_scheduler.SearchTask(0x1dbb5380),\n",
       " auto_scheduler.SearchTask(0x1dc05e40),\n",
       " auto_scheduler.SearchTask(0x1dc13100),\n",
       " auto_scheduler.SearchTask(0x1dc22ea0),\n",
       " auto_scheduler.SearchTask(0x1dc32120),\n",
       " auto_scheduler.SearchTask(0x1dc4e730),\n",
       " auto_scheduler.SearchTask(0x1db9b740),\n",
       " auto_scheduler.SearchTask(0x1dc610b0),\n",
       " auto_scheduler.SearchTask(0x1dc57220),\n",
       " auto_scheduler.SearchTask(0x1dc9b490),\n",
       " auto_scheduler.SearchTask(0x1dc454b0),\n",
       " auto_scheduler.SearchTask(0x1dc97400),\n",
       " auto_scheduler.SearchTask(0x1dc73740),\n",
       " auto_scheduler.SearchTask(0x1dca3f40),\n",
       " auto_scheduler.SearchTask(0x1dccb6e0),\n",
       " auto_scheduler.SearchTask(0x1dcec770),\n",
       " auto_scheduler.SearchTask(0x1dce6450),\n",
       " auto_scheduler.SearchTask(0x1dd11d20),\n",
       " auto_scheduler.SearchTask(0x1dd119e0),\n",
       " auto_scheduler.SearchTask(0x1dd35730),\n",
       " auto_scheduler.SearchTask(0x1dd1de20),\n",
       " auto_scheduler.SearchTask(0x1dd15160),\n",
       " auto_scheduler.SearchTask(0x1dd5c5a0),\n",
       " auto_scheduler.SearchTask(0x1dcfddc0),\n",
       " auto_scheduler.SearchTask(0x1dcf5d60),\n",
       " auto_scheduler.SearchTask(0x1dd8f2e0),\n",
       " auto_scheduler.SearchTask(0x1dd970c0),\n",
       " auto_scheduler.SearchTask(0x1de1bff0),\n",
       " auto_scheduler.SearchTask(0x1de1d700),\n",
       " auto_scheduler.SearchTask(0x1de037b0),\n",
       " auto_scheduler.SearchTask(0x1dda5b20),\n",
       " auto_scheduler.SearchTask(0x1de100c0),\n",
       " auto_scheduler.SearchTask(0x1de0fac0),\n",
       " auto_scheduler.SearchTask(0x1de2a380),\n",
       " auto_scheduler.SearchTask(0x1de43c40),\n",
       " auto_scheduler.SearchTask(0x1de436e0),\n",
       " auto_scheduler.SearchTask(0x1de6c550),\n",
       " auto_scheduler.SearchTask(0x1ddde540),\n",
       " auto_scheduler.SearchTask(0x1de6d350),\n",
       " auto_scheduler.SearchTask(0x1de8d3c0),\n",
       " auto_scheduler.SearchTask(0x1de97000),\n",
       " auto_scheduler.SearchTask(0x1de9dc00),\n",
       " auto_scheduler.SearchTask(0x1deb6100),\n",
       " auto_scheduler.SearchTask(0x1deb2230),\n",
       " auto_scheduler.SearchTask(0x1de82c50),\n",
       " auto_scheduler.SearchTask(0x1de895b0),\n",
       " auto_scheduler.SearchTask(0x1dee4fe0),\n",
       " auto_scheduler.SearchTask(0x1def8f00),\n",
       " auto_scheduler.SearchTask(0x1def51a0),\n",
       " auto_scheduler.SearchTask(0x1def1c90),\n",
       " auto_scheduler.SearchTask(0x1df25200),\n",
       " auto_scheduler.SearchTask(0x1df04f60),\n",
       " auto_scheduler.SearchTask(0x1decb100),\n",
       " auto_scheduler.SearchTask(0x1dedbcb0),\n",
       " auto_scheduler.SearchTask(0x1df71490),\n",
       " auto_scheduler.SearchTask(0x1df80f60),\n",
       " auto_scheduler.SearchTask(0x1df80490),\n",
       " auto_scheduler.SearchTask(0x1df9e960),\n",
       " auto_scheduler.SearchTask(0x1df95990),\n",
       " auto_scheduler.SearchTask(0x1df63d00),\n",
       " auto_scheduler.SearchTask(0x1df5f060),\n",
       " auto_scheduler.SearchTask(0x1dfd47b0),\n",
       " auto_scheduler.SearchTask(0x1dfe86f0),\n",
       " auto_scheduler.SearchTask(0x1dfe2880),\n",
       " auto_scheduler.SearchTask(0x1dfe1ec0),\n",
       " auto_scheduler.SearchTask(0x1dffe650),\n",
       " auto_scheduler.SearchTask(0x1e084f20),\n",
       " auto_scheduler.SearchTask(0x1e0f7c20),\n",
       " auto_scheduler.SearchTask(0x1e0f83a0),\n",
       " auto_scheduler.SearchTask(0x1dff35f0),\n",
       " auto_scheduler.SearchTask(0x1e027870),\n",
       " auto_scheduler.SearchTask(0x1dfc3fe0),\n",
       " auto_scheduler.SearchTask(0x1e072fa0),\n",
       " auto_scheduler.SearchTask(0x1dfcf780),\n",
       " auto_scheduler.SearchTask(0x1e10ee80),\n",
       " auto_scheduler.SearchTask(0x1e0ee9a0),\n",
       " auto_scheduler.SearchTask(0x1e011220),\n",
       " auto_scheduler.SearchTask(0x1e12c210),\n",
       " auto_scheduler.SearchTask(0x1e13ced0),\n",
       " auto_scheduler.SearchTask(0x1e141660),\n",
       " auto_scheduler.SearchTask(0x1e156a60),\n",
       " auto_scheduler.SearchTask(0x1e150ac0),\n",
       " auto_scheduler.SearchTask(0x1e1823e0),\n",
       " auto_scheduler.SearchTask(0x1e159710),\n",
       " auto_scheduler.SearchTask(0x1e1a7160),\n",
       " auto_scheduler.SearchTask(0x1e1a9f90),\n",
       " auto_scheduler.SearchTask(0x1e179c50),\n",
       " auto_scheduler.SearchTask(0x1e1cb230),\n",
       " auto_scheduler.SearchTask(0x1e124b50),\n",
       " auto_scheduler.SearchTask(0x1e19af60),\n",
       " auto_scheduler.SearchTask(0x1e19c3f0),\n",
       " auto_scheduler.SearchTask(0x1e1925f0),\n",
       " auto_scheduler.SearchTask(0x1e2157a0),\n",
       " auto_scheduler.SearchTask(0x1e28a680),\n",
       " auto_scheduler.SearchTask(0x1e2aa510),\n",
       " auto_scheduler.SearchTask(0x1e2b23c0),\n",
       " auto_scheduler.SearchTask(0x1e1da5f0),\n",
       " auto_scheduler.SearchTask(0x1e22f0d0),\n",
       " auto_scheduler.SearchTask(0x1e2a1840),\n",
       " auto_scheduler.SearchTask(0x1e1950a0),\n",
       " auto_scheduler.SearchTask(0x1e29cf30),\n",
       " auto_scheduler.SearchTask(0x1e2f43e0),\n",
       " auto_scheduler.SearchTask(0x1e2ee890),\n",
       " auto_scheduler.SearchTask(0x1e2e6e50),\n",
       " auto_scheduler.SearchTask(0x1e225020),\n",
       " auto_scheduler.SearchTask(0x1e30b120),\n",
       " auto_scheduler.SearchTask(0x1e2ffc60),\n",
       " auto_scheduler.SearchTask(0x1e3063f0),\n",
       " auto_scheduler.SearchTask(0x1e36f320),\n",
       " auto_scheduler.SearchTask(0x1e34ab70),\n",
       " auto_scheduler.SearchTask(0x1e301e70),\n",
       " auto_scheduler.SearchTask(0x1e388320),\n",
       " auto_scheduler.SearchTask(0x1e38fbe0),\n",
       " auto_scheduler.SearchTask(0x1e3b7f00),\n",
       " auto_scheduler.SearchTask(0x1e36bea0),\n",
       " auto_scheduler.SearchTask(0x1e3c9940),\n",
       " auto_scheduler.SearchTask(0x1e3c6470),\n",
       " auto_scheduler.SearchTask(0x1e3e65b0),\n",
       " auto_scheduler.SearchTask(0x1e3e9090),\n",
       " auto_scheduler.SearchTask(0x1e3ec410),\n",
       " auto_scheduler.SearchTask(0x1e30e330),\n",
       " auto_scheduler.SearchTask(0x1e3f2e00),\n",
       " auto_scheduler.SearchTask(0x1e415310),\n",
       " auto_scheduler.SearchTask(0x1e424fc0),\n",
       " auto_scheduler.SearchTask(0x1e3ef450),\n",
       " auto_scheduler.SearchTask(0x1e421cb0),\n",
       " auto_scheduler.SearchTask(0x1e452950),\n",
       " auto_scheduler.SearchTask(0x1e44eff0),\n",
       " auto_scheduler.SearchTask(0x1e46bbb0),\n",
       " auto_scheduler.SearchTask(0x1e47ada0),\n",
       " auto_scheduler.SearchTask(0x1e4715f0),\n",
       " auto_scheduler.SearchTask(0x1e444b90),\n",
       " auto_scheduler.SearchTask(0x1e4a9e30),\n",
       " auto_scheduler.SearchTask(0x1e4cf900),\n",
       " auto_scheduler.SearchTask(0x1e4c7620),\n",
       " auto_scheduler.SearchTask(0x1e4f0f10),\n",
       " auto_scheduler.SearchTask(0x1e4f5c60),\n",
       " auto_scheduler.SearchTask(0x1e4e3fa0),\n",
       " auto_scheduler.SearchTask(0x1e5143d0),\n",
       " auto_scheduler.SearchTask(0x1e511f20),\n",
       " auto_scheduler.SearchTask(0x1e533ed0),\n",
       " auto_scheduler.SearchTask(0x1e533cb0),\n",
       " auto_scheduler.SearchTask(0x1e55be20),\n",
       " auto_scheduler.SearchTask(0x1e564230),\n",
       " auto_scheduler.SearchTask(0x1e565d90),\n",
       " auto_scheduler.SearchTask(0x1e582400),\n",
       " auto_scheduler.SearchTask(0x1e5478c0),\n",
       " auto_scheduler.SearchTask(0x1e5a1c00),\n",
       " auto_scheduler.SearchTask(0x1e4bab70),\n",
       " auto_scheduler.SearchTask(0x1e59cff0),\n",
       " auto_scheduler.SearchTask(0x1d2f5980),\n",
       " auto_scheduler.SearchTask(0x1e542630),\n",
       " auto_scheduler.SearchTask(0x1e609cf0),\n",
       " auto_scheduler.SearchTask(0x1d2ee750),\n",
       " auto_scheduler.SearchTask(0x1e63f890),\n",
       " auto_scheduler.SearchTask(0x1e637040),\n",
       " auto_scheduler.SearchTask(0x1e65b2a0),\n",
       " auto_scheduler.SearchTask(0x1e610d70),\n",
       " auto_scheduler.SearchTask(0x1e67aa20),\n",
       " auto_scheduler.SearchTask(0x1e67f130),\n",
       " auto_scheduler.SearchTask(0x1e6220d0),\n",
       " auto_scheduler.SearchTask(0x1e620b70),\n",
       " auto_scheduler.SearchTask(0x1e6c4520),\n",
       " auto_scheduler.SearchTask(0x1e6c64c0),\n",
       " auto_scheduler.SearchTask(0x1e6b4370),\n",
       " auto_scheduler.SearchTask(0x1e6eccc0),\n",
       " auto_scheduler.SearchTask(0x1e6727f0),\n",
       " auto_scheduler.SearchTask(0x1e722ef0),\n",
       " auto_scheduler.SearchTask(0x1e665770),\n",
       " auto_scheduler.SearchTask(0x1e7312c0),\n",
       " auto_scheduler.SearchTask(0x1e73c5b0),\n",
       " auto_scheduler.SearchTask(0x1e749b30),\n",
       " auto_scheduler.SearchTask(0x1e760e70),\n",
       " auto_scheduler.SearchTask(0x1e74d680),\n",
       " auto_scheduler.SearchTask(0x1e779470),\n",
       " auto_scheduler.SearchTask(0x1e79bfd0),\n",
       " auto_scheduler.SearchTask(0x1e795460),\n",
       " auto_scheduler.SearchTask(0x1e7c4f30),\n",
       " auto_scheduler.SearchTask(0x1e6faff0),\n",
       " auto_scheduler.SearchTask(0x1e784b60),\n",
       " auto_scheduler.SearchTask(0x1e7860b0),\n",
       " auto_scheduler.SearchTask(0x1e7af190),\n",
       " auto_scheduler.SearchTask(0x1e7dab40),\n",
       " auto_scheduler.SearchTask(0x1e8156a0),\n",
       " auto_scheduler.SearchTask(0x1e824c50),\n",
       " auto_scheduler.SearchTask(0x1e7f3ae0),\n",
       " auto_scheduler.SearchTask(0x1e80f100),\n",
       " auto_scheduler.SearchTask(0x1e83f920),\n",
       " auto_scheduler.SearchTask(0x1e839f70),\n",
       " auto_scheduler.SearchTask(0x1e870a70),\n",
       " auto_scheduler.SearchTask(0x1e833f10),\n",
       " auto_scheduler.SearchTask(0x1e863de0),\n",
       " auto_scheduler.SearchTask(0x1e89a920),\n",
       " auto_scheduler.SearchTask(0x1e927ac0),\n",
       " auto_scheduler.SearchTask(0x1e91c7b0),\n",
       " auto_scheduler.SearchTask(0x1e896e90),\n",
       " auto_scheduler.SearchTask(0x1e91a1e0),\n",
       " auto_scheduler.SearchTask(0x1e90a440),\n",
       " auto_scheduler.SearchTask(0x1e8e53d0),\n",
       " auto_scheduler.SearchTask(0x1e935e30),\n",
       " auto_scheduler.SearchTask(0x1e8ec570),\n",
       " auto_scheduler.SearchTask(0x1e8f0f70),\n",
       " auto_scheduler.SearchTask(0x1e970be0),\n",
       " auto_scheduler.SearchTask(0x1e8d95f0),\n",
       " auto_scheduler.SearchTask(0x1e948030),\n",
       " auto_scheduler.SearchTask(0x1e994bb0),\n",
       " auto_scheduler.SearchTask(0x1e942b40),\n",
       " auto_scheduler.SearchTask(0x1e9b4cd0),\n",
       " auto_scheduler.SearchTask(0x1e9b1a60),\n",
       " auto_scheduler.SearchTask(0x1e9be1a0),\n",
       " auto_scheduler.SearchTask(0x1e9b89a0),\n",
       " auto_scheduler.SearchTask(0x1e9f15a0),\n",
       " auto_scheduler.SearchTask(0x1ea10c80),\n",
       " auto_scheduler.SearchTask(0x1e9ea150),\n",
       " auto_scheduler.SearchTask(0x1e9ede40),\n",
       " auto_scheduler.SearchTask(0x1e9a88e0),\n",
       " auto_scheduler.SearchTask(0x1ea21df0),\n",
       " auto_scheduler.SearchTask(0x1ea1a560),\n",
       " auto_scheduler.SearchTask(0x1ea46480),\n",
       " auto_scheduler.SearchTask(0x1ea620b0),\n",
       " auto_scheduler.SearchTask(0x1ea64210),\n",
       " auto_scheduler.SearchTask(0x1ea797a0),\n",
       " auto_scheduler.SearchTask(0x1ea791a0),\n",
       " auto_scheduler.SearchTask(0x1ea73170),\n",
       " auto_scheduler.SearchTask(0x1eaa6360),\n",
       " auto_scheduler.SearchTask(0x1eaa4080),\n",
       " auto_scheduler.SearchTask(0x1eac8120),\n",
       " auto_scheduler.SearchTask(0x1eac0d90),\n",
       " auto_scheduler.SearchTask(0x1eaf4ea0),\n",
       " auto_scheduler.SearchTask(0x1eaddb10),\n",
       " auto_scheduler.SearchTask(0x1eb0e8a0),\n",
       " auto_scheduler.SearchTask(0x1eade810),\n",
       " auto_scheduler.SearchTask(0x1ea55980),\n",
       " auto_scheduler.SearchTask(0x1eb40c40),\n",
       " auto_scheduler.SearchTask(0x1eb21aa0),\n",
       " auto_scheduler.SearchTask(0x1eaedaa0),\n",
       " auto_scheduler.SearchTask(0x1eb69c50),\n",
       " auto_scheduler.SearchTask(0x1eb7c800),\n",
       " auto_scheduler.SearchTask(0x1eb91480),\n",
       " auto_scheduler.SearchTask(0x1ec0f740),\n",
       " auto_scheduler.SearchTask(0x1eb80a10),\n",
       " auto_scheduler.SearchTask(0x1ebb8010),\n",
       " auto_scheduler.SearchTask(0x1ebe1990),\n",
       " auto_scheduler.SearchTask(0x1ebe39c0),\n",
       " auto_scheduler.SearchTask(0x1ebf0f50),\n",
       " auto_scheduler.SearchTask(0x1ec307f0),\n",
       " auto_scheduler.SearchTask(0x1eb4e350),\n",
       " auto_scheduler.SearchTask(0x1eb86400),\n",
       " auto_scheduler.SearchTask(0x1eba98a0),\n",
       " auto_scheduler.SearchTask(0x1ebf9f80),\n",
       " auto_scheduler.SearchTask(0x1eb99e50),\n",
       " auto_scheduler.SearchTask(0x1ec42590),\n",
       " auto_scheduler.SearchTask(0x1ecaa1f0),\n",
       " auto_scheduler.SearchTask(0x1ecb7ac0),\n",
       " auto_scheduler.SearchTask(0x1ecd0780),\n",
       " auto_scheduler.SearchTask(0x1ecce460),\n",
       " auto_scheduler.SearchTask(0x1ecf0340),\n",
       " auto_scheduler.SearchTask(0x1ec40bf0),\n",
       " auto_scheduler.SearchTask(0x1ed02750),\n",
       " auto_scheduler.SearchTask(0x1ed00b30),\n",
       " auto_scheduler.SearchTask(0x1ed29410),\n",
       " auto_scheduler.SearchTask(0x1ed08be0),\n",
       " auto_scheduler.SearchTask(0x1ed53280),\n",
       " auto_scheduler.SearchTask(0x1ed5dd90),\n",
       " auto_scheduler.SearchTask(0x1ed26200),\n",
       " auto_scheduler.SearchTask(0x1ed77960),\n",
       " auto_scheduler.SearchTask(0x1ed8e550),\n",
       " auto_scheduler.SearchTask(0x1ed9c700),\n",
       " auto_scheduler.SearchTask(0x1edab790),\n",
       " auto_scheduler.SearchTask(0x1edb5900),\n",
       " auto_scheduler.SearchTask(0x1edb0410),\n",
       " auto_scheduler.SearchTask(0x1edda990),\n",
       " auto_scheduler.SearchTask(0x1edef550),\n",
       " auto_scheduler.SearchTask(0x1ed7c720),\n",
       " auto_scheduler.SearchTask(0x1ee01c00),\n",
       " auto_scheduler.SearchTask(0x1ee16850),\n",
       " auto_scheduler.SearchTask(0x1ee397e0),\n",
       " auto_scheduler.SearchTask(0x1ee34d40),\n",
       " auto_scheduler.SearchTask(0x1edfbe50),\n",
       " auto_scheduler.SearchTask(0x1ee5ae40),\n",
       " auto_scheduler.SearchTask(0x1ee556b0),\n",
       " auto_scheduler.SearchTask(0x1eec5650),\n",
       " auto_scheduler.SearchTask(0x1edce5e0),\n",
       " auto_scheduler.SearchTask(0x1eeea8c0),\n",
       " auto_scheduler.SearchTask(0x1edff380),\n",
       " auto_scheduler.SearchTask(0x1edca6c0),\n",
       " auto_scheduler.SearchTask(0x1eef4370),\n",
       " auto_scheduler.SearchTask(0x1eee3260),\n",
       " auto_scheduler.SearchTask(0x1ef07250),\n",
       " auto_scheduler.SearchTask(0x1ef175e0),\n",
       " auto_scheduler.SearchTask(0x1ef26640),\n",
       " auto_scheduler.SearchTask(0x1ee721d0),\n",
       " auto_scheduler.SearchTask(0x1ee72730),\n",
       " auto_scheduler.SearchTask(0x1ef58300),\n",
       " auto_scheduler.SearchTask(0x1eefa1c0),\n",
       " auto_scheduler.SearchTask(0x1ef39bc0),\n",
       " auto_scheduler.SearchTask(0x1ef8f730),\n",
       " auto_scheduler.SearchTask(0x1ef3e5c0),\n",
       " auto_scheduler.SearchTask(0x1efad1a0),\n",
       " auto_scheduler.SearchTask(0x1ef35910),\n",
       " auto_scheduler.SearchTask(0x1efcb6b0),\n",
       " auto_scheduler.SearchTask(0x1efe0f00),\n",
       " auto_scheduler.SearchTask(0x1efeee20),\n",
       " auto_scheduler.SearchTask(0x1efd9930),\n",
       " auto_scheduler.SearchTask(0x1f012ba0),\n",
       " auto_scheduler.SearchTask(0x1effb360),\n",
       " auto_scheduler.SearchTask(0x1f02adf0),\n",
       " auto_scheduler.SearchTask(0x1f03a390),\n",
       " auto_scheduler.SearchTask(0x1efff620),\n",
       " auto_scheduler.SearchTask(0x1f051ac0),\n",
       " auto_scheduler.SearchTask(0x1f0653b0),\n",
       " auto_scheduler.SearchTask(0x1f020350),\n",
       " auto_scheduler.SearchTask(0x1f060a90),\n",
       " auto_scheduler.SearchTask(0x1f099250),\n",
       " auto_scheduler.SearchTask(0x1f091dd0),\n",
       " auto_scheduler.SearchTask(0x1f094c50),\n",
       " auto_scheduler.SearchTask(0x1f06fd50),\n",
       " auto_scheduler.SearchTask(0x1f0dda00),\n",
       " auto_scheduler.SearchTask(0x1f096e90),\n",
       " auto_scheduler.SearchTask(0x1f063d50),\n",
       " auto_scheduler.SearchTask(0x1f0d6140),\n",
       " auto_scheduler.SearchTask(0x1f0f5230),\n",
       " auto_scheduler.SearchTask(0x1f0b8c70),\n",
       " auto_scheduler.SearchTask(0x1f0ae0f0),\n",
       " auto_scheduler.SearchTask(0x1f0ee4b0),\n",
       " auto_scheduler.SearchTask(0x1f12cd10),\n",
       " auto_scheduler.SearchTask(0x1f1453a0),\n",
       " auto_scheduler.SearchTask(0x1f123280),\n",
       " auto_scheduler.SearchTask(0x1f124720),\n",
       " auto_scheduler.SearchTask(0x1f1784d0),\n",
       " auto_scheduler.SearchTask(0x1f186800),\n",
       " auto_scheduler.SearchTask(0x1f195110),\n",
       " auto_scheduler.SearchTask(0x1f1a1ed0),\n",
       " auto_scheduler.SearchTask(0x1f14bff0),\n",
       " auto_scheduler.SearchTask(0x1f181710),\n",
       " auto_scheduler.SearchTask(0x1f1a1b40),\n",
       " auto_scheduler.SearchTask(0x1f1d8570),\n",
       " auto_scheduler.SearchTask(0x1f1bde20),\n",
       " auto_scheduler.SearchTask(0x1f2191f0),\n",
       " auto_scheduler.SearchTask(0x1f210b30),\n",
       " auto_scheduler.SearchTask(0x1f234990),\n",
       " auto_scheduler.SearchTask(0x1f229bd0),\n",
       " auto_scheduler.SearchTask(0x1f252e30),\n",
       " auto_scheduler.SearchTask(0x1f26ac80),\n",
       " auto_scheduler.SearchTask(0x1f27a590),\n",
       " auto_scheduler.SearchTask(0x1f26e070),\n",
       " auto_scheduler.SearchTask(0x1f293390),\n",
       " auto_scheduler.SearchTask(0x1f2a2b40),\n",
       " auto_scheduler.SearchTask(0x1f2b0870),\n",
       " auto_scheduler.SearchTask(0x1f270da0),\n",
       " auto_scheduler.SearchTask(0x1f1efc90),\n",
       " auto_scheduler.SearchTask(0x1f1e7a80),\n",
       " auto_scheduler.SearchTask(0x1f1edaa0),\n",
       " auto_scheduler.SearchTask(0x1f2c45f0),\n",
       " auto_scheduler.SearchTask(0x1f30be80),\n",
       " auto_scheduler.SearchTask(0x1f3199f0),\n",
       " auto_scheduler.SearchTask(0x1f33a6a0),\n",
       " auto_scheduler.SearchTask(0x1f34a2b0),\n",
       " auto_scheduler.SearchTask(0x1f34ba00),\n",
       " auto_scheduler.SearchTask(0x1f32f410),\n",
       " auto_scheduler.SearchTask(0x1f36a270),\n",
       " auto_scheduler.SearchTask(0x1f328e00),\n",
       " auto_scheduler.SearchTask(0x1f31eae0),\n",
       " auto_scheduler.SearchTask(0x1f323eb0),\n",
       " auto_scheduler.SearchTask(0x1f325830),\n",
       " auto_scheduler.SearchTask(0x1f37b620),\n",
       " auto_scheduler.SearchTask(0x1f374e40),\n",
       " auto_scheduler.SearchTask(0x1f3dd870),\n",
       " auto_scheduler.SearchTask(0x1f3f64e0),\n",
       " auto_scheduler.SearchTask(0x1f3f6200),\n",
       " auto_scheduler.SearchTask(0x1f4089e0),\n",
       " auto_scheduler.SearchTask(0x1f416010),\n",
       " auto_scheduler.SearchTask(0x1f3ec2d0),\n",
       " auto_scheduler.SearchTask(0x1f3e3540),\n",
       " auto_scheduler.SearchTask(0x1f448750),\n",
       " auto_scheduler.SearchTask(0x1f458150),\n",
       " auto_scheduler.SearchTask(0x1f364e10),\n",
       " auto_scheduler.SearchTask(0x1f44cbf0),\n",
       " auto_scheduler.SearchTask(0x1f488900),\n",
       " auto_scheduler.SearchTask(0x1f4982c0),\n",
       " auto_scheduler.SearchTask(0x1f4a7a60),\n",
       " auto_scheduler.SearchTask(0x1f4c1c20),\n",
       " auto_scheduler.SearchTask(0x1f47ffd0),\n",
       " auto_scheduler.SearchTask(0x1f4db120),\n",
       " auto_scheduler.SearchTask(0x1f4e5120),\n",
       " auto_scheduler.SearchTask(0x1f4df900),\n",
       " auto_scheduler.SearchTask(0x1f50b760),\n",
       " auto_scheduler.SearchTask(0x1f50ccb0),\n",
       " auto_scheduler.SearchTask(0x1f4e6c20),\n",
       " auto_scheduler.SearchTask(0x1f5420e0),\n",
       " auto_scheduler.SearchTask(0x1f549cb0),\n",
       " auto_scheduler.SearchTask(0x1f553b60),\n",
       " auto_scheduler.SearchTask(0x1f54e250),\n",
       " auto_scheduler.SearchTask(0x1f577250),\n",
       " auto_scheduler.SearchTask(0x1f56e8a0),\n",
       " auto_scheduler.SearchTask(0x1f598a30),\n",
       " auto_scheduler.SearchTask(0x1f51a8d0),\n",
       " auto_scheduler.SearchTask(0x1f5b7c90),\n",
       " auto_scheduler.SearchTask(0x1f5a60f0),\n",
       " auto_scheduler.SearchTask(0x1f5e83f0),\n",
       " auto_scheduler.SearchTask(0x1f5ba8c0),\n",
       " auto_scheduler.SearchTask(0x1f5f4030),\n",
       " auto_scheduler.SearchTask(0x1f5fdd80),\n",
       " auto_scheduler.SearchTask(0x1f5f94e0),\n",
       " auto_scheduler.SearchTask(0x1f621950),\n",
       " auto_scheduler.SearchTask(0x1f5c37c0),\n",
       " auto_scheduler.SearchTask(0x1f5a1f80),\n",
       " auto_scheduler.SearchTask(0x1f6323e0),\n",
       " auto_scheduler.SearchTask(0x1f644900),\n",
       " auto_scheduler.SearchTask(0x1f670a70),\n",
       " auto_scheduler.SearchTask(0x1f67f7a0),\n",
       " auto_scheduler.SearchTask(0x1f68de70),\n",
       " auto_scheduler.SearchTask(0x1f6995b0),\n",
       " auto_scheduler.SearchTask(0x1f6963a0),\n",
       " auto_scheduler.SearchTask(0x1f6bb650),\n",
       " auto_scheduler.SearchTask(0x1f65f790),\n",
       " auto_scheduler.SearchTask(0x1f6e0d80),\n",
       " auto_scheduler.SearchTask(0x1f6eb950),\n",
       " auto_scheduler.SearchTask(0x1f6f97c0),\n",
       " auto_scheduler.SearchTask(0x1f707a50),\n",
       " auto_scheduler.SearchTask(0x1f6fbc60),\n",
       " auto_scheduler.SearchTask(0x1f728010),\n",
       " auto_scheduler.SearchTask(0x1f723e30),\n",
       " auto_scheduler.SearchTask(0x1f74c920),\n",
       " auto_scheduler.SearchTask(0x1f75d780),\n",
       " auto_scheduler.SearchTask(0x1f73b350),\n",
       " auto_scheduler.SearchTask(0x1f778b40),\n",
       " auto_scheduler.SearchTask(0x1f735950),\n",
       " auto_scheduler.SearchTask(0x1f79c340),\n",
       " auto_scheduler.SearchTask(0x1f7b06b0),\n",
       " auto_scheduler.SearchTask(0x1f7b90c0),\n",
       " auto_scheduler.SearchTask(0x1f7b4bc0),\n",
       " auto_scheduler.SearchTask(0x1f78be00),\n",
       " auto_scheduler.SearchTask(0x1f7e5fd0),\n",
       " auto_scheduler.SearchTask(0x1f786430),\n",
       " auto_scheduler.SearchTask(0x1f805e10),\n",
       " auto_scheduler.SearchTask(0x1f815150),\n",
       " auto_scheduler.SearchTask(0x1f82b6d0),\n",
       " auto_scheduler.SearchTask(0x1f7f6880),\n",
       " auto_scheduler.SearchTask(0x1f84b0a0),\n",
       " auto_scheduler.SearchTask(0x1f83ab10),\n",
       " auto_scheduler.SearchTask(0x1f8c9e70),\n",
       " auto_scheduler.SearchTask(0x1f8ea070),\n",
       " auto_scheduler.SearchTask(0x1f8f1270),\n",
       " auto_scheduler.SearchTask(0x1f831410),\n",
       " auto_scheduler.SearchTask(0x1f84f060),\n",
       " auto_scheduler.SearchTask(0x1f8503c0),\n",
       " auto_scheduler.SearchTask(0x1f85fe00),\n",
       " auto_scheduler.SearchTask(0x1f905360),\n",
       " auto_scheduler.SearchTask(0x1f913890),\n",
       " auto_scheduler.SearchTask(0x1f9236c0),\n",
       " auto_scheduler.SearchTask(0x1f9406b0),\n",
       " auto_scheduler.SearchTask(0x1f943ea0),\n",
       " auto_scheduler.SearchTask(0x1f943970),\n",
       " auto_scheduler.SearchTask(0x1f909510),\n",
       " auto_scheduler.SearchTask(0x1f8a5cf0),\n",
       " auto_scheduler.SearchTask(0x1f98c3d0),\n",
       " auto_scheduler.SearchTask(0x1f9bea70),\n",
       " auto_scheduler.SearchTask(0x1f960070),\n",
       " auto_scheduler.SearchTask(0x1f9a0f40),\n",
       " auto_scheduler.SearchTask(0x1f9dd8e0),\n",
       " auto_scheduler.SearchTask(0x1f9df2d0),\n",
       " auto_scheduler.SearchTask(0x1f9eeb30),\n",
       " auto_scheduler.SearchTask(0x1fa06670),\n",
       " auto_scheduler.SearchTask(0x1fa06bd0),\n",
       " auto_scheduler.SearchTask(0x1fa23c10),\n",
       " auto_scheduler.SearchTask(0x1fa3ac10),\n",
       " auto_scheduler.SearchTask(0x1fa1d420),\n",
       " auto_scheduler.SearchTask(0x1fa34310),\n",
       " auto_scheduler.SearchTask(0x1fa303c0),\n",
       " auto_scheduler.SearchTask(0x1f983a30),\n",
       " auto_scheduler.SearchTask(0x1fa63a70),\n",
       " auto_scheduler.SearchTask(0x1fa7ddb0),\n",
       " auto_scheduler.SearchTask(0x1fa87a00),\n",
       " auto_scheduler.SearchTask(0x1fa84480),\n",
       " auto_scheduler.SearchTask(0x1faa4330),\n",
       " auto_scheduler.SearchTask(0x1faa9fe0),\n",
       " auto_scheduler.SearchTask(0x1fac8900),\n",
       " auto_scheduler.SearchTask(0x1fa96e40),\n",
       " auto_scheduler.SearchTask(0x1faeb900),\n",
       " auto_scheduler.SearchTask(0x1faf6f40),\n",
       " auto_scheduler.SearchTask(0x1fafd1e0),\n",
       " auto_scheduler.SearchTask(0x1fafc5e0),\n",
       " auto_scheduler.SearchTask(0x1fb33680),\n",
       " auto_scheduler.SearchTask(0x1fb1f2d0),\n",
       " auto_scheduler.SearchTask(0x1fac0bc0),\n",
       " auto_scheduler.SearchTask(0x1fb5cc30),\n",
       " auto_scheduler.SearchTask(0x1fb489a0),\n",
       " auto_scheduler.SearchTask(0x1fb74190),\n",
       " auto_scheduler.SearchTask(0x1fb89530),\n",
       " auto_scheduler.SearchTask(0x1fb6e680),\n",
       " auto_scheduler.SearchTask(0x1fb9a120),\n",
       " auto_scheduler.SearchTask(0x1fb97390),\n",
       " auto_scheduler.SearchTask(0x1fbcda10),\n",
       " auto_scheduler.SearchTask(0x1fb97170),\n",
       " auto_scheduler.SearchTask(0x1fbf8940),\n",
       " auto_scheduler.SearchTask(0x1fb7f940),\n",
       " auto_scheduler.SearchTask(0x1fc06b80),\n",
       " auto_scheduler.SearchTask(0x1fbb9590),\n",
       " auto_scheduler.SearchTask(0x1fbf2290),\n",
       " auto_scheduler.SearchTask(0x1fc103a0),\n",
       " auto_scheduler.SearchTask(0x1fc0e600),\n",
       " auto_scheduler.SearchTask(0x1fc60e50),\n",
       " auto_scheduler.SearchTask(0x1fc451a0),\n",
       " auto_scheduler.SearchTask(0x1fc5c600),\n",
       " auto_scheduler.SearchTask(0x1fc401e0),\n",
       " auto_scheduler.SearchTask(0x1fd106c0),\n",
       " auto_scheduler.SearchTask(0x1fd06150),\n",
       " auto_scheduler.SearchTask(0x1fcf5350),\n",
       " auto_scheduler.SearchTask(0x1fd68040),\n",
       " auto_scheduler.SearchTask(0x1fdb5df0),\n",
       " auto_scheduler.SearchTask(0x1fd80940),\n",
       " auto_scheduler.SearchTask(0x1fcf9a60),\n",
       " auto_scheduler.SearchTask(0x1fd00650),\n",
       " auto_scheduler.SearchTask(0x1fd49d60),\n",
       " auto_scheduler.SearchTask(0x1fd4fb90),\n",
       " auto_scheduler.SearchTask(0x1fd4f380),\n",
       " auto_scheduler.SearchTask(0x1fc87010),\n",
       " auto_scheduler.SearchTask(0x1fda9310),\n",
       " auto_scheduler.SearchTask(0x1fdac8c0),\n",
       " auto_scheduler.SearchTask(0x1fd85cf0),\n",
       " auto_scheduler.SearchTask(0x1fd91d80),\n",
       " auto_scheduler.SearchTask(0x1fe1c6d0),\n",
       " auto_scheduler.SearchTask(0x1fe35370),\n",
       " auto_scheduler.SearchTask(0x1fe44d40),\n",
       " auto_scheduler.SearchTask(0x1fe52680),\n",
       " auto_scheduler.SearchTask(0x1fe5c320),\n",
       " auto_scheduler.SearchTask(0x1fe66660),\n",
       " auto_scheduler.SearchTask(0x1fe696e0),\n",
       " auto_scheduler.SearchTask(0x1fe844b0),\n",
       " auto_scheduler.SearchTask(0x1fe07c00),\n",
       " auto_scheduler.SearchTask(0x1fe62cf0),\n",
       " auto_scheduler.SearchTask(0x1fec1640),\n",
       " auto_scheduler.SearchTask(0x1fe97280),\n",
       " auto_scheduler.SearchTask(0x1fed9bf0),\n",
       " auto_scheduler.SearchTask(0x1fe921d0),\n",
       " auto_scheduler.SearchTask(0x1fefd230),\n",
       " auto_scheduler.SearchTask(0x1fe02da0),\n",
       " auto_scheduler.SearchTask(0x1ff1fa70),\n",
       " auto_scheduler.SearchTask(0x1ff29dc0),\n",
       " auto_scheduler.SearchTask(0x1feee4d0),\n",
       " auto_scheduler.SearchTask(0x1ff24370),\n",
       " auto_scheduler.SearchTask(0x1ff57eb0),\n",
       " auto_scheduler.SearchTask(0x1ff4e5e0),\n",
       " auto_scheduler.SearchTask(0x1ff6baa0),\n",
       " auto_scheduler.SearchTask(0x1ff6b000),\n",
       " auto_scheduler.SearchTask(0x1ff68d80),\n",
       " auto_scheduler.SearchTask(0x2008a2f0),\n",
       " auto_scheduler.SearchTask(0x1ff27b40),\n",
       " auto_scheduler.SearchTask(0x20098f60),\n",
       " auto_scheduler.SearchTask(0x1ff65af0),\n",
       " auto_scheduler.SearchTask(0x200556f0),\n",
       " auto_scheduler.SearchTask(0x200514a0),\n",
       " auto_scheduler.SearchTask(0x1ff98d10),\n",
       " auto_scheduler.SearchTask(0x1ff94250),\n",
       " auto_scheduler.SearchTask(0x1ffe76b0),\n",
       " auto_scheduler.SearchTask(0x200de900),\n",
       " auto_scheduler.SearchTask(0x1ffefc00),\n",
       " auto_scheduler.SearchTask(0x1ffecc30),\n",
       " auto_scheduler.SearchTask(0x200c4630),\n",
       " auto_scheduler.SearchTask(0x200d3920),\n",
       " auto_scheduler.SearchTask(0x20074760),\n",
       " auto_scheduler.SearchTask(0x2006e230),\n",
       " auto_scheduler.SearchTask(0x200f0920),\n",
       " auto_scheduler.SearchTask(0x20100f40),\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "sys.path.append(\"/root/work/tenset/scripts\")\n",
    "from print_programs import return_all_states\n",
    "from make_dataset import load_and_register_tasks\n",
    "from tvm import auto_scheduler\n",
    "from tvm.auto_scheduler.dataset import Dataset, make_dataset_from_log_file\n",
    "# json_file = \"/root/work/tenset/dataset/measure_records_tenset/k80/([0bcb8746286db050cd088f375c85372d,1,64,64,128,6,6,32,128,1,64,64,32],cuda).json\"\n",
    "json_file = \"/root/work/tenset/dataset/measure_records_tenset/k80/([0c9a5ba46ffc5e1a9e5641018527117f,4,7,7,160,1,1,160,960,1,1,1,960,4,7,7,960],cuda).json\"\n",
    "# json_file = \"/root/work/tenset/dataset/measure_records_tenset/k80/([3eb184d18885126bd13d564ef260c820,4,16,16,256,6,6,256,256,1,1,1,256,4,16,16,256,4,16,16,256],cuda).json\"\n",
    "# json_file = \"/root/work/tenset/dataset/measure_records_tenset/k80/([8c674f26f66543069d1e1c56cda249f9,4,60,60,256,1,1,256,512,1,1,1,512,4,30,30,512],cuda).json\"\n",
    "load_and_register_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "947647eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "states, costs = return_all_states(json_file)\n",
    "records_raw = list(map(lambda x: str(x).strip(), states))\n",
    "\n",
    "records = {\"schedules\": [], \"extents\": [], \"costs\": [], \"unroll\" : [], \"all\": []}\n",
    "\n",
    "for rec, cost in zip(records_raw, costs):\n",
    "    cost = np.array([c.value for c in cost])\n",
    "    cost = -np.log(np.mean(cost) + 1e-8)\n",
    "    schedule = rec.split(\"Placeholder\")[-1][2:]\n",
    "    \n",
    "    records[\"schedules\"].append(schedule)\n",
    "    records[\"costs\"].append(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5e40e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "placeholder, placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,42)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,7)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,40)\n",
      "      Conv2dOutput.local auto_unroll: 512\n",
      "      for nn_c.0 (0,1)\n",
      "        for yy_c.0 (0,1)\n",
      "          for xx_c.0 (0,1)\n",
      "            for ff_c.0 (0,1)\n",
      "              for nn_c.1 (0,1)\n",
      "                for yy_c.1 (0,1)\n",
      "                  for xx_c.1 (0,1)\n",
      "                    for ff_c.1 (0,1)\n",
      "                      for nn_c.2 (0,1)\n",
      "                        for yy_c.2 (0,1)\n",
      "                          for xx_c.2 (0,1)\n",
      "                            for ff_c.2 (0,1)\n",
      "                              for ry.0 (0,1)\n",
      "                                for rx.0 (0,1)\n",
      "                                  for rc.0 (0,5)\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,128)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,40)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          placeholder.shared = ...\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,23)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,40)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          PaddedInput.shared = ...\n",
      "                                    for ry.1 (0,1)\n",
      "                                      for rx.1 (0,1)\n",
      "                                        for rc.1 (0,1)\n",
      "                                          for nn_c.3 (0,2)\n",
      "                                            for yy_c.3 (0,1)\n",
      "                                              for xx_c.3 (0,1)\n",
      "                                                for ff_c.3 (0,2)\n",
      "                                                  for ry.2 (0,1)\n",
      "                                                    for rx.2 (0,1)\n",
      "                                                      for rc.2 (0,32)\n",
      "                                                        for nn_c.4 (0,2)\n",
      "                                                          for yy_c.4 (0,1)\n",
      "                                                            for xx_c.4 (0,1)\n",
      "                                                              for ff_c.4 (0,2)\n",
      "                                                                Conv2dOutput.local = ...\n",
      "      for nn.3 (0,4)\n",
      "        for yy.3 (0,1)\n",
      "          for xx.3 (0,1)\n",
      "            for ff.3 (0,4)\n",
      "              Conv2dOutput = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,5880)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_multiply = ...\n",
      "---------------------------------------------------\n",
      "placeholder, placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,1344)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,1)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,70)\n",
      "      Conv2dOutput.local auto_unroll: 64\n",
      "      for nn_c.0 (0,1)\n",
      "        for yy_c.0 (0,1)\n",
      "          for xx_c.0 (0,1)\n",
      "            for ff_c.0 (0,1)\n",
      "              for nn_c.1 (0,1)\n",
      "                for yy_c.1 (0,1)\n",
      "                  for xx_c.1 (0,1)\n",
      "                    for ff_c.1 (0,1)\n",
      "                      for nn_c.2 (0,1)\n",
      "                        for yy_c.2 (0,1)\n",
      "                          for xx_c.2 (0,1)\n",
      "                            for ff_c.2 (0,1)\n",
      "                              for ry.0 (0,1)\n",
      "                                for rx.0 (0,1)\n",
      "                                  for rc.0 (0,5)\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,1)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,70)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "                                          placeholder.shared = ...\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,13)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,70)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          PaddedInput.shared = ...\n",
      "                                    for ry.1 (0,1)\n",
      "                                      for rx.1 (0,1)\n",
      "                                        for rc.1 (0,4)\n",
      "                                          for nn_c.3 (0,2)\n",
      "                                            for yy_c.3 (0,1)\n",
      "                                              for xx_c.3 (0,1)\n",
      "                                                for ff_c.3 (0,1)\n",
      "                                                  for ry.2 (0,1)\n",
      "                                                    for rx.2 (0,1)\n",
      "                                                      for rc.2 (0,8)\n",
      "                                                        for nn_c.4 (0,1)\n",
      "                                                          for yy_c.4 (0,1)\n",
      "                                                            for xx_c.4 (0,1)\n",
      "                                                              for ff_c.4 (0,1)\n",
      "                                                                Conv2dOutput.local = ...\n",
      "      for nn.3 (0,2)\n",
      "        for yy.3 (0,1)\n",
      "          for xx.3 (0,1)\n",
      "            for ff.3 (0,1)\n",
      "              Conv2dOutput = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,5880)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_multiply = ...\n",
      "---------------------------------------------------\n",
      "placeholder, placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,60)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,1)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,56)\n",
      "      Conv2dOutput.local auto_unroll: 512\n",
      "      for nn_c.0 (0,1)\n",
      "        for yy_c.0 (0,1)\n",
      "          for xx_c.0 (0,1)\n",
      "            for ff_c.0 (0,1)\n",
      "              for nn_c.1 (0,1)\n",
      "                for yy_c.1 (0,1)\n",
      "                  for xx_c.1 (0,1)\n",
      "                    for ff_c.1 (0,1)\n",
      "                      for nn_c.2 (0,1)\n",
      "                        for yy_c.2 (0,1)\n",
      "                          for xx_c.2 (0,1)\n",
      "                            for ff_c.2 (0,1)\n",
      "                              for ry.0 (0,1)\n",
      "                                for rx.0 (0,1)\n",
      "                                  for rc.0 (0,40)\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,5)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          placeholder.shared = ...\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,1)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,14)\n",
      "                                          PaddedInput.shared = ...\n",
      "                                    for ry.1 (0,1)\n",
      "                                      for rx.1 (0,1)\n",
      "                                        for rc.1 (0,1)\n",
      "                                          for nn_c.3 (0,1)\n",
      "                                            for yy_c.3 (0,7)\n",
      "                                              for xx_c.3 (0,1)\n",
      "                                                for ff_c.3 (0,1)\n",
      "                                                  for ry.2 (0,1)\n",
      "                                                    for rx.2 (0,1)\n",
      "                                                      for rc.2 (0,4)\n",
      "                                                        for nn_c.4 (0,1)\n",
      "                                                          for yy_c.4 (0,1)\n",
      "                                                            for xx_c.4 (0,1)\n",
      "                                                              for ff_c.4 (0,8)\n",
      "                                                                Conv2dOutput.local = ...\n",
      "      for nn.3 (0,1)\n",
      "        for yy.3 (0,7)\n",
      "          for xx.3 (0,1)\n",
      "            for ff.3 (0,8)\n",
      "              Conv2dOutput = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,5880)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_multiply = ...\n",
      "---------------------------------------------------\n",
      "placeholder, placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,448)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,1)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,7)\n",
      "      Conv2dOutput.local auto_unroll: 16\n",
      "      for nn_c.0 (0,1)\n",
      "        for yy_c.0 (0,1)\n",
      "          for xx_c.0 (0,1)\n",
      "            for ff_c.0 (0,1)\n",
      "              for nn_c.1 (0,1)\n",
      "                for yy_c.1 (0,1)\n",
      "                  for xx_c.1 (0,1)\n",
      "                    for ff_c.1 (0,1)\n",
      "                      for nn_c.2 (0,1)\n",
      "                        for yy_c.2 (0,1)\n",
      "                          for xx_c.2 (0,1)\n",
      "                            for ff_c.2 (0,1)\n",
      "                              for ry.0 (0,1)\n",
      "                                for rx.0 (0,1)\n",
      "                                  for rc.0 (0,2)\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,343)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,7)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          placeholder.shared = ...\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,160)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,7)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          PaddedInput.shared = ...\n",
      "                                    for ry.1 (0,1)\n",
      "                                      for rx.1 (0,1)\n",
      "                                        for rc.1 (0,8)\n",
      "                                          for nn_c.3 (0,2)\n",
      "                                            for yy_c.3 (0,1)\n",
      "                                              for xx_c.3 (0,1)\n",
      "                                                for ff_c.3 (0,30)\n",
      "                                                  for ry.2 (0,1)\n",
      "                                                    for rx.2 (0,1)\n",
      "                                                      for rc.2 (0,10)\n",
      "                                                        for nn_c.4 (0,1)\n",
      "                                                          for yy_c.4 (0,1)\n",
      "                                                            for xx_c.4 (0,1)\n",
      "                                                              for ff_c.4 (0,1)\n",
      "                                                                Conv2dOutput.local = ...\n",
      "      for nn.3 (0,2)\n",
      "        for yy.3 (0,1)\n",
      "          for xx.3 (0,1)\n",
      "            for ff.3 (0,30)\n",
      "              Conv2dOutput = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,5880)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_multiply = ...\n",
      "---------------------------------------------------\n",
      "placeholder, placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,24)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,2)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,56)\n",
      "      Conv2dOutput.local auto_unroll: 64\n",
      "      for nn_c.0 (0,1)\n",
      "        for yy_c.0 (0,1)\n",
      "          for xx_c.0 (0,1)\n",
      "            for ff_c.0 (0,1)\n",
      "              for nn_c.1 (0,1)\n",
      "                for yy_c.1 (0,1)\n",
      "                  for xx_c.1 (0,1)\n",
      "                    for ff_c.1 (0,1)\n",
      "                      for nn_c.2 (0,1)\n",
      "                        for yy_c.2 (0,1)\n",
      "                          for xx_c.2 (0,1)\n",
      "                            for ff_c.2 (0,1)\n",
      "                              for ry.0 (0,1)\n",
      "                                for rx.0 (0,1)\n",
      "                                  for rc.0 (0,4)\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,29)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          placeholder.shared = ...\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,140)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          PaddedInput.shared = ...\n",
      "                                    for ry.1 (0,1)\n",
      "                                      for rx.1 (0,1)\n",
      "                                        for rc.1 (0,5)\n",
      "                                          for nn_c.3 (0,1)\n",
      "                                            for yy_c.3 (0,7)\n",
      "                                              for xx_c.3 (0,1)\n",
      "                                                for ff_c.3 (0,2)\n",
      "                                                  for ry.2 (0,1)\n",
      "                                                    for rx.2 (0,1)\n",
      "                                                      for rc.2 (0,8)\n",
      "                                                        for nn_c.4 (0,1)\n",
      "                                                          for yy_c.4 (0,1)\n",
      "                                                            for xx_c.4 (0,1)\n",
      "                                                              for ff_c.4 (0,5)\n",
      "                                                                Conv2dOutput.local = ...\n",
      "      for nn.3 (0,1)\n",
      "        for yy.3 (0,7)\n",
      "          for xx.3 (0,1)\n",
      "            for ff.3 (0,10)\n",
      "              Conv2dOutput = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,5880)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_multiply = ...\n",
      "---------------------------------------------------\n",
      "placeholder, placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,28)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,2)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,168)\n",
      "      Conv2dOutput.local auto_unroll: 64\n",
      "      for nn_c.0 (0,1)\n",
      "        for yy_c.0 (0,1)\n",
      "          for xx_c.0 (0,1)\n",
      "            for ff_c.0 (0,1)\n",
      "              for nn_c.1 (0,1)\n",
      "                for yy_c.1 (0,1)\n",
      "                  for xx_c.1 (0,1)\n",
      "                    for ff_c.1 (0,1)\n",
      "                      for nn_c.2 (0,1)\n",
      "                        for yy_c.2 (0,1)\n",
      "                          for xx_c.2 (0,1)\n",
      "                            for ff_c.2 (0,1)\n",
      "                              for ry.0 (0,1)\n",
      "                                for rx.0 (0,1)\n",
      "                                  for rc.0 (0,20)\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,12)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,168)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          placeholder.shared = ...\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,2)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,168)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          PaddedInput.shared = ...\n",
      "                                    for ry.1 (0,1)\n",
      "                                      for rx.1 (0,1)\n",
      "                                        for rc.1 (0,2)\n",
      "                                          for nn_c.3 (0,2)\n",
      "                                            for yy_c.3 (0,1)\n",
      "                                              for xx_c.3 (0,1)\n",
      "                                                for ff_c.3 (0,1)\n",
      "                                                  for ry.2 (0,1)\n",
      "                                                    for rx.2 (0,1)\n",
      "                                                      for rc.2 (0,4)\n",
      "                                                        for nn_c.4 (0,1)\n",
      "                                                          for yy_c.4 (0,1)\n",
      "                                                            for xx_c.4 (0,1)\n",
      "                                                              for ff_c.4 (0,10)\n",
      "                                                                Conv2dOutput.local = ...\n",
      "      for nn.3 (0,2)\n",
      "        for yy.3 (0,1)\n",
      "          for xx.3 (0,1)\n",
      "            for ff.3 (0,10)\n",
      "              Conv2dOutput = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,5880)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_multiply = ...\n",
      "---------------------------------------------------\n",
      "placeholder, placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,24)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,1)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,56)\n",
      "      Conv2dOutput.local auto_unroll: 512\n",
      "      for nn_c.0 (0,1)\n",
      "        for yy_c.0 (0,1)\n",
      "          for xx_c.0 (0,1)\n",
      "            for ff_c.0 (0,1)\n",
      "              for nn_c.1 (0,1)\n",
      "                for yy_c.1 (0,1)\n",
      "                  for xx_c.1 (0,1)\n",
      "                    for ff_c.1 (0,1)\n",
      "                      for nn_c.2 (0,1)\n",
      "                        for yy_c.2 (0,1)\n",
      "                          for xx_c.2 (0,1)\n",
      "                            for ff_c.2 (0,1)\n",
      "                              for ry.0 (0,1)\n",
      "                                for rx.0 (0,1)\n",
      "                                  for rc.0 (0,4)\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,29)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          placeholder.shared = ...\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,140)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          PaddedInput.shared = ...\n",
      "                                    for ry.1 (0,1)\n",
      "                                      for rx.1 (0,1)\n",
      "                                        for rc.1 (0,40)\n",
      "                                          for nn_c.3 (0,2)\n",
      "                                            for yy_c.3 (0,1)\n",
      "                                              for xx_c.3 (0,1)\n",
      "                                                for ff_c.3 (0,1)\n",
      "                                                  for ry.2 (0,1)\n",
      "                                                    for rx.2 (0,1)\n",
      "                                                      for rc.2 (0,1)\n",
      "                                                        for nn_c.4 (0,2)\n",
      "                                                          for yy_c.4 (0,1)\n",
      "                                                            for xx_c.4 (0,7)\n",
      "                                                              for ff_c.4 (0,5)\n",
      "                                                                Conv2dOutput.local = ...\n",
      "      for nn.3 (0,4)\n",
      "        for yy.3 (0,1)\n",
      "          for xx.3 (0,7)\n",
      "            for ff.3 (0,5)\n",
      "              Conv2dOutput = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,5880)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_multiply = ...\n",
      "---------------------------------------------------\n",
      "placeholder, placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,2)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,4)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,140)\n",
      "      Conv2dOutput.local auto_unroll: 16\n",
      "      for nn_c.0 (0,1)\n",
      "        for yy_c.0 (0,1)\n",
      "          for xx_c.0 (0,1)\n",
      "            for ff_c.0 (0,1)\n",
      "              for nn_c.1 (0,1)\n",
      "                for yy_c.1 (0,1)\n",
      "                  for xx_c.1 (0,1)\n",
      "                    for ff_c.1 (0,1)\n",
      "                      for nn_c.2 (0,1)\n",
      "                        for yy_c.2 (0,1)\n",
      "                          for xx_c.2 (0,1)\n",
      "                            for ff_c.2 (0,1)\n",
      "                              for ry.0 (0,1)\n",
      "                                for rx.0 (0,1)\n",
      "                                  for rc.0 (0,40)\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,14)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,140)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          placeholder.shared = ...\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,6)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,140)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          PaddedInput.shared = ...\n",
      "                                    for ry.1 (0,1)\n",
      "                                      for rx.1 (0,1)\n",
      "                                        for rc.1 (0,4)\n",
      "                                          for nn_c.3 (0,1)\n",
      "                                            for yy_c.3 (0,1)\n",
      "                                              for xx_c.3 (0,1)\n",
      "                                                for ff_c.3 (0,12)\n",
      "                                                  for ry.2 (0,1)\n",
      "                                                    for rx.2 (0,1)\n",
      "                                                      for rc.2 (0,1)\n",
      "                                                        for nn_c.4 (0,1)\n",
      "                                                          for yy_c.4 (0,1)\n",
      "                                                            for xx_c.4 (0,7)\n",
      "                                                              for ff_c.4 (0,2)\n",
      "                                                                Conv2dOutput.local = ...\n",
      "      for nn.3 (0,1)\n",
      "        for yy.3 (0,1)\n",
      "          for xx.3 (0,7)\n",
      "            for ff.3 (0,24)\n",
      "              Conv2dOutput = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,5880)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_multiply = ...\n",
      "---------------------------------------------------\n",
      "placeholder, placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,20)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,1)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,56)\n",
      "      Conv2dOutput.local auto_unroll: 512\n",
      "      for nn_c.0 (0,1)\n",
      "        for yy_c.0 (0,1)\n",
      "          for xx_c.0 (0,1)\n",
      "            for ff_c.0 (0,1)\n",
      "              for nn_c.1 (0,1)\n",
      "                for yy_c.1 (0,1)\n",
      "                  for xx_c.1 (0,1)\n",
      "                    for ff_c.1 (0,1)\n",
      "                      for nn_c.2 (0,1)\n",
      "                        for yy_c.2 (0,1)\n",
      "                          for xx_c.2 (0,1)\n",
      "                            for ff_c.2 (0,1)\n",
      "                              for ry.0 (0,1)\n",
      "                                for rx.0 (0,1)\n",
      "                                  for rc.0 (0,40)\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,14)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          placeholder.shared = ...\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,4)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          PaddedInput.shared = ...\n",
      "                                    for ry.1 (0,1)\n",
      "                                      for rx.1 (0,1)\n",
      "                                        for rc.1 (0,1)\n",
      "                                          for nn_c.3 (0,1)\n",
      "                                            for yy_c.3 (0,7)\n",
      "                                              for xx_c.3 (0,1)\n",
      "                                                for ff_c.3 (0,3)\n",
      "                                                  for ry.2 (0,1)\n",
      "                                                    for rx.2 (0,1)\n",
      "                                                      for rc.2 (0,4)\n",
      "                                                        for nn_c.4 (0,1)\n",
      "                                                          for yy_c.4 (0,1)\n",
      "                                                            for xx_c.4 (0,1)\n",
      "                                                              for ff_c.4 (0,8)\n",
      "                                                                Conv2dOutput.local = ...\n",
      "      for nn.3 (0,1)\n",
      "        for yy.3 (0,7)\n",
      "          for xx.3 (0,1)\n",
      "            for ff.3 (0,24)\n",
      "              Conv2dOutput = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,5880)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_multiply = ...\n",
      "---------------------------------------------------\n",
      "placeholder, placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,12)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,8)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,280)\n",
      "      for nn_c.0 (0,1)\n",
      "        for yy_c.0 (0,1)\n",
      "          for xx_c.0 (0,1)\n",
      "            for ff_c.0 (0,1)\n",
      "              for nn_c.1 (0,1)\n",
      "                for yy_c.1 (0,1)\n",
      "                  for xx_c.1 (0,1)\n",
      "                    for ff_c.1 (0,1)\n",
      "                      for nn_c.2 (0,1)\n",
      "                        for yy_c.2 (0,1)\n",
      "                          for xx_c.2 (0,1)\n",
      "                            for ff_c.2 (0,1)\n",
      "                              for ry.0 (0,1)\n",
      "                                for rx.0 (0,1)\n",
      "                                  for rc.0 (0,4)\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,12)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,280)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          placeholder.shared = ...\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,28)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,280)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          PaddedInput.shared = ...\n",
      "                                    for ry.1 (0,1)\n",
      "                                      for rx.1 (0,1)\n",
      "                                        for rc.1 (0,1)\n",
      "                                          for nn_c.3 (0,1)\n",
      "                                            for yy_c.3 (0,7)\n",
      "                                              for xx_c.3 (0,1)\n",
      "                                                for ff_c.3 (0,1)\n",
      "                                                  for ry.2 (0,1)\n",
      "                                                    for rx.2 (0,1)\n",
      "                                                      for rc.2 (0,40)\n",
      "                                                        for nn_c.4 (0,1)\n",
      "                                                          for yy_c.4 (0,1)\n",
      "                                                            for xx_c.4 (0,1)\n",
      "                                                              for ff_c.4 (0,1)\n",
      "                                                                Conv2dOutput.local = ...\n",
      "      for nn.3 (0,1)\n",
      "        for yy.3 (0,7)\n",
      "          for xx.3 (0,1)\n",
      "            for ff.3 (0,1)\n",
      "              Conv2dOutput = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,5880)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_multiply = ...\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for a in records[\"schedules\"][:10]:\n",
    "    print(a)\n",
    "    print(\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bff08adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0,1) for : {'ry.2', 'ry.0', 'ff_c.2', 'rx.2', 'nn_c.1', 'nn_c.0', 'ry.1', 'nn_c.2', 'yy_c.2', 'rx.1', 'yy_c.0', 'ff_c.0', 'ff_c.1', 'xx_c.0', 'xx_c.2', 'yy_c.1', 'xx_c.1', 'rx.0'}\n",
      "  : 18.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def find_common_for_loops(schedules):\n",
    "    \"\"\"\n",
    "        (0,1) for  \n",
    "    \"\"\"\n",
    "    common_vars = None\n",
    "    \n",
    "    for schedule in schedules:\n",
    "        lines = schedule.split('\\n')\n",
    "        vars_in_schedule = set()\n",
    "        \n",
    "        for line in lines:\n",
    "            stripped = line.lstrip()\n",
    "            match = re.match(r'for\\s+(\\S+)\\s+\\(0,\\s*1\\)', stripped)\n",
    "            if match:\n",
    "                vars_in_schedule.add(match.group(1))\n",
    "        \n",
    "        if common_vars is None:\n",
    "            common_vars = vars_in_schedule\n",
    "        else:\n",
    "            common_vars &= vars_in_schedule  # \n",
    "    \n",
    "    return common_vars if common_vars is not None else set()\n",
    "\n",
    "\n",
    "def remove_common_for_loops(schedule, common_vars):\n",
    "    \"\"\"\n",
    "        (0,1) for   \n",
    "    \"\"\"\n",
    "    lines = schedule.split('\\n')\n",
    "    result_lines = []\n",
    "    \n",
    "    #  for   \n",
    "    remove_indices = set()\n",
    "    for_loop_indents = {}  #  for  ->  \n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        stripped = line.lstrip()\n",
    "        indent_level = len(line) - len(stripped)\n",
    "        \n",
    "        # (0,1) for \n",
    "        match = re.match(r'for\\s+(\\S+)\\s+\\(0,\\s*1\\)', stripped)\n",
    "        if match and match.group(1) in common_vars:\n",
    "            remove_indices.add(i)\n",
    "            for_loop_indents[i] = indent_level\n",
    "    \n",
    "    #        \n",
    "    indent_reduction = [0] * len(lines)\n",
    "    \n",
    "    for idx in sorted(remove_indices):\n",
    "        base_indent = for_loop_indents[idx]\n",
    "        #  for       2 \n",
    "        for j in range(idx + 1, len(lines)):\n",
    "            if j in remove_indices:\n",
    "                continue\n",
    "            line = lines[j]\n",
    "            stripped = line.lstrip()\n",
    "            if not stripped:  #  \n",
    "                continue\n",
    "            current_indent = len(line) - len(stripped)\n",
    "            \n",
    "            #  for body  (   )\n",
    "            if current_indent > base_indent:\n",
    "                indent_reduction[j] += 2\n",
    "            else:\n",
    "                #       for  \n",
    "                break\n",
    "    \n",
    "    #        \n",
    "    for i, line in enumerate(lines):\n",
    "        if i in remove_indices:\n",
    "            continue\n",
    "        \n",
    "        if not line.strip():  #  \n",
    "            result_lines.append(line)\n",
    "            continue\n",
    "        \n",
    "        stripped = line.lstrip()\n",
    "        original_indent = len(line) - len(stripped)\n",
    "        new_indent = max(0, original_indent - indent_reduction[i])\n",
    "        result_lines.append(' ' * new_indent + stripped)\n",
    "    \n",
    "    return '\\n'.join(result_lines)\n",
    "\n",
    "\n",
    "common_for_loops = find_common_for_loops(records[\"schedules\"])\n",
    "print(f\"  (0,1) for : {common_for_loops}\")\n",
    "\n",
    "\n",
    "#   \n",
    "cleaned_schedules = []\n",
    "records[\"extents\"] = []\n",
    "records[\"unroll\"] = []\n",
    "records[\"all\"] = []\n",
    "for i, schedule in enumerate(records[\"schedules\"]):\n",
    "    extents = [float(x) for x in re.findall(r'\\(0,\\s*(\\d+)\\)', schedule)]\n",
    "\n",
    "for i, schedule in enumerate(records[\"schedules\"]):\n",
    "    extents = [float(x) for x in re.findall(r'\\(0,\\s*(\\d+)\\)', schedule)]\n",
    "    unrolls = [float(x) for x in re.findall(r'auto_unroll:\\s*(\\d+)', schedule)]\n",
    "    records[\"extents\"].append(extents)\n",
    "    if unrolls == []:\n",
    "        unrolls = [0.0]\n",
    "    records[\"unroll\"].append(unrolls)\n",
    "    feature = extents+unrolls\n",
    "    records[\"all\"].append(np.array(feature, dtype=np.float32))\n",
    "    \n",
    "    cleaned = remove_common_for_loops(schedule, common_for_loops)\n",
    "    cleaned_schedules.append(cleaned)\n",
    "records[\"cleaned_schedules\"] = cleaned_schedules\n",
    "\n",
    "\n",
    "total_removed = sum(len(orig.split('\\n')) - len(clean.split('\\n')) \n",
    "                    for orig, clean in zip(records['schedules'], cleaned_schedules))\n",
    "avg_removed = total_removed / len(cleaned_schedules)\n",
    "print(f\"  : {avg_removed:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed1663eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class FeatureRegressionDataset(Dataset):\n",
    "    def __init__(self, X, y, feature=None):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            self.X = torch.from_numpy(X).float()\n",
    "        else:\n",
    "            self.X = X\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        if self.y.ndim == 1:\n",
    "            self.y = self.y.unsqueeze(1)\n",
    "\n",
    "        self.feature = feature\n",
    "        if feature is not None:\n",
    "            if isinstance(feature, np.ndarray):\n",
    "                self.feature = torch.from_numpy(feature).float()\n",
    "            else:\n",
    "                self.feature = feature\n",
    "            \n",
    "            if self.feature.ndim == 1:\n",
    "                self.feature = self.feature.unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.feature is None:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        return self.X[idx], self.y[idx], self.feature[idx]\n",
    "\n",
    "\n",
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, X, feature=None):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            self.X = torch.from_numpy(X).float()\n",
    "        else:\n",
    "            self.X = X\n",
    "        \n",
    "        if isinstance(feature, np.ndarray):\n",
    "            self.feature = torch.from_numpy(feature).float()\n",
    "        else:\n",
    "            self.feature = feature\n",
    "        # feature shape (N,) (N,1)     \n",
    "        if self.feature is not None and self.feature.ndim == 1:\n",
    "            self.feature = self.feature.unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.feature is None:\n",
    "            return self.X[idx]\n",
    "        return self.X[idx], self.feature[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cd8864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class VAE_feature_head(nn.Module):\n",
    "    def __init__(self, input_dim, feature_dim=None, latent_dim=16, hidden_dim=128):\n",
    "        \"\"\"\n",
    "        input_dim: 2 * D (v_norm + is_zero concat )\n",
    "        latent_dim: latent space \n",
    "        hidden_dim: MLP hidden \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            \n",
    "            #   activation  \n",
    "        )\n",
    "\n",
    "        if feature_dim is None:\n",
    "            self.use_feature = False\n",
    "        else:\n",
    "            self.use_feature = True\n",
    "            self.feature_predictor = nn.Sequential(\n",
    "                nn.Linear(latent_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, feature_dim),  # features.shape[1] feature \n",
    "            )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def predict_feature(self, z):\n",
    "        return self.feature_predictor(z)\n",
    "\n",
    "    def forward(self, x, use_mean=True):\n",
    "        mu, logvar = self.encode(x)\n",
    "        if use_mean:\n",
    "            z = mu\n",
    "        else:\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        \n",
    "        if self.use_feature:\n",
    "            feature_pred = self.predict_feature(z)\n",
    "        else:\n",
    "            feature_pred = None\n",
    "        return x_recon, mu, logvar, z, feature_pred\n",
    "\n",
    "class L3Loss(torch.nn.Module):\n",
    "    def forward(self, pred, target):\n",
    "        return torch.mean(torch.abs(pred - target) ** 4)\n",
    "\n",
    "def vae_feature_loss(x_recon, x, mu, logvar, feature_pred, feature, alpha_recon=0, alpha_feature=0, beta=1.0):\n",
    "    \"\"\"\n",
    "    x, x_recon: (B, input_dim)\n",
    "    mu, logvar: (B, latent_dim)\n",
    "\n",
    "    beta: KL  (-VAE  )\n",
    "    \"\"\"\n",
    "    # reconstruction loss: MSE\n",
    "    recon_loss = F.mse_loss(x_recon, x, reduction=\"mean\")\n",
    "    # \n",
    "    # recon_loss = L3Loss()(x_recon, x)\n",
    "\n",
    "    feature_loss = F.mse_loss(feature_pred, feature, reduction=\"mean\") if feature_pred is not None else 0.0\n",
    "\n",
    "    # KL divergence: D_KL(q(z|x) || N(0, I))\n",
    "    kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    loss = alpha_recon * recon_loss + beta * kl + alpha_feature * feature_loss\n",
    "    return loss, recon_loss, kl, feature_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a08123a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b057bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_seed = 2023\n",
    "seed_everything(train_seed)\n",
    "\n",
    "\n",
    "input_data = np.log1p(np.array(records[\"all\"], dtype=np.float32))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "input_data_scaled = scaler.fit_transform(input_data)\n",
    "\n",
    "X_train, X_val = train_test_split(\n",
    "    input_data_scaled,  test_size=0.2, random_state=train_seed\n",
    ")\n",
    "\n",
    "\n",
    "# feature \n",
    "train_dataset = FeatureDataset(X_train)\n",
    "val_dataset   = FeatureDataset(X_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=False)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=512, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17270272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "Experiment 1/1\n",
      "beta=0.01, alpha_recon=1.0, alpha_feature=1.0,\n",
      "epochs=500, latent_dim=64, hidden_dim=256, lr=0.001\n",
      "epoch 500: loss=0.0102, recon=0.0030, kl=0.7265\n",
      "epoch 500: val loss=0.0092, val recon=0.0019, val kl=0.7319\n",
      "Recon R2 : 0.598098271486631, Feature R2 : None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import itertools\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[-1]\n",
    "latent_dim = 64\n",
    "hidden_dim = 256\n",
    "\n",
    "\n",
    "hyperparameter = {\n",
    "    'beta': [0.01],\n",
    "    'alpha_recon': [1.0],\n",
    "    'alpha_feature': [1.0],\n",
    "    'latent_dim': [64],\n",
    "    'lr': [1e-3],\n",
    "}\n",
    "\n",
    "cnt = 0\n",
    "epochs = 500\n",
    "\n",
    "\n",
    "for vals in itertools.product(*hyperparameter.values()):\n",
    "    (beta, alpha_recon, alpha_feature, latent_dim, lr) = vals\n",
    "    cnt += 1\n",
    "    print(\"=============================================\")\n",
    "    print(f\"Experiment {cnt}/{len(list(itertools.product(*hyperparameter.values())))}\")\n",
    "    print(f\"beta={beta}, alpha_recon={alpha_recon}, alpha_feature={alpha_feature},\\nepochs={epochs}, latent_dim={latent_dim}, hidden_dim={hidden_dim}, lr={lr}\")\n",
    "\n",
    "    seed_everything(train_seed)\n",
    "\n",
    "    vae = VAE_feature_head(input_dim=input_dim, latent_dim=latent_dim, hidden_dim=hidden_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(vae.parameters(), lr=lr)\n",
    "\n",
    "    # early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 30\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        vae.train()\n",
    "        for x_batch in train_loader:\n",
    "            if len(x_batch) == 2:\n",
    "                x_batch, feature_batch = x_batch\n",
    "                feature_batch = feature_batch.to(device)\n",
    "            else:\n",
    "                feature_batch = None\n",
    "            x_batch = x_batch.to(device)  # (N, D)\n",
    "            \n",
    "            \n",
    "\n",
    "            x_recon, mu, logvar, z, feature_pred = vae(x_batch, use_mean=False)\n",
    "\n",
    "            loss, recon_loss, kl, feature_loss = vae_feature_loss(x_recon, x_batch, mu, logvar, feature_pred, feature_batch, alpha_recon=alpha_recon, alpha_feature=alpha_feature, beta=beta)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        vae.eval()\n",
    "        for x_batch in val_loader:\n",
    "            if len(x_batch) == 2:\n",
    "                x_batch, feature_batch = x_batch\n",
    "                feature_batch = feature_batch.to(device)\n",
    "            else:\n",
    "                feature_batch = None\n",
    "            x_batch = x_batch.to(device)\n",
    "            if feature_batch is not None:\n",
    "                feature_batch = feature_batch.to(device)\n",
    "            x_recon, mu, logvar, z, feature_pred = vae(x_batch, use_mean=True)\n",
    "            val_loss, val_recon_loss, val_kl, val_feature_loss = vae_feature_loss(x_recon, x_batch, mu, logvar, feature_pred, feature_batch, alpha_recon=alpha_recon, alpha_feature=alpha_feature, beta=beta)\n",
    "            val_recon_r2 = r2_score(x_batch.detach().cpu().numpy(), x_recon.detach().cpu().numpy())\n",
    "            if feature_batch is not None:\n",
    "                val_feature_r2 = r2_score(feature_batch.detach().cpu().numpy(), feature_pred.detach().cpu().numpy())\n",
    "            else:\n",
    "                val_feature_r2 = None\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss.item()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "    print(f\"epoch {epoch}: loss={loss.item():.4f}, recon={recon_loss.item():.4f}, kl={kl.item():.4f}\")\n",
    "    print(f\"epoch {epoch}: val loss={val_loss.item():.4f}, val recon={val_recon_loss.item():.4f}, val kl={val_kl.item():.4f}\")\n",
    "\n",
    "    print(f\"Recon R2 : {val_recon_r2}, Feature R2 : {val_feature_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6310053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAECostPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    VAE  Cost Regression \n",
    "    \n",
    "    :\n",
    "    - input  segment_encoder  segment_sum  VAE encoder  z  cost_predictor  cost\n",
    "    \n",
    "    :\n",
    "    - Pretrained VAE encoder finetune ( learning rate)\n",
    "    - Cost predictor   learning rate \n",
    "    -  forward     (detach, stop_grad )\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, feature_dim=None, hidden_dim=256, latent_dim=64, \n",
    "                 predictor_hidden=256, predictor_layers=2, dropout=0.1, use_feature=False):\n",
    "        super(VAECostPredictor, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # ========== Cost Predictor ( ) ==========\n",
    "        predictor_modules = []\n",
    "        current_dim = latent_dim\n",
    "        for i in range(predictor_layers):\n",
    "            predictor_modules.extend([\n",
    "                nn.Linear(current_dim, predictor_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout) if i < predictor_layers - 1 else nn.Identity(),\n",
    "            ])\n",
    "            current_dim = predictor_hidden\n",
    "        predictor_modules.append(nn.Linear(predictor_hidden, 1))\n",
    "        \n",
    "        self.cost_predictor = nn.Sequential(*predictor_modules)\n",
    "\n",
    "        self.use_feature = use_feature\n",
    "        if self.use_feature:\n",
    "            pass\n",
    "            self.feature_predictor = nn.Sequential(\n",
    "                nn.Linear(latent_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, feature_dim),  # feature_dim feature \n",
    "            )\n",
    "        \n",
    "    \n",
    "    def encode(self, input_data):\n",
    "        \"\"\"\n",
    "        Full encoding path: features  z\n",
    "          \n",
    "        \"\"\"\n",
    "                \n",
    "        # VAE Encoder\n",
    "        h = self.encoder(input_data)\n",
    "        \n",
    "        mean = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        \n",
    "        return mean, logvar\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        \"\"\"Reparameterization trick -  \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "    \n",
    "    def predict_cost(self, z):\n",
    "        \"\"\"z  cost prediction -   \"\"\"\n",
    "        return self.cost_predictor(z).squeeze(-1)\n",
    "    \n",
    "    def predict_feature(self, z):\n",
    "        return self.feature_predictor(z)\n",
    "    \n",
    "    def forward(self, input_data, use_mean=True):\n",
    "        \"\"\"\n",
    "        Forward pass: input  z  cost\n",
    "        \n",
    "        Args:\n",
    "            use_mean: True reparameterize  mean  (inference)\n",
    "        \n",
    "        Returns:\n",
    "            cost_pred:  cost\n",
    "            mean: latent mean\n",
    "            logvar: latent log-variance\n",
    "            z: sampled/mean latent vector\n",
    "        \"\"\"\n",
    "        mean, logvar = self.encode(input_data)\n",
    "        \n",
    "        if use_mean:\n",
    "            z = mean  # Inference deterministic\n",
    "        else:\n",
    "            z = self.reparameterize(mean, logvar)  # Training stochastic\n",
    "        \n",
    "        cost_pred = self.predict_cost(z)\n",
    "        \n",
    "        return cost_pred, mean, logvar, z\n",
    "    \n",
    "    def get_encoder_params(self):\n",
    "        \"\"\"Encoder  ( lr)\"\"\"\n",
    "        encoder_params = []\n",
    "        encoder_params.extend(self.encoder.parameters())\n",
    "        encoder_params.extend(self.fc_mu.parameters())\n",
    "        encoder_params.extend(self.fc_logvar.parameters())\n",
    "        return encoder_params\n",
    "    \n",
    "    def get_cost_predictor_params(self):\n",
    "        \"\"\"Predictor  ( lr)\"\"\"\n",
    "        return self.cost_predictor.parameters()\n",
    "    \n",
    "    def get_feature_predictor_params(self):\n",
    "        \"\"\"Feature Predictor \"\"\"\n",
    "        return self.feature_predictor.parameters()\n",
    "\n",
    "    def load_pretrained_encoder(self, checkpoint):\n",
    "        \"\"\"Pretrained VAE encoder  \"\"\"\n",
    "        \n",
    "\n",
    "        vae_state = checkpoint\n",
    "        \n",
    "        #   \n",
    "        encoder_keys = ['encoder', 'fc_mu', 'fc_logvar']\n",
    "        own_state = self.state_dict()\n",
    "        \n",
    "        loaded_keys = []\n",
    "        for name, param in vae_state.items():\n",
    "            if any(name.startswith(k) for k in encoder_keys):\n",
    "                if name in own_state and own_state[name].shape == param.shape:\n",
    "                    own_state[name].copy_(param)\n",
    "                    loaded_keys.append(name)\n",
    "        \n",
    "        # print(f\"Loaded {len(loaded_keys)} parameters from pretrained VAE\")\n",
    "        # return loaded_keys\n",
    "\n",
    "    def _enable_dropout(self):\n",
    "        \"\"\" Dropout  train   \"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.train()\n",
    "\n",
    "    def mc_predict(self, input_tensor, T=20):\n",
    "        \"\"\"\n",
    "        MC Dropout   \n",
    "        \n",
    "        Args:\n",
    "            input_tensor:   (shape [N, input_dim])\n",
    "            T: MC  \n",
    "        \n",
    "        Returns:\n",
    "            mean: epistemic  cost (shape [N])\n",
    "            var: epistemic  (shape [N])\n",
    "        \"\"\"\n",
    "\n",
    "        self.eval()  #   eval \n",
    "        self._enable_dropout()  # Dropout train  \n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            \n",
    "            for _ in range(T):\n",
    "                # Encode\n",
    "                z, logvar = self.encode(input_tensor)\n",
    "                cost_pred = self.predict_cost(z)\n",
    "                predictions.append(cost_pred)\n",
    "            \n",
    "            predictions = torch.stack(predictions, dim=0)\n",
    "            \n",
    "            # epistemic mean & variance\n",
    "            mc_mean = predictions.mean(dim=0)\n",
    "            mc_var = predictions.var(dim=0)\n",
    "\n",
    "        return mc_mean, mc_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b584ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_loss_fn(cost_pred, cost_true, loss_type='mse'):\n",
    "    \"\"\"\n",
    "       (MSE  MAE)\n",
    "    \"\"\"\n",
    "    if loss_type == 'mse':\n",
    "        return F.mse_loss(cost_pred, cost_true)\n",
    "    else:  # mae\n",
    "        return F.l1_loss(cost_pred, cost_true)\n",
    "\n",
    "\n",
    "def pair_loss_fn(cost_pred, cost_true, margin=0.1):\n",
    "    \"\"\"\n",
    "    Pairwise ranking loss:  cost   .\n",
    "    cost_true[i] < cost_true[j]  cost_pred[i] < cost_pred[j] + margin\n",
    "    \"\"\"\n",
    "    batch_size = cost_pred.size(0)\n",
    "    if batch_size < 2:\n",
    "        return torch.tensor(0.0, device=cost_pred.device)\n",
    "    \n",
    "    #    ranking loss \n",
    "    idx = torch.arange(batch_size, device=cost_pred.device)\n",
    "    i_idx, j_idx = torch.meshgrid(idx, idx, indexing='ij')\n",
    "    mask = i_idx < j_idx  # upper triangular only\n",
    "    \n",
    "    pred_i = cost_pred[i_idx[mask]]\n",
    "    pred_j = cost_pred[j_idx[mask]]\n",
    "    true_i = cost_true[i_idx[mask]]\n",
    "    true_j = cost_true[j_idx[mask]]\n",
    "    \n",
    "    # label: 1 if true_i < true_j, -1 otherwise\n",
    "    labels = torch.sign(true_j - true_i).float()\n",
    "    \n",
    "    # Margin ranking loss\n",
    "    loss = F.margin_ranking_loss(pred_j.view(-1), pred_i.view(-1), labels.view(-1), margin=margin)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def smooth_loss_fn(model, z, noise_std=0.1):\n",
    "    \"\"\"\n",
    "    Smoothness loss: z        .\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z_noisy = z + noise_std * torch.randn_like(z)\n",
    "    \n",
    "    cost_original = model.predict_cost(z)\n",
    "    cost_noisy = model.predict_cost(z_noisy)\n",
    "    \n",
    "    smooth_loss = F.mse_loss(cost_original, cost_noisy)\n",
    "    return smooth_loss\n",
    "\n",
    "\n",
    "def kld_loss_fn(mean, logvar):\n",
    "    \"\"\"\n",
    "    KL Divergence: q(z|x) || N(0, I)\n",
    "    \"\"\"\n",
    "    kld = -0.5 * torch.mean(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "    return kld\n",
    "\n",
    "def feature_loss_fn(use_feature, feature_pred, feature_true, coef=0.1):\n",
    "    \"\"\"\n",
    "    Feature   (MSE)\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if not use_feature:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "    return F.mse_loss(feature_pred, feature_true) * coef\n",
    "\n",
    "\n",
    "def compute_total_loss(model, cost_pred, mean, logvar, z, labels, feature, config, return_components=True):\n",
    "    \"\"\"\n",
    "    Total loss  (Segment  ).\n",
    "    total_loss = reg_loss + _pair * pair_loss +  * smooth_loss +  * kld_loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # Individual losses\n",
    "    reg = reg_loss_fn(cost_pred, labels, loss_type=config.get('loss_type', 'mse'))\n",
    "    pair = pair_loss_fn(cost_pred.view(-1), labels.view(-1), margin=config.get('margin', 0.1))\n",
    "    smooth = smooth_loss_fn(model, z, noise_std=config.get('noise_std', 0.1))\n",
    "    kld = kld_loss_fn(mean, logvar)\n",
    "    feature_loss = feature_loss_fn(model.use_feature, None, feature, coef=0)\n",
    "    \n",
    "    # Weighted sum\n",
    "    total = config['lambda_reg'] * reg + config['lambda_pair'] * pair + config['gamma'] * smooth + config['beta'] * kld + feature_loss\n",
    "    \n",
    "    if return_components:\n",
    "        return total, {\n",
    "            'reg_loss': reg.item(),\n",
    "            'pair_loss': pair.item(),\n",
    "            'smooth_loss': smooth.item(),\n",
    "            'kld_loss': kld.item(),\n",
    "            'feature_loss': feature_loss.item(),\n",
    "        }\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ef7144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_accuracy(cost_pred, labels, rng=np.random.default_rng(42)):\n",
    "    \"\"\"\n",
    "    cost_pred, labels: (B,) \n",
    "    \"\"\"\n",
    "    n_samples = min(2000, len(cost_pred))\n",
    "    sample_indices = rng.choice(len(cost_pred), n_samples, replace=False)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i + 1, n_samples):\n",
    "            idx_i = sample_indices[i]\n",
    "            idx_j = sample_indices[j]\n",
    "            pred_diff = cost_pred[idx_i] - cost_pred[idx_j]\n",
    "            true_diff = labels[idx_i] - labels[idx_j]\n",
    "            if (pred_diff * true_diff) > 0:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy\n",
    "\n",
    "def recall_at_k(pred, labels, k=1):\n",
    "    true_best_idx = torch.argmax(labels)\n",
    "    topk_pred_idx = torch.topk(pred, k=k, largest=True).indices\n",
    "\n",
    "    return int((topk_pred_idx == true_best_idx).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a03fd72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_select_indices(xgb_all_preds, train_indices, test_indices, topk_size, eps_greedy_size, rng):\n",
    "    \"\"\"\n",
    "     2, xgb   62 \n",
    "    \"\"\"\n",
    "    #     random_select_size \n",
    "\n",
    "    remaining_indices = set(test_indices)\n",
    "\n",
    "    if topk_size + eps_greedy_size > test_indices.shape[0]:\n",
    "        remaining_indices.update(train_indices.tolist())\n",
    "        train_indices = np.array(list(remaining_indices), dtype=np.int64)\n",
    "        return train_indices, np.array([], dtype=np.int64)\n",
    "\n",
    "\n",
    "    top_indices, remaining_indices = select_topk_cost(xgb_all_preds, remaining_indices, topk_size)\n",
    "    random_indices, remaining_indices = random_select_indices(remaining_indices, eps_greedy_size, rng=rng)\n",
    "    test_indices = np.array(list(remaining_indices), dtype=np.int64)\n",
    "\n",
    "    selected_indices = np.concatenate([top_indices, random_indices])\n",
    "\n",
    "    train_indices = np.concatenate([train_indices, selected_indices])\n",
    "\n",
    "    return train_indices, test_indices\n",
    "\n",
    "\n",
    "\n",
    "def random_select_indices(remaining_indices, select_size, rng=np.random.default_rng(42)):\n",
    "    if select_size == 0:\n",
    "        return np.array([], dtype=np.int64), remaining_indices\n",
    "    \n",
    "    random_indices = rng.choice(list(remaining_indices), size=select_size, replace=False)\n",
    "\n",
    "    remaining_indices = util_update_remaining_indices(remaining_indices, random_indices)\n",
    "\n",
    "    return random_indices, remaining_indices\n",
    "\n",
    "\n",
    "\n",
    "def util_update_remaining_indices(remaining_indices, selected_indices):\n",
    "    \"\"\"\n",
    "       \n",
    "    util_update_remaining_indices selected_indices \n",
    "    \"\"\"\n",
    "    selected_indices = set(selected_indices)\n",
    "    remaining_indices.difference_update(selected_indices)\n",
    "\n",
    "    return remaining_indices\n",
    "\n",
    "\n",
    "\n",
    "def util_select_topk(predictions, remaining_indices, num_select):\n",
    "    \"\"\"\n",
    "         \n",
    "    \n",
    "    Args:\n",
    "        predictions:    ([N, ] )\n",
    "        remaining_indices:      (set)\n",
    "        num_select:   \n",
    "    \n",
    "    Returns:\n",
    "        selected_indices:    numpy \n",
    "        remaining_indices:     (set)\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction = np.asarray(predictions)  # [N]\n",
    "\n",
    "    remaining_np = np.array(list(remaining_indices), dtype=np.int64)\n",
    "    remaining_pred = prediction[remaining_np]\n",
    "\n",
    "    k = min(num_select, len(remaining_np))\n",
    "\n",
    "    topk_local = np.argsort(remaining_pred)[-k:]\n",
    "    selected_indices = remaining_np[topk_local]\n",
    "\n",
    "    # remaining \n",
    "    remaining_indices.difference_update(selected_indices.tolist())\n",
    "\n",
    "    return selected_indices, remaining_indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_topk_cost(cost_pred, remaining_indices, num_select):\n",
    "    \"\"\"\n",
    "     cost     \n",
    "    \n",
    "    Args:\n",
    "        model: VAECostPredictor \n",
    "        input_data_scaled:  input  ([N, input_dim] )\n",
    "        remaining_indices:      (set)\n",
    "        num_select:   \n",
    "    \n",
    "    \"\"\"\n",
    "    if num_select == 0:\n",
    "        return np.array([], dtype=np.int64), remaining_indices\n",
    "\n",
    "    if isinstance(cost_pred, torch.Tensor):\n",
    "        cost_pred = cost_pred.detach().cpu().numpy()  # [N]\n",
    "\n",
    "    topk_cost_indices, remaining_indices = util_select_topk(cost_pred, remaining_indices, num_select)\n",
    "    \n",
    "\n",
    "    return topk_cost_indices, remaining_indices\n",
    "\n",
    "\n",
    "def select_topk_z_grad(z, cost_pred, remaining_indices, num_select):\n",
    "    \"\"\"\n",
    "    z  cost gradient     \n",
    "    \n",
    "    Args:\n",
    "        model: VAECostPredictor \n",
    "        input_tensor:  input numpy  ([N, input_dim] )\n",
    "        remaining_indices:      (set)\n",
    "        num_select:   \n",
    "    \n",
    "    \"\"\"\n",
    "    if num_select == 0:\n",
    "        return np.array([], dtype=np.int64), remaining_indices\n",
    "\n",
    "    candidate_indices = np.array(list(remaining_indices), dtype=np.int64)\n",
    "\n",
    "    # z-gradient \n",
    "    z_grad = torch.autograd.grad(\n",
    "        outputs=cost_pred.sum(),\n",
    "        inputs=z,\n",
    "        retain_graph=False,\n",
    "        create_graph=False\n",
    "    )[0]  # [N, latent_dim]\n",
    "\n",
    "    z_grad_norm = torch.norm(z_grad, dim=1).detach().cpu().numpy()  # [N]\n",
    "\n",
    "    #   grad-norm top-k\n",
    "    candidate_grad = z_grad_norm[candidate_indices]\n",
    "    k = min(num_select, len(candidate_indices))\n",
    "\n",
    "    topk_local = np.argsort(candidate_grad)[-k:]\n",
    "    selected_indices = candidate_indices[topk_local]\n",
    "\n",
    "    # remaining \n",
    "    remaining_indices = set(remaining_indices)\n",
    "    remaining_indices.difference_update(selected_indices.tolist())\n",
    "\n",
    "    return selected_indices, remaining_indices\n",
    "\n",
    "\n",
    "def select_topk_uncertainty(model, input_tensor, remaining_indices, num_select, T_mc=10):\n",
    "    \"\"\"\n",
    "    MC Dropout       \n",
    "    \n",
    "    Args:\n",
    "        model: VAECostPredictor \n",
    "        input_data_scaled:  input  ([N, input_dim] )\n",
    "        remaining_indices:      (set)\n",
    "        num_select:   \n",
    "        T_mc: MC Dropout  \n",
    "    \n",
    "    Returns:\n",
    "        selected_indices:    \n",
    "    \"\"\"\n",
    "    if num_select == 0:\n",
    "        return np.array([], dtype=np.int64), remaining_indices\n",
    "\n",
    "\n",
    "    was_training = model.training\n",
    "    model.train()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, mc_var = model.mc_predict(input_tensor, T=T_mc)\n",
    "\n",
    "    if not was_training:\n",
    "        model.eval()  # \n",
    "\n",
    "    var_np = mc_var.detach().cpu().numpy()  # [N]\n",
    "\n",
    "    topk_uncertainty_indices, remaining_indices = util_select_topk(var_np, remaining_indices, num_select)\n",
    "\n",
    "    return topk_uncertainty_indices, remaining_indices\n",
    "\n",
    "\n",
    "def select_topk_latent_diversity(z, candidate_indices, used_indices, select_n_div, chunk_size=1024, eps=1e-12):\n",
    "    \"\"\"\n",
    "     candidates 320  .\n",
    "      topk_cost, topk_z_grad 40   .\n",
    "    latent diversity 40 + used_indices    24 280 .\n",
    "\n",
    "    z L2  , k-center greedy(farthest-first) diversity .\n",
    "      used_indices (  ).\n",
    "      \"  \"  candidate  .\n",
    "    \n",
    "    Args:\n",
    "        z: torch.Tensor [N, latent_dim]\n",
    "        candidate_indices: set(int)\n",
    "        used_indices: set(int)\n",
    "        select_n_div: int\n",
    "        chunk_size: int\n",
    "    Returns:\n",
    "        diverse_indices: np.ndarray (int64)\n",
    "        candidate_indices: set (   )\n",
    "    \"\"\"\n",
    "    if select_n_div == 0 or len(candidate_indices) == 0:\n",
    "        return np.array([], dtype=np.int64), candidate_indices\n",
    "\n",
    "\n",
    "    device = z.device\n",
    "\n",
    "    # 1) L2 normalize z  (  )\n",
    "    with torch.no_grad():\n",
    "        z_norm = z / (z.norm(dim=1, keepdim=True) + eps)\n",
    "\n",
    "    cand = np.array(list(candidate_indices), dtype=np.int64)\n",
    "    k = min(select_n_div, len(cand))\n",
    "\n",
    "    cand_t = torch.from_numpy(cand).to(device=device)\n",
    "    z_cand = z_norm[cand_t]  # [M, D], M=len(cand)\n",
    "\n",
    "    #  : used_indices (  )\n",
    "    used = np.array(list(used_indices), dtype=np.int64)\n",
    "    selected = []\n",
    "\n",
    "    # 2)  candidate \"   \"  init\n",
    "    #    used  +inf     (  ) \n",
    "    if len(used) > 0:\n",
    "        used_t = torch.from_numpy(used).to(device=device)\n",
    "        z_used = z_norm[used_t]  # [U, D]\n",
    "\n",
    "        # min_dists[j] = min_{u in used} ||z_cand[j] - z_used[u]||\n",
    "        min_dists = torch.empty(len(cand), device=device, dtype=torch.float32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for s in range(0, len(cand), chunk_size):\n",
    "                e = min(s + chunk_size, len(cand))\n",
    "                d = torch.cdist(z_cand[s:e], z_used, p=2)  # [B, U]\n",
    "                min_dists[s:e] = d.min(dim=1).values\n",
    "    else:\n",
    "        #          argmax 0   \n",
    "        #   / norm      ,\n",
    "        #  \"  min_dists\"  +inf .\n",
    "        min_dists = torch.full((len(cand),), float(\"inf\"), device=device, dtype=torch.float32)\n",
    "\n",
    "    # 3) k-center greedy \n",
    "    #     argmax(min_dists)   ->     -> min_dists \n",
    "    with torch.no_grad():\n",
    "        for _ in range(k):\n",
    "            j = torch.argmax(min_dists).item()     # cand  \n",
    "            sel_idx = cand[j]                      #  \n",
    "            selected.append(sel_idx)\n",
    "\n",
    "            #   \"\" :  candidate  dist_to_new_center   min \n",
    "            new_center = z_cand[j:j+1]  # [1, D]\n",
    "\n",
    "            #       min_dists -inf\n",
    "            min_dists[j] = -float(\"inf\")\n",
    "\n",
    "            #   min  \n",
    "            for s in range(0, len(cand), chunk_size):\n",
    "                e = min(s + chunk_size, len(cand))\n",
    "                d_new = torch.cdist(z_cand[s:e], new_center, p=2).squeeze(1)  # [B]\n",
    "                min_dists[s:e] = torch.minimum(min_dists[s:e], d_new)\n",
    "\n",
    "    diverse_indices = np.array(selected, dtype=np.int64)\n",
    "\n",
    "    candidate_indices = set(candidate_indices)\n",
    "    candidate_indices.difference_update(diverse_indices.tolist())\n",
    "\n",
    "    return diverse_indices, candidate_indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_programs(model, input_data_scaled, used_indices, remaining_indices, num_select=64, T_mc=10, uncertainty_topk=128,\n",
    "                    w_cost=0.5, w_unc=0.3, w_div=0.2, grad_num=2, rand_num=0, rng=np.random.default_rng(42), device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), topk_factor=5):\n",
    "    \"\"\"\n",
    "    Active learning     \n",
    "    \n",
    "    Args:\n",
    "        model: VAECostPredictor \n",
    "        input_data_scaled:  input  ([N, input_dim] )\n",
    "        used_indices:    (set)\n",
    "        remaining_indices:      (set)\n",
    "        num_select:   \n",
    "        T_mc: MC Dropout  \n",
    "        w_cost:    \n",
    "        w_unc: epistemic    \n",
    "        w_div: latent    \n",
    "        grad_num: z  cost gradient   \n",
    "        rand_num:    \n",
    "    \n",
    "    Returns:\n",
    "        selected_indices:    \n",
    "    \"\"\"\n",
    "\n",
    "    #  64 \n",
    "    total = num_select\n",
    "    budget = total - grad_num - rand_num\n",
    "\n",
    "    #    \n",
    "    if num_select == 0 and rand_num > 0:\n",
    "        rand_indices, remaining_indices = random_select_indices(remaining_indices, rand_num, rng=rng)\n",
    "        return rand_indices, remaining_indices\n",
    "    \n",
    "\n",
    "    select_n_cost = int(budget * w_cost)\n",
    "    select_n_unc  = int(budget * w_unc)\n",
    "    select_n_div  = int(budget * w_div)\n",
    "    select_n_grad = grad_num\n",
    "    s = select_n_cost + select_n_unc + select_n_div\n",
    "    if s < budget:\n",
    "        select_n_cost += budget - s\n",
    "\n",
    "    input_tensor = torch.tensor(input_data_scaled, dtype=torch.float32, device=device)\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z, _ = model.encode(input_tensor)\n",
    "    z = z.detach().requires_grad_(True)\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    cost_pred = model.predict_cost(z)\n",
    "    cost_pred = cost_pred.view(-1)\n",
    "    cost_np = cost_pred.detach().cpu().numpy()\n",
    "\n",
    "    remaining_np = np.array(list(remaining_indices), dtype=np.int64)\n",
    "    remaining_cost = cost_np[remaining_np]\n",
    "\n",
    "    k_pref = min(len(remaining_np), total * topk_factor)\n",
    "    top_local = np.argsort(remaining_cost)[-k_pref:]\n",
    "    candidate_indices = set(remaining_np[top_local].tolist())  #  remaining\n",
    "\n",
    "    # print(f\"Candidate pool size: {len(candidate_indices)}\")\n",
    "\n",
    "\n",
    "    #  \n",
    "    currently_used = set()\n",
    "    topk_cost_indices, candidate_indices = select_topk_cost(cost_pred, candidate_indices, select_n_cost)\n",
    "    currently_used.update(topk_cost_indices.tolist())\n",
    "    z_grad_indices, candidate_indices = select_topk_z_grad(z, cost_pred, candidate_indices, select_n_grad)\n",
    "    currently_used.update(z_grad_indices.tolist())\n",
    "\n",
    "    # if len(used_indices) / len(input_data_scaled) >= 0.1:\n",
    "    if len(used_indices) >= uncertainty_topk:\n",
    "        uncertainty_indices, candidate_indices = select_topk_uncertainty(model, input_tensor, candidate_indices, select_n_unc, T_mc=T_mc)\n",
    "    else:\n",
    "        pool_for_uncertainty = set(remaining_indices)\n",
    "        pool_for_uncertainty.difference_update(currently_used)\n",
    "        uncertainty_indices, _ = select_topk_uncertainty(model, input_tensor, pool_for_uncertainty, select_n_unc, T_mc=T_mc)\n",
    "        candidate_indices.difference_update(uncertainty_indices.tolist())\n",
    "\n",
    "\n",
    "    currently_used.update(uncertainty_indices.tolist())\n",
    "    used_local = set(used_indices)\n",
    "    used_local.update(currently_used)\n",
    "\n",
    "    diverse_indices, _ = select_topk_latent_diversity(z, candidate_indices, used_local, select_n_div)\n",
    "    currently_used.update(diverse_indices.tolist())\n",
    "\n",
    "\n",
    "    remaining_indices.difference_update(currently_used)\n",
    "\n",
    "\n",
    "    rand_indices, remaining_indices = random_select_indices(remaining_indices, rand_num, rng=rng)\n",
    "    currently_used.update(rand_indices.tolist())\n",
    "\n",
    "    \n",
    "\n",
    "    all_selected_indices = np.array(sorted(currently_used), dtype=np.int64)\n",
    "\n",
    "\n",
    "\n",
    "    return all_selected_indices, remaining_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe4fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vae_reg_dataloaders(input_data_scaled, costs, used_indices, remaining_indices):\n",
    "\n",
    "    train_indices = np.array(list(used_indices), dtype=np.int64)\n",
    "    val_indices = np.array(list(remaining_indices), dtype=np.int64)\n",
    "\n",
    "    X_train = input_data_scaled[train_indices]\n",
    "    X_val = input_data_scaled[val_indices]\n",
    "    y_train = costs[train_indices]\n",
    "    y_val = costs[val_indices]\n",
    "\n",
    "    train_dataset = FeatureRegressionDataset(X_train, y_train)\n",
    "    val_dataset   = FeatureRegressionDataset(X_val,   y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=False)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=512, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "    y_mean = y_train.mean()\n",
    "    y_std = y_train.std() + 1e-8  # 0     \n",
    "    print(f\"y_train mean: {y_mean}, std: {y_std}\")\n",
    "\n",
    "    \n",
    "    return train_loader, val_loader, y_mean, y_std\n",
    "\n",
    "def make_xgb_datasets(inputs, results, train_indices, test_indices):\n",
    "    f_inputs = []\n",
    "    f_results = []\n",
    "    r_costs = []\n",
    "    for inp, res in zip(inputs, results):\n",
    "        cost = np.mean([c.value for c in res.costs])\n",
    "        if cost < 1e10:\n",
    "            f_inputs.append(inp)\n",
    "            f_results.append(res)\n",
    "            r_costs.append(cost)\n",
    "    r_costs = np.array(r_costs, dtype=np.float32)\n",
    "    \n",
    "    dataset = auto_scheduler.dataset.Dataset()\n",
    "    dataset.update_from_measure_pairs(f_inputs, f_results)\n",
    "\n",
    "\n",
    "    raw_features = list(dataset.features.values())[0]\n",
    "    raw_throughputs = list(dataset.throughputs.values())[0]\n",
    "\n",
    "    # features_list = []  #   feature (seq_len, feature_dim)\n",
    "    dataset_costs = []\n",
    "\n",
    "    for feature, throughput in zip(raw_features, raw_throughputs):\n",
    "\n",
    "        if feature.shape[0] != 1 and throughput > 1e-8:\n",
    "            # features_list.append(feature)\n",
    "            dataset_costs.append(throughput)\n",
    "\n",
    "    dataset_costs = np.array(dataset_costs, dtype=np.float32)\n",
    "    if test_indices.shape[0] == 0:\n",
    "        train_set = dataset\n",
    "        test_set = None\n",
    "    \n",
    "    train_set, test_set = dataset.random_split_within_task(train_set_ratio=0, \n",
    "                                                        train_idxs=train_indices.tolist(), \n",
    "                                                        test_idxs=test_indices.tolist())\n",
    "    return train_set, test_set, dataset, raw_throughputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ac08a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vae_reg_model(vae, hyperparameter, input_dim, latent_dim, hidden_dim, y_std, verbose=True):\n",
    "\n",
    "\n",
    "    cnt = 0\n",
    "    for vals in itertools.product(*hyperparameter.values()):\n",
    "        (lambda_reg, lambda_pair, margin_scale, gamma, beta, noise_std, \n",
    "        encoder_lr, feature_predictor_lr, cost_predictor_lr,  epochs) = vals\n",
    "        cnt += 1\n",
    "        if verbose:\n",
    "            print(f\"Experiment {cnt}/{len(list(itertools.product(*hyperparameter.values())))}\")\n",
    "            print(f\"lambda_reg={lambda_reg}, lambda_pair={lambda_pair}, margin_scale={margin_scale}, \\\n",
    "              gamma={gamma}, beta={beta}, noise_std={noise_std}\\nencoder_lr={encoder_lr}, cost_predictor_lr={cost_predictor_lr}, epochs={epochs}\")\n",
    "        config = {\n",
    "                    'encoder_lr': encoder_lr,\n",
    "                    'feature_predictor_lr': feature_predictor_lr,\n",
    "                    'cost_predictor_lr': cost_predictor_lr,\n",
    "                    'lambda_reg' : lambda_reg,\n",
    "                    'lambda_pair': lambda_pair,\n",
    "                    'gamma': gamma,\n",
    "                    'beta': beta,\n",
    "                    'margin': margin_scale * y_std,\n",
    "                    'noise_std': noise_std,\n",
    "                    'loss_type': 'mse',\n",
    "                    'epochs': epochs,\n",
    "                }\n",
    "\n",
    "        vae_cost_model = VAECostPredictor(input_dim=input_dim, \n",
    "                                    latent_dim=latent_dim, \n",
    "                                    hidden_dim=hidden_dim, \n",
    "                                    predictor_layers=2,\n",
    "                                    dropout=0.1, use_feature=False).to(device)\n",
    "        vae_cost_model.load_pretrained_encoder(vae.state_dict())\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': vae_cost_model.get_encoder_params(), 'lr': config['encoder_lr']},\n",
    "            {'params': vae_cost_model.get_cost_predictor_params(), 'lr': config['cost_predictor_lr']}\n",
    "        ], weight_decay=1e-5)\n",
    "    return vae_cost_model, optimizer, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "47e0fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regression(vae_cost_model, optimizer, train_loader, val_loader, input_data_scaled, costs, config, top_k=10, use_rank=True):\n",
    "\n",
    "    print(\"Train size :\", len(train_loader.dataset))\n",
    "\n",
    "    # all_reg_results = []\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    for epoch in range(1, config['epochs']+1):\n",
    "        vae_cost_model.train()\n",
    "        for x_batch, labels in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            labels = labels.to(device).squeeze(-1)\n",
    "            \n",
    "        \n",
    "            cost_pred, mean, logvar, z = vae_cost_model(x_batch, use_mean=True)\n",
    "\n",
    "            train_loss, train_components = compute_total_loss(vae_cost_model, \n",
    "                                                    cost_pred, mean, logvar, z, labels, None, config)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(vae_cost_model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "        \n",
    "\n",
    "        if epoch % config['epochs'] == 0:\n",
    "            vae_cost_model.eval()\n",
    "            with torch.no_grad():\n",
    "                all_preds = []\n",
    "                all_labels = []\n",
    "                for x_batch, labels in val_loader:\n",
    "                    x_batch = x_batch.to(device)\n",
    "                    labels = labels.to(device).squeeze(-1)\n",
    "\n",
    "                    cost_pred, mean, logvar, z = vae_cost_model(x_batch, use_mean=True)\n",
    "                    all_preds.append(cost_pred)\n",
    "                    all_labels.append(labels)\n",
    "\n",
    "                    val_loss, val_components = compute_total_loss(vae_cost_model, cost_pred, mean, logvar, z, labels, None, config)\n",
    "                val_reg_r2 = r2_score(torch.cat(all_labels).detach().cpu().numpy(), torch.cat(all_preds).detach().cpu().numpy())\n",
    "                val_reg_r2 = round(val_reg_r2, 4)\n",
    "                \n",
    "                print(f\"Train loss epoch {epoch} : reg={train_components['reg_loss']: .4f} rank={train_components['pair_loss']: .4f} kl={train_components['kld_loss']: .4f}\")\n",
    "                print(f\"Val loss epoch {epoch}: reg={val_components['reg_loss']: .4f} rank={val_components['pair_loss']: .4f} kl={val_components['kld_loss']: .4f}\")\n",
    "                \n",
    "                print(f\"Regression R2 : {val_reg_r2:.4f}, \")\n",
    "        \n",
    "        # rank r2 \n",
    "        vae_cost_model.eval()\n",
    "        with torch.no_grad():\n",
    "            if epoch % config['epochs'] == 0:\n",
    "                input_data_tensor = torch.from_numpy(input_data_scaled).float().to(device)\n",
    "                all_preds = vae_cost_model(input_data_tensor, use_mean=True)[0].detach().cpu().numpy()\n",
    "                if use_rank:\n",
    "                    val_rank_r2 = pair_accuracy(all_preds, costs)\n",
    "                    val_rank_r2 = round(val_rank_r2, 4)\n",
    "                    print(f\"Rank R2 : {val_rank_r2:.4f}\")\n",
    "                else:\n",
    "                    val_rank_r2 = None\n",
    "                recall_top_k = recall_at_k(torch.tensor(all_preds), torch.from_numpy(costs), k=top_k)\n",
    "                \n",
    "                print(f\"Recall@{top_k} : {recall_top_k}\")\n",
    "                if recall_top_k:\n",
    "                    break_signal = True\n",
    "                else:\n",
    "                    break_signal = False\n",
    "\n",
    "    # print(\"=============================================\")\n",
    "    # all_reg_results.append({\n",
    "    #     \"lambda_reg\": lambda_reg,\n",
    "    #     \"lambda_pair\": lambda_pair,\n",
    "    #     \"margin_scale\": margin_scale,\n",
    "    #     \"gamma\": gamma,\n",
    "    #     \"beta\": beta,\n",
    "    #     \"noise_std\": noise_std,\n",
    "    #     \"encoder_lr\": encoder_lr,\n",
    "    #     \"feature_predictor_lr\": feature_predictor_lr,\n",
    "    #     \"cost_predictor_lr\": cost_predictor_lr,\n",
    "    #     \"seed\": seed,\n",
    "    #     \"reg_r2\": val_reg_r2,\n",
    "    #     \"rank_r2\": val_rank_r2,\n",
    "    #     \"recall@64\": recall_top_k\n",
    "    # })\n",
    "    return vae_cost_model, break_signal, val_reg_r2, val_rank_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fbf8d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weight_grid(step=0.1):\n",
    "    m = int(round(1.0 / step))  # step=0.1 -> 10\n",
    "    weights = []\n",
    "    for i in range(m + 1):\n",
    "        for j in range(m + 1):\n",
    "            k = m - i - j\n",
    "            if k < 0:\n",
    "                continue\n",
    "            weights.append((i/m, j/m, k/m))\n",
    "    return weights\n",
    "weights = generate_weight_grid(step=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "18ffb652",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_weights = []\n",
    "for w in weights:\n",
    "    w_cost, w_unc, w_div = w\n",
    "    if w_cost < 0.3:\n",
    "        continue\n",
    "    # if w_unc == 0.0 and w_cost > 0.0 and w_div > 0.0:\n",
    "    #     f_weights.append(w)\n",
    "    #     continue\n",
    "    # if w_div == 0.0 and w_cost > 0.0 and w_unc > 0.0:\n",
    "    #     f_weights.append(w)\n",
    "        # continue\n",
    "    f_weights.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "26db8d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########  1/16 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2000\n",
      "    : [   8   41   99  108  228  236  253  389  415  435  563  629  639  654\n",
      "  709  743  788  864  900  947  961  971  990  991 1121 1217 1239 1359\n",
      " 1511 1581 1665 1696 1719 1727 1828 1838 1841 1851 1863 1949 1974 1998\n",
      " 2006 2111 2124 2129 2223 2241 2256 2381 2411 2471 2585 2745 2885 3225\n",
      " 3248 3300 3320 3434 3542 3552 3654 3703]\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.1750946044921875, std: 1.899470339284668\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.6576 rank= 0.0216 kl= 0.1203\n",
      "Val loss epoch 1000: reg= 1.6982 rank= 0.2314 kl= 0.1329\n",
      "Regression R2 : 0.7609, \n",
      "Recall@1 : 0\n",
      "===============  Phase 2 ================\n",
      "y_train mean: 7.441629886627197, std: 1.8796640734536743\n",
      "Train size : 128\n",
      "Train loss epoch 1000 : reg= 2.2155 rank= 0.0251 kl= 0.1134\n",
      "Val loss epoch 1000: reg= 2.3909 rank= 0.2692 kl= 0.1283\n",
      "Regression R2 : 0.3829, \n",
      "Recall@1 : 0\n",
      "===============  Phase 3 ================\n",
      "y_train mean: 7.847609043121338, std: 1.6628451447351074\n",
      "Train size : 192\n",
      "Train loss epoch 1000 : reg= 1.9771 rank= 0.0250 kl= 0.1123\n",
      "Val loss epoch 1000: reg= 1.9825 rank= 0.1732 kl= 0.1483\n",
      "Regression R2 : 0.4567, \n",
      "Recall@1 : 1\n",
      " \n",
      "   : 192\n",
      "  : 10.81 \n",
      "=============================================\n",
      "##########  2/16 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2001\n",
      "    : [  26   74   81  121  217  371  395  412  420  440  579  602  697  714\n",
      "  745  748  809  811  817  892  945 1024 1104 1206 1210 1242 1474 1493\n",
      " 1555 1562 1589 1603 1620 1637 1667 1746 1752 1764 1811 1827 1901 1974\n",
      " 2066 2069 2082 2114 2119 2189 2210 2331 2527 2656 2713 2820 2886 3122\n",
      " 3130 3151 3168 3222 3462 3538 3651 3678]\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.41241455078125, std: 1.9297140936715698\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.5571 rank= 0.0234 kl= 0.1273\n",
      "Val loss epoch 1000: reg= 1.9901 rank= 0.3251 kl= 0.1482\n",
      "Regression R2 : 0.7088, \n",
      "Recall@1 : 0\n",
      "===============  Phase 2 ================\n",
      "y_train mean: 7.453821659088135, std: 1.7641641001565551\n",
      "Train size : 128\n",
      "Train loss epoch 1000 : reg= 2.0794 rank= 0.0236 kl= 0.1073\n",
      "Val loss epoch 1000: reg= 2.0114 rank= 0.2396 kl= 0.1340\n",
      "Regression R2 : 0.6631, \n",
      "Recall@1 : 1\n",
      " \n",
      "   : 128\n",
      "  : 7.22 \n",
      "=============================================\n",
      "##########  3/16 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2002\n",
      "    : [  15  135  166  294  337  392  399  406  465  490  507  528  556  560\n",
      "  632  711  728  730  831  860  877  915  921  928 1171 1198 1309 1385\n",
      " 1512 1535 1643 1675 1685 1768 1793 1881 1902 1918 2029 2124 2136 2178\n",
      " 2235 2276 2419 2538 2548 2842 3066 3102 3110 3263 3271 3306 3314 3354\n",
      " 3395 3396 3447 3465 3520 3529 3553 3595]\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.467670917510986, std: 1.6613528828485107\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.7171 rank= 0.0188 kl= 0.1186\n",
      "Val loss epoch 1000: reg= 1.1742 rank= 0.2121 kl= 0.1481\n",
      "Regression R2 : 0.7988, \n",
      "Recall@1 : 1\n",
      " \n",
      "   : 64\n",
      "  : 3.45 \n",
      "=============================================\n",
      "##########  4/16 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2003\n",
      "    : [  19   57   69  146  187  276  304  396  470  522  627  714  750  878\n",
      "  961  979  997 1048 1081 1115 1168 1181 1269 1297 1336 1432 1436 1558\n",
      " 1619 1712 1852 1933 2043 2063 2094 2132 2202 2223 2394 2402 2428 2449\n",
      " 2456 2467 2473 2622 2680 2920 2924 2934 2988 3171 3190 3230 3234 3237\n",
      " 3398 3409 3417 3418 3479 3614 3672 3711]\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.559206962585449, std: 1.5440037350518798\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.1998 rank= 0.0175 kl= 0.1129\n",
      "Val loss epoch 1000: reg= 1.1913 rank= 0.1822 kl= 0.1427\n",
      "Regression R2 : 0.7705, \n",
      "Recall@1 : 0\n",
      "===============  Phase 2 ================\n",
      "y_train mean: 7.533614635467529, std: 1.5222201447351074\n",
      "Train size : 128\n",
      "Train loss epoch 1000 : reg= 1.4352 rank= 0.0200 kl= 0.1039\n",
      "Val loss epoch 1000: reg= 1.3118 rank= 0.2359 kl= 0.1319\n",
      "Regression R2 : 0.6272, \n",
      "Recall@1 : 0\n",
      "===============  Phase 3 ================\n",
      "y_train mean: 7.9611968994140625, std: 1.3984904389245605\n",
      "Train size : 192\n",
      "Train loss epoch 1000 : reg= 1.5959 rank= 0.0194 kl= 0.1005\n",
      "Val loss epoch 1000: reg= 1.4973 rank= 0.1612 kl= 0.1560\n",
      "Regression R2 : 0.5206, \n",
      "Recall@1 : 1\n",
      " \n",
      "   : 192\n",
      "  : 10.59 \n",
      "=============================================\n",
      "##########  5/16 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2004\n",
      "    : [  10  111  113  116  194  252  340  497  502  543  618  684 1068 1084\n",
      " 1095 1133 1147 1177 1291 1297 1305 1394 1404 1427 1474 1553 1563 1580\n",
      " 1685 1815 1971 2019 2020 2063 2073 2137 2172 2248 2280 2286 2293 2350\n",
      " 2437 2492 2547 2597 2603 2607 2613 2625 2639 2784 2794 2954 3011 3055\n",
      " 3082 3111 3236 3368 3372 3420 3514 3538]\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.192169189453125, std: 1.9492085080010986\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.9176 rank= 0.0222 kl= 0.1178\n",
      "Val loss epoch 1000: reg= 2.0580 rank= 0.2961 kl= 0.1422\n",
      "Regression R2 : 0.7366, \n",
      "Recall@1 : 0\n",
      "===============  Phase 2 ================\n",
      "y_train mean: 7.33471155166626, std: 1.8165929417474365\n",
      "Train size : 128\n",
      "Train loss epoch 1000 : reg= 2.2969 rank= 0.0240 kl= 0.1092\n",
      "Val loss epoch 1000: reg= 3.4099 rank= 0.5100 kl= 0.1306\n",
      "Regression R2 : 0.5059, \n",
      "Recall@1 : 0\n",
      "===============  Phase 3 ================\n",
      "y_train mean: 7.784016132354736, std: 1.633615742192993\n",
      "Train size : 192\n",
      "Train loss epoch 1000 : reg= 2.0927 rank= 0.0229 kl= 0.1062\n",
      "Val loss epoch 1000: reg= 2.0373 rank= 0.2052 kl= 0.1554\n",
      "Regression R2 : 0.5161, \n",
      "Recall@1 : 1\n",
      " \n",
      "   : 192\n",
      "  : 10.56 \n",
      "=============================================\n",
      "##########  6/16 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2005\n",
      "    : [  12   33   61  108  110  360  420  485  567  675  701  715  720  737\n",
      "  770  786  838  963  982 1141 1173 1274 1307 1309 1389 1420 1445 1473\n",
      " 1722 1777 1866 1868 2044 2083 2103 2156 2178 2203 2266 2298 2345 2399\n",
      " 2457 2481 2517 2531 2635 2648 2669 2684 2737 2739 2788 2944 2959 2981\n",
      " 3143 3300 3320 3350 3466 3503 3517 3567]\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.259561061859131, std: 1.693363080487976\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.4767 rank= 0.0184 kl= 0.1184\n",
      "Val loss epoch 1000: reg= 1.8130 rank= 0.2153 kl= 0.1405\n",
      "Regression R2 : 0.7421, \n",
      "Recall@1 : 0\n",
      "===============  Phase 2 ================\n",
      "y_train mean: 7.387248992919922, std: 1.6907763581140136\n",
      "Train size : 128\n",
      "Train loss epoch 1000 : reg= 1.7819 rank= 0.0225 kl= 0.1118\n",
      "Val loss epoch 1000: reg= 1.3812 rank= 0.1811 kl= 0.1307\n",
      "Regression R2 : 0.6538, \n",
      "Recall@1 : 0\n",
      "===============  Phase 3 ================\n",
      "y_train mean: 7.819840908050537, std: 1.5547781090600585\n",
      "Train size : 192\n",
      "Train loss epoch 1000 : reg= 1.8921 rank= 0.0218 kl= 0.1144\n",
      "Val loss epoch 1000: reg= 1.9625 rank= 0.1758 kl= 0.1530\n",
      "Regression R2 : 0.4628, \n",
      "Recall@1 : 1\n",
      " \n",
      "   : 192\n",
      "  : 10.55 \n",
      "=============================================\n",
      "##########  7/16 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2006\n",
      "    : [  18   46   77  203  211  213  276  350  462  535  541  560  700  730\n",
      "  809 1086 1112 1113 1139 1284 1317 1324 1420 1509 1538 1571 1671 1675\n",
      " 1747 1840 2050 2054 2157 2185 2259 2336 2433 2445 2453 2489 2509 2555\n",
      " 2689 2829 2830 2899 3009 3037 3047 3152 3160 3183 3193 3201 3239 3305\n",
      " 3339 3342 3425 3491 3533 3563 3658 3689]\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.65336799621582, std: 1.9082207779748535\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.6391 rank= 0.0243 kl= 0.1156\n",
      "Val loss epoch 1000: reg= 1.6166 rank= 0.3030 kl= 0.1427\n",
      "Regression R2 : 0.7234, \n",
      "Recall@1 : 0\n",
      "===============  Phase 2 ================\n",
      "y_train mean: 7.5839924812316895, std: 1.6773076157434081\n",
      "Train size : 128\n",
      "Train loss epoch 1000 : reg= 2.0350 rank= 0.0222 kl= 0.1087\n",
      "Val loss epoch 1000: reg= 1.5331 rank= 0.1218 kl= 0.1328\n",
      "Regression R2 : 0.7435, \n",
      "Recall@1 : 0\n",
      "===============  Phase 3 ================\n",
      "y_train mean: 7.9054083824157715, std: 1.4955772261483764\n",
      "Train size : 192\n",
      "Train loss epoch 1000 : reg= 1.9992 rank= 0.0202 kl= 0.1067\n",
      "Val loss epoch 1000: reg= 1.6787 rank= 0.1645 kl= 0.1528\n",
      "Regression R2 : 0.5405, \n",
      "Recall@1 : 0\n",
      "===============  Phase 4 ================\n",
      "y_train mean: 8.115001678466797, std: 1.3618918757302856\n",
      "Train size : 256\n",
      "Train loss epoch 1000 : reg= 1.8265 rank= 0.0193 kl= 0.1048\n",
      "Val loss epoch 1000: reg= 1.4181 rank= 0.1220 kl= 0.1568\n",
      "Regression R2 : 0.5385, \n",
      "Recall@1 : 0\n",
      "===============  Phase 5 ================\n",
      "y_train mean: 8.255147933959961, std: 1.2613347868783569\n",
      "Train size : 320\n",
      "Train loss epoch 1000 : reg= 1.7288 rank= 0.0185 kl= 0.1047\n",
      "Val loss epoch 1000: reg= 1.3938 rank= 0.1235 kl= 0.1641\n",
      "Regression R2 : 0.4183, \n",
      "Recall@1 : 1\n",
      " \n",
      "   : 320\n",
      "  : 18.79 \n",
      "=============================================\n",
      "##########  8/16 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2007\n",
      "    : [  45   95  113  192  215  372  382  510  516  633  670  702  735  796\n",
      "  802  827  902 1005 1006 1038 1054 1057 1074 1082 1125 1140 1271 1322\n",
      " 1375 1378 1440 1442 1464 1674 1744 1771 1776 1913 2034 2084 2145 2186\n",
      " 2230 2240 2428 2475 2479 2482 2646 2673 2784 2790 2818 2894 2971 3032\n",
      " 3177 3225 3255 3260 3370 3383 3408 3663]\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.666424751281738, std: 1.8059845070703124\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.9755 rank= 0.0202 kl= 0.1113\n",
      "Val loss epoch 1000: reg= 1.8237 rank= 0.2468 kl= 0.1449\n",
      "Regression R2 : 0.7030, \n",
      "Recall@1 : 0\n",
      "===============  Phase 2 ================\n",
      "y_train mean: 7.526405334472656, std: 1.5928828816278076\n",
      "Train size : 128\n",
      "Train loss epoch 1000 : reg= 1.7915 rank= 0.0210 kl= 0.1032\n",
      "Val loss epoch 1000: reg= 1.3816 rank= 0.1115 kl= 0.1347\n",
      "Regression R2 : 0.7267, \n",
      "Recall@1 : 0\n",
      "===============  Phase 3 ================\n",
      "y_train mean: 7.885005950927734, std: 1.4405138592584228\n",
      "Train size : 192\n",
      "Train loss epoch 1000 : reg= 1.7867 rank= 0.0196 kl= 0.1062\n",
      "Val loss epoch 1000: reg= 1.8207 rank= 0.1831 kl= 0.1588\n",
      "Regression R2 : 0.4161, \n",
      "Recall@1 : 1\n",
      " \n",
      "   : 192\n",
      "  : 10.70 \n",
      "=============================================\n",
      "##########  9/16 ##########\n",
      "weights: (0.3, 0.3, 0.4)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2000\n",
      "    : [   8   41   99  108  228  236  253  389  415  435  563  629  639  654\n",
      "  709  743  788  864  900  947  961  971  990  991 1121 1217 1239 1359\n",
      " 1511 1581 1665 1696 1719 1727 1828 1838 1841 1851 1863 1949 1974 1998\n",
      " 2006 2111 2124 2129 2223 2241 2256 2381 2411 2471 2585 2745 2885 3225\n",
      " 3248 3300 3320 3434 3542 3552 3654 3703]\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.1750946044921875, std: 1.899470339284668\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.6576 rank= 0.0216 kl= 0.1203\n",
      "Val loss epoch 1000: reg= 1.6982 rank= 0.2314 kl= 0.1329\n",
      "Regression R2 : 0.7609, \n",
      "Recall@1 : 0\n",
      "===============  Phase 2 ================\n",
      "y_train mean: 7.413516998291016, std: 1.8635821442468261\n",
      "Train size : 128\n",
      "Train loss epoch 1000 : reg= 2.2392 rank= 0.0253 kl= 0.1142\n",
      "Val loss epoch 1000: reg= 1.7913 rank= 0.2348 kl= 0.1283\n",
      "Regression R2 : 0.5275, \n",
      "Recall@1 : 0\n",
      "===============  Phase 3 ================\n",
      "y_train mean: 7.839242458343506, std: 1.6615817646844482\n",
      "Train size : 192\n",
      "Train loss epoch 1000 : reg= 2.0417 rank= 0.0239 kl= 0.1125\n",
      "Val loss epoch 1000: reg= 1.9092 rank= 0.1643 kl= 0.1467\n",
      "Regression R2 : 0.4474, \n",
      "Recall@1 : 1\n",
      " \n",
      "   : 192\n",
      "  : 10.59 \n",
      "=============================================\n",
      "##########  10/16 ##########\n",
      "weights: (0.3, 0.3, 0.4)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2001\n",
      "    : [  26   74   81  121  217  371  395  412  420  440  579  602  697  714\n",
      "  745  748  809  811  817  892  945 1024 1104 1206 1210 1242 1474 1493\n",
      " 1555 1562 1589 1603 1620 1637 1667 1746 1752 1764 1811 1827 1901 1974\n",
      " 2066 2069 2082 2114 2119 2189 2210 2331 2527 2656 2713 2820 2886 3122\n",
      " 3130 3151 3168 3222 3462 3538 3651 3678]\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.41241455078125, std: 1.9297140936715698\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.5571 rank= 0.0234 kl= 0.1273\n",
      "Val loss epoch 1000: reg= 1.9901 rank= 0.3251 kl= 0.1482\n",
      "Regression R2 : 0.7088, \n",
      "Recall@1 : 0\n",
      "===============  Phase 2 ================\n",
      "y_train mean: 7.437097072601318, std: 1.7596581082208251\n",
      "Train size : 128\n",
      "Train loss epoch 1000 : reg= 2.1270 rank= 0.0227 kl= 0.1079\n",
      "Val loss epoch 1000: reg= 2.0480 rank= 0.2637 kl= 0.1308\n",
      "Regression R2 : 0.6555, \n",
      "Recall@1 : 1\n",
      " \n",
      "   : 128\n",
      "  : 6.88 \n",
      "=============================================\n",
      "##########  11/16 ##########\n",
      "weights: (0.3, 0.3, 0.4)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2002\n",
      "    : [  15  135  166  294  337  392  399  406  465  490  507  528  556  560\n",
      "  632  711  728  730  831  860  877  915  921  928 1171 1198 1309 1385\n",
      " 1512 1535 1643 1675 1685 1768 1793 1881 1902 1918 2029 2124 2136 2178\n",
      " 2235 2276 2419 2538 2548 2842 3066 3102 3110 3263 3271 3306 3314 3354\n",
      " 3395 3396 3447 3465 3520 3529 3553 3595]\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.467670917510986, std: 1.6613528828485107\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.7171 rank= 0.0188 kl= 0.1186\n",
      "Val loss epoch 1000: reg= 1.1742 rank= 0.2121 kl= 0.1481\n",
      "Regression R2 : 0.7988, \n",
      "Recall@1 : 1\n",
      " \n",
      "   : 64\n",
      "  : 3.32 \n",
      "=============================================\n",
      "##########  12/16 ##########\n",
      "weights: (0.3, 0.3, 0.4)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2003\n",
      "    : [  19   57   69  146  187  276  304  396  470  522  627  714  750  878\n",
      "  961  979  997 1048 1081 1115 1168 1181 1269 1297 1336 1432 1436 1558\n",
      " 1619 1712 1852 1933 2043 2063 2094 2132 2202 2223 2394 2402 2428 2449\n",
      " 2456 2467 2473 2622 2680 2920 2924 2934 2988 3171 3190 3230 3234 3237\n",
      " 3398 3409 3417 3418 3479 3614 3672 3711]\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.559206962585449, std: 1.5440037350518798\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.1998 rank= 0.0175 kl= 0.1129\n",
      "Val loss epoch 1000: reg= 1.1913 rank= 0.1822 kl= 0.1427\n",
      "Regression R2 : 0.7705, \n",
      "Recall@1 : 0\n",
      "===============  Phase 2 ================\n",
      "y_train mean: 7.511429786682129, std: 1.5161774258477783\n",
      "Train size : 128\n",
      "Train loss epoch 1000 : reg= 1.5051 rank= 0.0194 kl= 0.1039\n",
      "Val loss epoch 1000: reg= 1.7720 rank= 0.2376 kl= 0.1309\n",
      "Regression R2 : 0.5157, \n",
      "Recall@1 : 0\n",
      "===============  Phase 3 ================\n",
      "y_train mean: 7.915276050567627, std: 1.3901513914926147\n",
      "Train size : 192\n",
      "Train loss epoch 1000 : reg= 1.6692 rank= 0.0184 kl= 0.1016\n",
      "Val loss epoch 1000: reg= 1.7002 rank= 0.1722 kl= 0.1551\n",
      "Regression R2 : 0.4647, \n",
      "Recall@1 : 0\n",
      "===============  Phase 4 ================\n",
      "y_train mean: 8.137601852416992, std: 1.2887780766351318\n",
      "Train size : 256\n",
      "Train loss epoch 1000 : reg= 1.5003 rank= 0.0183 kl= 0.1013\n",
      "Val loss epoch 1000: reg= 1.4197 rank= 0.1537 kl= 0.1580\n",
      "Regression R2 : 0.4223, \n",
      "Recall@1 : 1\n",
      " \n",
      "   : 256\n",
      "  : 14.56 \n",
      "=============================================\n",
      "##########  13/16 ##########\n",
      "weights: (0.3, 0.3, 0.4)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2004\n",
      "    : [  10  111  113  116  194  252  340  497  502  543  618  684 1068 1084\n",
      " 1095 1133 1147 1177 1291 1297 1305 1394 1404 1427 1474 1553 1563 1580\n",
      " 1685 1815 1971 2019 2020 2063 2073 2137 2172 2248 2280 2286 2293 2350\n",
      " 2437 2492 2547 2597 2603 2607 2613 2625 2639 2784 2794 2954 3011 3055\n",
      " 3082 3111 3236 3368 3372 3420 3514 3538]\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.192169189453125, std: 1.9492085080010986\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.9176 rank= 0.0222 kl= 0.1178\n",
      "Val loss epoch 1000: reg= 2.0580 rank= 0.2961 kl= 0.1422\n",
      "Regression R2 : 0.7366, \n",
      "Recall@1 : 0\n",
      "===============  Phase 2 ================\n",
      "y_train mean: 7.344030380249023, std: 1.8230483631951904\n",
      "Train size : 128\n",
      "Train loss epoch 1000 : reg= 2.3471 rank= 0.0242 kl= 0.1102\n",
      "Val loss epoch 1000: reg= 3.7894 rank= 0.5694 kl= 0.1317\n",
      "Regression R2 : 0.4663, \n",
      "Recall@1 : 0\n",
      "===============  Phase 3 ================\n",
      "y_train mean: 7.747808456420898, std: 1.6373324494226074\n",
      "Train size : 192\n",
      "Train loss epoch 1000 : reg= 1.9697 rank= 0.0234 kl= 0.1096\n",
      "Val loss epoch 1000: reg= 1.7626 rank= 0.1588 kl= 0.1589\n",
      "Regression R2 : 0.5675, \n",
      "Recall@1 : 1\n",
      " \n",
      "   : 192\n",
      "  : 10.63 \n",
      "=============================================\n",
      "##########  14/16 ##########\n",
      "weights: (0.3, 0.3, 0.4)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2005\n",
      "    : [  12   33   61  108  110  360  420  485  567  675  701  715  720  737\n",
      "  770  786  838  963  982 1141 1173 1274 1307 1309 1389 1420 1445 1473\n",
      " 1722 1777 1866 1868 2044 2083 2103 2156 2178 2203 2266 2298 2345 2399\n",
      " 2457 2481 2517 2531 2635 2648 2669 2684 2737 2739 2788 2944 2959 2981\n",
      " 3143 3300 3320 3350 3466 3503 3517 3567]\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.259561061859131, std: 1.693363080487976\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.4767 rank= 0.0184 kl= 0.1184\n",
      "Val loss epoch 1000: reg= 1.8130 rank= 0.2153 kl= 0.1405\n",
      "Regression R2 : 0.7421, \n",
      "Recall@1 : 0\n",
      "===============  Phase 2 ================\n",
      "y_train mean: 7.403188705444336, std: 1.6970096926553344\n",
      "Train size : 128\n",
      "Train loss epoch 1000 : reg= 1.9203 rank= 0.0222 kl= 0.1125\n",
      "Val loss epoch 1000: reg= 1.7279 rank= 0.2786 kl= 0.1314\n",
      "Regression R2 : 0.6236, \n",
      "Recall@1 : 0\n",
      "===============  Phase 3 ================\n",
      "y_train mean: 7.789077281951904, std: 1.535672674642334\n",
      "Train size : 192\n",
      "Train loss epoch 1000 : reg= 1.8077 rank= 0.0213 kl= 0.1089\n",
      "Val loss epoch 1000: reg= 1.8155 rank= 0.1606 kl= 0.1461\n",
      "Regression R2 : 0.4497, \n",
      "Recall@1 : 1\n",
      " \n",
      "   : 192\n",
      "  : 10.63 \n",
      "=============================================\n",
      "##########  15/16 ##########\n",
      "weights: (0.3, 0.3, 0.4)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2006\n",
      "    : [  18   46   77  203  211  213  276  350  462  535  541  560  700  730\n",
      "  809 1086 1112 1113 1139 1284 1317 1324 1420 1509 1538 1571 1671 1675\n",
      " 1747 1840 2050 2054 2157 2185 2259 2336 2433 2445 2453 2489 2509 2555\n",
      " 2689 2829 2830 2899 3009 3037 3047 3152 3160 3183 3193 3201 3239 3305\n",
      " 3339 3342 3425 3491 3533 3563 3658 3689]\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.65336799621582, std: 1.9082207779748535\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.6391 rank= 0.0243 kl= 0.1156\n",
      "Val loss epoch 1000: reg= 1.6166 rank= 0.3030 kl= 0.1427\n",
      "Regression R2 : 0.7234, \n",
      "Recall@1 : 0\n",
      "===============  Phase 2 ================\n",
      "y_train mean: 7.604775428771973, std: 1.6911683182580566\n",
      "Train size : 128\n",
      "Train loss epoch 1000 : reg= 1.9647 rank= 0.0228 kl= 0.1099\n",
      "Val loss epoch 1000: reg= 1.9260 rank= 0.1196 kl= 0.1336\n",
      "Regression R2 : 0.7088, \n",
      "Recall@1 : 0\n",
      "===============  Phase 3 ================\n",
      "y_train mean: 7.93104887008667, std: 1.512060294614563\n",
      "Train size : 192\n",
      "Train loss epoch 1000 : reg= 2.2124 rank= 0.0204 kl= 0.1085\n",
      "Val loss epoch 1000: reg= 1.4764 rank= 0.1435 kl= 0.1528\n",
      "Regression R2 : 0.5862, \n",
      "Recall@1 : 0\n",
      "===============  Phase 4 ================\n",
      "y_train mean: 8.142184257507324, std: 1.3754900793893432\n",
      "Train size : 256\n",
      "Train loss epoch 1000 : reg= 1.7330 rank= 0.0198 kl= 0.1087\n",
      "Val loss epoch 1000: reg= 1.2772 rank= 0.1307 kl= 0.1573\n",
      "Regression R2 : 0.5244, \n",
      "Recall@1 : 0\n",
      "===============  Phase 5 ================\n",
      "y_train mean: 8.280851364135742, std: 1.2684450249536132\n",
      "Train size : 320\n",
      "Train loss epoch 1000 : reg= 1.7628 rank= 0.0181 kl= 0.1050\n",
      "Val loss epoch 1000: reg= 1.1891 rank= 0.1105 kl= 0.1598\n",
      "Regression R2 : 0.5077, \n",
      "Recall@1 : 1\n",
      " \n",
      "   : 320\n",
      "  : 19.28 \n",
      "=============================================\n",
      "##########  16/16 ##########\n",
      "weights: (0.3, 0.3, 0.4)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2007\n",
      "    : [  45   95  113  192  215  372  382  510  516  633  670  702  735  796\n",
      "  802  827  902 1005 1006 1038 1054 1057 1074 1082 1125 1140 1271 1322\n",
      " 1375 1378 1440 1442 1464 1674 1744 1771 1776 1913 2034 2084 2145 2186\n",
      " 2230 2240 2428 2475 2479 2482 2646 2673 2784 2790 2818 2894 2971 3032\n",
      " 3177 3225 3255 3260 3370 3383 3408 3663]\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.666424751281738, std: 1.8059845070703124\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.9755 rank= 0.0202 kl= 0.1113\n",
      "Val loss epoch 1000: reg= 1.8237 rank= 0.2468 kl= 0.1449\n",
      "Regression R2 : 0.7030, \n",
      "Recall@1 : 0\n",
      "===============  Phase 2 ================\n",
      "y_train mean: 7.518039703369141, std: 1.5955405335290527\n",
      "Train size : 128\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m vae_cost_model, optimizer, config \u001b[38;5;241m=\u001b[39m make_vae_reg_model(vae, hyperparameter, input_dim, latent_dim, hidden_dim, y_std, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    102\u001b[0m seed_everything(train_seed)\n\u001b[0;32m--> 103\u001b[0m vae_cost_model, topk_recall_signal, val_reg_r2, val_rank_r2 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvae_cost_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcosts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m reg_history\u001b[38;5;241m.\u001b[39mappend(val_reg_r2)\n\u001b[1;32m    106\u001b[0m rank_history\u001b[38;5;241m.\u001b[39mappend(val_rank_r2)\n",
      "Cell \u001b[0;32mIn[58], line 23\u001b[0m, in \u001b[0;36mtrain_regression\u001b[0;34m(vae_cost_model, optimizer, train_loader, val_loader, input_data_scaled, costs, config, top_k, use_rank)\u001b[0m\n\u001b[1;32m     19\u001b[0m train_loss, train_components \u001b[38;5;241m=\u001b[39m compute_total_loss(vae_cost_model, \n\u001b[1;32m     20\u001b[0m                                         cost_pred, mean, logvar, z, labels, \u001b[38;5;28;01mNone\u001b[39;00m, config)\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtrain_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(vae_cost_model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/work/tenset/.venv/lib/python3.8/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/tenset/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py:244\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    235\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    236\u001b[0m     (inputs,)\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[1;32m    241\u001b[0m )\n\u001b[1;32m    243\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 244\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/work/tenset/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py:127\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    121\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    122\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         )\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    126\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 127\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     )\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "#    numpy  \n",
    "all_indices = np.arange(len(input_data_scaled))\n",
    "costs = np.array(records[\"costs\"], dtype=np.float32)\n",
    "\n",
    "real_optimum_index = np.argmax(costs)\n",
    "\n",
    "top_k = 1\n",
    "\n",
    "train_seed = 2023\n",
    "\n",
    "\n",
    "sampling_hyper = {\n",
    "    \"measure_size\": [64],\n",
    "    \"weight\" : [\n",
    "            # (1.0, 0.0, 0.0),\n",
    "            # (0.7, 0.0, 0.3),\n",
    "            # (0.7, 0.3, 0.0),\n",
    "            # (0.6, 0.1, 0.3),\n",
    "            # (0.3, 0.4, 0.3),\n",
    "            (0.4, 0.3, 0.3),\n",
    "            (0.3, 0.3, 0.4),\n",
    "            # (0.5, 0.2, 0.3),\n",
    "            ],\n",
    "    \"uncertainty_topk\": [64],\n",
    "    # \"weight\" : f_weights,\n",
    "    \"grad_num\": [4],\n",
    "    \"rand_num\": [0],\n",
    "    \n",
    "    \"T_mc\": [20],\n",
    "    \"seed\" : range(2000, 2008),\n",
    "    # \"seed\" : [2023,2025],\n",
    "}\n",
    "\n",
    "random_indices_list = []\n",
    "all_results = []\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%m%d_%H%M\")\n",
    "filename = f\"result/vae_extent_search_results_{now}.csv\"\n",
    "\n",
    "for params in itertools.product(*sampling_hyper.values()):\n",
    "\n",
    "    cnt += 1\n",
    "    print(f\"##########  {cnt}/{len(list(itertools.product(*sampling_hyper.values())))} ##########\")\n",
    "\n",
    "    tic = time.time()\n",
    "    # used_indices :    . train_indices \n",
    "    # remaining_indices :     . val_indices \n",
    "    used_indices = set()\n",
    "    remaining_indices = set(all_indices)\n",
    "    \n",
    "    measure_size, weight, uncertainty_topk, grad_num, rand_num, T_mc, sampling_seed = params\n",
    "    w_cost, w_unc, w_div = weight\n",
    "    print(f\"weights: {weight}\")\n",
    "    print(f\"measure_size: {measure_size}, T_mc: {T_mc}, sampling_seed: {sampling_seed}\")\n",
    "\n",
    "    sampling_rng = np.random.default_rng(sampling_seed)\n",
    "\n",
    "    hyperparameter = {\n",
    "\n",
    "        'lambda_reg' : [0.01],\n",
    "        'lambda_pair': [3.0],\n",
    "        'margin_scale': [0.3],\n",
    "        'gamma': [0.01],\n",
    "        'beta': [0.01],\n",
    "        'noise_std': [0.001],\n",
    "\n",
    "        'encoder_lr': [1e-4],\n",
    "        'feature_predictor_lr': [0],\n",
    "        'cost_predictor_lr': [1e-2],\n",
    "        'epochs': [1000],\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    random_indices, remaining_indices = random_select_indices(remaining_indices, select_size=sampling_hyper[\"measure_size\"][0], rng=sampling_rng)\n",
    "    print(f\"    : {np.sort(random_indices)}\")\n",
    "    used_indices.update(random_indices)\n",
    "    random_indices_list.append(random_indices)\n",
    "\n",
    "    reg_history = []\n",
    "    rank_history = []\n",
    "\n",
    "    for phase in range(1, len(input_data_scaled) // measure_size + 1):\n",
    "\n",
    "        print(f\"===============  Phase {phase} ================\")\n",
    "\n",
    "\n",
    "        # DataLoader \n",
    "        seed_everything(train_seed)\n",
    "        train_loader, val_loader, y_mean, y_std = make_vae_reg_dataloaders(input_data_scaled, costs, used_indices, remaining_indices)\n",
    "\n",
    "        \n",
    "        vae_cost_model, optimizer, config = make_vae_reg_model(vae, hyperparameter, input_dim, latent_dim, hidden_dim, y_std, verbose=False)\n",
    "        \n",
    "        seed_everything(train_seed)\n",
    "        vae_cost_model, topk_recall_signal, val_reg_r2, val_rank_r2 = train_regression(vae_cost_model, optimizer, train_loader, val_loader, input_data_scaled, costs, config, top_k=top_k, use_rank=False)\n",
    "\n",
    "        reg_history.append(val_reg_r2)\n",
    "        rank_history.append(val_rank_r2)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        #    \n",
    "        selected_indices, remaining_indices = select_programs(\n",
    "            model=vae_cost_model,\n",
    "            input_data_scaled=input_data_scaled,\n",
    "            remaining_indices=remaining_indices,\n",
    "            used_indices=used_indices,\n",
    "            num_select=measure_size,\n",
    "            T_mc=T_mc,\n",
    "            w_cost=weight[0],\n",
    "            w_unc=weight[1],\n",
    "            w_div=weight[2],\n",
    "            # w_cost=0.3,\n",
    "            # w_unc=0.35,\n",
    "            # w_div=0.35,\n",
    "            uncertainty_topk=uncertainty_topk,\n",
    "            grad_num=grad_num,\n",
    "            rand_num=rand_num,\n",
    "            device=device,\n",
    "            rng=sampling_rng,\n",
    "            \n",
    "            topk_factor=5\n",
    "        )\n",
    "        # w_cost += 0.03\n",
    "        # w_unc -= 0.02\n",
    "        # w_div -= 0.01\n",
    "\n",
    "        # selected_indices: numpy \n",
    "        used_indices.update(selected_indices.tolist())\n",
    "\n",
    "        measured_optimum = True if real_optimum_index in used_indices else False\n",
    "\n",
    "\n",
    "        use_topk = True\n",
    "        \n",
    "\n",
    "        break_signal = False\n",
    "        if not use_topk and measured_optimum:\n",
    "            break_signal = True\n",
    "        elif use_topk and topk_recall_signal:\n",
    "            break_signal = True\n",
    "            filename= filename.replace(\"result/\", \"result_topk/topk_\")\n",
    "\n",
    "\n",
    "        if break_signal:\n",
    "            print(\" \")\n",
    "            print(\"   :\", len(used_indices)-measure_size)\n",
    "            used_time = time.time() - tic\n",
    "            print(f\"  : {used_time:.2f} \")\n",
    "            print(\"=============================================\")\n",
    "            all_results.append({\n",
    "                \"measure_size\": measure_size,\n",
    "                \"weights\": weight,\n",
    "                \"uncertainty_topk\": uncertainty_topk,\n",
    "                \"grad_num\": grad_num,\n",
    "                \"rand_num\": rand_num,\n",
    "                \"phase\" : phase,\n",
    "                \"used_time\": round(used_time, 2),\n",
    "                \"train_size\" : len(used_indices)-measure_size,\n",
    "                \"val_reg_r2\": reg_history,\n",
    "                \"val_rank_r2\": rank_history,\n",
    "                \"sampling_seed\": sampling_seed,\n",
    "                \n",
    "            })\n",
    "            if use_topk:\n",
    "                all_results[-1][\"top_k\"] = top_k\n",
    "\n",
    "            df_results = pd.DataFrame(all_results)\n",
    "            \n",
    "            \n",
    "            df_results.to_csv(filename, index=False)\n",
    "            \n",
    "            break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c10407a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "feb68db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure_size</th>\n",
       "      <th>weights</th>\n",
       "      <th>uncertainty_topk</th>\n",
       "      <th>grad_num</th>\n",
       "      <th>rand_num</th>\n",
       "      <th>phase</th>\n",
       "      <th>train_size</th>\n",
       "      <th>used_time</th>\n",
       "      <th>val_reg_r2</th>\n",
       "      <th>val_rank_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>(0.3, 0.3, 0.4)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000</td>\n",
       "      <td>192.0</td>\n",
       "      <td>10.841429</td>\n",
       "      <td>[0.7609, 0.5275, 0.4474]</td>\n",
       "      <td>[None, None, None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.875</td>\n",
       "      <td>184.0</td>\n",
       "      <td>10.333750</td>\n",
       "      <td>[0.7609, 0.3829, 0.4567]</td>\n",
       "      <td>[None, None, None]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   measure_size          weights  uncertainty_topk  grad_num  rand_num  phase  \\\n",
       "0            64  (0.3, 0.3, 0.4)                64         4         0  3.000   \n",
       "1            64  (0.4, 0.3, 0.3)                64         4         0  2.875   \n",
       "\n",
       "   train_size  used_time                val_reg_r2         val_rank_r2  \n",
       "0       192.0  10.841429  [0.7609, 0.5275, 0.4474]  [None, None, None]  \n",
       "1       184.0  10.333750  [0.7609, 0.3829, 0.4567]  [None, None, None]  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_cols = [\n",
    "    \"measure_size\",\n",
    "    \"weights\",\n",
    "    \"uncertainty_topk\",\n",
    "    \"grad_num\",\n",
    "    \"rand_num\",\n",
    "]\n",
    "\n",
    "agg_dict = {\n",
    "    \"phase\": \"mean\",\n",
    "    \"train_size\": \"mean\",\n",
    "    \"used_time\": \"mean\",\n",
    "    \"val_reg_r2\": \"first\",\n",
    "    \"val_rank_r2\": \"first\",\n",
    "}\n",
    "\n",
    "df_avg = (\n",
    "    df_results\n",
    "    .groupby(group_cols, as_index=False)\n",
    "    .agg(agg_dict)\n",
    ")\n",
    "df_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074df23e",
   "metadata": {},
   "source": [
    "## XGB test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f06d5436",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tvm.auto_scheduler.cost_model.xgb_model import XGBModelInternal\n",
    "\n",
    "\n",
    "inputs, results = auto_scheduler.RecordReader(json_file).read_lines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5970056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_inputs = []\n",
    "f_results = []\n",
    "r_costs = []\n",
    "for inp, res in zip(inputs, results):\n",
    "    cost = np.mean([c.value for c in res.costs])\n",
    "    if cost < 1e10:\n",
    "        f_inputs.append(inp)\n",
    "        f_results.append(res)\n",
    "        r_costs.append(cost)\n",
    "r_costs = np.array(r_costs, dtype=np.float32)\n",
    "\n",
    "dataset = auto_scheduler.dataset.Dataset()\n",
    "dataset.update_from_measure_pairs(f_inputs, f_results)\n",
    "\n",
    "\n",
    "raw_features = list(dataset.features.values())[0]\n",
    "raw_throughputs = list(dataset.throughputs.values())[0]\n",
    "\n",
    "# features_list = []  #   feature (seq_len, feature_dim)\n",
    "dataset_costs = []\n",
    "\n",
    "for feature, throughput in zip(raw_features, raw_throughputs):\n",
    "\n",
    "    if feature.shape[0] != 1 and throughput > 1e-8:\n",
    "        # features_list.append(feature)\n",
    "        dataset_costs.append(throughput)\n",
    "\n",
    "dataset_costs = np.array(dataset_costs, dtype=np.float32)\n",
    "# if test_indices.shape[0] == 0:\n",
    "#     train_set = dataset\n",
    "#     test_set = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a708e818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(LearningTask(workload_key='[\"0c9a5ba46ffc5e1a9e5641018527117f\", 4, 7, 7, 160, 1, 1, 160, 960, 1, 1, 1, 960, 4, 7, 7, 960]', target='cuda -keys=cuda,gpu -arch=sm_37 -max_num_threads=1024 -max_threads_per_block=1024 -registers_per_block=65536 -shared_memory_per_block=49152 -thread_warp_size=32'),\n",
       "              array([0.17120592, 0.27796993, 0.65570956, ..., 0.6159606 , 0.00544477,\n",
       "                     0.00569193]))])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.throughputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bfa4fafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   8   41   99  108  228  236  253  389  415  435  563  629  639  654\n",
      "  709  743  788  864  900  947  961  971  990  991 1121 1217 1239 1359\n",
      " 1511 1581 1665 1696 1719 1727 1828 1838 1841 1851 1863 1949 1974 1998\n",
      " 2006 2111 2124 2129 2223 2241 2256 2381 2411 2471 2585 2745 2885 3225\n",
      " 3248 3300 3320 3434 3542 3552 3654 3703]\n",
      "===============  Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.4548\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.5437\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 3 ================\n",
      "Fit a xgb booster. Train size: 192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.4947\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 4 ================\n",
      "Fit a xgb booster. Train size: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.4642\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 5 ================\n",
      "Fit a xgb booster. Train size: 320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.5580\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 6 ================\n",
      "Fit a xgb booster. Train size: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.6116\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 7 ================\n",
      "Fit a xgb booster. Train size: 448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.5327\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 8 ================\n",
      "Fit a xgb booster. Train size: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.6152\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 9 ================\n",
      "Fit a xgb booster. Train size: 576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.6029\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 10 ================\n",
      "Fit a xgb booster. Train size: 640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.6184\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 11 ================\n",
      "Fit a xgb booster. Train size: 704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.6914\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 12 ================\n",
      "Fit a xgb booster. Train size: 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.6592\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 13 ================\n",
      "Fit a xgb booster. Train size: 832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.6799\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 14 ================\n",
      "Fit a xgb booster. Train size: 896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.6959\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 15 ================\n",
      "Fit a xgb booster. Train size: 960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.6947\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 16 ================\n",
      "Fit a xgb booster. Train size: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.7150\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 17 ================\n",
      "Fit a xgb booster. Train size: 1088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.6855\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 18 ================\n",
      "Fit a xgb booster. Train size: 1152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.7493\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 19 ================\n",
      "Fit a xgb booster. Train size: 1216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.7109\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 20 ================\n",
      "Fit a xgb booster. Train size: 1280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.7133\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 21 ================\n",
      "Fit a xgb booster. Train size: 1344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.7072\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 22 ================\n",
      "Fit a xgb booster. Train size: 1408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.7174\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 23 ================\n",
      "Fit a xgb booster. Train size: 1472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.7696\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 24 ================\n",
      "Fit a xgb booster. Train size: 1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.7469\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 25 ================\n",
      "Fit a xgb booster. Train size: 1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.7739\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 26 ================\n",
      "Fit a xgb booster. Train size: 1664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.7875\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 27 ================\n",
      "Fit a xgb booster. Train size: 1728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.7727\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 28 ================\n",
      "Fit a xgb booster. Train size: 1792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8122\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 29 ================\n",
      "Fit a xgb booster. Train size: 1856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8095\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 30 ================\n",
      "Fit a xgb booster. Train size: 1920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8173\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 31 ================\n",
      "Fit a xgb booster. Train size: 1984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8270\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 32 ================\n",
      "Fit a xgb booster. Train size: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8275\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 33 ================\n",
      "Fit a xgb booster. Train size: 2112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8524\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 34 ================\n",
      "Fit a xgb booster. Train size: 2176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8392\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 35 ================\n",
      "Fit a xgb booster. Train size: 2240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8497\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 36 ================\n",
      "Fit a xgb booster. Train size: 2304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8569\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 37 ================\n",
      "Fit a xgb booster. Train size: 2368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8680\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 38 ================\n",
      "Fit a xgb booster. Train size: 2432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8725\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 39 ================\n",
      "Fit a xgb booster. Train size: 2496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8750\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 40 ================\n",
      "Fit a xgb booster. Train size: 2560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8863\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 41 ================\n",
      "Fit a xgb booster. Train size: 2624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8875\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 42 ================\n",
      "Fit a xgb booster. Train size: 2688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8965\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 43 ================\n",
      "Fit a xgb booster. Train size: 2752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8979\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 44 ================\n",
      "Fit a xgb booster. Train size: 2816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9027\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 45 ================\n",
      "Fit a xgb booster. Train size: 2880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9062\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 46 ================\n",
      "Fit a xgb booster. Train size: 2944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9078\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 47 ================\n",
      "Fit a xgb booster. Train size: 3008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9081\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 48 ================\n",
      "Fit a xgb booster. Train size: 3072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9167\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 49 ================\n",
      "Fit a xgb booster. Train size: 3136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9163\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 50 ================\n",
      "Fit a xgb booster. Train size: 3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9250\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 51 ================\n",
      "Fit a xgb booster. Train size: 3264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9272\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 52 ================\n",
      "Fit a xgb booster. Train size: 3328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9322\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 53 ================\n",
      "Fit a xgb booster. Train size: 3392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9313\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 54 ================\n",
      "Fit a xgb booster. Train size: 3456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9417\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 55 ================\n",
      "Fit a xgb booster. Train size: 3520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9476\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 56 ================\n",
      "Fit a xgb booster. Train size: 3584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9501\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 57 ================\n",
      "Fit a xgb booster. Train size: 3648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9582\n",
      "XGB Recall@1 : 0\n",
      "     \n",
      "[  26   74   81  121  217  371  395  412  420  440  579  602  697  714\n",
      "  745  748  809  811  817  892  945 1024 1104 1206 1210 1242 1474 1493\n",
      " 1555 1562 1589 1603 1620 1637 1667 1746 1752 1764 1811 1827 1901 1974\n",
      " 2066 2069 2082 2114 2119 2189 2210 2331 2527 2656 2713 2820 2886 3122\n",
      " 3130 3151 3168 3222 3462 3538 3651 3678]\n",
      "===============  Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.5684\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.5504\n",
      "XGB Recall@1 : 1\n",
      "XGB    \n",
      "  : 5.37 \n",
      "=============================================\n",
      "[  15  135  166  294  337  392  399  406  465  490  507  528  556  560\n",
      "  632  711  728  730  831  860  877  915  921  928 1171 1198 1309 1385\n",
      " 1512 1535 1643 1675 1685 1768 1793 1881 1902 1918 2029 2124 2136 2178\n",
      " 2235 2276 2419 2538 2548 2842 3066 3102 3110 3263 3271 3306 3314 3354\n",
      " 3395 3396 3447 3465 3520 3529 3553 3595]\n",
      "===============  Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.6232\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.5539\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 3 ================\n",
      "Fit a xgb booster. Train size: 192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.5662\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 4 ================\n",
      "Fit a xgb booster. Train size: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.5746\n",
      "XGB Recall@1 : 1\n",
      "XGB    \n",
      "  : 11.00 \n",
      "=============================================\n",
      "[  19   57   69  146  187  276  304  396  470  522  627  714  750  878\n",
      "  961  979  997 1048 1081 1115 1168 1181 1269 1297 1336 1432 1436 1558\n",
      " 1619 1712 1852 1933 2043 2063 2094 2132 2202 2223 2394 2402 2428 2449\n",
      " 2456 2467 2473 2622 2680 2920 2924 2934 2988 3171 3190 3230 3234 3237\n",
      " 3398 3409 3417 3418 3479 3614 3672 3711]\n",
      "===============  Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.5795\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.5620\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 3 ================\n",
      "Fit a xgb booster. Train size: 192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.6158\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 4 ================\n",
      "Fit a xgb booster. Train size: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.5386\n",
      "XGB Recall@1 : 1\n",
      "XGB    \n",
      "  : 10.86 \n",
      "=============================================\n",
      "[  10  111  113  116  194  252  340  497  502  543  618  684 1068 1084\n",
      " 1095 1133 1147 1177 1291 1297 1305 1394 1404 1427 1474 1553 1563 1580\n",
      " 1685 1815 1971 2019 2020 2063 2073 2137 2172 2248 2280 2286 2293 2350\n",
      " 2437 2492 2547 2597 2603 2607 2613 2625 2639 2784 2794 2954 3011 3055\n",
      " 3082 3111 3236 3368 3372 3420 3514 3538]\n",
      "===============  Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.4790\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.6334\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 3 ================\n",
      "Fit a xgb booster. Train size: 192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.5900\n",
      "XGB Recall@1 : 1\n",
      "XGB    \n",
      "  : 8.07 \n",
      "=============================================\n",
      "[  12   33   61  108  110  360  420  485  567  675  701  715  720  737\n",
      "  770  786  838  963  982 1141 1173 1274 1307 1309 1389 1420 1445 1473\n",
      " 1722 1777 1866 1868 2044 2083 2103 2156 2178 2203 2266 2298 2345 2399\n",
      " 2457 2481 2517 2531 2635 2648 2669 2684 2737 2739 2788 2944 2959 2981\n",
      " 3143 3300 3320 3350 3466 3503 3517 3567]\n",
      "===============  Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.3824\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.5249\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 3 ================\n",
      "Fit a xgb booster. Train size: 192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.5714\n",
      "XGB Recall@1 : 1\n",
      "XGB    \n",
      "  : 8.15 \n",
      "=============================================\n",
      "[  18   46   77  203  211  213  276  350  462  535  541  560  700  730\n",
      "  809 1086 1112 1113 1139 1284 1317 1324 1420 1509 1538 1571 1671 1675\n",
      " 1747 1840 2050 2054 2157 2185 2259 2336 2433 2445 2453 2489 2509 2555\n",
      " 2689 2829 2830 2899 3009 3037 3047 3152 3160 3183 3193 3201 3239 3305\n",
      " 3339 3342 3425 3491 3533 3563 3658 3689]\n",
      "===============  Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.5424\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.1518\n",
      "XGB Recall@1 : 1\n",
      "XGB    \n",
      "  : 5.41 \n",
      "=============================================\n",
      "[  45   95  113  192  215  372  382  510  516  633  670  702  735  796\n",
      "  802  827  902 1005 1006 1038 1054 1057 1074 1082 1125 1140 1271 1322\n",
      " 1375 1378 1440 1442 1464 1674 1744 1771 1776 1913 2034 2084 2145 2186\n",
      " 2230 2240 2428 2475 2479 2482 2646 2673 2784 2790 2818 2894 2971 3032\n",
      " 3177 3225 3255 3260 3370 3383 3408 3663]\n",
      "===============  Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.5049\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.4891\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 3 ================\n",
      "Fit a xgb booster. Train size: 192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.4107\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 4 ================\n",
      "Fit a xgb booster. Train size: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.5480\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 5 ================\n",
      "Fit a xgb booster. Train size: 320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.5860\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 6 ================\n",
      "Fit a xgb booster. Train size: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.5907\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 7 ================\n",
      "Fit a xgb booster. Train size: 448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.5394\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 8 ================\n",
      "Fit a xgb booster. Train size: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.6269\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 9 ================\n",
      "Fit a xgb booster. Train size: 576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.5634\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 10 ================\n",
      "Fit a xgb booster. Train size: 640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.5624\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 11 ================\n",
      "Fit a xgb booster. Train size: 704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.6160\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 12 ================\n",
      "Fit a xgb booster. Train size: 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.5627\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 13 ================\n",
      "Fit a xgb booster. Train size: 832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.5012\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 14 ================\n",
      "Fit a xgb booster. Train size: 896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.5402\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 15 ================\n",
      "Fit a xgb booster. Train size: 960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.6056\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 16 ================\n",
      "Fit a xgb booster. Train size: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.6375\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 17 ================\n",
      "Fit a xgb booster. Train size: 1088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.6311\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 18 ================\n",
      "Fit a xgb booster. Train size: 1152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.6253\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 19 ================\n",
      "Fit a xgb booster. Train size: 1216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.6181\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 20 ================\n",
      "Fit a xgb booster. Train size: 1280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.6594\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 21 ================\n",
      "Fit a xgb booster. Train size: 1344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.7287\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 22 ================\n",
      "Fit a xgb booster. Train size: 1408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.7175\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 23 ================\n",
      "Fit a xgb booster. Train size: 1472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.7343\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 24 ================\n",
      "Fit a xgb booster. Train size: 1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.7099\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 25 ================\n",
      "Fit a xgb booster. Train size: 1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.7510\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 26 ================\n",
      "Fit a xgb booster. Train size: 1664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.7464\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 27 ================\n",
      "Fit a xgb booster. Train size: 1728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.7649\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 28 ================\n",
      "Fit a xgb booster. Train size: 1792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.7876\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 29 ================\n",
      "Fit a xgb booster. Train size: 1856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.7808\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 30 ================\n",
      "Fit a xgb booster. Train size: 1920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.7871\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 31 ================\n",
      "Fit a xgb booster. Train size: 1984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8090\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 32 ================\n",
      "Fit a xgb booster. Train size: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8174\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 33 ================\n",
      "Fit a xgb booster. Train size: 2112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8144\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 34 ================\n",
      "Fit a xgb booster. Train size: 2176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8396\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 35 ================\n",
      "Fit a xgb booster. Train size: 2240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8538\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 36 ================\n",
      "Fit a xgb booster. Train size: 2304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8508\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 37 ================\n",
      "Fit a xgb booster. Train size: 2368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8664\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 38 ================\n",
      "Fit a xgb booster. Train size: 2432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8689\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 39 ================\n",
      "Fit a xgb booster. Train size: 2496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8706\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 40 ================\n",
      "Fit a xgb booster. Train size: 2560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8785\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 41 ================\n",
      "Fit a xgb booster. Train size: 2624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8877\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 42 ================\n",
      "Fit a xgb booster. Train size: 2688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8851\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 43 ================\n",
      "Fit a xgb booster. Train size: 2752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.8914\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 44 ================\n",
      "Fit a xgb booster. Train size: 2816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9027\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 45 ================\n",
      "Fit a xgb booster. Train size: 2880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9074\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 46 ================\n",
      "Fit a xgb booster. Train size: 2944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9067\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 47 ================\n",
      "Fit a xgb booster. Train size: 3008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9166\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 48 ================\n",
      "Fit a xgb booster. Train size: 3072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9221\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 49 ================\n",
      "Fit a xgb booster. Train size: 3136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9224\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 50 ================\n",
      "Fit a xgb booster. Train size: 3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9293\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 51 ================\n",
      "Fit a xgb booster. Train size: 3264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9263\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 52 ================\n",
      "Fit a xgb booster. Train size: 3328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9287\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 53 ================\n",
      "Fit a xgb booster. Train size: 3392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9339\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 54 ================\n",
      "Fit a xgb booster. Train size: 3456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9415\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 55 ================\n",
      "Fit a xgb booster. Train size: 3520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9489\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 56 ================\n",
      "Fit a xgb booster. Train size: 3584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9523\n",
      "XGB Recall@1 : 0\n",
      "===============  Phase 57 ================\n",
      "Fit a xgb booster. Train size: 3648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Reg R2 : 0.9554\n",
      "XGB Recall@1 : 0\n",
      "     \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "topk_size = int(measure_size * 0.95)\n",
    "eps_greedy_size = measure_size - topk_size\n",
    "\n",
    "\n",
    "seeds = sampling_hyper[\"seed\"]\n",
    "random_indices = random_indices_list[:len(seeds)]\n",
    "\n",
    "xgb_results = []\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%m%d_%H%M\")\n",
    "xgb_filename = f\"result_xgb/xgb_search_{now}.csv\"\n",
    "\n",
    "for i, seed in enumerate(seeds):\n",
    "\n",
    "    tic = time.time()\n",
    "    sample_rng = np.random.default_rng(seed)\n",
    "    \n",
    "    tenset_model = XGBModelInternal(seed=train_seed)\n",
    "\n",
    "    \n",
    "    used_indices = set(random_indices[i])\n",
    "    remaining_indices = set(all_indices)\n",
    "    remaining_indices.difference_update(used_indices)\n",
    "\n",
    "    train_indices = np.array(sorted(used_indices), dtype=np.int64)\n",
    "    test_indices = np.array(sorted(remaining_indices), dtype=np.int64)\n",
    "    print(train_indices)\n",
    "\n",
    "    reg_history = []\n",
    "    rank_history = []\n",
    "\n",
    "    for phase in range(1,  len(input_data_scaled) // measure_size + 1):\n",
    "\n",
    "        print(f\"===============  Phase {phase} ================\")\n",
    "\n",
    "        seed_everything(train_seed)\n",
    "        train_set, test_set, dataset, dataset_costs = make_xgb_datasets(inputs, results, train_indices, test_indices)\n",
    "        real_optimum_idx = np.argmax(dataset_costs)\n",
    "        seed_everything(train_seed)\n",
    "        tenset_model.fit_base(train_set=train_set)\n",
    "        xgb_all_preds = tenset_model.predict(dataset)\n",
    "        xgb_all_preds = np.array(list(xgb_all_preds.values())[0], dtype=np.float32)\n",
    "        \n",
    "        \n",
    "        xgb_reg_r2 = r2_score(dataset_costs, xgb_all_preds)\n",
    "        reg_history.append(round(xgb_reg_r2, 4))\n",
    "        print(f\"XGB Reg R2 : {xgb_reg_r2:.4f}\")\n",
    "\n",
    "        # xgb_rank_r2 = pair_accuracy(xgb_all_preds, dataset_costs)\n",
    "        # rank_history.append(round(xgb_rank_r2, 4))\n",
    "        # print(f\"XGB Rank R2 : {xgb_rank_r2:.4f}\")\n",
    "\n",
    "        recall_score = recall_at_k(torch.tensor(xgb_all_preds), torch.tensor(dataset_costs), k=10)        \n",
    "        print(f\"XGB Recall@{top_k} : {recall_score}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #    \n",
    "        train_indices, test_indices = xgb_select_indices(xgb_all_preds, \n",
    "                            train_indices, test_indices, topk_size=topk_size, eps_greedy_size=eps_greedy_size, rng=sample_rng)\n",
    "        measured_optimum = True if real_optimum_idx in train_indices else False\n",
    "\n",
    "        use_topk = True\n",
    "        \n",
    "\n",
    "        break_signal = False\n",
    "        if not use_topk and measured_optimum:\n",
    "            break_signal = True\n",
    "            \n",
    "        elif use_topk and recall_score:\n",
    "            break_signal = True\n",
    "            xgb_filename= xgb_filename.replace(\"result_xgb/\", \"result_xgb_topk/topk_\")\n",
    "\n",
    "\n",
    "        if break_signal:\n",
    "        # if recall_score:\n",
    "            print(\"XGB    \")\n",
    "            print(f\"  : {time.time() - tic:.2f} \")\n",
    "            print(\"=============================================\")\n",
    "            xgb_results.append({\n",
    "                \"measure_size\": measure_size,\n",
    "                \"phase\" : phase,\n",
    "                \"used_time\": round(time.time() - tic, 2),\n",
    "                \"train_size\" : len(train_indices) - measure_size,\n",
    "                \"val_reg_r2\": reg_history,\n",
    "                \"val_rank_r2\": rank_history,\n",
    "                \"sampling_seed\": seed,\n",
    "                \n",
    "            })\n",
    "            df_xgb_results = pd.DataFrame(xgb_results)\n",
    "            df_xgb_results.to_csv(xgb_filename, index=False)\n",
    "            # raise KeyboardInterrupt\n",
    "            break\n",
    "        \n",
    "        if test_indices.shape[0] < measure_size:\n",
    "            print(\"     \")\n",
    "            xgb_results.append({\n",
    "                \"measure_size\": measure_size,\n",
    "                \"phase\" : \"all but not found\",\n",
    "                \"used_time\": round(time.time() - tic, 2),\n",
    "                \"train_size\" : len(train_indices) - measure_size,\n",
    "                \"val_reg_r2\": reg_history,\n",
    "                \"val_rank_r2\": rank_history,\n",
    "                \"sampling_seed\": seed,\n",
    "                \n",
    "            })\n",
    "            df_xgb_results = pd.DataFrame(xgb_results)\n",
    "            df_xgb_results.to_csv(xgb_filename, index=False)\n",
    "            break\n",
    "            # raise KeyboardInterrupt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5d6f6988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure_size</th>\n",
       "      <th>train_size</th>\n",
       "      <th>used_time</th>\n",
       "      <th>val_reg_r2</th>\n",
       "      <th>val_rank_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>1056.0</td>\n",
       "      <td>63.9475</td>\n",
       "      <td>[0.4548, 0.5437, 0.4947, 0.4642, 0.558, 0.6116...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   measure_size  train_size  used_time  \\\n",
       "0            64      1056.0    63.9475   \n",
       "\n",
       "                                          val_reg_r2 val_rank_r2  \n",
       "0  [0.4548, 0.5437, 0.4947, 0.4642, 0.558, 0.6116...          []  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_cols = [\n",
    "    \"measure_size\",\n",
    "]\n",
    "\n",
    "agg_dict = {\n",
    "    # \"phase\": \"mean\",\n",
    "    \"train_size\": \"mean\",\n",
    "    \"used_time\": \"mean\",\n",
    "    \"val_reg_r2\": \"first\",\n",
    "    \"val_rank_r2\": \"first\",\n",
    "}\n",
    "\n",
    "df_avg = (\n",
    "    df_xgb_results\n",
    "    .groupby(group_cols, as_index=False)\n",
    "    .agg(agg_dict)\n",
    ")\n",
    "df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336e266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit a xgb booster. Train size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenset  Rank Accuracy: 0.8091\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tvm.auto_scheduler.cost_model.xgb_model import XGBModelInternal\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    tenset_model = XGBModelInternal()\n",
    "    tenset_model.fit_base(train_set, valid_set=test_set)\n",
    "    throughputs = np.array(list(test_set.throughputs.values()))\n",
    "\n",
    "    pred = tenset_model.predict(test_set)\n",
    "\n",
    "    true_biggest_index = np.argsort(throughputs[0])[-1]\n",
    "    biggest_indices_64 = np.argsort(list(pred.values())[0])[-64:]\n",
    "\n",
    "    # list(pred.values())[0]\n",
    "    if true_biggest_index in biggest_indices_64:\n",
    "        print(\" Tenset     throughput  !\")\n",
    "        break\n",
    "    break\n",
    "\n",
    "\n",
    "# pred, throughputs rank accuracy\n",
    "correct_pairs = 0\n",
    "total_pairs = 0\n",
    "n_samples = min(2000, throughputs.shape[-1])\n",
    "sample_indices = np.random.choice(throughputs.shape[-1], n_samples, replace=False)\n",
    "pred_values = list(pred.values())[0]\n",
    "throughput_values = throughputs.squeeze()\n",
    "rank_accuracy = pair_accuracy(pred_values, throughput_values)\n",
    "print(f\"Tenset  Rank Accuracy: {rank_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
