{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "d665df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "\n",
    "project_root = \"/root/work/tenset\"\n",
    "os.environ[\"TVM_HOME\"] = f\"{project_root}\"\n",
    "os.environ[\"TVM_LIBRARY_PATH\"] = f\"{project_root}/build\"\n",
    "if f\"{project_root}/python\" not in sys.path:\n",
    "    sys.path.insert(0, f\"{project_root}/python\")\n",
    "    \n",
    "\n",
    "sys.path = [p for p in sys.path if not p.startswith(f\"{project_root}/build\")]\n",
    "sys.path.append(f\"{project_root}/build\")\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = f\"{project_root}/build:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "fb3f4c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[auto_scheduler.SearchTask(0x87b3fad0),\n",
       " auto_scheduler.SearchTask(0x820ea480),\n",
       " auto_scheduler.SearchTask(0x834b3760),\n",
       " auto_scheduler.SearchTask(0x8040d8f0),\n",
       " auto_scheduler.SearchTask(0x7e05fe80),\n",
       " auto_scheduler.SearchTask(0x9994ca70),\n",
       " auto_scheduler.SearchTask(0x9620f600),\n",
       " auto_scheduler.SearchTask(0x4d9d4080),\n",
       " auto_scheduler.SearchTask(0x67f3a380),\n",
       " auto_scheduler.SearchTask(0x95fade50),\n",
       " auto_scheduler.SearchTask(0x7ca2c480),\n",
       " auto_scheduler.SearchTask(0x85744a00),\n",
       " auto_scheduler.SearchTask(0x927b0720),\n",
       " auto_scheduler.SearchTask(0x7d642060),\n",
       " auto_scheduler.SearchTask(0x7ec3b650),\n",
       " auto_scheduler.SearchTask(0x84a5b200),\n",
       " auto_scheduler.SearchTask(0x7c0a90c0),\n",
       " auto_scheduler.SearchTask(0x80c51c30),\n",
       " auto_scheduler.SearchTask(0x7f72c9c0),\n",
       " auto_scheduler.SearchTask(0x53acd9f0),\n",
       " auto_scheduler.SearchTask(0x98b19a70),\n",
       " auto_scheduler.SearchTask(0x54345740),\n",
       " auto_scheduler.SearchTask(0x76e60b60),\n",
       " auto_scheduler.SearchTask(0x6c9e2a90),\n",
       " auto_scheduler.SearchTask(0x86aff500),\n",
       " auto_scheduler.SearchTask(0x83c8e9c0),\n",
       " auto_scheduler.SearchTask(0x80f32fc0),\n",
       " auto_scheduler.SearchTask(0x4f779420),\n",
       " auto_scheduler.SearchTask(0x80c617b0),\n",
       " auto_scheduler.SearchTask(0x6c9720e0),\n",
       " auto_scheduler.SearchTask(0x582f5300),\n",
       " auto_scheduler.SearchTask(0x8b03e510),\n",
       " auto_scheduler.SearchTask(0x90807ea0),\n",
       " auto_scheduler.SearchTask(0x9ae11ea0),\n",
       " auto_scheduler.SearchTask(0x8bc21cc0),\n",
       " auto_scheduler.SearchTask(0x525263a0),\n",
       " auto_scheduler.SearchTask(0x6e0a9310),\n",
       " auto_scheduler.SearchTask(0x80a56bd0),\n",
       " auto_scheduler.SearchTask(0x67c8f6f0),\n",
       " auto_scheduler.SearchTask(0x811a7fe0),\n",
       " auto_scheduler.SearchTask(0xa7538af0),\n",
       " auto_scheduler.SearchTask(0x85387430),\n",
       " auto_scheduler.SearchTask(0x8bfe45c0),\n",
       " auto_scheduler.SearchTask(0x902d5ce0),\n",
       " auto_scheduler.SearchTask(0x8f51b550),\n",
       " auto_scheduler.SearchTask(0x93efaae0),\n",
       " auto_scheduler.SearchTask(0x8a26d920),\n",
       " auto_scheduler.SearchTask(0x6cb755b0),\n",
       " auto_scheduler.SearchTask(0x9940cc10),\n",
       " auto_scheduler.SearchTask(0x91138040),\n",
       " auto_scheduler.SearchTask(0x818862a0),\n",
       " auto_scheduler.SearchTask(0x88628260),\n",
       " auto_scheduler.SearchTask(0x8c4b0de0),\n",
       " auto_scheduler.SearchTask(0x850556d0),\n",
       " auto_scheduler.SearchTask(0x7c0adc80),\n",
       " auto_scheduler.SearchTask(0x817481c0),\n",
       " auto_scheduler.SearchTask(0x97da3d70),\n",
       " auto_scheduler.SearchTask(0x8eff0380),\n",
       " auto_scheduler.SearchTask(0x7f032860),\n",
       " auto_scheduler.SearchTask(0x9a164d30),\n",
       " auto_scheduler.SearchTask(0x96350a50),\n",
       " auto_scheduler.SearchTask(0x7e479c70),\n",
       " auto_scheduler.SearchTask(0x78d693d0),\n",
       " auto_scheduler.SearchTask(0x8f57b6e0),\n",
       " auto_scheduler.SearchTask(0x8def59a0),\n",
       " auto_scheduler.SearchTask(0x9af46060),\n",
       " auto_scheduler.SearchTask(0x905932f0),\n",
       " auto_scheduler.SearchTask(0x91874a40),\n",
       " auto_scheduler.SearchTask(0x8af6f5b0),\n",
       " auto_scheduler.SearchTask(0x8fadc9a0),\n",
       " auto_scheduler.SearchTask(0x89e77130),\n",
       " auto_scheduler.SearchTask(0x51cd8d40),\n",
       " auto_scheduler.SearchTask(0x91f1fa50),\n",
       " auto_scheduler.SearchTask(0x8a938000),\n",
       " auto_scheduler.SearchTask(0x765d53f0),\n",
       " auto_scheduler.SearchTask(0x8631f5d0),\n",
       " auto_scheduler.SearchTask(0x95de62f0),\n",
       " auto_scheduler.SearchTask(0x4fcaf570),\n",
       " auto_scheduler.SearchTask(0x54953530),\n",
       " auto_scheduler.SearchTask(0x6e2b5850),\n",
       " auto_scheduler.SearchTask(0x7b4913e0),\n",
       " auto_scheduler.SearchTask(0x8da81a80),\n",
       " auto_scheduler.SearchTask(0x861f3cd0),\n",
       " auto_scheduler.SearchTask(0x9901d380),\n",
       " auto_scheduler.SearchTask(0x827f06e0),\n",
       " auto_scheduler.SearchTask(0x7ca7fd90),\n",
       " auto_scheduler.SearchTask(0x7cdd8a40),\n",
       " auto_scheduler.SearchTask(0x4de80970),\n",
       " auto_scheduler.SearchTask(0x9520aad0),\n",
       " auto_scheduler.SearchTask(0x8bd3c1f0),\n",
       " auto_scheduler.SearchTask(0x8dbb0300),\n",
       " auto_scheduler.SearchTask(0x8db05c60),\n",
       " auto_scheduler.SearchTask(0x8dbb0270),\n",
       " auto_scheduler.SearchTask(0x935e3f00),\n",
       " auto_scheduler.SearchTask(0x8fda2020),\n",
       " auto_scheduler.SearchTask(0x80c199c0),\n",
       " auto_scheduler.SearchTask(0x548d22d0),\n",
       " auto_scheduler.SearchTask(0x93671e50),\n",
       " auto_scheduler.SearchTask(0x4cff4fd0),\n",
       " auto_scheduler.SearchTask(0x8169f610),\n",
       " auto_scheduler.SearchTask(0x7ee84190),\n",
       " auto_scheduler.SearchTask(0x4d154530),\n",
       " auto_scheduler.SearchTask(0x7e618940),\n",
       " auto_scheduler.SearchTask(0x54320a70),\n",
       " auto_scheduler.SearchTask(0x961e42e0),\n",
       " auto_scheduler.SearchTask(0x54017cf0),\n",
       " auto_scheduler.SearchTask(0x71cdcea0),\n",
       " auto_scheduler.SearchTask(0x86863960),\n",
       " auto_scheduler.SearchTask(0x97ec2910),\n",
       " auto_scheduler.SearchTask(0x82683870),\n",
       " auto_scheduler.SearchTask(0x4d463470),\n",
       " auto_scheduler.SearchTask(0x53bbe4c0),\n",
       " auto_scheduler.SearchTask(0x4cf73270),\n",
       " auto_scheduler.SearchTask(0x89ed2c30),\n",
       " auto_scheduler.SearchTask(0x8e378bb0),\n",
       " auto_scheduler.SearchTask(0x929981e0),\n",
       " auto_scheduler.SearchTask(0x98fedfe0),\n",
       " auto_scheduler.SearchTask(0x8eb43be0),\n",
       " auto_scheduler.SearchTask(0x4d9d0c40),\n",
       " auto_scheduler.SearchTask(0x8ddd5c80),\n",
       " auto_scheduler.SearchTask(0x7788cb30),\n",
       " auto_scheduler.SearchTask(0x8a235710),\n",
       " auto_scheduler.SearchTask(0x800f8a80),\n",
       " auto_scheduler.SearchTask(0x76270250),\n",
       " auto_scheduler.SearchTask(0x8f2ecd50),\n",
       " auto_scheduler.SearchTask(0x803ef530),\n",
       " auto_scheduler.SearchTask(0x801c3620),\n",
       " auto_scheduler.SearchTask(0x545f54d0),\n",
       " auto_scheduler.SearchTask(0x53e76010),\n",
       " auto_scheduler.SearchTask(0x9911f390),\n",
       " auto_scheduler.SearchTask(0x82a9b0d0),\n",
       " auto_scheduler.SearchTask(0x883d5820),\n",
       " auto_scheduler.SearchTask(0x99bc63a0),\n",
       " auto_scheduler.SearchTask(0x71028970),\n",
       " auto_scheduler.SearchTask(0x88b38070),\n",
       " auto_scheduler.SearchTask(0x7f84cc10),\n",
       " auto_scheduler.SearchTask(0x99c6a810),\n",
       " auto_scheduler.SearchTask(0x679e9e90),\n",
       " auto_scheduler.SearchTask(0x802b78f0),\n",
       " auto_scheduler.SearchTask(0x699953a0),\n",
       " auto_scheduler.SearchTask(0x9a0e6560),\n",
       " auto_scheduler.SearchTask(0x4f1ec500),\n",
       " auto_scheduler.SearchTask(0x9117d870),\n",
       " auto_scheduler.SearchTask(0x88a269e0),\n",
       " auto_scheduler.SearchTask(0x84773310),\n",
       " auto_scheduler.SearchTask(0x513d08d0),\n",
       " auto_scheduler.SearchTask(0x85224900),\n",
       " auto_scheduler.SearchTask(0x878c5200),\n",
       " auto_scheduler.SearchTask(0x4f895e00),\n",
       " auto_scheduler.SearchTask(0x84f95ac0),\n",
       " auto_scheduler.SearchTask(0x6541aef0),\n",
       " auto_scheduler.SearchTask(0x6f3fe680),\n",
       " auto_scheduler.SearchTask(0x4f29de10),\n",
       " auto_scheduler.SearchTask(0x711ae860),\n",
       " auto_scheduler.SearchTask(0x7fe1c270),\n",
       " auto_scheduler.SearchTask(0x67832af0),\n",
       " auto_scheduler.SearchTask(0x74e513b0),\n",
       " auto_scheduler.SearchTask(0x77747cb0),\n",
       " auto_scheduler.SearchTask(0x6bb24600),\n",
       " auto_scheduler.SearchTask(0x881b5600),\n",
       " auto_scheduler.SearchTask(0x6b7ed950),\n",
       " auto_scheduler.SearchTask(0x5e0e70a0),\n",
       " auto_scheduler.SearchTask(0x6cfcb310),\n",
       " auto_scheduler.SearchTask(0x754c9810),\n",
       " auto_scheduler.SearchTask(0x6c699720),\n",
       " auto_scheduler.SearchTask(0x4f721c70),\n",
       " auto_scheduler.SearchTask(0x505a1a80),\n",
       " auto_scheduler.SearchTask(0x8a44e260),\n",
       " auto_scheduler.SearchTask(0x5279cb10),\n",
       " auto_scheduler.SearchTask(0x90da79d0),\n",
       " auto_scheduler.SearchTask(0x91f6e580),\n",
       " auto_scheduler.SearchTask(0x516d74a0),\n",
       " auto_scheduler.SearchTask(0x8d503270),\n",
       " auto_scheduler.SearchTask(0x8e170be0),\n",
       " auto_scheduler.SearchTask(0x54dc0aa0),\n",
       " auto_scheduler.SearchTask(0x51f79770),\n",
       " auto_scheduler.SearchTask(0x77dfdd70),\n",
       " auto_scheduler.SearchTask(0x93a67070),\n",
       " auto_scheduler.SearchTask(0x9318f950),\n",
       " auto_scheduler.SearchTask(0x87de7550),\n",
       " auto_scheduler.SearchTask(0x5281fe70),\n",
       " auto_scheduler.SearchTask(0x7bd15470),\n",
       " auto_scheduler.SearchTask(0x54c18860),\n",
       " auto_scheduler.SearchTask(0x8ceef110),\n",
       " auto_scheduler.SearchTask(0x6df6d360),\n",
       " auto_scheduler.SearchTask(0x948f25e0),\n",
       " auto_scheduler.SearchTask(0x7e6bbfd0),\n",
       " auto_scheduler.SearchTask(0x73260840),\n",
       " auto_scheduler.SearchTask(0x8ac393b0),\n",
       " auto_scheduler.SearchTask(0x7ee4bc20),\n",
       " auto_scheduler.SearchTask(0x98165e80),\n",
       " auto_scheduler.SearchTask(0x80c80160),\n",
       " auto_scheduler.SearchTask(0x643f0890),\n",
       " auto_scheduler.SearchTask(0x7c3af910),\n",
       " auto_scheduler.SearchTask(0x8f12bbb0),\n",
       " auto_scheduler.SearchTask(0x92047b10),\n",
       " auto_scheduler.SearchTask(0x5e9c23b0),\n",
       " auto_scheduler.SearchTask(0x8a5f89d0),\n",
       " auto_scheduler.SearchTask(0x9840df40),\n",
       " auto_scheduler.SearchTask(0x87a7b4f0),\n",
       " auto_scheduler.SearchTask(0x68fc3150),\n",
       " auto_scheduler.SearchTask(0x8e62e1a0),\n",
       " auto_scheduler.SearchTask(0x8169e1a0),\n",
       " auto_scheduler.SearchTask(0x50946160),\n",
       " auto_scheduler.SearchTask(0x52255200),\n",
       " auto_scheduler.SearchTask(0x862c9da0),\n",
       " auto_scheduler.SearchTask(0x60d89de0),\n",
       " auto_scheduler.SearchTask(0x902aaf90),\n",
       " auto_scheduler.SearchTask(0x86df24c0),\n",
       " auto_scheduler.SearchTask(0x827f0040),\n",
       " auto_scheduler.SearchTask(0x855435b0),\n",
       " auto_scheduler.SearchTask(0x7e879570),\n",
       " auto_scheduler.SearchTask(0x53ef6570),\n",
       " auto_scheduler.SearchTask(0x4d30c0d0),\n",
       " auto_scheduler.SearchTask(0x80f3d640),\n",
       " auto_scheduler.SearchTask(0x68733c50),\n",
       " auto_scheduler.SearchTask(0x7e43a4b0),\n",
       " auto_scheduler.SearchTask(0x83227c30),\n",
       " auto_scheduler.SearchTask(0x8623b800),\n",
       " auto_scheduler.SearchTask(0x863ce440),\n",
       " auto_scheduler.SearchTask(0x8c051080),\n",
       " auto_scheduler.SearchTask(0x7ea91bc0),\n",
       " auto_scheduler.SearchTask(0x8bae2de0),\n",
       " auto_scheduler.SearchTask(0x84425970),\n",
       " auto_scheduler.SearchTask(0x80545540),\n",
       " auto_scheduler.SearchTask(0x7bf5c180),\n",
       " auto_scheduler.SearchTask(0x878cec50),\n",
       " auto_scheduler.SearchTask(0x86bed830),\n",
       " auto_scheduler.SearchTask(0x958ac6f0),\n",
       " auto_scheduler.SearchTask(0x88dcbee0),\n",
       " auto_scheduler.SearchTask(0x6cccd790),\n",
       " auto_scheduler.SearchTask(0x86b5c6d0),\n",
       " auto_scheduler.SearchTask(0x98925090),\n",
       " auto_scheduler.SearchTask(0x6e3e9b30),\n",
       " auto_scheduler.SearchTask(0x9585c150),\n",
       " auto_scheduler.SearchTask(0x4e15b430),\n",
       " auto_scheduler.SearchTask(0x957807d0),\n",
       " auto_scheduler.SearchTask(0x4da6b9e0),\n",
       " auto_scheduler.SearchTask(0x880dc770),\n",
       " auto_scheduler.SearchTask(0x4cd80ac0),\n",
       " auto_scheduler.SearchTask(0x7e67f6c0),\n",
       " auto_scheduler.SearchTask(0x94bff690),\n",
       " auto_scheduler.SearchTask(0x937e8fc0),\n",
       " auto_scheduler.SearchTask(0x679297a0),\n",
       " auto_scheduler.SearchTask(0x6fbf3800),\n",
       " auto_scheduler.SearchTask(0x7d40fb90),\n",
       " auto_scheduler.SearchTask(0x810e7f40),\n",
       " auto_scheduler.SearchTask(0x81d0cbe0),\n",
       " auto_scheduler.SearchTask(0x524e9a90),\n",
       " auto_scheduler.SearchTask(0x67ed1a30),\n",
       " auto_scheduler.SearchTask(0x5481c9e0),\n",
       " auto_scheduler.SearchTask(0x97cd18c0),\n",
       " auto_scheduler.SearchTask(0x813f3ae0),\n",
       " auto_scheduler.SearchTask(0x98de7960),\n",
       " auto_scheduler.SearchTask(0x81e504b0),\n",
       " auto_scheduler.SearchTask(0x8513aee0),\n",
       " auto_scheduler.SearchTask(0x87210080),\n",
       " auto_scheduler.SearchTask(0x7c5bd760),\n",
       " auto_scheduler.SearchTask(0x7c019da0),\n",
       " auto_scheduler.SearchTask(0x4d9c8590),\n",
       " auto_scheduler.SearchTask(0x6d792ed0),\n",
       " auto_scheduler.SearchTask(0x71edc590),\n",
       " auto_scheduler.SearchTask(0x57ccc2a0),\n",
       " auto_scheduler.SearchTask(0x7cd5e150),\n",
       " auto_scheduler.SearchTask(0x4d27b110),\n",
       " auto_scheduler.SearchTask(0x4d27b1a0),\n",
       " auto_scheduler.SearchTask(0x95c7d250),\n",
       " auto_scheduler.SearchTask(0x83ad9d10),\n",
       " auto_scheduler.SearchTask(0x838f51b0),\n",
       " auto_scheduler.SearchTask(0x46e17970),\n",
       " auto_scheduler.SearchTask(0x86061ed0),\n",
       " auto_scheduler.SearchTask(0x54631ba0),\n",
       " auto_scheduler.SearchTask(0x7eb6cc00),\n",
       " auto_scheduler.SearchTask(0x82688810),\n",
       " auto_scheduler.SearchTask(0x95e58d70),\n",
       " auto_scheduler.SearchTask(0x549489c0),\n",
       " auto_scheduler.SearchTask(0x4e324880),\n",
       " auto_scheduler.SearchTask(0x9548e300),\n",
       " auto_scheduler.SearchTask(0x8509ad70),\n",
       " auto_scheduler.SearchTask(0x99aac610),\n",
       " auto_scheduler.SearchTask(0x7bc2b2e0),\n",
       " auto_scheduler.SearchTask(0x7d202940),\n",
       " auto_scheduler.SearchTask(0x94e75750),\n",
       " auto_scheduler.SearchTask(0x7cc5f9d0),\n",
       " auto_scheduler.SearchTask(0x81734b00),\n",
       " auto_scheduler.SearchTask(0x99ffdf80),\n",
       " auto_scheduler.SearchTask(0x99ffe360),\n",
       " auto_scheduler.SearchTask(0x859b7c50),\n",
       " auto_scheduler.SearchTask(0x84f2c8c0),\n",
       " auto_scheduler.SearchTask(0x95740c90),\n",
       " auto_scheduler.SearchTask(0x7d409960),\n",
       " auto_scheduler.SearchTask(0x7c3e4f10),\n",
       " auto_scheduler.SearchTask(0x83d8c430),\n",
       " auto_scheduler.SearchTask(0x83d8c210),\n",
       " auto_scheduler.SearchTask(0x82b5ba10),\n",
       " auto_scheduler.SearchTask(0x53edc380),\n",
       " auto_scheduler.SearchTask(0x7fb55cb0),\n",
       " auto_scheduler.SearchTask(0x83453880),\n",
       " auto_scheduler.SearchTask(0x84278ac0),\n",
       " auto_scheduler.SearchTask(0x4d73b000),\n",
       " auto_scheduler.SearchTask(0x97c805d0),\n",
       " auto_scheduler.SearchTask(0x837c9bd0),\n",
       " auto_scheduler.SearchTask(0x8335b2e0),\n",
       " auto_scheduler.SearchTask(0x4dadbbb0),\n",
       " auto_scheduler.SearchTask(0x8897efe0),\n",
       " auto_scheduler.SearchTask(0x87363940),\n",
       " auto_scheduler.SearchTask(0x7ca0e4f0),\n",
       " auto_scheduler.SearchTask(0x98246480),\n",
       " auto_scheduler.SearchTask(0x82d2f170),\n",
       " auto_scheduler.SearchTask(0x875aa900),\n",
       " auto_scheduler.SearchTask(0x7e486d10),\n",
       " auto_scheduler.SearchTask(0x5d568810),\n",
       " auto_scheduler.SearchTask(0x826f1ef0),\n",
       " auto_scheduler.SearchTask(0x99dfe700),\n",
       " auto_scheduler.SearchTask(0x7c39fa60),\n",
       " auto_scheduler.SearchTask(0x4d406dc0),\n",
       " auto_scheduler.SearchTask(0x4d8651c0),\n",
       " auto_scheduler.SearchTask(0x98089de0),\n",
       " auto_scheduler.SearchTask(0x53d76a10),\n",
       " auto_scheduler.SearchTask(0x99178da0),\n",
       " auto_scheduler.SearchTask(0x823c0fa0),\n",
       " auto_scheduler.SearchTask(0x81bd4c10),\n",
       " auto_scheduler.SearchTask(0x84addb80),\n",
       " auto_scheduler.SearchTask(0x88ab93f0),\n",
       " auto_scheduler.SearchTask(0x6df6e2c0),\n",
       " auto_scheduler.SearchTask(0x83aff000),\n",
       " auto_scheduler.SearchTask(0x7fe41210),\n",
       " auto_scheduler.SearchTask(0x4dee8350),\n",
       " auto_scheduler.SearchTask(0x80ec9590),\n",
       " auto_scheduler.SearchTask(0x7c83f6d0),\n",
       " auto_scheduler.SearchTask(0x7eddfe70),\n",
       " auto_scheduler.SearchTask(0x4dc18a30),\n",
       " auto_scheduler.SearchTask(0x99743c40),\n",
       " auto_scheduler.SearchTask(0x839a2640),\n",
       " auto_scheduler.SearchTask(0x87c2d250),\n",
       " auto_scheduler.SearchTask(0x81f8bb90),\n",
       " auto_scheduler.SearchTask(0x811573a0),\n",
       " auto_scheduler.SearchTask(0x87fbd240),\n",
       " auto_scheduler.SearchTask(0x7fc9b690),\n",
       " auto_scheduler.SearchTask(0x94c6eaa0),\n",
       " auto_scheduler.SearchTask(0x95a7f880),\n",
       " auto_scheduler.SearchTask(0x6addab90),\n",
       " auto_scheduler.SearchTask(0x7c640dd0),\n",
       " auto_scheduler.SearchTask(0x986787a0),\n",
       " auto_scheduler.SearchTask(0x7d932ad0),\n",
       " auto_scheduler.SearchTask(0x6c8b0920),\n",
       " auto_scheduler.SearchTask(0x86b60390),\n",
       " auto_scheduler.SearchTask(0x53dbd790),\n",
       " auto_scheduler.SearchTask(0x5dffbca0),\n",
       " auto_scheduler.SearchTask(0x7e7413e0),\n",
       " auto_scheduler.SearchTask(0x7bf7ebe0),\n",
       " auto_scheduler.SearchTask(0x83c3a030),\n",
       " auto_scheduler.SearchTask(0x846bad40),\n",
       " auto_scheduler.SearchTask(0x702aa0a0),\n",
       " auto_scheduler.SearchTask(0x7bc74e20),\n",
       " auto_scheduler.SearchTask(0x84f07a70),\n",
       " auto_scheduler.SearchTask(0x7ce6ce40),\n",
       " auto_scheduler.SearchTask(0x4dca5b00),\n",
       " auto_scheduler.SearchTask(0x8473b030),\n",
       " auto_scheduler.SearchTask(0x83ad4ec0),\n",
       " auto_scheduler.SearchTask(0x6ee7cb40),\n",
       " auto_scheduler.SearchTask(0x80cb8910),\n",
       " auto_scheduler.SearchTask(0x99397560),\n",
       " auto_scheduler.SearchTask(0x7fe986b0),\n",
       " auto_scheduler.SearchTask(0x980fa4e0),\n",
       " auto_scheduler.SearchTask(0x7d459630),\n",
       " auto_scheduler.SearchTask(0x6e8ddb70),\n",
       " auto_scheduler.SearchTask(0x71735d50),\n",
       " auto_scheduler.SearchTask(0x6b6c2da0),\n",
       " auto_scheduler.SearchTask(0x6daff110),\n",
       " auto_scheduler.SearchTask(0x963fc180),\n",
       " auto_scheduler.SearchTask(0x98825050),\n",
       " auto_scheduler.SearchTask(0x6c8d2610),\n",
       " auto_scheduler.SearchTask(0x7d29a470),\n",
       " auto_scheduler.SearchTask(0x68863860),\n",
       " auto_scheduler.SearchTask(0x4d777870),\n",
       " auto_scheduler.SearchTask(0x80ea4680),\n",
       " auto_scheduler.SearchTask(0x7d8c2090),\n",
       " auto_scheduler.SearchTask(0x7ac9ef50),\n",
       " auto_scheduler.SearchTask(0x7ac9ebb0),\n",
       " auto_scheduler.SearchTask(0x83c19490),\n",
       " auto_scheduler.SearchTask(0x959b37c0),\n",
       " auto_scheduler.SearchTask(0x81f7bb40),\n",
       " auto_scheduler.SearchTask(0x6dfb14e0),\n",
       " auto_scheduler.SearchTask(0x6beaa6a0),\n",
       " auto_scheduler.SearchTask(0x981a2f30),\n",
       " auto_scheduler.SearchTask(0x6d2c4020),\n",
       " auto_scheduler.SearchTask(0x7bdd8bb0),\n",
       " auto_scheduler.SearchTask(0x8035fa60),\n",
       " auto_scheduler.SearchTask(0x83944da0),\n",
       " auto_scheduler.SearchTask(0x7bba5ae0),\n",
       " auto_scheduler.SearchTask(0x99791cf0),\n",
       " auto_scheduler.SearchTask(0x6d1f74d0),\n",
       " auto_scheduler.SearchTask(0x85a9f990),\n",
       " auto_scheduler.SearchTask(0x99eaee70),\n",
       " auto_scheduler.SearchTask(0x8155fce0),\n",
       " auto_scheduler.SearchTask(0x7e970170),\n",
       " auto_scheduler.SearchTask(0x957689c0),\n",
       " auto_scheduler.SearchTask(0x98a01640),\n",
       " auto_scheduler.SearchTask(0x9922d300),\n",
       " auto_scheduler.SearchTask(0x85216ba0),\n",
       " auto_scheduler.SearchTask(0x7c7a0ed0),\n",
       " auto_scheduler.SearchTask(0x94ed6e80),\n",
       " auto_scheduler.SearchTask(0x6ab59730),\n",
       " auto_scheduler.SearchTask(0x81cdc300),\n",
       " auto_scheduler.SearchTask(0x7f1c42b0),\n",
       " auto_scheduler.SearchTask(0x92ae9830),\n",
       " auto_scheduler.SearchTask(0x80075240),\n",
       " auto_scheduler.SearchTask(0x86b76cf0),\n",
       " auto_scheduler.SearchTask(0x86d339d0),\n",
       " auto_scheduler.SearchTask(0x4d155be0),\n",
       " auto_scheduler.SearchTask(0x545200c0),\n",
       " auto_scheduler.SearchTask(0x4ddb7e30),\n",
       " auto_scheduler.SearchTask(0x9587d120),\n",
       " auto_scheduler.SearchTask(0x9918b670),\n",
       " auto_scheduler.SearchTask(0x7c0789b0),\n",
       " auto_scheduler.SearchTask(0x7ccc8670),\n",
       " auto_scheduler.SearchTask(0x72642120),\n",
       " auto_scheduler.SearchTask(0x800b0a20),\n",
       " auto_scheduler.SearchTask(0x85816f70),\n",
       " auto_scheduler.SearchTask(0x704fe030),\n",
       " auto_scheduler.SearchTask(0x7dfbc560),\n",
       " auto_scheduler.SearchTask(0x88801500),\n",
       " auto_scheduler.SearchTask(0x4e0245e0),\n",
       " auto_scheduler.SearchTask(0x4f76f600),\n",
       " auto_scheduler.SearchTask(0x7fa076a0),\n",
       " auto_scheduler.SearchTask(0x70e9b450),\n",
       " auto_scheduler.SearchTask(0x7e062db0),\n",
       " auto_scheduler.SearchTask(0x88ea0ef0),\n",
       " auto_scheduler.SearchTask(0x981279a0),\n",
       " auto_scheduler.SearchTask(0x85c13e80),\n",
       " auto_scheduler.SearchTask(0x86ef45f0),\n",
       " auto_scheduler.SearchTask(0x7f9b5870),\n",
       " auto_scheduler.SearchTask(0x82f34f90),\n",
       " auto_scheduler.SearchTask(0x808f83c0),\n",
       " auto_scheduler.SearchTask(0x8717b120),\n",
       " auto_scheduler.SearchTask(0x80efea00),\n",
       " auto_scheduler.SearchTask(0x8aba9320),\n",
       " auto_scheduler.SearchTask(0x8c888040),\n",
       " auto_scheduler.SearchTask(0x6cc2a890),\n",
       " auto_scheduler.SearchTask(0x6dc1c0a0),\n",
       " auto_scheduler.SearchTask(0x4d8de370),\n",
       " auto_scheduler.SearchTask(0x80b00250),\n",
       " auto_scheduler.SearchTask(0x98dc4eb0),\n",
       " auto_scheduler.SearchTask(0x7a22cfd0),\n",
       " auto_scheduler.SearchTask(0x95de0c90),\n",
       " auto_scheduler.SearchTask(0x8007ea20),\n",
       " auto_scheduler.SearchTask(0x924d70e0),\n",
       " auto_scheduler.SearchTask(0x8454f3e0),\n",
       " auto_scheduler.SearchTask(0x724a1970),\n",
       " auto_scheduler.SearchTask(0x832b21f0),\n",
       " auto_scheduler.SearchTask(0x7d7e36e0),\n",
       " auto_scheduler.SearchTask(0x98406c10),\n",
       " auto_scheduler.SearchTask(0x4d3cb750),\n",
       " auto_scheduler.SearchTask(0x6d067ca0),\n",
       " auto_scheduler.SearchTask(0x7bc11f70),\n",
       " auto_scheduler.SearchTask(0x4d421ae0),\n",
       " auto_scheduler.SearchTask(0x4e177a90),\n",
       " auto_scheduler.SearchTask(0x4e230ba0),\n",
       " auto_scheduler.SearchTask(0x7d7da2b0),\n",
       " auto_scheduler.SearchTask(0x7f9aa980),\n",
       " auto_scheduler.SearchTask(0x7e66fcf0),\n",
       " auto_scheduler.SearchTask(0x548fd810),\n",
       " auto_scheduler.SearchTask(0x800d0450),\n",
       " auto_scheduler.SearchTask(0x7e53f3d0),\n",
       " auto_scheduler.SearchTask(0x7ea5ec30),\n",
       " auto_scheduler.SearchTask(0x85dc1d00),\n",
       " auto_scheduler.SearchTask(0x7cb4b600),\n",
       " auto_scheduler.SearchTask(0x82753910),\n",
       " auto_scheduler.SearchTask(0x990462b0),\n",
       " auto_scheduler.SearchTask(0x886a1db0),\n",
       " auto_scheduler.SearchTask(0x950266a0),\n",
       " auto_scheduler.SearchTask(0x85410580),\n",
       " auto_scheduler.SearchTask(0x7fa6c2a0),\n",
       " auto_scheduler.SearchTask(0x840ccd30),\n",
       " auto_scheduler.SearchTask(0x90bcd6d0),\n",
       " auto_scheduler.SearchTask(0x80df9060),\n",
       " auto_scheduler.SearchTask(0x820c36d0),\n",
       " auto_scheduler.SearchTask(0x6d6a35a0),\n",
       " auto_scheduler.SearchTask(0x85d35820),\n",
       " auto_scheduler.SearchTask(0x853143d0),\n",
       " auto_scheduler.SearchTask(0x99150d80),\n",
       " auto_scheduler.SearchTask(0x8156fe80),\n",
       " auto_scheduler.SearchTask(0x84fe64d0),\n",
       " auto_scheduler.SearchTask(0x7eb3b000),\n",
       " auto_scheduler.SearchTask(0xa464d250),\n",
       " auto_scheduler.SearchTask(0x84f6c840),\n",
       " auto_scheduler.SearchTask(0x98af3c60),\n",
       " auto_scheduler.SearchTask(0x7be48a70),\n",
       " auto_scheduler.SearchTask(0x87917020),\n",
       " auto_scheduler.SearchTask(0x80797a90),\n",
       " auto_scheduler.SearchTask(0x7ef747d0),\n",
       " auto_scheduler.SearchTask(0x7d9c3770),\n",
       " auto_scheduler.SearchTask(0x8701ce20),\n",
       " auto_scheduler.SearchTask(0x7e9e1ac0),\n",
       " auto_scheduler.SearchTask(0x8798ab60),\n",
       " auto_scheduler.SearchTask(0x826bc9a0),\n",
       " auto_scheduler.SearchTask(0x9586a6e0),\n",
       " auto_scheduler.SearchTask(0x5ba493b0),\n",
       " auto_scheduler.SearchTask(0x54703c30),\n",
       " auto_scheduler.SearchTask(0x918989f0),\n",
       " auto_scheduler.SearchTask(0x8d1bf090),\n",
       " auto_scheduler.SearchTask(0x95d37290),\n",
       " auto_scheduler.SearchTask(0x672aca40),\n",
       " auto_scheduler.SearchTask(0x5dd8cef0),\n",
       " auto_scheduler.SearchTask(0x78d603f0),\n",
       " auto_scheduler.SearchTask(0x93463c30),\n",
       " auto_scheduler.SearchTask(0x77024f50),\n",
       " auto_scheduler.SearchTask(0x78651020),\n",
       " auto_scheduler.SearchTask(0x99a03b60),\n",
       " auto_scheduler.SearchTask(0x8abd2130),\n",
       " auto_scheduler.SearchTask(0x8b7e9bc0),\n",
       " auto_scheduler.SearchTask(0x8d92fad0),\n",
       " auto_scheduler.SearchTask(0x9039a440),\n",
       " auto_scheduler.SearchTask(0x9252cd90),\n",
       " auto_scheduler.SearchTask(0x524aabd0),\n",
       " auto_scheduler.SearchTask(0x83798cb0),\n",
       " auto_scheduler.SearchTask(0x74368110),\n",
       " auto_scheduler.SearchTask(0x89b14820),\n",
       " auto_scheduler.SearchTask(0x867d4630),\n",
       " auto_scheduler.SearchTask(0x83742420),\n",
       " auto_scheduler.SearchTask(0x7c14da50),\n",
       " auto_scheduler.SearchTask(0x7fd8cac0),\n",
       " auto_scheduler.SearchTask(0x7fd8c640),\n",
       " auto_scheduler.SearchTask(0x8c8a7c50),\n",
       " auto_scheduler.SearchTask(0x95775b10),\n",
       " auto_scheduler.SearchTask(0x9525ee90),\n",
       " auto_scheduler.SearchTask(0x85420e30),\n",
       " auto_scheduler.SearchTask(0x57ccf670),\n",
       " auto_scheduler.SearchTask(0x83eb6c70),\n",
       " auto_scheduler.SearchTask(0x5421d870),\n",
       " auto_scheduler.SearchTask(0x808ba090),\n",
       " auto_scheduler.SearchTask(0x4dce00e0),\n",
       " auto_scheduler.SearchTask(0x99bfbc90),\n",
       " auto_scheduler.SearchTask(0x7d229a10),\n",
       " auto_scheduler.SearchTask(0x81e05c80),\n",
       " auto_scheduler.SearchTask(0x8331b590),\n",
       " auto_scheduler.SearchTask(0x9900d3a0),\n",
       " auto_scheduler.SearchTask(0x57cf0710),\n",
       " auto_scheduler.SearchTask(0x7f4ab1e0),\n",
       " auto_scheduler.SearchTask(0x5d7a4310),\n",
       " auto_scheduler.SearchTask(0x88d42160),\n",
       " auto_scheduler.SearchTask(0x4db97a00),\n",
       " auto_scheduler.SearchTask(0x86283ba0),\n",
       " auto_scheduler.SearchTask(0x84aaa2c0),\n",
       " auto_scheduler.SearchTask(0x990c5710),\n",
       " auto_scheduler.SearchTask(0x97c921d0),\n",
       " auto_scheduler.SearchTask(0x82bc56d0),\n",
       " auto_scheduler.SearchTask(0x82bc5420),\n",
       " auto_scheduler.SearchTask(0x964dac60),\n",
       " auto_scheduler.SearchTask(0x874d8ae0),\n",
       " auto_scheduler.SearchTask(0x83f87a80),\n",
       " auto_scheduler.SearchTask(0x82fedda0),\n",
       " auto_scheduler.SearchTask(0x95bab1e0),\n",
       " auto_scheduler.SearchTask(0x80912170),\n",
       " auto_scheduler.SearchTask(0x959bd3e0),\n",
       " auto_scheduler.SearchTask(0x993b2fa0),\n",
       " auto_scheduler.SearchTask(0x84713a20),\n",
       " auto_scheduler.SearchTask(0x4da219e0),\n",
       " auto_scheduler.SearchTask(0x7db16370),\n",
       " auto_scheduler.SearchTask(0x87ad0180),\n",
       " auto_scheduler.SearchTask(0x80ac1e60),\n",
       " auto_scheduler.SearchTask(0x7d343b90),\n",
       " auto_scheduler.SearchTask(0x506fe810),\n",
       " auto_scheduler.SearchTask(0x832e15f0),\n",
       " auto_scheduler.SearchTask(0x7d5f5450),\n",
       " auto_scheduler.SearchTask(0x7e07edc0),\n",
       " auto_scheduler.SearchTask(0x832401f0),\n",
       " auto_scheduler.SearchTask(0x81182e10),\n",
       " auto_scheduler.SearchTask(0x4e1c7400),\n",
       " auto_scheduler.SearchTask(0x7e4495b0),\n",
       " auto_scheduler.SearchTask(0x85271590),\n",
       " auto_scheduler.SearchTask(0x7e105ce0),\n",
       " auto_scheduler.SearchTask(0x4cfb1ee0),\n",
       " auto_scheduler.SearchTask(0x85c4e550),\n",
       " auto_scheduler.SearchTask(0x67754df0),\n",
       " auto_scheduler.SearchTask(0x98ddf200),\n",
       " auto_scheduler.SearchTask(0x855d3480),\n",
       " auto_scheduler.SearchTask(0x993dab80),\n",
       " auto_scheduler.SearchTask(0x8174e9e0),\n",
       " auto_scheduler.SearchTask(0x885b3400),\n",
       " auto_scheduler.SearchTask(0x5485a510),\n",
       " auto_scheduler.SearchTask(0x81ff6150),\n",
       " auto_scheduler.SearchTask(0x6eeb9660),\n",
       " auto_scheduler.SearchTask(0x86aeab60),\n",
       " auto_scheduler.SearchTask(0x54302be0),\n",
       " auto_scheduler.SearchTask(0x875432a0),\n",
       " auto_scheduler.SearchTask(0x9895d4c0),\n",
       " auto_scheduler.SearchTask(0x98546670),\n",
       " auto_scheduler.SearchTask(0x6606d280),\n",
       " auto_scheduler.SearchTask(0x80bd30e0),\n",
       " auto_scheduler.SearchTask(0x81e659d0),\n",
       " auto_scheduler.SearchTask(0x7d5f2600),\n",
       " auto_scheduler.SearchTask(0x4e29e050),\n",
       " auto_scheduler.SearchTask(0x4d269290),\n",
       " auto_scheduler.SearchTask(0x94c68a80),\n",
       " auto_scheduler.SearchTask(0x8674ec90),\n",
       " auto_scheduler.SearchTask(0x83da3560),\n",
       " auto_scheduler.SearchTask(0x4dba69f0),\n",
       " auto_scheduler.SearchTask(0x731dd260),\n",
       " auto_scheduler.SearchTask(0x97e9ae20),\n",
       " auto_scheduler.SearchTask(0x7fdc28a0),\n",
       " auto_scheduler.SearchTask(0x7f87f6b0),\n",
       " auto_scheduler.SearchTask(0x84a72de0),\n",
       " auto_scheduler.SearchTask(0x4d93e8f0),\n",
       " auto_scheduler.SearchTask(0x83e36b50),\n",
       " auto_scheduler.SearchTask(0x4e2319b0),\n",
       " auto_scheduler.SearchTask(0x541ddf60),\n",
       " auto_scheduler.SearchTask(0x82465c20),\n",
       " auto_scheduler.SearchTask(0x6b0a0ac0),\n",
       " auto_scheduler.SearchTask(0x88dec7d0),\n",
       " auto_scheduler.SearchTask(0x83e8aa30),\n",
       " auto_scheduler.SearchTask(0x85813bd0),\n",
       " auto_scheduler.SearchTask(0x6e1464d0),\n",
       " auto_scheduler.SearchTask(0x88ddce10),\n",
       " auto_scheduler.SearchTask(0x837a9e10),\n",
       " auto_scheduler.SearchTask(0x85ad4630),\n",
       " auto_scheduler.SearchTask(0x80634500),\n",
       " auto_scheduler.SearchTask(0x57e2d1b0),\n",
       " auto_scheduler.SearchTask(0x722a0240),\n",
       " auto_scheduler.SearchTask(0x81063350),\n",
       " auto_scheduler.SearchTask(0x85844290),\n",
       " auto_scheduler.SearchTask(0x7cf34e30),\n",
       " auto_scheduler.SearchTask(0x7db18f20),\n",
       " auto_scheduler.SearchTask(0x4d9b6880),\n",
       " auto_scheduler.SearchTask(0x87431770),\n",
       " auto_scheduler.SearchTask(0x7ce74520),\n",
       " auto_scheduler.SearchTask(0x9868dd30),\n",
       " auto_scheduler.SearchTask(0x683bada0),\n",
       " auto_scheduler.SearchTask(0x88eac810),\n",
       " auto_scheduler.SearchTask(0x54118490),\n",
       " auto_scheduler.SearchTask(0x54117a40),\n",
       " auto_scheduler.SearchTask(0x7e1d9910),\n",
       " auto_scheduler.SearchTask(0x68041f60),\n",
       " auto_scheduler.SearchTask(0x835972b0),\n",
       " auto_scheduler.SearchTask(0x662f0180),\n",
       " auto_scheduler.SearchTask(0x823d5910),\n",
       " auto_scheduler.SearchTask(0x57c85fa0),\n",
       " auto_scheduler.SearchTask(0x6d38d600),\n",
       " auto_scheduler.SearchTask(0x7cf39280),\n",
       " auto_scheduler.SearchTask(0x948f1050),\n",
       " auto_scheduler.SearchTask(0x4d831290),\n",
       " auto_scheduler.SearchTask(0x7be54200),\n",
       " auto_scheduler.SearchTask(0x860a1df0),\n",
       " auto_scheduler.SearchTask(0x83cfa4c0),\n",
       " auto_scheduler.SearchTask(0x83d3e8a0),\n",
       " auto_scheduler.SearchTask(0x85b11ea0),\n",
       " auto_scheduler.SearchTask(0x88b22520),\n",
       " auto_scheduler.SearchTask(0x72f3b3d0),\n",
       " auto_scheduler.SearchTask(0x71f5cfb0),\n",
       " auto_scheduler.SearchTask(0x7f5df420),\n",
       " auto_scheduler.SearchTask(0x87079fe0),\n",
       " auto_scheduler.SearchTask(0x870797a0),\n",
       " auto_scheduler.SearchTask(0x8007baf0),\n",
       " auto_scheduler.SearchTask(0x87b2ebb0),\n",
       " auto_scheduler.SearchTask(0x83350110),\n",
       " auto_scheduler.SearchTask(0x85d44120),\n",
       " auto_scheduler.SearchTask(0x805db760),\n",
       " auto_scheduler.SearchTask(0x7d88ba60),\n",
       " auto_scheduler.SearchTask(0x53ed98b0),\n",
       " auto_scheduler.SearchTask(0x7265f350),\n",
       " auto_scheduler.SearchTask(0x672e4c10),\n",
       " auto_scheduler.SearchTask(0x84f46bd0),\n",
       " auto_scheduler.SearchTask(0x988d8f60),\n",
       " auto_scheduler.SearchTask(0x880a6f80),\n",
       " auto_scheduler.SearchTask(0x95fe75d0),\n",
       " auto_scheduler.SearchTask(0x99b66030),\n",
       " auto_scheduler.SearchTask(0x7ffef310),\n",
       " auto_scheduler.SearchTask(0x98704da0),\n",
       " auto_scheduler.SearchTask(0x83336370),\n",
       " auto_scheduler.SearchTask(0x4d07c3b0),\n",
       " auto_scheduler.SearchTask(0x86698410),\n",
       " auto_scheduler.SearchTask(0x9818d010),\n",
       " auto_scheduler.SearchTask(0x829341c0),\n",
       " auto_scheduler.SearchTask(0x716ffc20),\n",
       " auto_scheduler.SearchTask(0x84d3f770),\n",
       " auto_scheduler.SearchTask(0x888bf4d0),\n",
       " auto_scheduler.SearchTask(0x7e2fac00),\n",
       " auto_scheduler.SearchTask(0x7eae3e60),\n",
       " auto_scheduler.SearchTask(0x84ef3610),\n",
       " auto_scheduler.SearchTask(0x98120220),\n",
       " auto_scheduler.SearchTask(0x88faa5a0),\n",
       " auto_scheduler.SearchTask(0x7c7d69d0),\n",
       " auto_scheduler.SearchTask(0x8627e8f0),\n",
       " auto_scheduler.SearchTask(0x71d30fd0),\n",
       " auto_scheduler.SearchTask(0x714863e0),\n",
       " auto_scheduler.SearchTask(0x98876ff0),\n",
       " auto_scheduler.SearchTask(0x80e1f8c0),\n",
       " auto_scheduler.SearchTask(0x642af9f0),\n",
       " auto_scheduler.SearchTask(0x98876d80),\n",
       " auto_scheduler.SearchTask(0x81d56940),\n",
       " auto_scheduler.SearchTask(0x96496da0),\n",
       " auto_scheduler.SearchTask(0x99c56e10),\n",
       " auto_scheduler.SearchTask(0x7c03afe0),\n",
       " auto_scheduler.SearchTask(0x81ab10a0),\n",
       " auto_scheduler.SearchTask(0x7c453bc0),\n",
       " auto_scheduler.SearchTask(0x8738aec0),\n",
       " auto_scheduler.SearchTask(0x57d05140),\n",
       " auto_scheduler.SearchTask(0x842c31f0),\n",
       " auto_scheduler.SearchTask(0x84c9e950),\n",
       " auto_scheduler.SearchTask(0x5e5559a0),\n",
       " auto_scheduler.SearchTask(0x83e49170),\n",
       " auto_scheduler.SearchTask(0x94996b50),\n",
       " auto_scheduler.SearchTask(0x9943e300),\n",
       " auto_scheduler.SearchTask(0x7e691d80),\n",
       " auto_scheduler.SearchTask(0x81d7d8b0),\n",
       " auto_scheduler.SearchTask(0x7ef1ad60),\n",
       " auto_scheduler.SearchTask(0x991474a0),\n",
       " auto_scheduler.SearchTask(0x53b97ca0),\n",
       " auto_scheduler.SearchTask(0x7c32cec0),\n",
       " auto_scheduler.SearchTask(0x81acccc0),\n",
       " auto_scheduler.SearchTask(0x7e56b7b0),\n",
       " auto_scheduler.SearchTask(0x88c34010),\n",
       " auto_scheduler.SearchTask(0x8840fd00),\n",
       " auto_scheduler.SearchTask(0x881d2a60),\n",
       " auto_scheduler.SearchTask(0x4df41030),\n",
       " auto_scheduler.SearchTask(0x8421e2d0),\n",
       " auto_scheduler.SearchTask(0x99da07c0),\n",
       " auto_scheduler.SearchTask(0x7f28a760),\n",
       " auto_scheduler.SearchTask(0x80b6b570),\n",
       " auto_scheduler.SearchTask(0x97c3af50),\n",
       " auto_scheduler.SearchTask(0x7c34f5a0),\n",
       " auto_scheduler.SearchTask(0x837c5ab0),\n",
       " auto_scheduler.SearchTask(0x85a550f0),\n",
       " auto_scheduler.SearchTask(0x8756a4a0),\n",
       " auto_scheduler.SearchTask(0x86376d00),\n",
       " auto_scheduler.SearchTask(0x890eeac0),\n",
       " auto_scheduler.SearchTask(0x87f01b90),\n",
       " auto_scheduler.SearchTask(0x98cb5810),\n",
       " auto_scheduler.SearchTask(0x732cc410),\n",
       " auto_scheduler.SearchTask(0x4d050660),\n",
       " auto_scheduler.SearchTask(0x84045880),\n",
       " auto_scheduler.SearchTask(0x7e5d6350),\n",
       " auto_scheduler.SearchTask(0x53a0a1b0),\n",
       " auto_scheduler.SearchTask(0x4dc52be0),\n",
       " auto_scheduler.SearchTask(0x90d26c60),\n",
       " auto_scheduler.SearchTask(0x84c50ea0),\n",
       " auto_scheduler.SearchTask(0x83aec7b0),\n",
       " auto_scheduler.SearchTask(0x7f9be440),\n",
       " auto_scheduler.SearchTask(0x98956520),\n",
       " auto_scheduler.SearchTask(0x99611390),\n",
       " auto_scheduler.SearchTask(0x80671850),\n",
       " auto_scheduler.SearchTask(0x7ff400f0),\n",
       " auto_scheduler.SearchTask(0x867cc370),\n",
       " auto_scheduler.SearchTask(0x850d3100),\n",
       " auto_scheduler.SearchTask(0x73288af0),\n",
       " auto_scheduler.SearchTask(0x87ea42f0),\n",
       " auto_scheduler.SearchTask(0x7271ca20),\n",
       " auto_scheduler.SearchTask(0x7326a250),\n",
       " auto_scheduler.SearchTask(0x7c618070),\n",
       " auto_scheduler.SearchTask(0x95df4850),\n",
       " auto_scheduler.SearchTask(0x99bf5090),\n",
       " auto_scheduler.SearchTask(0x937c9f00),\n",
       " auto_scheduler.SearchTask(0x882960d0),\n",
       " auto_scheduler.SearchTask(0x7cd82260),\n",
       " auto_scheduler.SearchTask(0x4cf94cc0),\n",
       " auto_scheduler.SearchTask(0x7d2c1f80),\n",
       " auto_scheduler.SearchTask(0x991a4b90),\n",
       " auto_scheduler.SearchTask(0x952855d0),\n",
       " auto_scheduler.SearchTask(0x95ca08e0),\n",
       " auto_scheduler.SearchTask(0x83b113d0),\n",
       " auto_scheduler.SearchTask(0x7ef96500),\n",
       " auto_scheduler.SearchTask(0x6ef4e510),\n",
       " auto_scheduler.SearchTask(0x86081900),\n",
       " auto_scheduler.SearchTask(0x91414fd0),\n",
       " auto_scheduler.SearchTask(0x8148fa90),\n",
       " auto_scheduler.SearchTask(0x7c320500),\n",
       " auto_scheduler.SearchTask(0x8446ec50),\n",
       " auto_scheduler.SearchTask(0x7de6a700),\n",
       " auto_scheduler.SearchTask(0x97f910c0),\n",
       " auto_scheduler.SearchTask(0x84704ce0),\n",
       " auto_scheduler.SearchTask(0x99572aa0),\n",
       " auto_scheduler.SearchTask(0x6e46a380),\n",
       " auto_scheduler.SearchTask(0x7dcde200),\n",
       " auto_scheduler.SearchTask(0x7dbe5400),\n",
       " auto_scheduler.SearchTask(0x950900d0),\n",
       " auto_scheduler.SearchTask(0x99b1d980),\n",
       " auto_scheduler.SearchTask(0x95553b10),\n",
       " auto_scheduler.SearchTask(0x98000390),\n",
       " auto_scheduler.SearchTask(0x86d13c60),\n",
       " auto_scheduler.SearchTask(0x6d9c0cf0),\n",
       " auto_scheduler.SearchTask(0x7c5d5f10),\n",
       " auto_scheduler.SearchTask(0x827a0140),\n",
       " auto_scheduler.SearchTask(0x7e269850),\n",
       " auto_scheduler.SearchTask(0x874a9050),\n",
       " auto_scheduler.SearchTask(0x80faf270),\n",
       " auto_scheduler.SearchTask(0x6eb1a060),\n",
       " auto_scheduler.SearchTask(0x858c05f0),\n",
       " auto_scheduler.SearchTask(0x546c3e70),\n",
       " auto_scheduler.SearchTask(0x4da00870),\n",
       " auto_scheduler.SearchTask(0x9858def0),\n",
       " auto_scheduler.SearchTask(0x84597b90),\n",
       " auto_scheduler.SearchTask(0x6ca68b20),\n",
       " auto_scheduler.SearchTask(0x80e27b20),\n",
       " auto_scheduler.SearchTask(0x7cca01c0),\n",
       " auto_scheduler.SearchTask(0x985144e0),\n",
       " auto_scheduler.SearchTask(0x9495a2d0),\n",
       " auto_scheduler.SearchTask(0x88c6a450),\n",
       " auto_scheduler.SearchTask(0x88c6a010),\n",
       " auto_scheduler.SearchTask(0x88477870),\n",
       " auto_scheduler.SearchTask(0x88a1c0b0),\n",
       " auto_scheduler.SearchTask(0x87ea58b0),\n",
       " auto_scheduler.SearchTask(0x86904e10),\n",
       " auto_scheduler.SearchTask(0x8095c3e0),\n",
       " auto_scheduler.SearchTask(0x879f75b0),\n",
       " auto_scheduler.SearchTask(0x8095c950),\n",
       " auto_scheduler.SearchTask(0x7f448cb0),\n",
       " auto_scheduler.SearchTask(0x51d41ca0),\n",
       " auto_scheduler.SearchTask(0x803b2960),\n",
       " auto_scheduler.SearchTask(0x80377e30),\n",
       " auto_scheduler.SearchTask(0x98168a80),\n",
       " auto_scheduler.SearchTask(0x84fac6e0),\n",
       " auto_scheduler.SearchTask(0x87554eb0),\n",
       " auto_scheduler.SearchTask(0x824715d0),\n",
       " auto_scheduler.SearchTask(0x879cbe30),\n",
       " auto_scheduler.SearchTask(0x847acc10),\n",
       " auto_scheduler.SearchTask(0x87a3d610),\n",
       " auto_scheduler.SearchTask(0x86a17290),\n",
       " auto_scheduler.SearchTask(0x869bd840),\n",
       " auto_scheduler.SearchTask(0x7eb6e0f0),\n",
       " auto_scheduler.SearchTask(0x6799c360),\n",
       " auto_scheduler.SearchTask(0x845cee10),\n",
       " auto_scheduler.SearchTask(0x845cf380),\n",
       " auto_scheduler.SearchTask(0x859559d0),\n",
       " auto_scheduler.SearchTask(0x7ff896d0),\n",
       " auto_scheduler.SearchTask(0x82a04be0),\n",
       " auto_scheduler.SearchTask(0x9843bc30),\n",
       " auto_scheduler.SearchTask(0x86bffe90),\n",
       " auto_scheduler.SearchTask(0x8109c660),\n",
       " auto_scheduler.SearchTask(0x80830470),\n",
       " auto_scheduler.SearchTask(0x881a3a60),\n",
       " auto_scheduler.SearchTask(0x870bb2d0),\n",
       " auto_scheduler.SearchTask(0x81292880),\n",
       " auto_scheduler.SearchTask(0x81292df0),\n",
       " auto_scheduler.SearchTask(0x817ea040),\n",
       " auto_scheduler.SearchTask(0x87e0e270),\n",
       " auto_scheduler.SearchTask(0x823d6510),\n",
       " auto_scheduler.SearchTask(0x7d6bbb30),\n",
       " auto_scheduler.SearchTask(0x4d8640c0),\n",
       " auto_scheduler.SearchTask(0x998b3ba0),\n",
       " auto_scheduler.SearchTask(0x5430b4a0),\n",
       " auto_scheduler.SearchTask(0x7f569460),\n",
       " auto_scheduler.SearchTask(0x87805c00),\n",
       " auto_scheduler.SearchTask(0x884d8f00),\n",
       " auto_scheduler.SearchTask(0x95f89420),\n",
       " auto_scheduler.SearchTask(0x99d0b280),\n",
       " auto_scheduler.SearchTask(0x80c297a0),\n",
       " auto_scheduler.SearchTask(0x7dc59950),\n",
       " auto_scheduler.SearchTask(0x7cc0e100),\n",
       " auto_scheduler.SearchTask(0x872b4b60),\n",
       " auto_scheduler.SearchTask(0x7e7ffe50),\n",
       " auto_scheduler.SearchTask(0x860234c0),\n",
       " auto_scheduler.SearchTask(0x7eee4090),\n",
       " auto_scheduler.SearchTask(0x82a0b360),\n",
       " auto_scheduler.SearchTask(0x7c9d2710),\n",
       " auto_scheduler.SearchTask(0x7bbb9b10),\n",
       " auto_scheduler.SearchTask(0x7ca8e890),\n",
       " auto_scheduler.SearchTask(0x4cfb9800),\n",
       " auto_scheduler.SearchTask(0x99900760),\n",
       " auto_scheduler.SearchTask(0x60e51980),\n",
       " auto_scheduler.SearchTask(0x87c7af70),\n",
       " auto_scheduler.SearchTask(0x53e26c80),\n",
       " auto_scheduler.SearchTask(0x7fd8abe0),\n",
       " auto_scheduler.SearchTask(0x805468e0),\n",
       " auto_scheduler.SearchTask(0x9873a550),\n",
       " auto_scheduler.SearchTask(0x9541e9b0),\n",
       " auto_scheduler.SearchTask(0x6dda4020),\n",
       " auto_scheduler.SearchTask(0x862b1170),\n",
       " auto_scheduler.SearchTask(0x82288640),\n",
       " auto_scheduler.SearchTask(0x7cffb930),\n",
       " auto_scheduler.SearchTask(0x7cffb6c0),\n",
       " auto_scheduler.SearchTask(0x87b604a0),\n",
       " auto_scheduler.SearchTask(0x8335b880),\n",
       " auto_scheduler.SearchTask(0x7c8ffeb0),\n",
       " auto_scheduler.SearchTask(0x8507edc0),\n",
       " auto_scheduler.SearchTask(0x877f0810),\n",
       " auto_scheduler.SearchTask(0x7f4302f0),\n",
       " auto_scheduler.SearchTask(0x990b80d0),\n",
       " auto_scheduler.SearchTask(0x821917d0),\n",
       " auto_scheduler.SearchTask(0x6d96d670),\n",
       " auto_scheduler.SearchTask(0x86c27430),\n",
       " auto_scheduler.SearchTask(0x7f260250),\n",
       " auto_scheduler.SearchTask(0x9997a950),\n",
       " auto_scheduler.SearchTask(0x97ff1d60),\n",
       " auto_scheduler.SearchTask(0x80c16de0),\n",
       " auto_scheduler.SearchTask(0x87d1acc0),\n",
       " auto_scheduler.SearchTask(0x7d7f7100),\n",
       " auto_scheduler.SearchTask(0x7a7dc9c0),\n",
       " auto_scheduler.SearchTask(0x8572b980),\n",
       " auto_scheduler.SearchTask(0x54091080),\n",
       " auto_scheduler.SearchTask(0x8490d410),\n",
       " auto_scheduler.SearchTask(0x4dc6c630),\n",
       " auto_scheduler.SearchTask(0x6cb53d90),\n",
       " auto_scheduler.SearchTask(0x86c16c40),\n",
       " auto_scheduler.SearchTask(0x98048c70),\n",
       " auto_scheduler.SearchTask(0x95fd20a0),\n",
       " auto_scheduler.SearchTask(0x7c1085b0),\n",
       " auto_scheduler.SearchTask(0x7f441530),\n",
       " auto_scheduler.SearchTask(0x86c4fe70),\n",
       " auto_scheduler.SearchTask(0x85f19170),\n",
       " auto_scheduler.SearchTask(0x8610d780),\n",
       " auto_scheduler.SearchTask(0x7f789c40),\n",
       " auto_scheduler.SearchTask(0x82d714f0),\n",
       " auto_scheduler.SearchTask(0x8083b9e0),\n",
       " auto_scheduler.SearchTask(0x86ceee50),\n",
       " auto_scheduler.SearchTask(0x8576a020),\n",
       " auto_scheduler.SearchTask(0x81483d70),\n",
       " auto_scheduler.SearchTask(0x81885910),\n",
       " auto_scheduler.SearchTask(0x7d0813a0),\n",
       " auto_scheduler.SearchTask(0x703908f0),\n",
       " auto_scheduler.SearchTask(0x99104510),\n",
       " auto_scheduler.SearchTask(0x70f55cb0),\n",
       " auto_scheduler.SearchTask(0x83ae0630),\n",
       " auto_scheduler.SearchTask(0x83adfc70),\n",
       " auto_scheduler.SearchTask(0x7ec0fe90),\n",
       " auto_scheduler.SearchTask(0x81054990),\n",
       " auto_scheduler.SearchTask(0x87e8a2b0),\n",
       " auto_scheduler.SearchTask(0x70184f20),\n",
       " auto_scheduler.SearchTask(0x6d132ba0),\n",
       " auto_scheduler.SearchTask(0x8104e500),\n",
       " auto_scheduler.SearchTask(0x703c9ab0),\n",
       " auto_scheduler.SearchTask(0x88d0ba50),\n",
       " auto_scheduler.SearchTask(0x57dd5d80),\n",
       " auto_scheduler.SearchTask(0x7c721c20),\n",
       " auto_scheduler.SearchTask(0x7e455310),\n",
       " auto_scheduler.SearchTask(0x7e938490),\n",
       " auto_scheduler.SearchTask(0x88662a80),\n",
       " auto_scheduler.SearchTask(0x7d805aa0),\n",
       " auto_scheduler.SearchTask(0x99e754a0),\n",
       " auto_scheduler.SearchTask(0x95c2a9c0),\n",
       " auto_scheduler.SearchTask(0x71ef5720),\n",
       " auto_scheduler.SearchTask(0x8386a870),\n",
       " auto_scheduler.SearchTask(0x81042ef0),\n",
       " auto_scheduler.SearchTask(0x7d321280),\n",
       " auto_scheduler.SearchTask(0x643db0a0),\n",
       " auto_scheduler.SearchTask(0x7d30b190),\n",
       " auto_scheduler.SearchTask(0x63a05690),\n",
       " auto_scheduler.SearchTask(0x87a9c950),\n",
       " auto_scheduler.SearchTask(0x984b82b0),\n",
       " auto_scheduler.SearchTask(0x99f631d0),\n",
       " auto_scheduler.SearchTask(0x81aa5c80),\n",
       " auto_scheduler.SearchTask(0x87381790),\n",
       " auto_scheduler.SearchTask(0x87c4c9e0),\n",
       " auto_scheduler.SearchTask(0x811d1880),\n",
       " auto_scheduler.SearchTask(0x6b446df0),\n",
       " auto_scheduler.SearchTask(0x851232f0),\n",
       " auto_scheduler.SearchTask(0x4d032830),\n",
       " auto_scheduler.SearchTask(0x95f88820),\n",
       " auto_scheduler.SearchTask(0x99ad4f10),\n",
       " auto_scheduler.SearchTask(0x70b6c810),\n",
       " auto_scheduler.SearchTask(0x7bf14900),\n",
       " auto_scheduler.SearchTask(0x4e311b40),\n",
       " auto_scheduler.SearchTask(0x6e0196d0),\n",
       " auto_scheduler.SearchTask(0x7cd980f0),\n",
       " auto_scheduler.SearchTask(0x88457480),\n",
       " auto_scheduler.SearchTask(0x819bc570),\n",
       " auto_scheduler.SearchTask(0x7d44dba0),\n",
       " auto_scheduler.SearchTask(0x83782bc0),\n",
       " auto_scheduler.SearchTask(0x82fd09a0),\n",
       " auto_scheduler.SearchTask(0x81367f40),\n",
       " auto_scheduler.SearchTask(0x86e483e0),\n",
       " auto_scheduler.SearchTask(0x716a5740),\n",
       " auto_scheduler.SearchTask(0x70f8cfc0),\n",
       " auto_scheduler.SearchTask(0x88ec39f0),\n",
       " auto_scheduler.SearchTask(0x884b25f0),\n",
       " auto_scheduler.SearchTask(0x99ad90b0),\n",
       " auto_scheduler.SearchTask(0x53b53d50),\n",
       " auto_scheduler.SearchTask(0x53ab0110),\n",
       " auto_scheduler.SearchTask(0x958cd7c0),\n",
       " auto_scheduler.SearchTask(0x6f30bad0),\n",
       " auto_scheduler.SearchTask(0x70f27e00),\n",
       " auto_scheduler.SearchTask(0x8047ae80),\n",
       " auto_scheduler.SearchTask(0x82b09880),\n",
       " auto_scheduler.SearchTask(0x6d59ce70),\n",
       " auto_scheduler.SearchTask(0x84de2ac0),\n",
       " auto_scheduler.SearchTask(0x8028ae80),\n",
       " auto_scheduler.SearchTask(0x86829eb0),\n",
       " auto_scheduler.SearchTask(0x85779f40),\n",
       " auto_scheduler.SearchTask(0x54f85670),\n",
       " auto_scheduler.SearchTask(0x95638260),\n",
       " auto_scheduler.SearchTask(0x7fc46a50),\n",
       " auto_scheduler.SearchTask(0x993fd190),\n",
       " auto_scheduler.SearchTask(0x88a31350),\n",
       " auto_scheduler.SearchTask(0x4e19b390),\n",
       " auto_scheduler.SearchTask(0x8853aba0),\n",
       " auto_scheduler.SearchTask(0x825eca90),\n",
       " auto_scheduler.SearchTask(0x984dd430),\n",
       " auto_scheduler.SearchTask(0x881146e0),\n",
       " auto_scheduler.SearchTask(0x7ee655d0),\n",
       " auto_scheduler.SearchTask(0x53e5b900),\n",
       " auto_scheduler.SearchTask(0x70df3390),\n",
       " auto_scheduler.SearchTask(0x4d30ab80),\n",
       " auto_scheduler.SearchTask(0x8315c060),\n",
       " auto_scheduler.SearchTask(0x86ad1f30),\n",
       " auto_scheduler.SearchTask(0x86b5ee60),\n",
       " auto_scheduler.SearchTask(0x8249fba0),\n",
       " auto_scheduler.SearchTask(0x818ae3f0),\n",
       " auto_scheduler.SearchTask(0x963a52a0),\n",
       " auto_scheduler.SearchTask(0x98bc29d0),\n",
       " auto_scheduler.SearchTask(0x8700a640),\n",
       " ...]"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "sys.path.append(\"/root/work/tenset/scripts\")\n",
    "from print_programs import return_all_states\n",
    "from make_dataset import load_and_register_tasks\n",
    "from tvm import auto_scheduler\n",
    "from tvm.auto_scheduler.dataset import Dataset, make_dataset_from_log_file\n",
    "# json_file = \"/root/work/tenset/dataset/measure_records_tenset/k80/([0bcb8746286db050cd088f375c85372d,1,64,64,128,6,6,32,128,1,64,64,32],cuda).json\"\n",
    "json_file = \"/root/work/tenset/dataset/measure_records_tenset/k80/([0c9a5ba46ffc5e1a9e5641018527117f,4,7,7,160,1,1,160,960,1,1,1,960,4,7,7,960],cuda).json\"\n",
    "# json_file = \"/root/work/tenset/dataset/measure_records_tenset/k80/([3eb184d18885126bd13d564ef260c820,4,16,16,256,6,6,256,256,1,1,1,256,4,16,16,256,4,16,16,256],cuda).json\"\n",
    "# json_file = \"/root/work/tenset/dataset/measure_records_tenset/k80/([8c674f26f66543069d1e1c56cda249f9,4,60,60,256,1,1,256,512,1,1,1,512,4,30,30,512],cuda).json\"\n",
    "load_and_register_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "947647eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "states, costs = return_all_states(json_file)\n",
    "records_raw = list(map(lambda x: str(x).strip(), states))\n",
    "\n",
    "records = {\"schedules\": [], \"extents\": [], \"costs\": [], \"unroll\" : [], \"all\": []}\n",
    "\n",
    "for rec, cost in zip(records_raw, costs):\n",
    "    cost = np.array([c.value for c in cost])\n",
    "    cost = -np.log(np.mean(cost) + 1e-8)\n",
    "    schedule = rec.split(\"Placeholder\")[-1][2:]\n",
    "    \n",
    "    records[\"schedules\"].append(schedule)\n",
    "    records[\"costs\"].append(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "c5e40e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "placeholder, placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,42)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,7)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,40)\n",
      "      Conv2dOutput.local auto_unroll: 512\n",
      "      for nn_c.0 (0,1)\n",
      "        for yy_c.0 (0,1)\n",
      "          for xx_c.0 (0,1)\n",
      "            for ff_c.0 (0,1)\n",
      "              for nn_c.1 (0,1)\n",
      "                for yy_c.1 (0,1)\n",
      "                  for xx_c.1 (0,1)\n",
      "                    for ff_c.1 (0,1)\n",
      "                      for nn_c.2 (0,1)\n",
      "                        for yy_c.2 (0,1)\n",
      "                          for xx_c.2 (0,1)\n",
      "                            for ff_c.2 (0,1)\n",
      "                              for ry.0 (0,1)\n",
      "                                for rx.0 (0,1)\n",
      "                                  for rc.0 (0,5)\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,128)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,40)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          placeholder.shared = ...\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,23)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,40)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          PaddedInput.shared = ...\n",
      "                                    for ry.1 (0,1)\n",
      "                                      for rx.1 (0,1)\n",
      "                                        for rc.1 (0,1)\n",
      "                                          for nn_c.3 (0,2)\n",
      "                                            for yy_c.3 (0,1)\n",
      "                                              for xx_c.3 (0,1)\n",
      "                                                for ff_c.3 (0,2)\n",
      "                                                  for ry.2 (0,1)\n",
      "                                                    for rx.2 (0,1)\n",
      "                                                      for rc.2 (0,32)\n",
      "                                                        for nn_c.4 (0,2)\n",
      "                                                          for yy_c.4 (0,1)\n",
      "                                                            for xx_c.4 (0,1)\n",
      "                                                              for ff_c.4 (0,2)\n",
      "                                                                Conv2dOutput.local = ...\n",
      "      for nn.3 (0,4)\n",
      "        for yy.3 (0,1)\n",
      "          for xx.3 (0,1)\n",
      "            for ff.3 (0,4)\n",
      "              Conv2dOutput = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,5880)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_multiply = ...\n",
      "---------------------------------------------------\n",
      "placeholder, placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,1344)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,1)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,70)\n",
      "      Conv2dOutput.local auto_unroll: 64\n",
      "      for nn_c.0 (0,1)\n",
      "        for yy_c.0 (0,1)\n",
      "          for xx_c.0 (0,1)\n",
      "            for ff_c.0 (0,1)\n",
      "              for nn_c.1 (0,1)\n",
      "                for yy_c.1 (0,1)\n",
      "                  for xx_c.1 (0,1)\n",
      "                    for ff_c.1 (0,1)\n",
      "                      for nn_c.2 (0,1)\n",
      "                        for yy_c.2 (0,1)\n",
      "                          for xx_c.2 (0,1)\n",
      "                            for ff_c.2 (0,1)\n",
      "                              for ry.0 (0,1)\n",
      "                                for rx.0 (0,1)\n",
      "                                  for rc.0 (0,5)\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,1)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,70)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "                                          placeholder.shared = ...\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,13)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,70)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          PaddedInput.shared = ...\n",
      "                                    for ry.1 (0,1)\n",
      "                                      for rx.1 (0,1)\n",
      "                                        for rc.1 (0,4)\n",
      "                                          for nn_c.3 (0,2)\n",
      "                                            for yy_c.3 (0,1)\n",
      "                                              for xx_c.3 (0,1)\n",
      "                                                for ff_c.3 (0,1)\n",
      "                                                  for ry.2 (0,1)\n",
      "                                                    for rx.2 (0,1)\n",
      "                                                      for rc.2 (0,8)\n",
      "                                                        for nn_c.4 (0,1)\n",
      "                                                          for yy_c.4 (0,1)\n",
      "                                                            for xx_c.4 (0,1)\n",
      "                                                              for ff_c.4 (0,1)\n",
      "                                                                Conv2dOutput.local = ...\n",
      "      for nn.3 (0,2)\n",
      "        for yy.3 (0,1)\n",
      "          for xx.3 (0,1)\n",
      "            for ff.3 (0,1)\n",
      "              Conv2dOutput = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,5880)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_multiply = ...\n",
      "---------------------------------------------------\n",
      "placeholder, placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,60)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,1)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,56)\n",
      "      Conv2dOutput.local auto_unroll: 512\n",
      "      for nn_c.0 (0,1)\n",
      "        for yy_c.0 (0,1)\n",
      "          for xx_c.0 (0,1)\n",
      "            for ff_c.0 (0,1)\n",
      "              for nn_c.1 (0,1)\n",
      "                for yy_c.1 (0,1)\n",
      "                  for xx_c.1 (0,1)\n",
      "                    for ff_c.1 (0,1)\n",
      "                      for nn_c.2 (0,1)\n",
      "                        for yy_c.2 (0,1)\n",
      "                          for xx_c.2 (0,1)\n",
      "                            for ff_c.2 (0,1)\n",
      "                              for ry.0 (0,1)\n",
      "                                for rx.0 (0,1)\n",
      "                                  for rc.0 (0,40)\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,5)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          placeholder.shared = ...\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,1)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,14)\n",
      "                                          PaddedInput.shared = ...\n",
      "                                    for ry.1 (0,1)\n",
      "                                      for rx.1 (0,1)\n",
      "                                        for rc.1 (0,1)\n",
      "                                          for nn_c.3 (0,1)\n",
      "                                            for yy_c.3 (0,7)\n",
      "                                              for xx_c.3 (0,1)\n",
      "                                                for ff_c.3 (0,1)\n",
      "                                                  for ry.2 (0,1)\n",
      "                                                    for rx.2 (0,1)\n",
      "                                                      for rc.2 (0,4)\n",
      "                                                        for nn_c.4 (0,1)\n",
      "                                                          for yy_c.4 (0,1)\n",
      "                                                            for xx_c.4 (0,1)\n",
      "                                                              for ff_c.4 (0,8)\n",
      "                                                                Conv2dOutput.local = ...\n",
      "      for nn.3 (0,1)\n",
      "        for yy.3 (0,7)\n",
      "          for xx.3 (0,1)\n",
      "            for ff.3 (0,8)\n",
      "              Conv2dOutput = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,5880)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_multiply = ...\n",
      "---------------------------------------------------\n",
      "placeholder, placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,448)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,1)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,7)\n",
      "      Conv2dOutput.local auto_unroll: 16\n",
      "      for nn_c.0 (0,1)\n",
      "        for yy_c.0 (0,1)\n",
      "          for xx_c.0 (0,1)\n",
      "            for ff_c.0 (0,1)\n",
      "              for nn_c.1 (0,1)\n",
      "                for yy_c.1 (0,1)\n",
      "                  for xx_c.1 (0,1)\n",
      "                    for ff_c.1 (0,1)\n",
      "                      for nn_c.2 (0,1)\n",
      "                        for yy_c.2 (0,1)\n",
      "                          for xx_c.2 (0,1)\n",
      "                            for ff_c.2 (0,1)\n",
      "                              for ry.0 (0,1)\n",
      "                                for rx.0 (0,1)\n",
      "                                  for rc.0 (0,2)\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,343)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,7)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          placeholder.shared = ...\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,160)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,7)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          PaddedInput.shared = ...\n",
      "                                    for ry.1 (0,1)\n",
      "                                      for rx.1 (0,1)\n",
      "                                        for rc.1 (0,8)\n",
      "                                          for nn_c.3 (0,2)\n",
      "                                            for yy_c.3 (0,1)\n",
      "                                              for xx_c.3 (0,1)\n",
      "                                                for ff_c.3 (0,30)\n",
      "                                                  for ry.2 (0,1)\n",
      "                                                    for rx.2 (0,1)\n",
      "                                                      for rc.2 (0,10)\n",
      "                                                        for nn_c.4 (0,1)\n",
      "                                                          for yy_c.4 (0,1)\n",
      "                                                            for xx_c.4 (0,1)\n",
      "                                                              for ff_c.4 (0,1)\n",
      "                                                                Conv2dOutput.local = ...\n",
      "      for nn.3 (0,2)\n",
      "        for yy.3 (0,1)\n",
      "          for xx.3 (0,1)\n",
      "            for ff.3 (0,30)\n",
      "              Conv2dOutput = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,5880)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_multiply = ...\n",
      "---------------------------------------------------\n",
      "placeholder, placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,24)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,2)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,56)\n",
      "      Conv2dOutput.local auto_unroll: 64\n",
      "      for nn_c.0 (0,1)\n",
      "        for yy_c.0 (0,1)\n",
      "          for xx_c.0 (0,1)\n",
      "            for ff_c.0 (0,1)\n",
      "              for nn_c.1 (0,1)\n",
      "                for yy_c.1 (0,1)\n",
      "                  for xx_c.1 (0,1)\n",
      "                    for ff_c.1 (0,1)\n",
      "                      for nn_c.2 (0,1)\n",
      "                        for yy_c.2 (0,1)\n",
      "                          for xx_c.2 (0,1)\n",
      "                            for ff_c.2 (0,1)\n",
      "                              for ry.0 (0,1)\n",
      "                                for rx.0 (0,1)\n",
      "                                  for rc.0 (0,4)\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,29)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          placeholder.shared = ...\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,140)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          PaddedInput.shared = ...\n",
      "                                    for ry.1 (0,1)\n",
      "                                      for rx.1 (0,1)\n",
      "                                        for rc.1 (0,5)\n",
      "                                          for nn_c.3 (0,1)\n",
      "                                            for yy_c.3 (0,7)\n",
      "                                              for xx_c.3 (0,1)\n",
      "                                                for ff_c.3 (0,2)\n",
      "                                                  for ry.2 (0,1)\n",
      "                                                    for rx.2 (0,1)\n",
      "                                                      for rc.2 (0,8)\n",
      "                                                        for nn_c.4 (0,1)\n",
      "                                                          for yy_c.4 (0,1)\n",
      "                                                            for xx_c.4 (0,1)\n",
      "                                                              for ff_c.4 (0,5)\n",
      "                                                                Conv2dOutput.local = ...\n",
      "      for nn.3 (0,1)\n",
      "        for yy.3 (0,7)\n",
      "          for xx.3 (0,1)\n",
      "            for ff.3 (0,10)\n",
      "              Conv2dOutput = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,5880)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_multiply = ...\n",
      "---------------------------------------------------\n",
      "placeholder, placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,28)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,2)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,168)\n",
      "      Conv2dOutput.local auto_unroll: 64\n",
      "      for nn_c.0 (0,1)\n",
      "        for yy_c.0 (0,1)\n",
      "          for xx_c.0 (0,1)\n",
      "            for ff_c.0 (0,1)\n",
      "              for nn_c.1 (0,1)\n",
      "                for yy_c.1 (0,1)\n",
      "                  for xx_c.1 (0,1)\n",
      "                    for ff_c.1 (0,1)\n",
      "                      for nn_c.2 (0,1)\n",
      "                        for yy_c.2 (0,1)\n",
      "                          for xx_c.2 (0,1)\n",
      "                            for ff_c.2 (0,1)\n",
      "                              for ry.0 (0,1)\n",
      "                                for rx.0 (0,1)\n",
      "                                  for rc.0 (0,20)\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,12)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,168)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          placeholder.shared = ...\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,2)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,168)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          PaddedInput.shared = ...\n",
      "                                    for ry.1 (0,1)\n",
      "                                      for rx.1 (0,1)\n",
      "                                        for rc.1 (0,2)\n",
      "                                          for nn_c.3 (0,2)\n",
      "                                            for yy_c.3 (0,1)\n",
      "                                              for xx_c.3 (0,1)\n",
      "                                                for ff_c.3 (0,1)\n",
      "                                                  for ry.2 (0,1)\n",
      "                                                    for rx.2 (0,1)\n",
      "                                                      for rc.2 (0,4)\n",
      "                                                        for nn_c.4 (0,1)\n",
      "                                                          for yy_c.4 (0,1)\n",
      "                                                            for xx_c.4 (0,1)\n",
      "                                                              for ff_c.4 (0,10)\n",
      "                                                                Conv2dOutput.local = ...\n",
      "      for nn.3 (0,2)\n",
      "        for yy.3 (0,1)\n",
      "          for xx.3 (0,1)\n",
      "            for ff.3 (0,10)\n",
      "              Conv2dOutput = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,5880)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_multiply = ...\n",
      "---------------------------------------------------\n",
      "placeholder, placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,24)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,1)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,56)\n",
      "      Conv2dOutput.local auto_unroll: 512\n",
      "      for nn_c.0 (0,1)\n",
      "        for yy_c.0 (0,1)\n",
      "          for xx_c.0 (0,1)\n",
      "            for ff_c.0 (0,1)\n",
      "              for nn_c.1 (0,1)\n",
      "                for yy_c.1 (0,1)\n",
      "                  for xx_c.1 (0,1)\n",
      "                    for ff_c.1 (0,1)\n",
      "                      for nn_c.2 (0,1)\n",
      "                        for yy_c.2 (0,1)\n",
      "                          for xx_c.2 (0,1)\n",
      "                            for ff_c.2 (0,1)\n",
      "                              for ry.0 (0,1)\n",
      "                                for rx.0 (0,1)\n",
      "                                  for rc.0 (0,4)\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,29)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          placeholder.shared = ...\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,140)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          PaddedInput.shared = ...\n",
      "                                    for ry.1 (0,1)\n",
      "                                      for rx.1 (0,1)\n",
      "                                        for rc.1 (0,40)\n",
      "                                          for nn_c.3 (0,2)\n",
      "                                            for yy_c.3 (0,1)\n",
      "                                              for xx_c.3 (0,1)\n",
      "                                                for ff_c.3 (0,1)\n",
      "                                                  for ry.2 (0,1)\n",
      "                                                    for rx.2 (0,1)\n",
      "                                                      for rc.2 (0,1)\n",
      "                                                        for nn_c.4 (0,2)\n",
      "                                                          for yy_c.4 (0,1)\n",
      "                                                            for xx_c.4 (0,7)\n",
      "                                                              for ff_c.4 (0,5)\n",
      "                                                                Conv2dOutput.local = ...\n",
      "      for nn.3 (0,4)\n",
      "        for yy.3 (0,1)\n",
      "          for xx.3 (0,7)\n",
      "            for ff.3 (0,5)\n",
      "              Conv2dOutput = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,5880)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_multiply = ...\n",
      "---------------------------------------------------\n",
      "placeholder, placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,2)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,4)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,140)\n",
      "      Conv2dOutput.local auto_unroll: 16\n",
      "      for nn_c.0 (0,1)\n",
      "        for yy_c.0 (0,1)\n",
      "          for xx_c.0 (0,1)\n",
      "            for ff_c.0 (0,1)\n",
      "              for nn_c.1 (0,1)\n",
      "                for yy_c.1 (0,1)\n",
      "                  for xx_c.1 (0,1)\n",
      "                    for ff_c.1 (0,1)\n",
      "                      for nn_c.2 (0,1)\n",
      "                        for yy_c.2 (0,1)\n",
      "                          for xx_c.2 (0,1)\n",
      "                            for ff_c.2 (0,1)\n",
      "                              for ry.0 (0,1)\n",
      "                                for rx.0 (0,1)\n",
      "                                  for rc.0 (0,40)\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,14)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,140)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          placeholder.shared = ...\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,6)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,140)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          PaddedInput.shared = ...\n",
      "                                    for ry.1 (0,1)\n",
      "                                      for rx.1 (0,1)\n",
      "                                        for rc.1 (0,4)\n",
      "                                          for nn_c.3 (0,1)\n",
      "                                            for yy_c.3 (0,1)\n",
      "                                              for xx_c.3 (0,1)\n",
      "                                                for ff_c.3 (0,12)\n",
      "                                                  for ry.2 (0,1)\n",
      "                                                    for rx.2 (0,1)\n",
      "                                                      for rc.2 (0,1)\n",
      "                                                        for nn_c.4 (0,1)\n",
      "                                                          for yy_c.4 (0,1)\n",
      "                                                            for xx_c.4 (0,7)\n",
      "                                                              for ff_c.4 (0,2)\n",
      "                                                                Conv2dOutput.local = ...\n",
      "      for nn.3 (0,1)\n",
      "        for yy.3 (0,1)\n",
      "          for xx.3 (0,7)\n",
      "            for ff.3 (0,24)\n",
      "              Conv2dOutput = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,5880)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_multiply = ...\n",
      "---------------------------------------------------\n",
      "placeholder, placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,20)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,1)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,56)\n",
      "      Conv2dOutput.local auto_unroll: 512\n",
      "      for nn_c.0 (0,1)\n",
      "        for yy_c.0 (0,1)\n",
      "          for xx_c.0 (0,1)\n",
      "            for ff_c.0 (0,1)\n",
      "              for nn_c.1 (0,1)\n",
      "                for yy_c.1 (0,1)\n",
      "                  for xx_c.1 (0,1)\n",
      "                    for ff_c.1 (0,1)\n",
      "                      for nn_c.2 (0,1)\n",
      "                        for yy_c.2 (0,1)\n",
      "                          for xx_c.2 (0,1)\n",
      "                            for ff_c.2 (0,1)\n",
      "                              for ry.0 (0,1)\n",
      "                                for rx.0 (0,1)\n",
      "                                  for rc.0 (0,40)\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,14)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          placeholder.shared = ...\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,4)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          PaddedInput.shared = ...\n",
      "                                    for ry.1 (0,1)\n",
      "                                      for rx.1 (0,1)\n",
      "                                        for rc.1 (0,1)\n",
      "                                          for nn_c.3 (0,1)\n",
      "                                            for yy_c.3 (0,7)\n",
      "                                              for xx_c.3 (0,1)\n",
      "                                                for ff_c.3 (0,3)\n",
      "                                                  for ry.2 (0,1)\n",
      "                                                    for rx.2 (0,1)\n",
      "                                                      for rc.2 (0,4)\n",
      "                                                        for nn_c.4 (0,1)\n",
      "                                                          for yy_c.4 (0,1)\n",
      "                                                            for xx_c.4 (0,1)\n",
      "                                                              for ff_c.4 (0,8)\n",
      "                                                                Conv2dOutput.local = ...\n",
      "      for nn.3 (0,1)\n",
      "        for yy.3 (0,7)\n",
      "          for xx.3 (0,1)\n",
      "            for ff.3 (0,24)\n",
      "              Conv2dOutput = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,5880)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_multiply = ...\n",
      "---------------------------------------------------\n",
      "placeholder, placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,12)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,8)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,280)\n",
      "      for nn_c.0 (0,1)\n",
      "        for yy_c.0 (0,1)\n",
      "          for xx_c.0 (0,1)\n",
      "            for ff_c.0 (0,1)\n",
      "              for nn_c.1 (0,1)\n",
      "                for yy_c.1 (0,1)\n",
      "                  for xx_c.1 (0,1)\n",
      "                    for ff_c.1 (0,1)\n",
      "                      for nn_c.2 (0,1)\n",
      "                        for yy_c.2 (0,1)\n",
      "                          for xx_c.2 (0,1)\n",
      "                            for ff_c.2 (0,1)\n",
      "                              for ry.0 (0,1)\n",
      "                                for rx.0 (0,1)\n",
      "                                  for rc.0 (0,4)\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,12)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,280)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          placeholder.shared = ...\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,28)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,280)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          PaddedInput.shared = ...\n",
      "                                    for ry.1 (0,1)\n",
      "                                      for rx.1 (0,1)\n",
      "                                        for rc.1 (0,1)\n",
      "                                          for nn_c.3 (0,1)\n",
      "                                            for yy_c.3 (0,7)\n",
      "                                              for xx_c.3 (0,1)\n",
      "                                                for ff_c.3 (0,1)\n",
      "                                                  for ry.2 (0,1)\n",
      "                                                    for rx.2 (0,1)\n",
      "                                                      for rc.2 (0,40)\n",
      "                                                        for nn_c.4 (0,1)\n",
      "                                                          for yy_c.4 (0,1)\n",
      "                                                            for xx_c.4 (0,1)\n",
      "                                                              for ff_c.4 (0,1)\n",
      "                                                                Conv2dOutput.local = ...\n",
      "      for nn.3 (0,1)\n",
      "        for yy.3 (0,7)\n",
      "          for xx.3 (0,1)\n",
      "            for ff.3 (0,1)\n",
      "              Conv2dOutput = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,5880)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_multiply = ...\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for a in records[\"schedules\"][:10]:\n",
    "    print(a)\n",
    "    print(\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "bff08adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0,1) for : {'rx.2', 'nn_c.2', 'ff_c.0', 'ff_c.2', 'xx_c.1', 'yy_c.1', 'ry.0', 'ff_c.1', 'ry.1', 'rx.1', 'xx_c.0', 'nn_c.1', 'ry.2', 'xx_c.2', 'rx.0', 'nn_c.0', 'yy_c.2', 'yy_c.0'}\n",
      "  : 18.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def find_common_for_loops(schedules):\n",
    "    \"\"\"\n",
    "        (0,1) for  \n",
    "    \"\"\"\n",
    "    common_vars = None\n",
    "    \n",
    "    for schedule in schedules:\n",
    "        lines = schedule.split('\\n')\n",
    "        vars_in_schedule = set()\n",
    "        \n",
    "        for line in lines:\n",
    "            stripped = line.lstrip()\n",
    "            match = re.match(r'for\\s+(\\S+)\\s+\\(0,\\s*1\\)', stripped)\n",
    "            if match:\n",
    "                vars_in_schedule.add(match.group(1))\n",
    "        \n",
    "        if common_vars is None:\n",
    "            common_vars = vars_in_schedule\n",
    "        else:\n",
    "            common_vars &= vars_in_schedule  # \n",
    "    \n",
    "    return common_vars if common_vars is not None else set()\n",
    "\n",
    "\n",
    "def remove_common_for_loops(schedule, common_vars):\n",
    "    \"\"\"\n",
    "        (0,1) for   \n",
    "    \"\"\"\n",
    "    lines = schedule.split('\\n')\n",
    "    result_lines = []\n",
    "    \n",
    "    #  for   \n",
    "    remove_indices = set()\n",
    "    for_loop_indents = {}  #  for  ->  \n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        stripped = line.lstrip()\n",
    "        indent_level = len(line) - len(stripped)\n",
    "        \n",
    "        # (0,1) for \n",
    "        match = re.match(r'for\\s+(\\S+)\\s+\\(0,\\s*1\\)', stripped)\n",
    "        if match and match.group(1) in common_vars:\n",
    "            remove_indices.add(i)\n",
    "            for_loop_indents[i] = indent_level\n",
    "    \n",
    "    #        \n",
    "    indent_reduction = [0] * len(lines)\n",
    "    \n",
    "    for idx in sorted(remove_indices):\n",
    "        base_indent = for_loop_indents[idx]\n",
    "        #  for       2 \n",
    "        for j in range(idx + 1, len(lines)):\n",
    "            if j in remove_indices:\n",
    "                continue\n",
    "            line = lines[j]\n",
    "            stripped = line.lstrip()\n",
    "            if not stripped:  #  \n",
    "                continue\n",
    "            current_indent = len(line) - len(stripped)\n",
    "            \n",
    "            #  for body  (   )\n",
    "            if current_indent > base_indent:\n",
    "                indent_reduction[j] += 2\n",
    "            else:\n",
    "                #       for  \n",
    "                break\n",
    "    \n",
    "    #        \n",
    "    for i, line in enumerate(lines):\n",
    "        if i in remove_indices:\n",
    "            continue\n",
    "        \n",
    "        if not line.strip():  #  \n",
    "            result_lines.append(line)\n",
    "            continue\n",
    "        \n",
    "        stripped = line.lstrip()\n",
    "        original_indent = len(line) - len(stripped)\n",
    "        new_indent = max(0, original_indent - indent_reduction[i])\n",
    "        result_lines.append(' ' * new_indent + stripped)\n",
    "    \n",
    "    return '\\n'.join(result_lines)\n",
    "\n",
    "\n",
    "common_for_loops = find_common_for_loops(records[\"schedules\"])\n",
    "print(f\"  (0,1) for : {common_for_loops}\")\n",
    "\n",
    "\n",
    "#   \n",
    "cleaned_schedules = []\n",
    "records[\"extents\"] = []\n",
    "records[\"unroll\"] = []\n",
    "records[\"all\"] = []\n",
    "for i, schedule in enumerate(records[\"schedules\"]):\n",
    "    extents = [float(x) for x in re.findall(r'\\(0,\\s*(\\d+)\\)', schedule)]\n",
    "\n",
    "for i, schedule in enumerate(records[\"schedules\"]):\n",
    "    extents = [float(x) for x in re.findall(r'\\(0,\\s*(\\d+)\\)', schedule)]\n",
    "    unrolls = [float(x) for x in re.findall(r'auto_unroll:\\s*(\\d+)', schedule)]\n",
    "    records[\"extents\"].append(extents)\n",
    "    if unrolls == []:\n",
    "        unrolls = [0.0]\n",
    "    records[\"unroll\"].append(unrolls)\n",
    "    feature = extents+unrolls\n",
    "    records[\"all\"].append(np.array(feature, dtype=np.float32))\n",
    "    \n",
    "    cleaned = remove_common_for_loops(schedule, common_for_loops)\n",
    "    cleaned_schedules.append(cleaned)\n",
    "records[\"cleaned_schedules\"] = cleaned_schedules\n",
    "\n",
    "\n",
    "total_removed = sum(len(orig.split('\\n')) - len(clean.split('\\n')) \n",
    "                    for orig, clean in zip(records['schedules'], cleaned_schedules))\n",
    "avg_removed = total_removed / len(cleaned_schedules)\n",
    "print(f\"  : {avg_removed:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "ed1663eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class FeatureRegressionDataset(Dataset):\n",
    "    def __init__(self, X, y, feature=None):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            self.X = torch.from_numpy(X).float()\n",
    "        else:\n",
    "            self.X = X\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        if self.y.ndim == 1:\n",
    "            self.y = self.y.unsqueeze(1)\n",
    "\n",
    "        self.feature = feature\n",
    "        if feature is not None:\n",
    "            if isinstance(feature, np.ndarray):\n",
    "                self.feature = torch.from_numpy(feature).float()\n",
    "            else:\n",
    "                self.feature = feature\n",
    "            \n",
    "            if self.feature.ndim == 1:\n",
    "                self.feature = self.feature.unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.feature is None:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        return self.X[idx], self.y[idx], self.feature[idx]\n",
    "\n",
    "\n",
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, X, feature=None):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            self.X = torch.from_numpy(X).float()\n",
    "        else:\n",
    "            self.X = X\n",
    "        \n",
    "        if isinstance(feature, np.ndarray):\n",
    "            self.feature = torch.from_numpy(feature).float()\n",
    "        else:\n",
    "            self.feature = feature\n",
    "        # feature shape (N,) (N,1)     \n",
    "        if self.feature is not None and self.feature.ndim == 1:\n",
    "            self.feature = self.feature.unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.feature is None:\n",
    "            return self.X[idx]\n",
    "        return self.X[idx], self.feature[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "4cd8864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class VAE_feature_head(nn.Module):\n",
    "    def __init__(self, input_dim, feature_dim=None, latent_dim=16, hidden_dim=128):\n",
    "        \"\"\"\n",
    "        input_dim: 2 * D (v_norm + is_zero concat )\n",
    "        latent_dim: latent space \n",
    "        hidden_dim: MLP hidden \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            \n",
    "            #   activation  \n",
    "        )\n",
    "\n",
    "        if feature_dim is None:\n",
    "            self.use_feature = False\n",
    "        else:\n",
    "            self.use_feature = True\n",
    "            self.feature_predictor = nn.Sequential(\n",
    "                nn.Linear(latent_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, feature_dim),  # features.shape[1] feature \n",
    "            )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def predict_feature(self, z):\n",
    "        return self.feature_predictor(z)\n",
    "\n",
    "    def forward(self, x, use_mean=True):\n",
    "        mu, logvar = self.encode(x)\n",
    "        if use_mean:\n",
    "            z = mu\n",
    "        else:\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        \n",
    "        if self.use_feature:\n",
    "            feature_pred = self.predict_feature(z)\n",
    "        else:\n",
    "            feature_pred = None\n",
    "        return x_recon, mu, logvar, z, feature_pred\n",
    "\n",
    "class L3Loss(torch.nn.Module):\n",
    "    def forward(self, pred, target):\n",
    "        return torch.mean(torch.abs(pred - target) ** 4)\n",
    "\n",
    "def vae_feature_loss(x_recon, x, mu, logvar, feature_pred, feature, alpha_recon=0, alpha_feature=0, beta=1.0):\n",
    "    \"\"\"\n",
    "    x, x_recon: (B, input_dim)\n",
    "    mu, logvar: (B, latent_dim)\n",
    "\n",
    "    beta: KL  (-VAE  )\n",
    "    \"\"\"\n",
    "    # reconstruction loss: MSE\n",
    "    recon_loss = F.mse_loss(x_recon, x, reduction=\"mean\")\n",
    "    # \n",
    "    # recon_loss = L3Loss()(x_recon, x)\n",
    "\n",
    "    feature_loss = F.mse_loss(feature_pred, feature, reduction=\"mean\") if feature_pred is not None else 0.0\n",
    "\n",
    "    # KL divergence: D_KL(q(z|x) || N(0, I))\n",
    "    kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    loss = alpha_recon * recon_loss + beta * kl + alpha_feature * feature_loss\n",
    "    return loss, recon_loss, kl, feature_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "a08123a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "6b057bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "input_data = np.log1p(np.array(records[\"all\"], dtype=np.float32))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "input_data_scaled = scaler.fit_transform(input_data)\n",
    "\n",
    "X_train, X_val = train_test_split(\n",
    "    input_data_scaled,  test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# feature \n",
    "train_dataset = FeatureDataset(X_train)\n",
    "val_dataset   = FeatureDataset(X_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=False)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=512, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "17270272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "Experiment 1/1\n",
      "beta=0.01, alpha_recon=1.0, alpha_feature=1.0,\n",
      "epochs=500, latent_dim=64, hidden_dim=256, lr=0.001\n",
      "epoch 500: loss=0.0098, recon=0.0023, kl=0.7403\n",
      "epoch 500: val loss=0.0096, val recon=0.0022, val kl=0.7337\n",
      "Recon R2 : 0.5977115061509273, Feature R2 : None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import itertools\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[-1]\n",
    "latent_dim = 64\n",
    "hidden_dim = 256\n",
    "\n",
    "\n",
    "hyperparameter = {\n",
    "    'beta': [0.01],\n",
    "    'alpha_recon': [1.0],\n",
    "    'alpha_feature': [1.0],\n",
    "    'latent_dim': [64],\n",
    "    'lr': [1e-3],\n",
    "}\n",
    "\n",
    "cnt = 0\n",
    "epochs = 500\n",
    "\n",
    "\n",
    "for vals in itertools.product(*hyperparameter.values()):\n",
    "    (beta, alpha_recon, alpha_feature, latent_dim, lr) = vals\n",
    "    cnt += 1\n",
    "    print(\"=============================================\")\n",
    "    print(f\"Experiment {cnt}/{len(list(itertools.product(*hyperparameter.values())))}\")\n",
    "    print(f\"beta={beta}, alpha_recon={alpha_recon}, alpha_feature={alpha_feature},\\nepochs={epochs}, latent_dim={latent_dim}, hidden_dim={hidden_dim}, lr={lr}\")\n",
    "\n",
    "    seed_everything(42)\n",
    "\n",
    "    vae = VAE_feature_head(input_dim=input_dim, latent_dim=latent_dim, hidden_dim=hidden_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(vae.parameters(), lr=lr)\n",
    "\n",
    "    # early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 30\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        vae.train()\n",
    "        for x_batch in train_loader:\n",
    "            if len(x_batch) == 2:\n",
    "                x_batch, feature_batch = x_batch\n",
    "                feature_batch = feature_batch.to(device)\n",
    "            else:\n",
    "                feature_batch = None\n",
    "            x_batch = x_batch.to(device)  # (N, D)\n",
    "            \n",
    "            \n",
    "\n",
    "            x_recon, mu, logvar, z, feature_pred = vae(x_batch, use_mean=False)\n",
    "\n",
    "            loss, recon_loss, kl, feature_loss = vae_feature_loss(x_recon, x_batch, mu, logvar, feature_pred, feature_batch, alpha_recon=alpha_recon, alpha_feature=alpha_feature, beta=beta)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        vae.eval()\n",
    "        for x_batch in val_loader:\n",
    "            if len(x_batch) == 2:\n",
    "                x_batch, feature_batch = x_batch\n",
    "                feature_batch = feature_batch.to(device)\n",
    "            else:\n",
    "                feature_batch = None\n",
    "            x_batch = x_batch.to(device)\n",
    "            if feature_batch is not None:\n",
    "                feature_batch = feature_batch.to(device)\n",
    "            x_recon, mu, logvar, z, feature_pred = vae(x_batch, use_mean=True)\n",
    "            val_loss, val_recon_loss, val_kl, val_feature_loss = vae_feature_loss(x_recon, x_batch, mu, logvar, feature_pred, feature_batch, alpha_recon=alpha_recon, alpha_feature=alpha_feature, beta=beta)\n",
    "            val_recon_r2 = r2_score(x_batch.detach().cpu().numpy(), x_recon.detach().cpu().numpy())\n",
    "            if feature_batch is not None:\n",
    "                val_feature_r2 = r2_score(feature_batch.detach().cpu().numpy(), feature_pred.detach().cpu().numpy())\n",
    "            else:\n",
    "                val_feature_r2 = None\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss.item()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "    print(f\"epoch {epoch}: loss={loss.item():.4f}, recon={recon_loss.item():.4f}, kl={kl.item():.4f}\")\n",
    "    print(f\"epoch {epoch}: val loss={val_loss.item():.4f}, val recon={val_recon_loss.item():.4f}, val kl={val_kl.item():.4f}\")\n",
    "\n",
    "    print(f\"Recon R2 : {val_recon_r2}, Feature R2 : {val_feature_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "6310053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAECostPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    VAE  Cost Regression \n",
    "    \n",
    "    :\n",
    "    - input  segment_encoder  segment_sum  VAE encoder  z  cost_predictor  cost\n",
    "    \n",
    "    :\n",
    "    - Pretrained VAE encoder finetune ( learning rate)\n",
    "    - Cost predictor   learning rate \n",
    "    -  forward     (detach, stop_grad )\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, feature_dim=None, hidden_dim=256, latent_dim=64, \n",
    "                 predictor_hidden=256, predictor_layers=2, dropout=0.1, use_feature=False):\n",
    "        super(VAECostPredictor, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # ========== Cost Predictor ( ) ==========\n",
    "        predictor_modules = []\n",
    "        current_dim = latent_dim\n",
    "        for i in range(predictor_layers):\n",
    "            predictor_modules.extend([\n",
    "                nn.Linear(current_dim, predictor_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout) if i < predictor_layers - 1 else nn.Identity(),\n",
    "            ])\n",
    "            current_dim = predictor_hidden\n",
    "        predictor_modules.append(nn.Linear(predictor_hidden, 1))\n",
    "        \n",
    "        self.cost_predictor = nn.Sequential(*predictor_modules)\n",
    "\n",
    "        self.use_feature = use_feature\n",
    "        if self.use_feature:\n",
    "            pass\n",
    "            self.feature_predictor = nn.Sequential(\n",
    "                nn.Linear(latent_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, feature_dim),  # feature_dim feature \n",
    "            )\n",
    "        \n",
    "    \n",
    "    def encode(self, input_data):\n",
    "        \"\"\"\n",
    "        Full encoding path: features  z\n",
    "          \n",
    "        \"\"\"\n",
    "                \n",
    "        # VAE Encoder\n",
    "        h = self.encoder(input_data)\n",
    "        \n",
    "        mean = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        \n",
    "        return mean, logvar\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        \"\"\"Reparameterization trick -  \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "    \n",
    "    def predict_cost(self, z):\n",
    "        \"\"\"z  cost prediction -   \"\"\"\n",
    "        return self.cost_predictor(z).squeeze(-1)\n",
    "    \n",
    "    def predict_feature(self, z):\n",
    "        return self.feature_predictor(z)\n",
    "    \n",
    "    def forward(self, input_data, use_mean=True):\n",
    "        \"\"\"\n",
    "        Forward pass: input  z  cost\n",
    "        \n",
    "        Args:\n",
    "            use_mean: True reparameterize  mean  (inference)\n",
    "        \n",
    "        Returns:\n",
    "            cost_pred:  cost\n",
    "            mean: latent mean\n",
    "            logvar: latent log-variance\n",
    "            z: sampled/mean latent vector\n",
    "        \"\"\"\n",
    "        mean, logvar = self.encode(input_data)\n",
    "        \n",
    "        if use_mean:\n",
    "            z = mean  # Inference deterministic\n",
    "        else:\n",
    "            z = self.reparameterize(mean, logvar)  # Training stochastic\n",
    "        \n",
    "        cost_pred = self.predict_cost(z)\n",
    "        \n",
    "        return cost_pred, mean, logvar, z\n",
    "    \n",
    "    def get_encoder_params(self):\n",
    "        \"\"\"Encoder  ( lr)\"\"\"\n",
    "        encoder_params = []\n",
    "        encoder_params.extend(self.encoder.parameters())\n",
    "        encoder_params.extend(self.fc_mu.parameters())\n",
    "        encoder_params.extend(self.fc_logvar.parameters())\n",
    "        return encoder_params\n",
    "    \n",
    "    def get_cost_predictor_params(self):\n",
    "        \"\"\"Predictor  ( lr)\"\"\"\n",
    "        return self.cost_predictor.parameters()\n",
    "    \n",
    "    def get_feature_predictor_params(self):\n",
    "        \"\"\"Feature Predictor \"\"\"\n",
    "        return self.feature_predictor.parameters()\n",
    "\n",
    "    def load_pretrained_encoder(self, checkpoint):\n",
    "        \"\"\"Pretrained VAE encoder  \"\"\"\n",
    "        \n",
    "\n",
    "        vae_state = checkpoint\n",
    "        \n",
    "        #   \n",
    "        encoder_keys = ['encoder', 'fc_mu', 'fc_logvar']\n",
    "        own_state = self.state_dict()\n",
    "        \n",
    "        loaded_keys = []\n",
    "        for name, param in vae_state.items():\n",
    "            if any(name.startswith(k) for k in encoder_keys):\n",
    "                if name in own_state and own_state[name].shape == param.shape:\n",
    "                    own_state[name].copy_(param)\n",
    "                    loaded_keys.append(name)\n",
    "        \n",
    "        # print(f\"Loaded {len(loaded_keys)} parameters from pretrained VAE\")\n",
    "        # return loaded_keys\n",
    "\n",
    "    def _enable_dropout(self):\n",
    "        \"\"\" Dropout  train   \"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.train()\n",
    "\n",
    "    def mc_predict(self, input_tensor, T=20):\n",
    "        \"\"\"\n",
    "        MC Dropout   \n",
    "        \n",
    "        Args:\n",
    "            input_tensor:   (shape [N, input_dim])\n",
    "            T: MC  \n",
    "        \n",
    "        Returns:\n",
    "            mean: epistemic  cost (shape [N])\n",
    "            var: epistemic  (shape [N])\n",
    "        \"\"\"\n",
    "\n",
    "        self.eval()  #   eval \n",
    "        self._enable_dropout()  # Dropout train  \n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            \n",
    "            for _ in range(T):\n",
    "                # Encode\n",
    "                z, logvar = self.encode(input_tensor)\n",
    "                cost_pred = self.predict_cost(z)\n",
    "                predictions.append(cost_pred)\n",
    "            \n",
    "            predictions = torch.stack(predictions, dim=0)\n",
    "            \n",
    "            # epistemic mean & variance\n",
    "            mc_mean = predictions.mean(dim=0)\n",
    "            mc_var = predictions.var(dim=0)\n",
    "\n",
    "        return mc_mean, mc_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "1b584ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_loss_fn(cost_pred, cost_true, loss_type='mse'):\n",
    "    \"\"\"\n",
    "       (MSE  MAE)\n",
    "    \"\"\"\n",
    "    if loss_type == 'mse':\n",
    "        return F.mse_loss(cost_pred, cost_true)\n",
    "    else:  # mae\n",
    "        return F.l1_loss(cost_pred, cost_true)\n",
    "\n",
    "\n",
    "def pair_loss_fn(cost_pred, cost_true, margin=0.1):\n",
    "    \"\"\"\n",
    "    Pairwise ranking loss:  cost   .\n",
    "    cost_true[i] < cost_true[j]  cost_pred[i] < cost_pred[j] + margin\n",
    "    \"\"\"\n",
    "    batch_size = cost_pred.size(0)\n",
    "    if batch_size < 2:\n",
    "        return torch.tensor(0.0, device=cost_pred.device)\n",
    "    \n",
    "    #    ranking loss \n",
    "    idx = torch.arange(batch_size, device=cost_pred.device)\n",
    "    i_idx, j_idx = torch.meshgrid(idx, idx, indexing='ij')\n",
    "    mask = i_idx < j_idx  # upper triangular only\n",
    "    \n",
    "    pred_i = cost_pred[i_idx[mask]]\n",
    "    pred_j = cost_pred[j_idx[mask]]\n",
    "    true_i = cost_true[i_idx[mask]]\n",
    "    true_j = cost_true[j_idx[mask]]\n",
    "    \n",
    "    # label: 1 if true_i < true_j, -1 otherwise\n",
    "    labels = torch.sign(true_j - true_i).float()\n",
    "    \n",
    "    # Margin ranking loss\n",
    "    loss = F.margin_ranking_loss(pred_j.view(-1), pred_i.view(-1), labels.view(-1), margin=margin)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def smooth_loss_fn(model, z, noise_std=0.1):\n",
    "    \"\"\"\n",
    "    Smoothness loss: z        .\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z_noisy = z + noise_std * torch.randn_like(z)\n",
    "    \n",
    "    cost_original = model.predict_cost(z)\n",
    "    cost_noisy = model.predict_cost(z_noisy)\n",
    "    \n",
    "    smooth_loss = F.mse_loss(cost_original, cost_noisy)\n",
    "    return smooth_loss\n",
    "\n",
    "\n",
    "def kld_loss_fn(mean, logvar):\n",
    "    \"\"\"\n",
    "    KL Divergence: q(z|x) || N(0, I)\n",
    "    \"\"\"\n",
    "    kld = -0.5 * torch.mean(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "    return kld\n",
    "\n",
    "def feature_loss_fn(use_feature, feature_pred, feature_true, coef=0.1):\n",
    "    \"\"\"\n",
    "    Feature   (MSE)\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if not use_feature:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "    return F.mse_loss(feature_pred, feature_true) * coef\n",
    "\n",
    "\n",
    "def compute_total_loss(model, cost_pred, mean, logvar, z, labels, feature, config, return_components=True):\n",
    "    \"\"\"\n",
    "    Total loss  (Segment  ).\n",
    "    total_loss = reg_loss + _pair * pair_loss +  * smooth_loss +  * kld_loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # Individual losses\n",
    "    reg = reg_loss_fn(cost_pred, labels, loss_type=config.get('loss_type', 'mse'))\n",
    "    pair = pair_loss_fn(cost_pred.view(-1), labels.view(-1), margin=config.get('margin', 0.1))\n",
    "    smooth = smooth_loss_fn(model, z, noise_std=config.get('noise_std', 0.1))\n",
    "    kld = kld_loss_fn(mean, logvar)\n",
    "    feature_loss = feature_loss_fn(model.use_feature, None, feature, coef=0)\n",
    "    \n",
    "    # Weighted sum\n",
    "    total = config['lambda_reg'] * reg + config['lambda_pair'] * pair + config['gamma'] * smooth + config['beta'] * kld + feature_loss\n",
    "    \n",
    "    if return_components:\n",
    "        return total, {\n",
    "            'reg_loss': reg.item(),\n",
    "            'pair_loss': pair.item(),\n",
    "            'smooth_loss': smooth.item(),\n",
    "            'kld_loss': kld.item(),\n",
    "            'feature_loss': feature_loss.item(),\n",
    "        }\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "6ef7144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_accuracy(cost_pred, labels):\n",
    "    \"\"\"\n",
    "    cost_pred, labels: (B,) \n",
    "    \"\"\"\n",
    "    seed_everything(42)\n",
    "    n_samples = min(2000, len(cost_pred))\n",
    "    sample_indices = np.random.choice(len(cost_pred), n_samples, replace=False)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i + 1, n_samples):\n",
    "            idx_i = sample_indices[i]\n",
    "            idx_j = sample_indices[j]\n",
    "            pred_diff = cost_pred[idx_i] - cost_pred[idx_j]\n",
    "            true_diff = labels[idx_i] - labels[idx_j]\n",
    "            if (pred_diff * true_diff) > 0:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy\n",
    "\n",
    "def recall_at_k(pred, labels, k=1):\n",
    "    true_best_idx = torch.argmax(labels)\n",
    "    topk_pred_idx = torch.topk(pred, k=k, largest=True).indices\n",
    "\n",
    "    return int((topk_pred_idx == true_best_idx).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "a03fd72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_select_indices(xgb_all_preds, train_indices, test_indices, topk_size, eps_greedy_size):\n",
    "    \"\"\"\n",
    "     2, xgb   62 \n",
    "    \"\"\"\n",
    "    #     random_select_size \n",
    "\n",
    "    remaining_indices = set(test_indices)\n",
    "\n",
    "    top_indices, remaining_indices = select_topk_cost(xgb_all_preds, remaining_indices, topk_size)\n",
    "    random_indices, remaining_indices = random_select_indices(remaining_indices, eps_greedy_size)\n",
    "\n",
    "    test_indices = np.array(list(remaining_indices), dtype=np.int64)\n",
    "\n",
    "    selected_indices = np.concatenate([top_indices, random_indices])\n",
    "\n",
    "    train_indices = np.concatenate([train_indices, selected_indices])\n",
    "\n",
    "    return train_indices, test_indices\n",
    "\n",
    "\n",
    "\n",
    "def random_select_indices(remaining_indices, select_size):\n",
    "    if select_size == 0:\n",
    "        return np.array([], dtype=np.int64), remaining_indices\n",
    "    \n",
    "    random_indices = np.random.choice(list(remaining_indices), size=select_size, replace=False)\n",
    "\n",
    "    remaining_indices = util_update_remaining_indices(remaining_indices, random_indices)\n",
    "\n",
    "    return random_indices, remaining_indices\n",
    "\n",
    "\n",
    "\n",
    "def util_update_remaining_indices(remaining_indices, selected_indices):\n",
    "    \"\"\"\n",
    "       \n",
    "    util_update_remaining_indices selected_indices \n",
    "    \"\"\"\n",
    "    selected_indices = set(selected_indices)\n",
    "    remaining_indices.difference_update(selected_indices)\n",
    "\n",
    "    return remaining_indices\n",
    "\n",
    "\n",
    "\n",
    "def util_select_topk(predictions, remaining_indices, num_select):\n",
    "    \"\"\"\n",
    "         \n",
    "    \n",
    "    Args:\n",
    "        predictions:    ([N, ] )\n",
    "        remaining_indices:      (set)\n",
    "        num_select:   \n",
    "    \n",
    "    Returns:\n",
    "        selected_indices:    numpy \n",
    "        remaining_indices:     (set)\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction = np.asarray(predictions)  # [N]\n",
    "\n",
    "    remaining_np = np.array(list(remaining_indices), dtype=np.int64)\n",
    "    remaining_pred = prediction[remaining_np]\n",
    "\n",
    "    k = min(num_select, len(remaining_np))\n",
    "\n",
    "    topk_local = np.argsort(remaining_pred)[-k:]\n",
    "    selected_indices = remaining_np[topk_local]\n",
    "\n",
    "    # remaining \n",
    "    remaining_indices.difference_update(selected_indices.tolist())\n",
    "\n",
    "    return selected_indices, remaining_indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_topk_cost(cost_pred, remaining_indices, num_select):\n",
    "    \"\"\"\n",
    "     cost     \n",
    "    \n",
    "    Args:\n",
    "        model: VAECostPredictor \n",
    "        input_data_scaled:  input  ([N, input_dim] )\n",
    "        remaining_indices:      (set)\n",
    "        num_select:   \n",
    "    \n",
    "    \"\"\"\n",
    "    if num_select == 0:\n",
    "        return np.array([], dtype=np.int64), remaining_indices\n",
    "\n",
    "    if isinstance(cost_pred, torch.Tensor):\n",
    "        cost_pred = cost_pred.detach().cpu().numpy()  # [N]\n",
    "\n",
    "    topk_cost_indices, remaining_indices = util_select_topk(cost_pred, remaining_indices, num_select)\n",
    "    \n",
    "\n",
    "    return topk_cost_indices, remaining_indices\n",
    "\n",
    "\n",
    "def select_topk_z_grad(z, cost_pred, remaining_indices, num_select):\n",
    "    \"\"\"\n",
    "    z  cost gradient     \n",
    "    \n",
    "    Args:\n",
    "        model: VAECostPredictor \n",
    "        input_tensor:  input numpy  ([N, input_dim] )\n",
    "        remaining_indices:      (set)\n",
    "        num_select:   \n",
    "    \n",
    "    \"\"\"\n",
    "    if num_select == 0:\n",
    "        return np.array([], dtype=np.int64), remaining_indices\n",
    "\n",
    "    candidate_indices = np.array(list(remaining_indices), dtype=np.int64)\n",
    "\n",
    "    # z-gradient \n",
    "    z_grad = torch.autograd.grad(\n",
    "        outputs=cost_pred.sum(),\n",
    "        inputs=z,\n",
    "        retain_graph=False,\n",
    "        create_graph=False\n",
    "    )[0]  # [N, latent_dim]\n",
    "\n",
    "    z_grad_norm = torch.norm(z_grad, dim=1).detach().cpu().numpy()  # [N]\n",
    "\n",
    "    #   grad-norm top-k\n",
    "    candidate_grad = z_grad_norm[candidate_indices]\n",
    "    k = min(num_select, len(candidate_indices))\n",
    "\n",
    "    topk_local = np.argsort(candidate_grad)[-k:]\n",
    "    selected_indices = candidate_indices[topk_local]\n",
    "\n",
    "    # remaining \n",
    "    remaining_indices = set(remaining_indices)\n",
    "    remaining_indices.difference_update(selected_indices.tolist())\n",
    "\n",
    "    return selected_indices, remaining_indices\n",
    "\n",
    "\n",
    "def select_topk_uncertainty(model, input_tensor, remaining_indices, num_select, T_mc=10):\n",
    "    \"\"\"\n",
    "    MC Dropout       \n",
    "    \n",
    "    Args:\n",
    "        model: VAECostPredictor \n",
    "        input_data_scaled:  input  ([N, input_dim] )\n",
    "        remaining_indices:      (set)\n",
    "        num_select:   \n",
    "        T_mc: MC Dropout  \n",
    "    \n",
    "    Returns:\n",
    "        selected_indices:    \n",
    "    \"\"\"\n",
    "    if num_select == 0:\n",
    "        return np.array([], dtype=np.int64), remaining_indices\n",
    "\n",
    "    was_training = model.training\n",
    "    model.train()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, mc_var = model.mc_predict(input_tensor, T=T_mc)\n",
    "\n",
    "    if not was_training:\n",
    "        model.eval()  # \n",
    "\n",
    "    var_np = mc_var.detach().cpu().numpy()  # [N]\n",
    "\n",
    "    topk_uncertainty_indices, remaining_indices = util_select_topk(var_np, remaining_indices, num_select)\n",
    "\n",
    "    return topk_uncertainty_indices, remaining_indices\n",
    "\n",
    "\n",
    "def select_topk_latent_diversity(z, candidate_indices, used_indices, select_n_div, chunk_size=1024, eps=1e-12):\n",
    "    \"\"\"\n",
    "     candidates 320  .\n",
    "      topk_cost, topk_z_grad 40   .\n",
    "    latent diversity 40 + used_indices    24 280 .\n",
    "\n",
    "    z L2  , k-center greedy(farthest-first) diversity .\n",
    "      used_indices (  ).\n",
    "      \"  \"  candidate  .\n",
    "    \n",
    "    Args:\n",
    "        z: torch.Tensor [N, latent_dim]\n",
    "        candidate_indices: set(int)\n",
    "        used_indices: set(int)\n",
    "        select_n_div: int\n",
    "        chunk_size: int\n",
    "    Returns:\n",
    "        diverse_indices: np.ndarray (int64)\n",
    "        candidate_indices: set (   )\n",
    "    \"\"\"\n",
    "    if select_n_div == 0 or len(candidate_indices) == 0:\n",
    "        return np.array([], dtype=np.int64), candidate_indices\n",
    "\n",
    "    device = z.device\n",
    "\n",
    "    # 1) L2 normalize z  (  )\n",
    "    with torch.no_grad():\n",
    "        z_norm = z / (z.norm(dim=1, keepdim=True) + eps)\n",
    "\n",
    "    cand = np.array(list(candidate_indices), dtype=np.int64)\n",
    "    k = min(select_n_div, len(cand))\n",
    "\n",
    "    cand_t = torch.from_numpy(cand).to(device=device)\n",
    "    z_cand = z_norm[cand_t]  # [M, D], M=len(cand)\n",
    "\n",
    "    #  : used_indices (  )\n",
    "    used = np.array(list(used_indices), dtype=np.int64)\n",
    "    selected = []\n",
    "\n",
    "    # 2)  candidate \"   \"  init\n",
    "    #    used  +inf     (  ) \n",
    "    if len(used) > 0:\n",
    "        used_t = torch.from_numpy(used).to(device=device)\n",
    "        z_used = z_norm[used_t]  # [U, D]\n",
    "\n",
    "        # min_dists[j] = min_{u in used} ||z_cand[j] - z_used[u]||\n",
    "        min_dists = torch.empty(len(cand), device=device, dtype=torch.float32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for s in range(0, len(cand), chunk_size):\n",
    "                e = min(s + chunk_size, len(cand))\n",
    "                d = torch.cdist(z_cand[s:e], z_used, p=2)  # [B, U]\n",
    "                min_dists[s:e] = d.min(dim=1).values\n",
    "    else:\n",
    "        #          argmax 0   \n",
    "        #   / norm      ,\n",
    "        #  \"  min_dists\"  +inf .\n",
    "        min_dists = torch.full((len(cand),), float(\"inf\"), device=device, dtype=torch.float32)\n",
    "\n",
    "    # 3) k-center greedy \n",
    "    #     argmax(min_dists)   ->     -> min_dists \n",
    "    with torch.no_grad():\n",
    "        for _ in range(k):\n",
    "            j = torch.argmax(min_dists).item()     # cand  \n",
    "            sel_idx = cand[j]                      #  \n",
    "            selected.append(sel_idx)\n",
    "\n",
    "            #   \"\" :  candidate  dist_to_new_center   min \n",
    "            new_center = z_cand[j:j+1]  # [1, D]\n",
    "\n",
    "            #       min_dists -inf\n",
    "            min_dists[j] = -float(\"inf\")\n",
    "\n",
    "            #   min  \n",
    "            for s in range(0, len(cand), chunk_size):\n",
    "                e = min(s + chunk_size, len(cand))\n",
    "                d_new = torch.cdist(z_cand[s:e], new_center, p=2).squeeze(1)  # [B]\n",
    "                min_dists[s:e] = torch.minimum(min_dists[s:e], d_new)\n",
    "\n",
    "    diverse_indices = np.array(selected, dtype=np.int64)\n",
    "\n",
    "    candidate_indices = set(candidate_indices)\n",
    "    candidate_indices.difference_update(diverse_indices.tolist())\n",
    "\n",
    "    return diverse_indices, candidate_indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_programs(model, input_data_scaled, used_indices, remaining_indices, num_select=64, T_mc=10, \n",
    "                    w_cost=0.5, w_unc=0.3, w_div=0.2, grad_num=2, rand_num=0, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), topk_factor=5):\n",
    "    \"\"\"\n",
    "    Active learning     \n",
    "    \n",
    "    Args:\n",
    "        model: VAECostPredictor \n",
    "        input_data_scaled:  input  ([N, input_dim] )\n",
    "        used_indices:    (set)\n",
    "        remaining_indices:      (set)\n",
    "        num_select:   \n",
    "        T_mc: MC Dropout  \n",
    "        w_cost:    \n",
    "        w_unc: epistemic    \n",
    "        w_div: latent    \n",
    "        grad_num: z  cost gradient   \n",
    "        rand_num:    \n",
    "    \n",
    "    Returns:\n",
    "        selected_indices:    \n",
    "    \"\"\"\n",
    "\n",
    "    #  64 \n",
    "    total = num_select\n",
    "    budget = total - grad_num - rand_num\n",
    "\n",
    "    #    \n",
    "    if num_select == 0 and rand_num > 0:\n",
    "        rand_indices, remaining_indices = random_select_indices(remaining_indices, rand_num)\n",
    "        return rand_indices, remaining_indices\n",
    "    \n",
    "\n",
    "    select_n_cost = int(budget * w_cost)\n",
    "    select_n_unc  = int(budget * w_unc)\n",
    "    select_n_div  = int(budget * w_div)\n",
    "    select_n_grad = grad_num\n",
    "    s = select_n_cost + select_n_unc + select_n_div\n",
    "    if s < budget:\n",
    "        select_n_cost += budget - s\n",
    "\n",
    "    input_tensor = torch.tensor(input_data_scaled, dtype=torch.float32, device=device)\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z, _ = model.encode(input_tensor)\n",
    "    z = z.detach().requires_grad_(True)\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    cost_pred = model.predict_cost(z)\n",
    "    cost_pred = cost_pred.view(-1)\n",
    "    cost_np = cost_pred.detach().cpu().numpy()\n",
    "\n",
    "    remaining_np = np.array(list(remaining_indices), dtype=np.int64)\n",
    "    remaining_cost = cost_np[remaining_np]\n",
    "\n",
    "    k_pref = min(len(remaining_np), total * topk_factor)\n",
    "    top_local = np.argsort(remaining_cost)[-k_pref:]\n",
    "    candidate_indices = set(remaining_np[top_local].tolist())  #  remaining\n",
    "\n",
    "    # print(f\"Candidate pool size: {len(candidate_indices)}\")\n",
    "\n",
    "\n",
    "    #  \n",
    "    currently_used = set()\n",
    "    topk_cost_indices, candidate_indices = select_topk_cost(cost_pred, candidate_indices, select_n_cost)\n",
    "    currently_used.update(topk_cost_indices.tolist())\n",
    "    z_grad_indices, candidate_indices = select_topk_z_grad(z, cost_pred, candidate_indices, select_n_grad)\n",
    "    currently_used.update(z_grad_indices.tolist())\n",
    "\n",
    "    # if len(used_indices) / len(input_data_scaled) >= 0.1:\n",
    "    if len(used_indices) >= 0:\n",
    "        uncertainty_indices, candidate_indices = select_topk_uncertainty(model, input_tensor, candidate_indices, select_n_unc, T_mc=T_mc)\n",
    "    else:\n",
    "        pool_for_uncertainty = set(remaining_indices)\n",
    "        pool_for_uncertainty.difference_update(currently_used)\n",
    "        uncertainty_indices, _ = select_topk_uncertainty(model, input_tensor, pool_for_uncertainty, select_n_unc, T_mc=T_mc)\n",
    "        candidate_indices.difference_update(uncertainty_indices.tolist())\n",
    "\n",
    "\n",
    "    currently_used.update(uncertainty_indices.tolist())\n",
    "    used_local = set(used_indices)\n",
    "    used_local.update(currently_used)\n",
    "\n",
    "    diverse_indices, _ = select_topk_latent_diversity(z, candidate_indices, used_local, select_n_div)\n",
    "    currently_used.update(diverse_indices.tolist())\n",
    "\n",
    "\n",
    "    remaining_indices.difference_update(currently_used)\n",
    "\n",
    "\n",
    "    rand_indices, remaining_indices = random_select_indices(remaining_indices, rand_num)\n",
    "    currently_used.update(rand_indices.tolist())\n",
    "\n",
    "    \n",
    "\n",
    "    all_selected_indices = np.array(sorted(currently_used), dtype=np.int64)\n",
    "\n",
    "\n",
    "\n",
    "    return all_selected_indices, remaining_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "3fe4fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vae_reg_dataloaders(input_data_scaled, costs, used_indices, remaining_indices):\n",
    "\n",
    "    train_indices = np.array(list(used_indices), dtype=np.int64)\n",
    "    val_indices = np.array(list(remaining_indices), dtype=np.int64)\n",
    "\n",
    "    X_train = input_data_scaled[train_indices]\n",
    "    X_val = input_data_scaled[val_indices]\n",
    "    y_train = costs[train_indices]\n",
    "    y_val = costs[val_indices]\n",
    "\n",
    "    train_dataset = FeatureRegressionDataset(X_train, y_train)\n",
    "    val_dataset   = FeatureRegressionDataset(X_val,   y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=False)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=512, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "    y_mean = y_train.mean()\n",
    "    y_std = y_train.std() + 1e-8  # 0     \n",
    "    print(f\"y_train mean: {y_mean}, std: {y_std}\")\n",
    "\n",
    "    \n",
    "    return train_loader, val_loader, y_mean, y_std\n",
    "\n",
    "def make_xgb_datasets(inputs, results, train_indices, test_indices):\n",
    "    f_inputs = []\n",
    "    f_results = []\n",
    "    for inp, res in zip(inputs, results):\n",
    "        cost = np.mean([c.value for c in res.costs])\n",
    "        if cost < 1e10:\n",
    "            f_inputs.append(inp)\n",
    "            f_results.append(res)\n",
    "    dataset = auto_scheduler.dataset.Dataset()\n",
    "    dataset.update_from_measure_pairs(f_inputs, f_results)\n",
    "\n",
    "\n",
    "    raw_features = list(dataset.features.values())[0]\n",
    "    raw_throughputs = list(dataset.throughputs.values())[0]\n",
    "\n",
    "    # features_list = []  #   feature (seq_len, feature_dim)\n",
    "    dataset_costs = []\n",
    "\n",
    "    for feature, throughput in zip(raw_features, raw_throughputs):\n",
    "\n",
    "        if feature.shape[0] != 1 and throughput > 1e-8:\n",
    "            # features_list.append(feature)\n",
    "            dataset_costs.append(throughput)\n",
    "\n",
    "    dataset_costs = np.array(dataset_costs, dtype=np.float32)\n",
    "    train_set, test_set = dataset.random_split_within_task(train_set_ratio=0, \n",
    "                                                        train_idxs=train_indices.tolist(), \n",
    "                                                        test_idxs=test_indices.tolist())\n",
    "    return train_set, test_set, dataset, dataset_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "ac08a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vae_reg_model(vae, hyperparameter, input_dim, latent_dim, hidden_dim, y_std, verbose=True):\n",
    "\n",
    "    cnt = 0\n",
    "    for vals in itertools.product(*hyperparameter.values()):\n",
    "        (lambda_reg, lambda_pair, margin_scale, gamma, beta, noise_std, \n",
    "        encoder_lr, feature_predictor_lr, cost_predictor_lr, seed, epochs) = vals\n",
    "        cnt += 1\n",
    "        if verbose:\n",
    "            print(f\"Experiment {cnt}/{len(list(itertools.product(*hyperparameter.values())))}\")\n",
    "            print(f\"lambda_reg={lambda_reg}, lambda_pair={lambda_pair}, margin_scale={margin_scale}, \\\n",
    "              gamma={gamma}, beta={beta}, noise_std={noise_std}\\nencoder_lr={encoder_lr}, cost_predictor_lr={cost_predictor_lr}, seed={seed}, epochs={epochs}\")\n",
    "        config = {\n",
    "                    'encoder_lr': encoder_lr,\n",
    "                    'feature_predictor_lr': feature_predictor_lr,\n",
    "                    'cost_predictor_lr': cost_predictor_lr,\n",
    "                    'lambda_reg' : lambda_reg,\n",
    "                    'lambda_pair': lambda_pair,\n",
    "                    'gamma': gamma,\n",
    "                    'beta': beta,\n",
    "                    'margin': margin_scale * y_std,\n",
    "                    'noise_std': noise_std,\n",
    "                    'loss_type': 'mse',\n",
    "                    'seed' : seed,\n",
    "                    'epochs': epochs,\n",
    "                }\n",
    "\n",
    "        vae_cost_model = VAECostPredictor(input_dim=input_dim, \n",
    "                                    latent_dim=latent_dim, \n",
    "                                    hidden_dim=hidden_dim, \n",
    "                                    predictor_layers=2,\n",
    "                                    dropout=0.1, use_feature=False).to(device)\n",
    "        vae_cost_model.load_pretrained_encoder(vae.state_dict())\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': vae_cost_model.get_encoder_params(), 'lr': config['encoder_lr']},\n",
    "            {'params': vae_cost_model.get_cost_predictor_params(), 'lr': config['cost_predictor_lr']}\n",
    "        ], weight_decay=1e-5)\n",
    "    return vae_cost_model, optimizer, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e0fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regression(vae_cost_model, optimizer, train_loader, val_loader, input_data_scaled, costs, config, top_k=10):\n",
    "\n",
    "    print(\"Train size :\", len(train_loader.dataset))\n",
    "\n",
    "    # all_reg_results = []\n",
    "\n",
    "    seed_everything(config['seed'])\n",
    "\n",
    "    \n",
    "\n",
    "    for epoch in range(1, config['epochs']+1):\n",
    "        vae_cost_model.train()\n",
    "        for x_batch, labels in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            labels = labels.to(device).squeeze(-1)\n",
    "            \n",
    "        \n",
    "            cost_pred, mean, logvar, z = vae_cost_model(x_batch, use_mean=True)\n",
    "\n",
    "            train_loss, train_components = compute_total_loss(vae_cost_model, \n",
    "                                                    cost_pred, mean, logvar, z, labels, None, config)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(vae_cost_model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "        \n",
    "\n",
    "        if epoch % config['epochs'] == 0:\n",
    "            vae_cost_model.eval()\n",
    "            with torch.no_grad():\n",
    "                all_preds = []\n",
    "                all_labels = []\n",
    "                for x_batch, labels in val_loader:\n",
    "                    x_batch = x_batch.to(device)\n",
    "                    labels = labels.to(device).squeeze(-1)\n",
    "\n",
    "                    cost_pred, mean, logvar, z = vae_cost_model(x_batch, use_mean=True)\n",
    "\n",
    "                    val_loss, val_components = compute_total_loss(vae_cost_model, cost_pred, mean, logvar, z, labels, None, config)\n",
    "                val_reg_r2 = r2_score(cost_pred.detach().cpu().numpy(), labels.detach().cpu().numpy())\n",
    "                \n",
    "                print(f\"Train loss epoch {epoch} : reg={train_components['reg_loss']: .4f} rank={train_components['pair_loss']: .4f} kl={train_components['kld_loss']: .4f}\")\n",
    "                print(f\"Val loss epoch {epoch}: reg={val_components['reg_loss']: .4f} rank={val_components['pair_loss']: .4f} kl={val_components['kld_loss']: .4f}\")\n",
    "                \n",
    "                print(f\"Regression R2 : {val_reg_r2:.4f}, \")\n",
    "        \n",
    "        # rank r2 \n",
    "        vae_cost_model.eval()\n",
    "        with torch.no_grad():\n",
    "            if epoch % config['epochs'] == 0:\n",
    "                input_data_tensor = torch.from_numpy(input_data_scaled).float().to(device)\n",
    "                all_preds = vae_cost_model(input_data_tensor, use_mean=True)[0].detach().cpu().numpy()\n",
    "                val_rank_r2 = pair_accuracy(all_preds, costs)\n",
    "                recall_top_k = recall_at_k(torch.tensor(all_preds), torch.from_numpy(costs), k=top_k)\n",
    "                print(f\"Rank R2 : {val_rank_r2:.4f}\")\n",
    "                print(f\"Recall@{top_k} : {recall_top_k}\")\n",
    "                if recall_top_k:\n",
    "                    break_signal = True\n",
    "                else:\n",
    "                    break_signal = False\n",
    "\n",
    "    # print(\"=============================================\")\n",
    "    # all_reg_results.append({\n",
    "    #     \"lambda_reg\": lambda_reg,\n",
    "    #     \"lambda_pair\": lambda_pair,\n",
    "    #     \"margin_scale\": margin_scale,\n",
    "    #     \"gamma\": gamma,\n",
    "    #     \"beta\": beta,\n",
    "    #     \"noise_std\": noise_std,\n",
    "    #     \"encoder_lr\": encoder_lr,\n",
    "    #     \"feature_predictor_lr\": feature_predictor_lr,\n",
    "    #     \"cost_predictor_lr\": cost_predictor_lr,\n",
    "    #     \"seed\": seed,\n",
    "    #     \"reg_r2\": val_reg_r2,\n",
    "    #     \"rank_r2\": val_rank_r2,\n",
    "    #     \"recall@64\": recall_top_k\n",
    "    # })\n",
    "    return vae_cost_model, break_signal, val_reg_r2, val_rank_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "fbf8d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weight_grid(step=0.1):\n",
    "    m = int(round(1.0 / step))  # step=0.1 -> 10\n",
    "    weights = []\n",
    "    for i in range(m + 1):\n",
    "        for j in range(m + 1):\n",
    "            k = m - i - j\n",
    "            if k < 0:\n",
    "                continue\n",
    "            weights.append((i/m, j/m, k/m))\n",
    "    return weights\n",
    "weights = generate_weight_grid(step=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "18ffb652",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_weights = []\n",
    "for w in weights:\n",
    "    w_cost, w_unc, w_div = w\n",
    "    if w_cost < 0.3:\n",
    "        continue\n",
    "    # if w_unc == 0.0 and w_cost > 0.0 and w_div > 0.0:\n",
    "    #     f_weights.append(w)\n",
    "    #     continue\n",
    "    # if w_div == 0.0 and w_cost > 0.0 and w_unc > 0.0:\n",
    "    #     f_weights.append(w)\n",
    "        # continue\n",
    "    f_weights.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "26db8d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########  1/10 ##########\n",
      "weights: (0.7, 0.0, 0.3)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2000\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.791375160217285, std: 1.7239548067910766\n",
      "Train size : 64\n",
      "Train loss epoch 2000 : reg= 1.6341 rank= 0.0184 kl= 0.0762\n",
      "Val loss epoch 2000: reg= 1.7975 rank= 0.2700 kl= 0.0985\n",
      "Regression R2 : 0.7079, \n",
      "Rank R2 : 0.8524\n",
      " \n",
      "   : 64\n",
      "  : 10.09 \n",
      "=============================================\n",
      "##########  2/10 ##########\n",
      "weights: (0.7, 0.0, 0.3)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2001\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.149184226989746, std: 1.9302955965859985\n",
      "Train size : 64\n",
      "Train loss epoch 2001 : reg= 1.9355 rank= 0.0215 kl= 0.0777\n",
      "Val loss epoch 2001: reg= 2.7350 rank= 0.2840 kl= 0.0982\n",
      "Regression R2 : 0.6933, \n",
      "Rank R2 : 0.8419\n",
      "===============  Phase 2 ================\n",
      "y_train mean: 7.343400955200195, std: 1.8623713354928588\n",
      "Train size : 128\n",
      "Train loss epoch 2001 : reg= 1.9934 rank= 0.0249 kl= 0.0741\n",
      "Val loss epoch 2001: reg= 2.2627 rank= 0.2166 kl= 0.0988\n",
      "Regression R2 : 0.6476, \n",
      "Rank R2 : 0.8722\n",
      " \n",
      "   : 128\n",
      "  : 20.43 \n",
      "=============================================\n",
      "##########  3/10 ##########\n",
      "weights: (0.7, 0.0, 0.3)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2002\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.833639144897461, std: 1.9446176390512084\n",
      "Train size : 64\n",
      "Train loss epoch 2002 : reg= 1.8442 rank= 0.0222 kl= 0.0720\n",
      "Val loss epoch 2002: reg= 2.3414 rank= 0.3398 kl= 0.0986\n",
      "Regression R2 : 0.6667, \n",
      "Rank R2 : 0.8554\n",
      "===============  Phase 2 ================\n",
      "y_train mean: 7.755500793457031, std: 1.7064603667123412\n",
      "Train size : 128\n",
      "Train loss epoch 2002 : reg= 2.1943 rank= 0.0219 kl= 0.0733\n",
      "Val loss epoch 2002: reg= 1.2892 rank= 0.1764 kl= 0.1043\n",
      "Regression R2 : 0.6988, \n",
      "Rank R2 : 0.8596\n",
      "===============  Phase 3 ================\n",
      "y_train mean: 8.041020393371582, std: 1.4933526615960693\n",
      "Train size : 192\n",
      "Train loss epoch 2002 : reg= 2.0722 rank= 0.0198 kl= 0.0721\n",
      "Val loss epoch 2002: reg= 1.5893 rank= 0.1333 kl= 0.1225\n",
      "Regression R2 : 0.4444, \n",
      "Rank R2 : 0.8831\n",
      "===============  Phase 4 ================\n",
      "y_train mean: 8.236481666564941, std: 1.344919453130493\n",
      "Train size : 256\n",
      "Train loss epoch 2002 : reg= 1.8535 rank= 0.0186 kl= 0.0710\n",
      "Val loss epoch 2002: reg= 1.4648 rank= 0.1259 kl= 0.1262\n",
      "Regression R2 : 0.3362, \n",
      "Rank R2 : 0.8862\n",
      " \n",
      "   : 256\n",
      "  : 42.56 \n",
      "=============================================\n",
      "##########  4/10 ##########\n",
      "weights: (0.7, 0.0, 0.3)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2003\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.372060775756836, std: 1.62344492481781\n",
      "Train size : 64\n",
      "Train loss epoch 2003 : reg= 1.3987 rank= 0.0170 kl= 0.0819\n",
      "Val loss epoch 2003: reg= 1.3292 rank= 0.2696 kl= 0.1041\n",
      "Regression R2 : 0.7224, \n",
      "Rank R2 : 0.8484\n",
      "===============  Phase 2 ================\n",
      "y_train mean: 7.445337772369385, std: 1.6067586045129394\n",
      "Train size : 128\n",
      "Train loss epoch 2003 : reg= 1.9449 rank= 0.0195 kl= 0.0763\n",
      "Val loss epoch 2003: reg= 2.7055 rank= 0.4019 kl= 0.0968\n",
      "Regression R2 : 0.5386, \n",
      "Rank R2 : 0.8723\n",
      " \n",
      "   : 128\n",
      "  : 20.96 \n",
      "=============================================\n",
      "##########  5/10 ##########\n",
      "weights: (0.7, 0.0, 0.3)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2004\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.641780376434326, std: 1.6638903717858886\n",
      "Train size : 64\n",
      "Train loss epoch 2004 : reg= 1.5873 rank= 0.0170 kl= 0.0740\n",
      "Val loss epoch 2004: reg= 1.6935 rank= 0.2855 kl= 0.0951\n",
      "Regression R2 : 0.7041, \n",
      "Rank R2 : 0.8576\n",
      " \n",
      "   : 64\n",
      "  : 10.08 \n",
      "=============================================\n",
      "##########  6/10 ##########\n",
      "weights: (0.7, 0.3, 0.0)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2000\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.791375160217285, std: 1.7239548067910766\n",
      "Train size : 64\n",
      "Train loss epoch 2000 : reg= 1.6341 rank= 0.0184 kl= 0.0762\n",
      "Val loss epoch 2000: reg= 1.7975 rank= 0.2700 kl= 0.0985\n",
      "Regression R2 : 0.7079, \n",
      "Rank R2 : 0.8524\n",
      " \n",
      "   : 64\n",
      "  : 9.93 \n",
      "=============================================\n",
      "##########  7/10 ##########\n",
      "weights: (0.7, 0.3, 0.0)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2001\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.149184226989746, std: 1.9302955965859985\n",
      "Train size : 64\n",
      "Train loss epoch 2001 : reg= 1.9355 rank= 0.0215 kl= 0.0777\n",
      "Val loss epoch 2001: reg= 2.7350 rank= 0.2840 kl= 0.0982\n",
      "Regression R2 : 0.6933, \n",
      "Rank R2 : 0.8419\n",
      "===============  Phase 2 ================\n",
      "y_train mean: 7.362670421600342, std: 1.8550961117608642\n",
      "Train size : 128\n",
      "Train loss epoch 2001 : reg= 2.1696 rank= 0.0244 kl= 0.0743\n",
      "Val loss epoch 2001: reg= 2.2796 rank= 0.1664 kl= 0.0973\n",
      "Regression R2 : 0.6005, \n",
      "Rank R2 : 0.8679\n",
      " \n",
      "   : 128\n",
      "  : 20.30 \n",
      "=============================================\n",
      "##########  8/10 ##########\n",
      "weights: (0.7, 0.3, 0.0)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2002\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.833639144897461, std: 1.9446176390512084\n",
      "Train size : 64\n",
      "Train loss epoch 2002 : reg= 1.8442 rank= 0.0222 kl= 0.0720\n",
      "Val loss epoch 2002: reg= 2.3414 rank= 0.3398 kl= 0.0986\n",
      "Regression R2 : 0.6667, \n",
      "Rank R2 : 0.8554\n",
      "===============  Phase 2 ================\n",
      "y_train mean: 7.724505424499512, std: 1.696966538892517\n",
      "Train size : 128\n",
      "Train loss epoch 2002 : reg= 2.2826 rank= 0.0213 kl= 0.0718\n",
      "Val loss epoch 2002: reg= 1.3393 rank= 0.1298 kl= 0.1052\n",
      "Regression R2 : 0.7366, \n",
      "Rank R2 : 0.8634\n",
      "===============  Phase 3 ================\n",
      "y_train mean: 8.069327354431152, std: 1.4817503790719604\n",
      "Train size : 192\n",
      "Train loss epoch 2002 : reg= 2.1280 rank= 0.0198 kl= 0.0688\n",
      "Val loss epoch 2002: reg= 1.8682 rank= 0.1477 kl= 0.1238\n",
      "Regression R2 : 0.3834, \n",
      "Rank R2 : 0.8763\n",
      "===============  Phase 4 ================\n",
      "y_train mean: 8.257848739624023, std: 1.336142907605896\n",
      "Train size : 256\n",
      "Train loss epoch 2002 : reg= 2.0073 rank= 0.0181 kl= 0.0716\n",
      "Val loss epoch 2002: reg= 1.8031 rank= 0.1361 kl= 0.1337\n",
      "Regression R2 : 0.2227, \n",
      "Rank R2 : 0.8794\n",
      " \n",
      "   : 256\n",
      "  : 42.62 \n",
      "=============================================\n",
      "##########  9/10 ##########\n",
      "weights: (0.7, 0.3, 0.0)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2003\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.372060775756836, std: 1.62344492481781\n",
      "Train size : 64\n",
      "Train loss epoch 2003 : reg= 1.3987 rank= 0.0170 kl= 0.0819\n",
      "Val loss epoch 2003: reg= 1.3292 rank= 0.2696 kl= 0.1041\n",
      "Regression R2 : 0.7224, \n",
      "Rank R2 : 0.8484\n",
      "===============  Phase 2 ================\n",
      "y_train mean: 7.468230247497559, std: 1.6120978693826293\n",
      "Train size : 128\n",
      "Train loss epoch 2003 : reg= 1.6952 rank= 0.0205 kl= 0.0729\n",
      "Val loss epoch 2003: reg= 3.4861 rank= 0.5398 kl= 0.0975\n",
      "Regression R2 : 0.3968, \n",
      "Rank R2 : 0.8544\n",
      "===============  Phase 3 ================\n",
      "y_train mean: 7.716707229614258, std: 1.481520662770996\n",
      "Train size : 192\n",
      "Train loss epoch 2003 : reg= 1.7705 rank= 0.0190 kl= 0.0750\n",
      "Val loss epoch 2003: reg= 1.9376 rank= 0.1675 kl= 0.1229\n",
      "Regression R2 : 0.4294, \n",
      "Rank R2 : 0.8757\n",
      "===============  Phase 4 ================\n",
      "y_train mean: 7.993947982788086, std: 1.3875513176782226\n",
      "Train size : 256\n",
      "Train loss epoch 2003 : reg= 1.6438 rank= 0.0188 kl= 0.0749\n",
      "Val loss epoch 2003: reg= 1.9106 rank= 0.1293 kl= 0.1280\n",
      "Regression R2 : 0.1715, \n",
      "Rank R2 : 0.8911\n",
      "===============  Phase 5 ================\n",
      "y_train mean: 8.163276672363281, std: 1.2944512467248535\n",
      "Train size : 320\n",
      "Train loss epoch 2003 : reg= 1.6402 rank= 0.0177 kl= 0.0757\n",
      "Val loss epoch 2003: reg= 1.6740 rank= 0.1150 kl= 0.1336\n",
      "Regression R2 : 0.1977, \n",
      "Rank R2 : 0.8982\n",
      "===============  Phase 6 ================\n",
      "y_train mean: 8.276766777038574, std: 1.2147011856896972\n",
      "Train size : 384\n",
      "Train loss epoch 2003 : reg= 1.5805 rank= 0.0169 kl= 0.0760\n",
      "Val loss epoch 2003: reg= 1.5609 rank= 0.1016 kl= 0.1362\n",
      "Regression R2 : 0.1653, \n",
      "Rank R2 : 0.9041\n",
      " \n",
      "   : 384\n",
      "  : 66.49 \n",
      "=============================================\n",
      "##########  10/10 ##########\n",
      "weights: (0.7, 0.3, 0.0)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2004\n",
      "===============  Phase 1 ================\n",
      "y_train mean: 6.641780376434326, std: 1.6638903717858886\n",
      "Train size : 64\n",
      "Train loss epoch 2004 : reg= 1.5873 rank= 0.0170 kl= 0.0740\n",
      "Val loss epoch 2004: reg= 1.6935 rank= 0.2855 kl= 0.0951\n",
      "Regression R2 : 0.7041, \n",
      "Rank R2 : 0.8576\n",
      "===============  Phase 2 ================\n",
      "y_train mean: 7.6614580154418945, std: 1.5986578564508056\n",
      "Train size : 128\n",
      "Train loss epoch 2004 : reg= 1.9737 rank= 0.0194 kl= 0.0718\n",
      "Val loss epoch 2004: reg= 2.1089 rank= 0.4282 kl= 0.1010\n",
      "Regression R2 : 0.3847, \n",
      "Rank R2 : 0.8483\n",
      " \n",
      "   : 128\n",
      "  : 20.33 \n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "#    numpy  \n",
    "all_indices = np.arange(len(input_data_scaled))\n",
    "costs = np.array(records[\"costs\"], dtype=np.float32)\n",
    "\n",
    "real_optimum_index = np.argmax(costs)\n",
    "\n",
    "top_k = 10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sampling_hyper = {\n",
    "    \"measure_size\": [64],\n",
    "    \"weight\" : [\n",
    "            # (1.0, 0.0, 0.0),\n",
    "            (0.7, 0.0, 0.3),\n",
    "            (0.7, 0.3, 0.0),\n",
    "            # (0.6, 0.1, 0.3),\n",
    "            # (0.3, 0.4, 0.3),\n",
    "            # (0.3, 0.3, 0.4),\n",
    "            # (0.5, 0.2, 0.3),\n",
    "            # (0.4, 0.3, 0.3),\n",
    "            # (1.0, 0.0, 0.0),\n",
    "            ],\n",
    "    # \"weight\" : f_weights,\n",
    "    \"grad_num\": [2],\n",
    "    \"rand_num\": [0],\n",
    "    \n",
    "    \"T_mc\": [20],\n",
    "    \"seed\" : range(2000, 2005),\n",
    "    # \"seed\" : [2023,2025],\n",
    "}\n",
    "\n",
    "random_indices_list = []\n",
    "all_results = []\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%m%d_%H%M\")\n",
    "filename = f\"result_csv/vae_extent_search_results_{now}.csv\"\n",
    "\n",
    "for params in itertools.product(*sampling_hyper.values()):\n",
    "\n",
    "    cnt += 1\n",
    "    print(f\"##########  {cnt}/{len(list(itertools.product(*sampling_hyper.values())))} ##########\")\n",
    "\n",
    "    tic = time.time()\n",
    "    # used_indices :    . train_indices \n",
    "    # remaining_indices :     . val_indices \n",
    "    used_indices = set()\n",
    "    remaining_indices = set(all_indices)\n",
    "    \n",
    "    measure_size, weight, grad_num, rand_num, T_mc, sampling_seed = params\n",
    "    w_cost, w_unc, w_div = weight\n",
    "    print(f\"weights: {weight}\")\n",
    "    print(f\"measure_size: {measure_size}, T_mc: {T_mc}, sampling_seed: {sampling_seed}\")\n",
    "    seed_everything(sampling_seed)\n",
    "\n",
    "    hyperparameter = {\n",
    "\n",
    "        'lambda_reg' : [0.01],\n",
    "        'lambda_pair': [3.0],\n",
    "        'margin_scale': [0.3],\n",
    "        'gamma': [0.01],\n",
    "        'beta': [0.01],\n",
    "        'noise_std': [0.001],\n",
    "\n",
    "        'encoder_lr': [1e-4],\n",
    "        'feature_predictor_lr': [0],\n",
    "        'cost_predictor_lr': [1e-2],\n",
    "        'epochs': [1000],\n",
    "        'seed': [sampling_seed],\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    random_indices, remaining_indices = random_select_indices(remaining_indices, select_size=sampling_hyper[\"measure_size\"][0])\n",
    "    used_indices.update(random_indices)\n",
    "    random_indices_list.append(random_indices)\n",
    "\n",
    "    reg_history = []\n",
    "    rank_history = []\n",
    "\n",
    "    for phase in range(len(input_data_scaled) // measure_size):\n",
    "\n",
    "        print(f\"===============  Phase {phase+1} ================\")\n",
    "\n",
    "        # DataLoader \n",
    "        train_loader, val_loader, y_mean, y_std = make_vae_reg_dataloaders(input_data_scaled, costs, used_indices, remaining_indices)\n",
    "        \n",
    "        #   \n",
    "        vae_cost_model, optimizer, config = make_vae_reg_model(vae, hyperparameter, input_dim, latent_dim, hidden_dim, y_std, verbose=False)\n",
    "        vae_cost_model, topk_recall_signal, val_reg_r2, val_rank_r2 = train_regression(vae_cost_model, optimizer, train_loader, val_loader, input_data_scaled, costs, config, top_k=10)\n",
    "\n",
    "        reg_history.append(round(val_reg_r2, 4))\n",
    "        rank_history.append(round(val_rank_r2, 4))\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        #    \n",
    "        selected_indices, remaining_indices = select_programs(\n",
    "            model=vae_cost_model,\n",
    "            input_data_scaled=input_data_scaled,\n",
    "            remaining_indices=remaining_indices,\n",
    "            used_indices=used_indices,\n",
    "            num_select=measure_size,\n",
    "            T_mc=T_mc,\n",
    "            w_cost=weight[0],\n",
    "            w_unc=weight[1],\n",
    "            w_div=weight[2],\n",
    "            # w_cost=0.3,\n",
    "            # w_unc=0.35,\n",
    "            # w_div=0.35,\n",
    "            grad_num=grad_num,\n",
    "            rand_num=rand_num,\n",
    "            device=device,\n",
    "            topk_factor=5\n",
    "        )\n",
    "        # w_cost += 0.03\n",
    "        # w_unc -= 0.02\n",
    "        # w_div -= 0.01\n",
    "\n",
    "        # selected_indices: numpy \n",
    "        used_indices.update(selected_indices.tolist())\n",
    "\n",
    "        measured_optimum = True if real_optimum_index in used_indices else False\n",
    "        if measured_optimum:\n",
    "            print(\" \")\n",
    "            print(\"   :\", len(used_indices)-measure_size)\n",
    "            used_time = time.time() - tic\n",
    "            print(f\"  : {used_time:.2f} \")\n",
    "            print(\"=============================================\")\n",
    "            all_results.append({\n",
    "                \"measure_size\": measure_size,\n",
    "                \"weights\": weight,\n",
    "                \"grad_num\": grad_num,\n",
    "                \"rand_num\": rand_num,\n",
    "                \"phase\" : phase+1,\n",
    "                \"used_time\": round(used_time, 2),\n",
    "                \"train_size\" : len(used_indices)-measure_size,\n",
    "                \"val_reg_r2\": reg_history,\n",
    "                \"val_rank_r2\": rank_history,\n",
    "                \"sampling_seed\": sampling_seed,\n",
    "            })\n",
    "            df_results = pd.DataFrame(all_results)\n",
    "            \n",
    "            \n",
    "            df_results.to_csv(filename, index=False)\n",
    "            \n",
    "            break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074df23e",
   "metadata": {},
   "source": [
    "## XGB test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f06d5436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm.auto_scheduler.cost_model.xgb_model import XGBModelInternal\n",
    "\n",
    "tenset_model = XGBModelInternal()\n",
    "inputs, results = auto_scheduler.RecordReader(json_file).read_lines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa4fafb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Traceback (most recent call last):\n  4: TVMFuncCall\n  3: _ZNSt17_Function_handlerIFvN3t\n  2: tvm::NodeGetAttr(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  1: tvm::ReflectionVTable::GetAttr(tvm::runtime::Object*, tvm::runtime::String const&) const\n  0: tvm::ReflectionVTable::VisitAttrs(tvm::runtime::Object*, tvm::AttrVisitor*) const\n  File \"/root/work/tenset/include/tvm/node/reflection.h\", line 390\nTypeError: Array is not registered via TVM_REGISTER_NODE_TYPE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[345], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m used_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(random_indices)\n\u001b[1;32m      2\u001b[0m remaining_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(all_indices) \u001b[38;5;241m-\u001b[39m used_indices\n\u001b[0;32m----> 4\u001b[0m real_optimum_idx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcosts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m topk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(measure_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.95\u001b[39m)\n\u001b[1;32m      7\u001b[0m eps_greedy_size \u001b[38;5;241m=\u001b[39m measure_size \u001b[38;5;241m-\u001b[39m topk_size\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/work/tenset/.venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:1216\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m-> 1216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/tenset/.venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     52\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/work/tenset/.venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m, method)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
      "File \u001b[0;32m~/work/tenset/python/tvm/runtime/object.py:63\u001b[0m, in \u001b[0;36mObject.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ffi_node_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNodeGetAttr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)), name))\n",
      "File \u001b[0;32m~/work/tenset/python/tvm/_ffi/_ctypes/packed_func.py:237\u001b[0m, in \u001b[0;36mPackedFuncBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    225\u001b[0m ret_tcode \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int()\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    227\u001b[0m     _LIB\u001b[38;5;241m.\u001b[39mTVMFuncCall(\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    236\u001b[0m ):\n\u001b[0;32m--> 237\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m get_last_ffi_error()\n\u001b[1;32m    238\u001b[0m _ \u001b[38;5;241m=\u001b[39m temp_args\n\u001b[1;32m    239\u001b[0m _ \u001b[38;5;241m=\u001b[39m args\n",
      "\u001b[0;31mTypeError\u001b[0m: Traceback (most recent call last):\n  4: TVMFuncCall\n  3: _ZNSt17_Function_handlerIFvN3t\n  2: tvm::NodeGetAttr(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  1: tvm::ReflectionVTable::GetAttr(tvm::runtime::Object*, tvm::runtime::String const&) const\n  0: tvm::ReflectionVTable::VisitAttrs(tvm::runtime::Object*, tvm::AttrVisitor*) const\n  File \"/root/work/tenset/include/tvm/node/reflection.h\", line 390\nTypeError: Array is not registered via TVM_REGISTER_NODE_TYPE"
     ]
    }
   ],
   "source": [
    "used_indices = set(random_indices)\n",
    "remaining_indices = set(all_indices) - used_indices\n",
    "\n",
    "real_optimum_idx = np.argmax(costs)\n",
    "\n",
    "topk_size = int(measure_size * 0.95)\n",
    "eps_greedy_size = measure_size - topk_size\n",
    "\n",
    "\n",
    "seeds = sampling_hyper[\"seed\"]\n",
    "random_indices_seed_num = random_indices_list[:len(seeds)]\n",
    "\n",
    "for i, seed in enumerate(seeds):\n",
    "\n",
    "    tic = time.time()\n",
    "\n",
    "    print(f\"===============  Phase 1 ================\")\n",
    "    used_indices = set(random_indices_list[i])\n",
    "    remaining_indices = set(all_indices) - used_indices\n",
    "\n",
    "    train_indices = np.array(list(used_indices), dtype=np.int64)\n",
    "    test_indices = np.array(list(remaining_indices), dtype=np.int64)\n",
    "\n",
    "    for phase in range(len(input_data_scaled) // measure_size):\n",
    "\n",
    "\n",
    "        train_set, test_set, dataset, dataset_costs = make_xgb_datasets(inputs, results, train_indices, test_indices)\n",
    "        tenset_model.fit_base(train_set=train_set, valid_set=test_set)\n",
    "        xgb_all_preds = tenset_model.predict(dataset)\n",
    "        xgb_all_preds = np.array(list(xgb_all_preds.values())[0], dtype=np.float32)\n",
    "        \n",
    "        val_rank_r2 = pair_accuracy(xgb_all_preds, dataset_costs)\n",
    "        xgb_reg_r2 = r2_score(dataset_costs, xgb_all_preds)\n",
    "        # recall_score = recall_at_k(torch.tensor(xgb_all_preds), torch.tensor(dataset_costs), k=top_k)\n",
    "        \n",
    "\n",
    "        print(f\"XGB Rank R2 : {val_rank_r2:.4f}, Reg R2 : {xgb_reg_r2:.4f}\")\n",
    "        # print(f\"XGB Recall@{top_k} : {recall_score}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #    \n",
    "        train_indices, test_indices = xgb_select_indices(xgb_all_preds, \n",
    "                            train_indices, test_indices, topk_size=topk_size, eps_greedy_size=eps_greedy_size)\n",
    "        \n",
    "        measured_optimum = True if real_optimum_idx in train_indices else False\n",
    "        # break_signal = measured_optimum or recall_score == 1\n",
    "        if measured_optimum:\n",
    "            print(\"XGB    \")\n",
    "            print(f\"  : {time.time() - tic:.2f} \")\n",
    "            print(\"=============================================\")\n",
    "            break\n",
    "\n",
    "        print(f\"===============  Phase {phase+2} ================\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6f6988",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3670 is out of bounds for axis 0 with size 3670",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mutil_select_topk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxgb_all_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopk_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[75], line 61\u001b[0m, in \u001b[0;36mutil_select_topk\u001b[0;34m(predictions, remaining_indices, num_select)\u001b[0m\n\u001b[1;32m     58\u001b[0m prediction \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(predictions)  \u001b[38;5;66;03m# [N]\u001b[39;00m\n\u001b[1;32m     60\u001b[0m remaining_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(remaining_indices), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64)\n\u001b[0;32m---> 61\u001b[0m remaining_pred \u001b[38;5;241m=\u001b[39m \u001b[43mprediction\u001b[49m\u001b[43m[\u001b[49m\u001b[43mremaining_np\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     63\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(num_select, \u001b[38;5;28mlen\u001b[39m(remaining_np))\n\u001b[1;32m     65\u001b[0m topk_local \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(remaining_pred)[\u001b[38;5;241m-\u001b[39mk:]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3670 is out of bounds for axis 0 with size 3670"
     ]
    }
   ],
   "source": [
    "util_select_topk(xgb_all_preds, set(test_indices), topk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d2f722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3670,)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(set(test_indices)), dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ad34d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenset_model.fit_base(train_set=train_set, valid_set=test_set)\n",
    "\n",
    "xgb_all_preds = np.array(list(xgb_all_preds.values())[0], dtype=np.float32)\n",
    "val_rank_r2 = pair_accuracy(xgb_all_preds, dataset_costs)\n",
    "xgb_reg_r2 = r2_score(dataset_costs, xgb_all_preds)\n",
    "print(f\"Tenset XGB Model Rank R2: {val_rank_r2:.4f}\")\n",
    "print(f\"Tenset XGB Model Reg R2: {xgb_reg_r2:.4f}\")\n",
    "\n",
    "k = 64\n",
    "\n",
    "recall_score = recall_at_k(torch.tensor(xgb_all_preds), torch.tensor(dataset_costs), k=top_k)\n",
    "print(f\"Top-{top_k} Recall: {recall_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336e266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit a xgb booster. Train size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenset  Rank Accuracy: 0.8091\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tvm.auto_scheduler.cost_model.xgb_model import XGBModelInternal\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    tenset_model = XGBModelInternal()\n",
    "    tenset_model.fit_base(train_set, valid_set=test_set)\n",
    "    throughputs = np.array(list(test_set.throughputs.values()))\n",
    "\n",
    "    pred = tenset_model.predict(test_set)\n",
    "\n",
    "    true_biggest_index = np.argsort(throughputs[0])[-1]\n",
    "    biggest_indices_64 = np.argsort(list(pred.values())[0])[-64:]\n",
    "\n",
    "    # list(pred.values())[0]\n",
    "    if true_biggest_index in biggest_indices_64:\n",
    "        print(\" Tenset     throughput  !\")\n",
    "        break\n",
    "    break\n",
    "\n",
    "\n",
    "# pred, throughputs rank accuracy\n",
    "correct_pairs = 0\n",
    "total_pairs = 0\n",
    "n_samples = min(2000, throughputs.shape[-1])\n",
    "sample_indices = np.random.choice(throughputs.shape[-1], n_samples, replace=False)\n",
    "pred_values = list(pred.values())[0]\n",
    "throughput_values = throughputs.squeeze()\n",
    "rank_accuracy = pair_accuracy(pred_values, throughput_values)\n",
    "print(f\"Tenset  Rank Accuracy: {rank_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
