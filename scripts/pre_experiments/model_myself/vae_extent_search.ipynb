{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d665df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "\n",
    "project_root = \"/root/work/tenset\"\n",
    "os.environ[\"TVM_HOME\"] = f\"{project_root}\"\n",
    "os.environ[\"TVM_LIBRARY_PATH\"] = f\"{project_root}/build\"\n",
    "if f\"{project_root}/python\" not in sys.path:\n",
    "    sys.path.insert(0, f\"{project_root}/python\")\n",
    "    \n",
    "\n",
    "sys.path = [p for p in sys.path if not p.startswith(f\"{project_root}/build\")]\n",
    "sys.path.append(f\"{project_root}/build\")\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = f\"{project_root}/build:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb3f4c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from /root/work/tenset/dataset/measure_records_tenset/k80/([0bcb8746286db050cd088f375c85372d,1,64,64,128,6,6,32,128,1,64,64,32],cuda).json\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sys.path.append(\"/root/work/tenset/scripts\")\n",
    "from print_programs import return_all_states\n",
    "from make_dataset import load_and_register_tasks\n",
    "from tvm import auto_scheduler\n",
    "from tvm.auto_scheduler.dataset import Dataset, make_dataset_from_log_file\n",
    "json_file = \"/root/work/tenset/dataset/measure_records_tenset/k80/([0bcb8746286db050cd088f375c85372d,1,64,64,128,6,6,32,128,1,64,64,32],cuda).json\"\n",
    "# json_file = \"/root/work/tenset/dataset/measure_records_tenset/k80/([0c9a5ba46ffc5e1a9e5641018527117f,4,7,7,160,1,1,160,960,1,1,1,960,4,7,7,960],cuda).json\"\n",
    "# json_file = \"/root/work/tenset/dataset/measure_records_tenset/k80/([3eb184d18885126bd13d564ef260c820,4,16,16,256,6,6,256,256,1,1,1,256,4,16,16,256,4,16,16,256],cuda).json\"\n",
    "# json_file = \"/root/work/tenset/dataset/measure_records_tenset/k80/([8c674f26f66543069d1e1c56cda249f9,4,60,60,256,1,1,256,512,1,1,1,512,4,30,30,512],cuda).json\"\n",
    "load_and_register_tasks()\n",
    "print(\"Loading dataset from\", json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "947647eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "states, costs = return_all_states(json_file)\n",
    "records_raw = list(map(lambda x: str(x).strip(), states))\n",
    "\n",
    "records = {\"schedules\": [], \"extents\": [], \"costs\": [], \"unroll\" : [], \"all\": []}\n",
    "\n",
    "for rec, cost in zip(records_raw, costs):\n",
    "    cost = np.array([c.value for c in cost])\n",
    "    cost = -np.log(np.mean(cost) + 1e-8)\n",
    "    schedule = rec.split(\"Placeholder\")[-1][2:]\n",
    "    \n",
    "    records[\"schedules\"].append(schedule)\n",
    "    records[\"costs\"].append(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5e40e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "placeholder, placeholder\n",
      "data_pack auto_unroll: 1024\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,1024)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,6)\n",
      "      for nu (0,6)\n",
      "        for p (((floordiv(((blockIdx.x*32) + threadIdx.x), 8192)*64) + floordiv(floormod(((blockIdx.x*32) + threadIdx.x), 4096), 64)),1)\n",
      "          for ci (((floordiv(floormod(((blockIdx.x*32) + threadIdx.x), 8192), 4096)*64) + floormod(((blockIdx.x*32) + threadIdx.x), 64)),1)\n",
      "            input_tile = ...\n",
      "    unroll eps (0,6)\n",
      "      unroll nu (0,6)\n",
      "        unroll r_a (0,6)\n",
      "          unroll r_b (0,6)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,48)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,4)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,64)\n",
      "      bgemm.local auto_unroll: 16\n",
      "      for eps_c.0 (0,1)\n",
      "        for nu_c.0 (0,1)\n",
      "          for p_c.0 (0,1)\n",
      "            for co_c.0 (0,1)\n",
      "              for eps_c.1 (0,1)\n",
      "                for nu_c.1 (0,1)\n",
      "                  for p_c.1 (0,1)\n",
      "                    for co_c.1 (0,1)\n",
      "                      for eps_c.2 (0,1)\n",
      "                        for nu_c.2 (0,1)\n",
      "                          for p_c.2 (0,1)\n",
      "                            for co_c.2 (0,1)\n",
      "                              for ci.0 (0,8)\n",
      "                                for ax0@ax1@ax2@ax3@.0.0 (0,48)\n",
      "                                  threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)\n",
      "                                    vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                      placeholder.shared = ...\n",
      "                                for ax0@ax1@ax2@ax3@.0.0 (0,48)\n",
      "                                  threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)\n",
      "                                    vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                      data_pack.shared = ...\n",
      "                                for ci.1 (0,16)\n",
      "                                  for eps_c.3 (0,1)\n",
      "                                    for nu_c.3 (0,2)\n",
      "                                      for p_c.3 (0,1)\n",
      "                                        for co_c.3 (0,1)\n",
      "                                          for ci.2 (0,1)\n",
      "                                            for eps_c.4 (0,3)\n",
      "                                              for nu_c.4 (0,1)\n",
      "                                                for p_c.4 (0,2)\n",
      "                                                  for co_c.4 (0,2)\n",
      "                                                    bgemm.local = ...\n",
      "      for eps.3 (0,3)\n",
      "        for nu.3 (0,2)\n",
      "          for p.3 (0,2)\n",
      "            for co.3 (0,2)\n",
      "              bgemm = ...\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,256)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,4)\n",
      "      unroll vw (0,4)\n",
      "        unroll r_a (0,6)\n",
      "          unroll r_b (0,6)\n",
      "            inverse = ...\n",
      "blockIdx.x n@h@w@co@.0 (0,4096)\n",
      "  threadIdx.x n@h@w@co@.1 (0,32)\n",
      "    conv2d_winograd = ...\n",
      "---------------------------------------------------\n",
      "placeholder, placeholder\n",
      "data_pack auto_unroll: 16\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,1024)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,6)\n",
      "      for nu (0,6)\n",
      "        for p (((floordiv(((blockIdx.x*8) + floordiv(threadIdx.x, 4)), 2048)*64) + floormod(((blockIdx.x*8) + floordiv(threadIdx.x, 4)), 64)),1)\n",
      "          for ci (((floordiv(floormod(((blockIdx.x*8) + floordiv(threadIdx.x, 4)), 2048), 64)*4) + floormod(threadIdx.x, 4)),1)\n",
      "            input_tile = ...\n",
      "    unroll eps (0,6)\n",
      "      unroll nu (0,6)\n",
      "        unroll r_a (0,6)\n",
      "          unroll r_b (0,6)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,768)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,1)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,32)\n",
      "      bgemm.local auto_unroll: 1024\n",
      "      for eps_c.0 (0,1)\n",
      "        for nu_c.0 (0,1)\n",
      "          for p_c.0 (0,1)\n",
      "            for co_c.0 (0,1)\n",
      "              for eps_c.1 (0,1)\n",
      "                for nu_c.1 (0,1)\n",
      "                  for p_c.1 (0,1)\n",
      "                    for co_c.1 (0,1)\n",
      "                      for eps_c.2 (0,1)\n",
      "                        for nu_c.2 (0,1)\n",
      "                          for p_c.2 (0,1)\n",
      "                            for co_c.2 (0,1)\n",
      "                              for ci.0 (0,8)\n",
      "                                for ax0@ax1@ax2@ax3@.0.0 (0,12)\n",
      "                                  threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "                                    vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                      placeholder.shared = ...\n",
      "                                for ax0@ax1@ax2@ax3@.0.0 (0,96)\n",
      "                                  threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "                                    vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                      data_pack.shared = ...\n",
      "                                for ci.1 (0,8)\n",
      "                                  for eps_c.3 (0,1)\n",
      "                                    for nu_c.3 (0,1)\n",
      "                                      for p_c.3 (0,1)\n",
      "                                        for co_c.3 (0,1)\n",
      "                                          for ci.2 (0,2)\n",
      "                                            for eps_c.4 (0,6)\n",
      "                                              for nu_c.4 (0,1)\n",
      "                                                for p_c.4 (0,2)\n",
      "                                                  for co_c.4 (0,1)\n",
      "                                                    bgemm.local = ...\n",
      "      for eps.3 (0,6)\n",
      "        for nu.3 (0,1)\n",
      "          for p.3 (0,2)\n",
      "            for co.3 (0,1)\n",
      "              bgemm = ...\n",
      "inverse auto_unroll: 16\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,256)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,4)\n",
      "      unroll vw (0,4)\n",
      "        unroll r_a (0,6)\n",
      "          unroll r_b (0,6)\n",
      "            inverse = ...\n",
      "blockIdx.x n@h@w@co@.0 (0,4096)\n",
      "  threadIdx.x n@h@w@co@.1 (0,32)\n",
      "    conv2d_winograd = ...\n",
      "---------------------------------------------------\n",
      "placeholder, placeholder\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,1024)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,6)\n",
      "      for nu (0,6)\n",
      "        for p (((floordiv(((blockIdx.x*2) + floordiv(threadIdx.x, 16)), 32)*4) + floordiv(floormod(threadIdx.x, 16), 4)),1)\n",
      "          for ci (((floormod(((blockIdx.x*2) + floordiv(threadIdx.x, 16)), 32)*4) + floormod(threadIdx.x, 4)),1)\n",
      "            input_tile = ...\n",
      "    unroll eps (0,6)\n",
      "      unroll nu (0,6)\n",
      "        unroll r_a (0,6)\n",
      "          unroll r_b (0,6)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,8)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,1)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,36)\n",
      "      bgemm.local auto_unroll: 64\n",
      "      for eps_c.0 (0,1)\n",
      "        for nu_c.0 (0,1)\n",
      "          for p_c.0 (0,1)\n",
      "            for co_c.0 (0,1)\n",
      "              for eps_c.1 (0,1)\n",
      "                for nu_c.1 (0,1)\n",
      "                  for p_c.1 (0,1)\n",
      "                    for co_c.1 (0,1)\n",
      "                      for eps_c.2 (0,1)\n",
      "                        for nu_c.2 (0,1)\n",
      "                          for p_c.2 (0,1)\n",
      "                            for co_c.2 (0,1)\n",
      "                              for ci.0 (0,32)\n",
      "                                for ax0@ax1@ax2@ax3@.0.0 (0,64)\n",
      "                                  threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,36)\n",
      "                                    vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                      placeholder.shared = ...\n",
      "                                for ax0@ax1@ax2@ax3@.0.0 (0,128)\n",
      "                                  threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,36)\n",
      "                                    vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                      data_pack.shared = ...\n",
      "                                for ci.1 (0,4)\n",
      "                                  for eps_c.3 (0,1)\n",
      "                                    for nu_c.3 (0,1)\n",
      "                                      for p_c.3 (0,4)\n",
      "                                        for co_c.3 (0,4)\n",
      "                                          for ci.2 (0,1)\n",
      "                                            for eps_c.4 (0,1)\n",
      "                                              for nu_c.4 (0,1)\n",
      "                                                for p_c.4 (0,8)\n",
      "                                                  for co_c.4 (0,8)\n",
      "                                                    bgemm.local = ...\n",
      "      for eps.3 (0,1)\n",
      "        for nu.3 (0,1)\n",
      "          for p.3 (0,32)\n",
      "            for co.3 (0,32)\n",
      "              bgemm = ...\n",
      "inverse auto_unroll: 64\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,512)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,16)\n",
      "    unroll vh (0,4)\n",
      "      unroll vw (0,4)\n",
      "        unroll r_a (0,6)\n",
      "          unroll r_b (0,6)\n",
      "            inverse = ...\n",
      "blockIdx.x n@h@w@co@.0 (0,4096)\n",
      "  threadIdx.x n@h@w@co@.1 (0,32)\n",
      "    conv2d_winograd = ...\n",
      "---------------------------------------------------\n",
      "placeholder, placeholder\n",
      "data_pack auto_unroll: 512\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,1024)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,32)\n",
      "    for eps (0,6)\n",
      "      for nu (0,6)\n",
      "        for p (((floordiv(((blockIdx.x*8) + floordiv(threadIdx.x, 4)), 128)*4) + floormod(threadIdx.x, 4)),1)\n",
      "          for ci (floormod(((blockIdx.x*8) + floordiv(threadIdx.x, 4)), 128),1)\n",
      "            input_tile = ...\n",
      "    unroll eps (0,6)\n",
      "      unroll nu (0,6)\n",
      "        unroll r_a (0,6)\n",
      "          unroll r_b (0,6)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,12)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,8)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,32)\n",
      "      bgemm.local auto_unroll: 512\n",
      "      for eps_c.0 (0,1)\n",
      "        for nu_c.0 (0,1)\n",
      "          for p_c.0 (0,1)\n",
      "            for co_c.0 (0,1)\n",
      "              for eps_c.1 (0,1)\n",
      "                for nu_c.1 (0,1)\n",
      "                  for p_c.1 (0,1)\n",
      "                    for co_c.1 (0,1)\n",
      "                      for eps_c.2 (0,1)\n",
      "                        for nu_c.2 (0,1)\n",
      "                          for p_c.2 (0,1)\n",
      "                            for co_c.2 (0,1)\n",
      "                              for ci.0 (0,32)\n",
      "                                for ax0@ax1@ax2@ax3@.0.0 (0,24)\n",
      "                                  threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "                                    vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                      placeholder.shared = ...\n",
      "                                for ax0@ax1@ax2@ax3@.0.0 (0,96)\n",
      "                                  threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "                                    vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                      data_pack.shared = ...\n",
      "                                for ci.1 (0,2)\n",
      "                                  for eps_c.3 (0,1)\n",
      "                                    for nu_c.3 (0,1)\n",
      "                                      for p_c.3 (0,16)\n",
      "                                        for co_c.3 (0,2)\n",
      "                                          for ci.2 (0,2)\n",
      "                                            for eps_c.4 (0,3)\n",
      "                                              for nu_c.4 (0,1)\n",
      "                                                for p_c.4 (0,1)\n",
      "                                                  for co_c.4 (0,1)\n",
      "                                                    bgemm.local = ...\n",
      "      for eps.3 (0,3)\n",
      "        for nu.3 (0,1)\n",
      "          for p.3 (0,16)\n",
      "            for co.3 (0,2)\n",
      "              bgemm = ...\n",
      "inverse auto_unroll: 1024\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,256)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,4)\n",
      "      unroll vw (0,4)\n",
      "        unroll r_a (0,6)\n",
      "          unroll r_b (0,6)\n",
      "            inverse = ...\n",
      "blockIdx.x n@h@w@co@.0 (0,4096)\n",
      "  threadIdx.x n@h@w@co@.1 (0,32)\n",
      "    conv2d_winograd = ...\n",
      "---------------------------------------------------\n",
      "placeholder, placeholder\n",
      "data_pack auto_unroll: 64\n",
      "blockIdx.x p.0@ci.0@p.1@ci.1@.0 (0,8192)\n",
      "  threadIdx.x p.0@ci.0@p.1@ci.1@.1 (0,4)\n",
      "    for eps (0,6)\n",
      "      for nu (0,6)\n",
      "        for p (((floordiv(blockIdx.x, 64)*2) + floormod(blockIdx.x, 2)),1)\n",
      "          for ci (((floordiv(floormod(blockIdx.x, 64), 2)*4) + threadIdx.x),1)\n",
      "            input_tile = ...\n",
      "    unroll eps (0,6)\n",
      "      unroll nu (0,6)\n",
      "        unroll r_a (0,6)\n",
      "          unroll r_b (0,6)\n",
      "            data_pack = ...\n",
      "blockIdx.x eps.0@nu.0@p.0@co.0@ (0,768)\n",
      "  vthread eps.1@nu.1@p.1@co.1@ (0,1)\n",
      "    threadIdx.x eps.2@nu.2@p.2@co.2@ (0,32)\n",
      "      for eps_c.0 (0,1)\n",
      "        for nu_c.0 (0,1)\n",
      "          for p_c.0 (0,1)\n",
      "            for co_c.0 (0,1)\n",
      "              for eps_c.1 (0,1)\n",
      "                for nu_c.1 (0,1)\n",
      "                  for p_c.1 (0,1)\n",
      "                    for co_c.1 (0,1)\n",
      "                      for eps_c.2 (0,1)\n",
      "                        for nu_c.2 (0,1)\n",
      "                          for p_c.2 (0,1)\n",
      "                            for co_c.2 (0,1)\n",
      "                              for ci.0 (0,2)\n",
      "                                for ax0@ax1@ax2@ax3@.0.0 (0,48)\n",
      "                                  threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "                                    vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                      placeholder.shared = ...\n",
      "                                for ax0@ax1@ax2@ax3@.0.0 (0,192)\n",
      "                                  threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)\n",
      "                                    vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                      data_pack.shared = ...\n",
      "                                for ci.1 (0,32)\n",
      "                                  for eps_c.3 (0,1)\n",
      "                                    for nu_c.3 (0,1)\n",
      "                                      for p_c.3 (0,1)\n",
      "                                        for co_c.3 (0,1)\n",
      "                                          for ci.2 (0,2)\n",
      "                                            for eps_c.4 (0,2)\n",
      "                                              for nu_c.4 (0,3)\n",
      "                                                for p_c.4 (0,1)\n",
      "                                                  for co_c.4 (0,2)\n",
      "                                                    bgemm.local = ...\n",
      "      for eps.3 (0,2)\n",
      "        for nu.3 (0,3)\n",
      "          for p.3 (0,1)\n",
      "            for co.3 (0,2)\n",
      "              bgemm = ...\n",
      "inverse auto_unroll: 512\n",
      "blockIdx.x p.0@co.0@p.1@co.1@.0 (0,256)\n",
      "  threadIdx.x p.0@co.0@p.1@co.1@.1 (0,32)\n",
      "    unroll vh (0,4)\n",
      "      unroll vw (0,4)\n",
      "        unroll r_a (0,6)\n",
      "          unroll r_b (0,6)\n",
      "            inverse = ...\n",
      "blockIdx.x n@h@w@co@.0 (0,4096)\n",
      "  threadIdx.x n@h@w@co@.1 (0,32)\n",
      "    conv2d_winograd = ...\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for a in records[\"schedules\"][:5]:\n",
    "    print(a)\n",
    "    print(\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "bff08adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "발견된 공통 (0,1) for문 변수: {'xx_c.1', 'yy_c.1', 'yy_c.2', 'xx_c.2', 'nn_c.1', 'ry.1', 'rx.0', 'ff_c.0', 'ff_c.1', 'nn_c.0', 'yy_c.0', 'ff_c.2', 'ry.0', 'xx_c.0', 'ry.2', 'rx.1', 'rx.2', 'nn_c.2'}\n",
      "제거된 줄 수: 18.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def find_common_for_loops(schedules):\n",
    "    \"\"\"\n",
    "    모든 스케줄에서 공통으로 나타나는 (0,1) for문 변수명을 찾음\n",
    "    \"\"\"\n",
    "    common_vars = None\n",
    "    \n",
    "    for schedule in schedules:\n",
    "        lines = schedule.split('\\n')\n",
    "        vars_in_schedule = set()\n",
    "        \n",
    "        for line in lines:\n",
    "            stripped = line.lstrip()\n",
    "            match = re.match(r'for\\s+(\\S+)\\s+\\(0,\\s*1\\)', stripped)\n",
    "            if match:\n",
    "                vars_in_schedule.add(match.group(1))\n",
    "        \n",
    "        if common_vars is None:\n",
    "            common_vars = vars_in_schedule\n",
    "        else:\n",
    "            common_vars &= vars_in_schedule  # 교집합\n",
    "    \n",
    "    return common_vars if common_vars is not None else set()\n",
    "\n",
    "\n",
    "def remove_common_for_loops(schedule, common_vars):\n",
    "    \"\"\"\n",
    "    스케줄 코드에서 공통으로 나타나는 (0,1) for문을 제거하고 들여쓰기를 정리\n",
    "    \"\"\"\n",
    "    lines = schedule.split('\\n')\n",
    "    result_lines = []\n",
    "    \n",
    "    # 제거할 for문의 인덱스들을 먼저 찾기\n",
    "    remove_indices = set()\n",
    "    for_loop_indents = {}  # 제거될 for문의 인덱스 -> 들여쓰기 레벨\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        stripped = line.lstrip()\n",
    "        indent_level = len(line) - len(stripped)\n",
    "        \n",
    "        # (0,1) for문인지 확인\n",
    "        match = re.match(r'for\\s+(\\S+)\\s+\\(0,\\s*1\\)', stripped)\n",
    "        if match and match.group(1) in common_vars:\n",
    "            remove_indices.add(i)\n",
    "            for_loop_indents[i] = indent_level\n",
    "    \n",
    "    # 각 줄에 대해 들여쓰기를 얼마나 줄여야 하는지 계산\n",
    "    indent_reduction = [0] * len(lines)\n",
    "    \n",
    "    for idx in sorted(remove_indices):\n",
    "        base_indent = for_loop_indents[idx]\n",
    "        # 이 for문 다음부터 같거나 작은 들여쓰기가 나올 때까지 2칸씩 줄이기\n",
    "        for j in range(idx + 1, len(lines)):\n",
    "            if j in remove_indices:\n",
    "                continue\n",
    "            line = lines[j]\n",
    "            stripped = line.lstrip()\n",
    "            if not stripped:  # 빈 줄\n",
    "                continue\n",
    "            current_indent = len(line) - len(stripped)\n",
    "            \n",
    "            # 이 for문의 body인 경우 (들여쓰기가 더 큰 경우)\n",
    "            if current_indent > base_indent:\n",
    "                indent_reduction[j] += 2\n",
    "            else:\n",
    "                # 같거나 작은 들여쓰기 레벨이 나오면 이 for문 블록 종료\n",
    "                break\n",
    "    \n",
    "    # 제거하지 않는 줄들에 대해 들여쓰기를 조정하여 결과 생성\n",
    "    for i, line in enumerate(lines):\n",
    "        if i in remove_indices:\n",
    "            continue\n",
    "        \n",
    "        if not line.strip():  # 빈 줄\n",
    "            result_lines.append(line)\n",
    "            continue\n",
    "        \n",
    "        stripped = line.lstrip()\n",
    "        original_indent = len(line) - len(stripped)\n",
    "        new_indent = max(0, original_indent - indent_reduction[i])\n",
    "        result_lines.append(' ' * new_indent + stripped)\n",
    "    \n",
    "    return '\\n'.join(result_lines)\n",
    "\n",
    "\n",
    "common_for_loops = find_common_for_loops(records[\"schedules\"])\n",
    "print(f\"발견된 공통 (0,1) for문 변수: {common_for_loops}\")\n",
    "\n",
    "\n",
    "# 모든 스케줄에 적용\n",
    "cleaned_schedules = []\n",
    "records[\"extents\"] = []\n",
    "records[\"unroll\"] = []\n",
    "records[\"all\"] = []\n",
    "for i, schedule in enumerate(records[\"schedules\"]):\n",
    "    extents = [float(x) for x in re.findall(r'\\(0,\\s*(\\d+)\\)', schedule)]\n",
    "\n",
    "for i, schedule in enumerate(records[\"schedules\"]):\n",
    "    extents = [float(x) for x in re.findall(r'\\(0,\\s*(\\d+)\\)', schedule)]\n",
    "    unrolls = [float(x) for x in re.findall(r'auto_unroll:\\s*(\\d+)', schedule)]\n",
    "    records[\"extents\"].append(extents)\n",
    "    if unrolls == []:\n",
    "        unrolls = [0.0]\n",
    "    records[\"unroll\"].append(unrolls)\n",
    "    feature = extents+unrolls\n",
    "    records[\"all\"].append(np.array(feature, dtype=np.float32))\n",
    "    \n",
    "    cleaned = remove_common_for_loops(schedule, common_for_loops)\n",
    "    cleaned_schedules.append(cleaned)\n",
    "records[\"cleaned_schedules\"] = cleaned_schedules\n",
    "\n",
    "\n",
    "total_removed = sum(len(orig.split('\\n')) - len(clean.split('\\n')) \n",
    "                    for orig, clean in zip(records['schedules'], cleaned_schedules))\n",
    "avg_removed = total_removed / len(cleaned_schedules)\n",
    "print(f\"제거된 줄 수: {avg_removed:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "ed1663eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class FeatureRegressionDataset(Dataset):\n",
    "    def __init__(self, X, y, feature=None):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            self.X = torch.from_numpy(X).float()\n",
    "        else:\n",
    "            self.X = X\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        if self.y.ndim == 1:\n",
    "            self.y = self.y.unsqueeze(1)\n",
    "\n",
    "        self.feature = feature\n",
    "        if feature is not None:\n",
    "            if isinstance(feature, np.ndarray):\n",
    "                self.feature = torch.from_numpy(feature).float()\n",
    "            else:\n",
    "                self.feature = feature\n",
    "            \n",
    "            if self.feature.ndim == 1:\n",
    "                self.feature = self.feature.unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.feature is None:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        return self.X[idx], self.y[idx], self.feature[idx]\n",
    "\n",
    "\n",
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, X, feature=None):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            self.X = torch.from_numpy(X).float()\n",
    "        else:\n",
    "            self.X = X\n",
    "        \n",
    "        if isinstance(feature, np.ndarray):\n",
    "            self.feature = torch.from_numpy(feature).float()\n",
    "        else:\n",
    "            self.feature = feature\n",
    "        # feature shape이 (N,)이면 (N,1)로 바꿔주는 게 편할 때가 많음\n",
    "        if self.feature is not None and self.feature.ndim == 1:\n",
    "            self.feature = self.feature.unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.feature is None:\n",
    "            return self.X[idx]\n",
    "        return self.X[idx], self.feature[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "4cd8864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class VAE_feature_head(nn.Module):\n",
    "    def __init__(self, input_dim, feature_dim=None, latent_dim=16, hidden_dim=128):\n",
    "        \"\"\"\n",
    "        input_dim: 2 * D (v_norm + is_zero concat한 차원)\n",
    "        latent_dim: latent space 차원\n",
    "        hidden_dim: MLP hidden 크기\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            \n",
    "            # 출력은 연속값이니까 activation 없이 그대로\n",
    "        )\n",
    "\n",
    "        if feature_dim is None:\n",
    "            self.use_feature = False\n",
    "        else:\n",
    "            self.use_feature = True\n",
    "            self.feature_predictor = nn.Sequential(\n",
    "                nn.Linear(latent_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, feature_dim),  # features.shape[1]는 feature 차원\n",
    "            )\n",
    "\n",
    "    def encode(self, x, use_mean=False):\n",
    "\n",
    "        h = self.encoder(x)\n",
    "        mean = self.fc_mu(h)\n",
    "        if not use_mean:\n",
    "            logvar = self.fc_logvar(h)\n",
    "        else:\n",
    "            return mean\n",
    "        \n",
    "        return mean, logvar\n",
    "\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def predict_feature(self, z):\n",
    "        return self.feature_predictor(z)\n",
    "\n",
    "    def forward(self, x, use_mean=True):\n",
    "        mu, logvar = self.encode(x)\n",
    "        if use_mean:\n",
    "            z = mu\n",
    "        else:\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        \n",
    "        if self.use_feature:\n",
    "            feature_pred = self.predict_feature(z)\n",
    "        else:\n",
    "            feature_pred = None\n",
    "        return x_recon, mu, logvar, z, feature_pred\n",
    "\n",
    "class L3Loss(torch.nn.Module):\n",
    "    def forward(self, pred, target):\n",
    "        return torch.mean(torch.abs(pred - target) ** 4)\n",
    "\n",
    "def vae_feature_loss(x_recon, x, mu, logvar, feature_pred, feature, alpha_recon=0, alpha_feature=0, beta=1.0):\n",
    "    \"\"\"\n",
    "    x, x_recon: (B, input_dim)\n",
    "    mu, logvar: (B, latent_dim)\n",
    "\n",
    "    beta: KL 가중치 (β-VAE 스타일로 조절)\n",
    "    \"\"\"\n",
    "    # reconstruction loss: MSE\n",
    "    recon_loss = F.mse_loss(x_recon, x, reduction=\"mean\")\n",
    "    # \n",
    "    # recon_loss = L3Loss()(x_recon, x)\n",
    "\n",
    "    feature_loss = F.mse_loss(feature_pred, feature, reduction=\"mean\") if feature_pred is not None else 0.0\n",
    "\n",
    "    # KL divergence: D_KL(q(z|x) || N(0, I))\n",
    "    kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    loss = alpha_recon * recon_loss + beta * kl + alpha_feature * feature_loss\n",
    "    return loss, recon_loss, kl, feature_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "a08123a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "6b057bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_seed = 2023\n",
    "seed_everything(train_seed)\n",
    "\n",
    "\n",
    "input_data = np.log1p(np.array(records[\"all\"], dtype=np.float32))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "input_data_scaled = scaler.fit_transform(input_data)\n",
    "\n",
    "X_train, X_val = train_test_split(\n",
    "    input_data_scaled,  test_size=0.2, random_state=train_seed\n",
    ")\n",
    "\n",
    "\n",
    "# feature 없음\n",
    "train_dataset = FeatureDataset(X_train)\n",
    "val_dataset   = FeatureDataset(X_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=False)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=512, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "17270272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "Experiment 1/1\n",
      "beta=0.01, alpha_recon=1.0, alpha_feature=0.0,\n",
      "epochs=500, latent_dim=64, hidden_dim=256, lr=0.001\n",
      "Early stopping at epoch 428\n",
      "epoch 428: loss=0.0130, recon=0.0043, kl=0.8697\n",
      "epoch 428: val loss=0.0156, val recon=0.0069, val kl=0.8681\n",
      "Recon R2 : 0.592185834803539, Feature R2 : None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import itertools\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[-1]\n",
    "latent_dim = 64\n",
    "hidden_dim = 256\n",
    "\n",
    "\n",
    "hyperparameter = {\n",
    "    'beta': [0.01],\n",
    "    'alpha_recon': [1.0],\n",
    "    'alpha_feature': [0.0],\n",
    "    'latent_dim': [64],\n",
    "    'lr': [1e-3],\n",
    "}\n",
    "\n",
    "cnt = 0\n",
    "epochs = 500\n",
    "\n",
    "\n",
    "for vals in itertools.product(*hyperparameter.values()):\n",
    "    (beta, alpha_recon, alpha_feature, latent_dim, lr) = vals\n",
    "    cnt += 1\n",
    "    print(\"=============================================\")\n",
    "    print(f\"Experiment {cnt}/{len(list(itertools.product(*hyperparameter.values())))}\")\n",
    "    print(f\"beta={beta}, alpha_recon={alpha_recon}, alpha_feature={alpha_feature},\\nepochs={epochs}, latent_dim={latent_dim}, hidden_dim={hidden_dim}, lr={lr}\")\n",
    "\n",
    "    seed_everything(train_seed)\n",
    "\n",
    "    vae = VAE_feature_head(input_dim=input_dim, latent_dim=latent_dim, hidden_dim=hidden_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(vae.parameters(), lr=lr)\n",
    "\n",
    "    # early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 30\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        vae.train()\n",
    "        for x_batch in train_loader:\n",
    "            if len(x_batch) == 2:\n",
    "                x_batch, feature_batch = x_batch\n",
    "                feature_batch = feature_batch.to(device)\n",
    "            else:\n",
    "                feature_batch = None\n",
    "            x_batch = x_batch.to(device)  # (N, D)\n",
    "            \n",
    "            \n",
    "\n",
    "            x_recon, mu, logvar, z, feature_pred = vae(x_batch, use_mean=False)\n",
    "\n",
    "            loss, recon_loss, kl, feature_loss = vae_feature_loss(x_recon, x_batch, mu, logvar, feature_pred, feature_batch, alpha_recon=alpha_recon, alpha_feature=alpha_feature, beta=beta)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        vae.eval()\n",
    "        for x_batch in val_loader:\n",
    "            if len(x_batch) == 2:\n",
    "                x_batch, feature_batch = x_batch\n",
    "                feature_batch = feature_batch.to(device)\n",
    "            else:\n",
    "                feature_batch = None\n",
    "            x_batch = x_batch.to(device)\n",
    "            if feature_batch is not None:\n",
    "                feature_batch = feature_batch.to(device)\n",
    "            x_recon, mu, logvar, z, feature_pred = vae(x_batch, use_mean=True)\n",
    "            val_loss, val_recon_loss, val_kl, val_feature_loss = vae_feature_loss(x_recon, x_batch, mu, logvar, feature_pred, feature_batch, alpha_recon=alpha_recon, alpha_feature=alpha_feature, beta=beta)\n",
    "            val_recon_r2 = r2_score(x_batch.detach().cpu().numpy(), x_recon.detach().cpu().numpy())\n",
    "            if feature_batch is not None:\n",
    "                val_feature_r2 = r2_score(feature_batch.detach().cpu().numpy(), feature_pred.detach().cpu().numpy())\n",
    "            else:\n",
    "                val_feature_r2 = None\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss.item()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "    print(f\"epoch {epoch}: loss={loss.item():.4f}, recon={recon_loss.item():.4f}, kl={kl.item():.4f}\")\n",
    "    print(f\"epoch {epoch}: val loss={val_loss.item():.4f}, val recon={val_recon_loss.item():.4f}, val kl={val_kl.item():.4f}\")\n",
    "\n",
    "    print(f\"Recon R2 : {val_recon_r2}, Feature R2 : {val_feature_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "6310053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAECostPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    VAE 기반 Cost Regression 모델\n",
    "    \n",
    "    구조:\n",
    "    - input → segment_encoder → segment_sum → VAE encoder → z → cost_predictor → cost\n",
    "    \n",
    "    특징:\n",
    "    - Pretrained VAE encoder를 finetune (작은 learning rate)\n",
    "    - Cost predictor는 더 큰 learning rate로 학습\n",
    "    - 전체 forward 경로가 완전히 미분 가능 (detach, stop_grad 없음)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, feature_dim=None, hidden_dim=256, latent_dim=64, \n",
    "                 predictor_hidden=256, predictor_layers=2, dropout=0.1, use_feature=False):\n",
    "        super(VAECostPredictor, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # ========== Cost Predictor (새로 학습) ==========\n",
    "        predictor_modules = []\n",
    "        current_dim = latent_dim\n",
    "        for i in range(predictor_layers):\n",
    "            predictor_modules.extend([\n",
    "                nn.Linear(current_dim, predictor_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout) if i < predictor_layers - 1 else nn.Identity(),\n",
    "            ])\n",
    "            current_dim = predictor_hidden\n",
    "        predictor_modules.append(nn.Linear(predictor_hidden, 1))\n",
    "        \n",
    "        self.cost_predictor = nn.Sequential(*predictor_modules)\n",
    "\n",
    "        self.use_feature = use_feature\n",
    "        if self.use_feature:\n",
    "            pass\n",
    "            self.feature_predictor = nn.Sequential(\n",
    "                nn.Linear(latent_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, feature_dim),  # feature_dim는 feature 차원\n",
    "            )\n",
    "        \n",
    "    \n",
    "    def encode(self, input_data, use_mean=False):\n",
    "        \"\"\"\n",
    "        Full encoding path: features → z\n",
    "        완전히 미분 가능\n",
    "        \"\"\"\n",
    "                \n",
    "        # VAE Encoder\n",
    "        h = self.encoder(input_data)\n",
    "        \n",
    "        mean = self.fc_mu(h)\n",
    "        if not use_mean:\n",
    "            logvar = self.fc_logvar(h)\n",
    "        else:\n",
    "            return mean\n",
    "        \n",
    "        return mean, logvar\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        \"\"\"Reparameterization trick - 미분 가능\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "    \n",
    "    def predict_cost(self, z):\n",
    "        \"\"\"z → cost prediction - 완전히 미분 가능\"\"\"\n",
    "        return self.cost_predictor(z).squeeze(-1)\n",
    "    \n",
    "    def predict_feature(self, z):\n",
    "        return self.feature_predictor(z)\n",
    "    \n",
    "    def forward(self, input_data, use_mean=True):\n",
    "        \"\"\"\n",
    "        Forward pass: input → z → cost\n",
    "        \n",
    "        Args:\n",
    "            use_mean: True면 reparameterize 대신 mean 사용 (inference용)\n",
    "        \n",
    "        Returns:\n",
    "            cost_pred: 예측된 cost\n",
    "            mean: latent mean\n",
    "            logvar: latent log-variance\n",
    "            z: sampled/mean latent vector\n",
    "        \"\"\"\n",
    "        mean, logvar = self.encode(input_data)\n",
    "        \n",
    "        if use_mean:\n",
    "            z = mean  # Inference시 deterministic\n",
    "        else:\n",
    "            z = self.reparameterize(mean, logvar)  # Training시 stochastic\n",
    "        \n",
    "        cost_pred = self.predict_cost(z)\n",
    "        \n",
    "        return cost_pred, mean, logvar, z\n",
    "    \n",
    "    def get_encoder_params(self):\n",
    "        \"\"\"Encoder 파라미터 (작은 lr)\"\"\"\n",
    "        encoder_params = []\n",
    "        encoder_params.extend(self.encoder.parameters())\n",
    "        encoder_params.extend(self.fc_mu.parameters())\n",
    "        encoder_params.extend(self.fc_logvar.parameters())\n",
    "        return encoder_params\n",
    "    \n",
    "    def get_cost_predictor_params(self):\n",
    "        \"\"\"Predictor 파라미터 (큰 lr)\"\"\"\n",
    "        return self.cost_predictor.parameters()\n",
    "    \n",
    "    def get_feature_predictor_params(self):\n",
    "        \"\"\"Feature Predictor 파라미터\"\"\"\n",
    "        return self.feature_predictor.parameters()\n",
    "\n",
    "    def load_pretrained_encoder(self, checkpoint):\n",
    "        \"\"\"Pretrained VAE encoder 가중치 로드\"\"\"\n",
    "        \n",
    "\n",
    "        vae_state = checkpoint\n",
    "        \n",
    "        # 매칭되는 키만 로드\n",
    "        encoder_keys = ['encoder', 'fc_mu', 'fc_logvar']\n",
    "        own_state = self.state_dict()\n",
    "        \n",
    "        loaded_keys = []\n",
    "        for name, param in vae_state.items():\n",
    "            if any(name.startswith(k) for k in encoder_keys):\n",
    "                if name in own_state and own_state[name].shape == param.shape:\n",
    "                    own_state[name].copy_(param)\n",
    "                    loaded_keys.append(name)\n",
    "        \n",
    "        # print(f\"Loaded {len(loaded_keys)} parameters from pretrained VAE\")\n",
    "        # return loaded_keys\n",
    "\n",
    "    def _enable_dropout(self):\n",
    "        \"\"\"모든 Dropout 모듈을 train 모드로 강제 활성화\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.train()\n",
    "\n",
    "    def mc_predict(self, input_tensor, T=20):\n",
    "        \"\"\"\n",
    "        MC Dropout 기반 불확실성 추정\n",
    "        \n",
    "        Args:\n",
    "            input_tensor: 입력 텐서 (shape [N, input_dim])\n",
    "            T: MC 샘플 수\n",
    "        \n",
    "        Returns:\n",
    "            mean: epistemic 평균 cost (shape [N])\n",
    "            var: epistemic 분산 (shape [N])\n",
    "        \"\"\"\n",
    "\n",
    "        self.eval()  # 전체 모델을 eval 모드로\n",
    "        self._enable_dropout()  # Dropout만 train 모드로 활성화\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            \n",
    "            for _ in range(T):\n",
    "                # Encode\n",
    "                z, logvar = self.encode(input_tensor)\n",
    "                cost_pred = self.predict_cost(z)\n",
    "                predictions.append(cost_pred)\n",
    "            \n",
    "            predictions = torch.stack(predictions, dim=0)\n",
    "            \n",
    "            # epistemic mean & variance\n",
    "            mc_mean = predictions.mean(dim=0)\n",
    "            mc_var = predictions.var(dim=0)\n",
    "\n",
    "        return mc_mean, mc_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "1b584ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_loss_fn(cost_pred, cost_true, loss_type='mse'):\n",
    "    \"\"\"\n",
    "    기본 회귀 손실 (MSE 또는 MAE)\n",
    "    \"\"\"\n",
    "    if loss_type == 'mse':\n",
    "        return F.mse_loss(cost_pred, cost_true)\n",
    "    else:  # mae\n",
    "        return F.l1_loss(cost_pred, cost_true)\n",
    "\n",
    "\n",
    "def pair_loss_fn(cost_pred, cost_true, margin=0.1):\n",
    "    \"\"\"\n",
    "    Pairwise ranking loss: 실제 cost 순서를 예측이 유지하도록.\n",
    "    cost_true[i] < cost_true[j] 이면 cost_pred[i] < cost_pred[j] + margin\n",
    "    \"\"\"\n",
    "    batch_size = cost_pred.size(0)\n",
    "    if batch_size < 2:\n",
    "        return torch.tensor(0.0, device=cost_pred.device)\n",
    "    \n",
    "    # 모든 쌍에 대해 ranking loss 계산\n",
    "    idx = torch.arange(batch_size, device=cost_pred.device)\n",
    "    i_idx, j_idx = torch.meshgrid(idx, idx, indexing='ij')\n",
    "    mask = i_idx < j_idx  # upper triangular only\n",
    "    \n",
    "    pred_i = cost_pred[i_idx[mask]]\n",
    "    pred_j = cost_pred[j_idx[mask]]\n",
    "    true_i = cost_true[i_idx[mask]]\n",
    "    true_j = cost_true[j_idx[mask]]\n",
    "    \n",
    "    # label: 1 if true_i < true_j, -1 otherwise\n",
    "    labels = torch.sign(true_j - true_i).float()\n",
    "    \n",
    "    # Margin ranking loss\n",
    "    loss = F.margin_ranking_loss(pred_j.view(-1), pred_i.view(-1), labels.view(-1), margin=margin)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def smooth_loss_fn(model, z, noise_std=0.1):\n",
    "    \"\"\"\n",
    "    Smoothness loss: z에 작은 노이즈를 더했을 때 예측이 크게 변하지 않도록.\n",
    "    \"\"\"\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    \n",
    "    z_noisy = z + noise_std * torch.randn_like(z)\n",
    "    \n",
    "    cost_original = model.predict_cost(z)\n",
    "    cost_noisy = model.predict_cost(z_noisy)\n",
    "    \n",
    "    smooth_loss = F.mse_loss(cost_original, cost_noisy)\n",
    "    \n",
    "    if was_training:\n",
    "        model.train()\n",
    "    \n",
    "    return smooth_loss\n",
    "\n",
    "\n",
    "def kld_loss_fn(mean, logvar):\n",
    "    \"\"\"\n",
    "    KL Divergence: q(z|x) || N(0, I)\n",
    "    \"\"\"\n",
    "    kld = -0.5 * torch.mean(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "    return kld\n",
    "\n",
    "def feature_loss_fn(use_feature, feature_pred, feature_true, coef=0.1):\n",
    "    \"\"\"\n",
    "    Feature 예측 손실 (MSE)\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if not use_feature:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "    return F.mse_loss(feature_pred, feature_true) * coef\n",
    "\n",
    "\n",
    "def compute_total_loss(model, cost_pred, mean, logvar, z, labels, feature, config, return_components=True):\n",
    "    \"\"\"\n",
    "    Total loss 계산 (Segment 기반 데이터용).\n",
    "    total_loss = reg_loss + λ_pair * pair_loss + γ * smooth_loss + β * kld_loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # Individual losses\n",
    "    reg = reg_loss_fn(cost_pred, labels, loss_type=config.get('loss_type', 'mse'))\n",
    "    pair = pair_loss_fn(cost_pred.view(-1), labels.view(-1), margin=config.get('margin', 0.1))\n",
    "    smooth = smooth_loss_fn(model, z, noise_std=config.get('noise_std', 0.1))\n",
    "    kld = kld_loss_fn(mean, logvar)\n",
    "    feature_loss = feature_loss_fn(model.use_feature, None, feature, coef=0)\n",
    "    \n",
    "    # Weighted sum\n",
    "    total = config['lambda_reg'] * reg + config['lambda_pair'] * pair + config['gamma'] * smooth + config['beta'] * kld + feature_loss\n",
    "    \n",
    "    if return_components:\n",
    "        return total, {\n",
    "            'reg_loss': reg.item(),\n",
    "            'pair_loss': pair.item(),\n",
    "            'smooth_loss': smooth.item(),\n",
    "            'kld_loss': kld.item(),\n",
    "            'feature_loss': feature_loss.item(),\n",
    "        }\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "6ef7144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_accuracy(cost_pred, labels, rng=np.random.default_rng(42)):\n",
    "    \"\"\"\n",
    "    cost_pred, labels: (B,) 텐서\n",
    "    \"\"\"\n",
    "    n_samples = min(1000, len(cost_pred))\n",
    "    sample_indices = rng.choice(len(cost_pred), n_samples, replace=False)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i + 1, n_samples):\n",
    "            idx_i = sample_indices[i]\n",
    "            idx_j = sample_indices[j]\n",
    "            pred_diff = cost_pred[idx_i] - cost_pred[idx_j]\n",
    "            true_diff = labels[idx_i] - labels[idx_j]\n",
    "            if (pred_diff * true_diff) > 0:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy\n",
    "\n",
    "def recall_at_k(pred, labels, k=1):\n",
    "    true_best_idx = torch.argmax(labels)\n",
    "    topk_pred_idx = torch.topk(pred, k=k, largest=True).indices\n",
    "\n",
    "    return int((topk_pred_idx == true_best_idx).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "a03fd72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_select_indices(xgb_all_preds, train_indices, test_indices, topk_size, eps_greedy_size, rng):\n",
    "    \"\"\"\n",
    "    랜덤으로 2개, xgb 모델로 상위 62개 선택\n",
    "    \"\"\"\n",
    "    # 남은 인덱스 중에서 무작위로 random_select_size개 선택\n",
    "\n",
    "    remaining_indices = set(test_indices)\n",
    "\n",
    "    if topk_size + eps_greedy_size > test_indices.shape[0]:\n",
    "        remaining_indices.update(train_indices.tolist())\n",
    "        train_indices = np.array(list(remaining_indices), dtype=np.int64)\n",
    "        return train_indices, np.array([], dtype=np.int64)\n",
    "\n",
    "\n",
    "    top_indices, remaining_indices = select_topk_cost(xgb_all_preds, remaining_indices, topk_size)\n",
    "    random_indices, remaining_indices = random_select_indices(remaining_indices, eps_greedy_size, rng=rng)\n",
    "    test_indices = np.array(list(remaining_indices), dtype=np.int64)\n",
    "\n",
    "    selected_indices = np.concatenate([top_indices, random_indices])\n",
    "\n",
    "    train_indices = np.concatenate([train_indices, selected_indices])\n",
    "\n",
    "    return train_indices, test_indices\n",
    "\n",
    "\n",
    "\n",
    "def random_select_indices(remaining_indices, select_size, rng=np.random.default_rng(42)):\n",
    "    if select_size == 0:\n",
    "        return np.array([], dtype=np.int64), remaining_indices\n",
    "    \n",
    "    random_indices = rng.choice(list(remaining_indices), size=select_size, replace=False)\n",
    "\n",
    "    remaining_indices = util_update_remaining_indices(remaining_indices, random_indices)\n",
    "\n",
    "    return random_indices, remaining_indices\n",
    "\n",
    "\n",
    "\n",
    "def util_update_remaining_indices(remaining_indices, selected_indices):\n",
    "    \"\"\"\n",
    "    남은 인덱스 집합 업데이트\n",
    "    util_update_remaining_indices에서 selected_indices 제거\n",
    "    \"\"\"\n",
    "    selected_indices = set(selected_indices)\n",
    "    remaining_indices.difference_update(selected_indices)\n",
    "\n",
    "    return remaining_indices\n",
    "\n",
    "\n",
    "\n",
    "def util_select_topk(predictions, remaining_indices, num_select):\n",
    "    \"\"\"\n",
    "    예측값 기반 다음 측정할 샘플 선택\n",
    "    \n",
    "    Args:\n",
    "        predictions: 전체 예측값 리스트 ([N, ] 형태)\n",
    "        remaining_indices: 아직 측정되지 않은 인덱스 집합 (set)\n",
    "        num_select: 선택할 샘플 수\n",
    "    \n",
    "    Returns:\n",
    "        selected_indices: 선택된 샘플의 인덱스 numpy 배열\n",
    "        remaining_indices: 업데이트된 남은 인덱스 집합 (set)\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction = np.asarray(predictions)  # [N]\n",
    "\n",
    "    remaining_np = np.array(list(remaining_indices), dtype=np.int64)\n",
    "    remaining_pred = prediction[remaining_np]\n",
    "\n",
    "    k = min(num_select, len(remaining_np))\n",
    "\n",
    "    topk_local = np.argsort(remaining_pred)[-k:]\n",
    "    selected_indices = remaining_np[topk_local]\n",
    "\n",
    "    # remaining 업데이트\n",
    "    remaining_indices.difference_update(selected_indices.tolist())\n",
    "\n",
    "    return selected_indices, remaining_indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_topk_cost(cost_pred, remaining_indices, num_select):\n",
    "    \"\"\"\n",
    "    예측된 cost 기반 다음 측정할 샘플 선택\n",
    "    \n",
    "    Args:\n",
    "        model: VAECostPredictor 모델\n",
    "        input_data_scaled: 전체 input 리스트 ([N, input_dim] 형태)\n",
    "        remaining_indices: 아직 측정되지 않은 인덱스 집합 (set)\n",
    "        num_select: 선택할 샘플 수\n",
    "    \n",
    "    \"\"\"\n",
    "    if num_select == 0:\n",
    "        return np.array([], dtype=np.int64), remaining_indices\n",
    "\n",
    "    if isinstance(cost_pred, torch.Tensor):\n",
    "        cost_pred = cost_pred.detach().cpu().numpy()  # [N]\n",
    "\n",
    "    topk_cost_indices, remaining_indices = util_select_topk(cost_pred, remaining_indices, num_select)\n",
    "    \n",
    "\n",
    "    return topk_cost_indices, remaining_indices\n",
    "\n",
    "\n",
    "def select_topk_z_grad(z, cost_pred, remaining_indices, num_select):\n",
    "    \"\"\"\n",
    "    z에 대한 cost gradient 기반 다음 측정할 샘플 선택\n",
    "    \n",
    "    Args:\n",
    "        model: VAECostPredictor 모델\n",
    "        input_tensor: 전체 input numpy 배열 ([N, input_dim] 형태)\n",
    "        remaining_indices: 아직 측정되지 않은 인덱스 집합 (set)\n",
    "        num_select: 선택할 샘플 수\n",
    "    \n",
    "    \"\"\"\n",
    "    if num_select == 0:\n",
    "        return np.array([], dtype=np.int64), remaining_indices\n",
    "\n",
    "    candidate_indices = np.array(list(remaining_indices), dtype=np.int64)\n",
    "\n",
    "    # z-gradient 계산\n",
    "    z_grad = torch.autograd.grad(\n",
    "        outputs=cost_pred.sum(),\n",
    "        inputs=z,\n",
    "        retain_graph=False,\n",
    "        create_graph=False\n",
    "    )[0]  # [N, latent_dim]\n",
    "\n",
    "    z_grad_norm = torch.norm(z_grad, dim=1).detach().cpu().numpy()  # [N]\n",
    "\n",
    "    # 후보 중 grad-norm top-k\n",
    "    candidate_grad = z_grad_norm[candidate_indices]\n",
    "    k = min(num_select, len(candidate_indices))\n",
    "\n",
    "    topk_local = np.argsort(candidate_grad)[-k:]\n",
    "    selected_indices = candidate_indices[topk_local]\n",
    "\n",
    "    # remaining 업데이트\n",
    "    remaining_indices = set(remaining_indices)\n",
    "    remaining_indices.difference_update(selected_indices.tolist())\n",
    "\n",
    "    return selected_indices, remaining_indices\n",
    "\n",
    "\n",
    "def select_topk_uncertainty(model, input_tensor, remaining_indices, num_select, T_mc=10):\n",
    "    \"\"\"\n",
    "    MC Dropout 기반 불확실성 추정으로 다음 측정할 샘플 선택\n",
    "    \n",
    "    Args:\n",
    "        model: VAECostPredictor 모델\n",
    "        input_data_scaled: 전체 input 리스트 ([N, input_dim] 형태)\n",
    "        remaining_indices: 아직 측정되지 않은 인덱스 집합 (set)\n",
    "        num_select: 선택할 샘플 수\n",
    "        T_mc: MC Dropout 샘플 수\n",
    "    \n",
    "    Returns:\n",
    "        selected_indices: 선택된 샘플의 인덱스 리스트\n",
    "    \"\"\"\n",
    "    if num_select == 0:\n",
    "        return np.array([], dtype=np.int64), remaining_indices\n",
    "\n",
    "\n",
    "    was_training = model.training\n",
    "    model.train()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, mc_var = model.mc_predict(input_tensor, T=T_mc)\n",
    "\n",
    "    if not was_training:\n",
    "        model.eval()  # 원복\n",
    "\n",
    "    var_np = mc_var.detach().cpu().numpy()  # [N]\n",
    "\n",
    "    topk_uncertainty_indices, remaining_indices = util_select_topk(var_np, remaining_indices, num_select)\n",
    "\n",
    "    return topk_uncertainty_indices, remaining_indices\n",
    "\n",
    "\n",
    "def select_topk_latent_diversity(z, candidate_indices, used_indices, select_n_div, chunk_size=1024, eps=1e-12):\n",
    "    \"\"\"\n",
    "    먼저 candidates 320개를 뽑았다고 치자.\n",
    "    이후 앞에서 topk_cost, topk_z_grad로 40개 정도를 뽑았다고 치자.\n",
    "    latent diversity는 40개 + used_indices로부터 가장 멀리 떨어진 24개를 280개에서 뽑는다.\n",
    "\n",
    "    z를 L2 정규화한 뒤, k-center greedy(farthest-first)로 diversity 선택.\n",
    "    초기 센터는 used_indices (이미 측정된 점들).\n",
    "    매 스텝마다 \"센터 집합까지의 최소거리\"가 최대인 candidate를 하나씩 추가.\n",
    "    \n",
    "    Args:\n",
    "        z: torch.Tensor [N, latent_dim]\n",
    "        candidate_indices: set(int)\n",
    "        used_indices: set(int)\n",
    "        select_n_div: int\n",
    "        chunk_size: int\n",
    "    Returns:\n",
    "        diverse_indices: np.ndarray (int64)\n",
    "        candidate_indices: set (선택된 인덱스 제거된 상태)\n",
    "    \"\"\"\n",
    "    if select_n_div == 0 or len(candidate_indices) == 0:\n",
    "        return np.array([], dtype=np.int64), candidate_indices\n",
    "\n",
    "\n",
    "    device = z.device\n",
    "\n",
    "    # 1) L2 normalize z  (각 벡터를 단위벡터로)\n",
    "    with torch.no_grad():\n",
    "        z_norm = z / (z.norm(dim=1, keepdim=True) + eps)\n",
    "\n",
    "    cand = np.array(list(candidate_indices), dtype=np.int64)\n",
    "    k = min(select_n_div, len(cand))\n",
    "\n",
    "    cand_t = torch.from_numpy(cand).to(device=device)\n",
    "    z_cand = z_norm[cand_t]  # [M, D], M=len(cand)\n",
    "\n",
    "    # 초기 센터: used_indices (비어있을 수도 있음)\n",
    "    used = np.array(list(used_indices), dtype=np.int64)\n",
    "    selected = []\n",
    "\n",
    "    # 2) 각 candidate의 \"현재 센터 집합까지 최소거리\" 벡터 init\n",
    "    #    used가 비어있으면 +inf로 시작해서 임의 첫 점을 뽑게(가장 큰 값) 만들기\n",
    "    if len(used) > 0:\n",
    "        used_t = torch.from_numpy(used).to(device=device)\n",
    "        z_used = z_norm[used_t]  # [U, D]\n",
    "\n",
    "        # min_dists[j] = min_{u in used} ||z_cand[j] - z_used[u]||\n",
    "        min_dists = torch.empty(len(cand), device=device, dtype=torch.float32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for s in range(0, len(cand), chunk_size):\n",
    "                e = min(s + chunk_size, len(cand))\n",
    "                d = torch.cdist(z_cand[s:e], z_used, p=2)  # [B, U]\n",
    "                min_dists[s:e] = d.min(dim=1).values\n",
    "    else:\n",
    "        # 센터가 없으면 모두 동일하게 시작 → 첫 선택은 아래 argmax가 0번째로 갈 수 있음\n",
    "        # 다양성 목적이면 랜덤/최대 norm 등으로 첫 점을 정할 수도 있지만,\n",
    "        # 여기서는 \"가장 큰 min_dists\"를 위해 +inf로 둔다.\n",
    "        min_dists = torch.full((len(cand),), float(\"inf\"), device=device, dtype=torch.float32)\n",
    "\n",
    "    # 3) k-center greedy 반복\n",
    "    #    매번 argmax(min_dists) 하나 선택 -> 그 점을 센터에 추가 -> min_dists 갱신\n",
    "    with torch.no_grad():\n",
    "        for _ in range(k):\n",
    "            j = torch.argmax(min_dists).item()     # cand 내부 위치\n",
    "            sel_idx = cand[j]                      # 원본 인덱스\n",
    "            selected.append(sel_idx)\n",
    "\n",
    "            # 선택된 점을 \"센터\"로 추가: 모든 candidate에 대해 dist_to_new_center 계산 후 min 갱신\n",
    "            new_center = z_cand[j:j+1]  # [1, D]\n",
    "\n",
    "            # 방금 뽑은 점은 다시 뽑히지 않게 min_dists를 -inf로\n",
    "            min_dists[j] = -float(\"inf\")\n",
    "\n",
    "            # 나머지 후보들의 min 거리 업데이트\n",
    "            for s in range(0, len(cand), chunk_size):\n",
    "                e = min(s + chunk_size, len(cand))\n",
    "                d_new = torch.cdist(z_cand[s:e], new_center, p=2).squeeze(1)  # [B]\n",
    "                min_dists[s:e] = torch.minimum(min_dists[s:e], d_new)\n",
    "\n",
    "    diverse_indices = np.array(selected, dtype=np.int64)\n",
    "\n",
    "    candidate_indices = set(candidate_indices)\n",
    "    candidate_indices.difference_update(diverse_indices.tolist())\n",
    "\n",
    "    return diverse_indices, candidate_indices\n",
    "\n",
    "\n",
    "def select_init_latent_diversity(model, input_data_scaled, remaining_indices, select_num, device):\n",
    "    model.eval()\n",
    "\n",
    "    # remaining indices를 리스트로 고정\n",
    "    rem_idx = np.array(list(remaining_indices), dtype=np.int64)\n",
    "    select_num = min(select_num, len(rem_idx))\n",
    "\n",
    "    input_tensor = torch.tensor(\n",
    "        input_data_scaled[rem_idx],\n",
    "        dtype=torch.float32,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(input_tensor, use_mean=True)  # (M, D)\n",
    "\n",
    "    z = z.detach()\n",
    "    M = z.size(0)\n",
    "\n",
    "    selected_local = []\n",
    "\n",
    "    # 1) 첫 점 랜덤 (remaining 내부)\n",
    "    first = torch.randint(0, M, (1,), device=device).item()\n",
    "    selected_local.append(first)\n",
    "\n",
    "    # 2) farthest-point greedy (remaining 내부)\n",
    "    dist = torch.cdist(z, z[[first]])[:, 0]  # (M,)\n",
    "\n",
    "    for _ in range(1, select_num):\n",
    "        idx = torch.argmax(dist).item()\n",
    "        selected_local.append(idx)\n",
    "\n",
    "        new_dist = torch.cdist(z, z[[idx]])[:, 0]\n",
    "        dist = torch.minimum(dist, new_dist)\n",
    "\n",
    "    # local index → global index\n",
    "    selected_global = rem_idx[selected_local]\n",
    "\n",
    "    # remaining 업데이트\n",
    "    remaining_indices.difference_update(selected_global.tolist())\n",
    "\n",
    "    return selected_global, remaining_indices\n",
    "\n",
    "\n",
    "def select_representative_kmeans(model, input_data_scaled, remaining_indices, select_num, device, iters=10):\n",
    "    model.eval()\n",
    "    x = torch.tensor(input_data_scaled, dtype=torch.float32, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(x, use_mean=True)  # (N, D)\n",
    "    z = z.detach()\n",
    "    N, D = z.shape\n",
    "    k = min(select_num, N)\n",
    "\n",
    "    # --- kmeans++ 초기화 (center는 실제 데이터 포인트로 잡음) ---\n",
    "    centers_idx = []\n",
    "    first = torch.randint(0, N, (1,), device=z.device).item()\n",
    "    centers_idx.append(first)\n",
    "\n",
    "    dist = torch.cdist(z, z[[first]])[:, 0]  # (N,)\n",
    "    for _ in range(1, k):\n",
    "        probs = (dist ** 2)\n",
    "        probs = probs / probs.sum().clamp_min(1e-12)\n",
    "        idx = torch.multinomial(probs, 1).item()\n",
    "        centers_idx.append(idx)\n",
    "        dist = torch.minimum(dist, torch.cdist(z, z[[idx]])[:, 0])\n",
    "\n",
    "    centers = z[centers_idx].clone()  # (k, D)\n",
    "\n",
    "    # --- Lloyd iterations ---\n",
    "    for _ in range(iters):\n",
    "        d = torch.cdist(z, centers)          # (N, k)\n",
    "        assign = torch.argmin(d, dim=1)      # (N,)\n",
    "        new_centers = centers.clone()\n",
    "        for j in range(k):\n",
    "            mask = (assign == j)\n",
    "            if mask.any():\n",
    "                new_centers[j] = z[mask].mean(dim=0)\n",
    "        centers = new_centers\n",
    "\n",
    "    # --- 각 중심에 가장 가까운 실제 데이터 인덱스 선택 ---\n",
    "    d = torch.cdist(z, centers)  # (N, k)\n",
    "    selected = []\n",
    "    used = set()\n",
    "    for j in range(k):\n",
    "        # 중심 j에 가장 가까운 점부터 시도하되, 중복 방지\n",
    "        order = torch.argsort(d[:, j])\n",
    "        for idx in order.tolist():\n",
    "            if idx not in used:\n",
    "                used.add(idx)\n",
    "                selected.append(idx)\n",
    "                break\n",
    "\n",
    "    remaining_indices.difference_update(selected)\n",
    "    k_means_indices = np.array(selected, dtype=np.int64)\n",
    "\n",
    "    return k_means_indices, remaining_indices\n",
    "\n",
    "def select_programs(model, input_data_scaled, used_indices, remaining_indices, num_select=64, T_mc=10, uncertainty_topk=128,\n",
    "                    w_cost=0.5, w_unc=0.3, w_div=0.2, grad_num=2, rand_num=0, rng=np.random.default_rng(42), device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), topk_factor=5):\n",
    "    \"\"\"\n",
    "    Active learning 기반 다음 측정할 샘플 선택\n",
    "    \n",
    "    Args:\n",
    "        model: VAECostPredictor 모델\n",
    "        input_data_scaled: 전체 input 리스트 ([N, input_dim] 형태)\n",
    "        used_indices: 이미 측정된 인덱스 집합(set)\n",
    "        remaining_indices: 아직 측정되지 않은 인덱스 집합 (set)\n",
    "        num_select: 선택할 샘플 수\n",
    "        T_mc: MC Dropout 샘플 수\n",
    "        w_cost: 예측값이 큰 샘플 가중치\n",
    "        w_unc: epistemic 불확실성이 높은 샘플 가중치\n",
    "        w_div: latent 다양성이 높은 샘플 가중치\n",
    "        grad_num: z에 대한 cost의 gradient가 큰 샘플 수\n",
    "        rand_num: 무작위로 선택할 샘플 수\n",
    "    \n",
    "    Returns:\n",
    "        selected_indices: 선택된 샘플의 인덱스 리스트\n",
    "    \"\"\"\n",
    "\n",
    "    # 합쳐서 64개 선택\n",
    "    total = num_select\n",
    "    budget = total - grad_num - rand_num\n",
    "\n",
    "    # 랜덤 선택만 할 경우\n",
    "    if num_select == 0 and rand_num > 0:\n",
    "        rand_indices, remaining_indices = random_select_indices(remaining_indices, rand_num, rng=rng)\n",
    "        return rand_indices, remaining_indices\n",
    "    \n",
    "\n",
    "    select_n_cost = int(budget * w_cost)\n",
    "    select_n_unc  = int(budget * w_unc)\n",
    "    select_n_div  = int(budget * w_div)\n",
    "    select_n_grad = grad_num\n",
    "    s = select_n_cost + select_n_unc + select_n_div\n",
    "    if s < budget:\n",
    "        select_n_cost += budget - s\n",
    "\n",
    "    input_tensor = torch.tensor(input_data_scaled, dtype=torch.float32, device=device)\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z, _ = model.encode(input_tensor)\n",
    "    z = z.detach().requires_grad_(True)\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    cost_pred = model.predict_cost(z)\n",
    "    cost_pred = cost_pred.view(-1)\n",
    "    cost_np = cost_pred.detach().cpu().numpy()\n",
    "\n",
    "    remaining_np = np.array(list(remaining_indices), dtype=np.int64)\n",
    "    remaining_cost = cost_np[remaining_np]\n",
    "\n",
    "    k_pref = min(len(remaining_np), total * topk_factor)\n",
    "    top_local = np.argsort(remaining_cost)[-k_pref:]\n",
    "    candidate_indices = set(remaining_np[top_local].tolist())  # 작업용 remaining\n",
    "\n",
    "    # print(f\"Candidate pool size: {len(candidate_indices)}\")\n",
    "\n",
    "\n",
    "    # 중복 방지용\n",
    "    currently_used = set()\n",
    "    topk_cost_indices, candidate_indices = select_topk_cost(cost_pred, candidate_indices, select_n_cost)\n",
    "    currently_used.update(topk_cost_indices.tolist())\n",
    "    z_grad_indices, candidate_indices = select_topk_z_grad(z, cost_pred, candidate_indices, select_n_grad)\n",
    "    currently_used.update(z_grad_indices.tolist())\n",
    "\n",
    "    # if len(used_indices) / len(input_data_scaled) >= 0.1:\n",
    "    if len(used_indices) >= uncertainty_topk:\n",
    "        uncertainty_indices, candidate_indices = select_topk_uncertainty(model, input_tensor, candidate_indices, select_n_unc, T_mc=T_mc)\n",
    "    else:\n",
    "        pool_for_uncertainty = set(remaining_indices)\n",
    "        pool_for_uncertainty.difference_update(currently_used)\n",
    "        uncertainty_indices, _ = select_topk_uncertainty(model, input_tensor, pool_for_uncertainty, select_n_unc, T_mc=T_mc)\n",
    "        candidate_indices.difference_update(uncertainty_indices.tolist())\n",
    "\n",
    "\n",
    "    currently_used.update(uncertainty_indices.tolist())\n",
    "    used_local = set(used_indices)\n",
    "    used_local.update(currently_used)\n",
    "\n",
    "    diverse_indices, _ = select_topk_latent_diversity(z, candidate_indices, used_local, select_n_div)\n",
    "    currently_used.update(diverse_indices.tolist())\n",
    "\n",
    "\n",
    "    remaining_indices.difference_update(currently_used)\n",
    "\n",
    "\n",
    "    rand_indices, remaining_indices = random_select_indices(remaining_indices, rand_num, rng=rng)\n",
    "    currently_used.update(rand_indices.tolist())\n",
    "\n",
    "    \n",
    "\n",
    "    all_selected_indices = np.array(sorted(currently_used), dtype=np.int64)\n",
    "\n",
    "\n",
    "\n",
    "    return all_selected_indices, remaining_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "3fe4fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vae_reg_dataloaders(input_data_scaled, costs, used_indices, remaining_indices):\n",
    "\n",
    "    train_indices = np.array(list(used_indices), dtype=np.int64)\n",
    "    val_indices = np.array(list(remaining_indices), dtype=np.int64)\n",
    "\n",
    "    X_train = input_data_scaled[train_indices]\n",
    "    X_val = input_data_scaled[val_indices]\n",
    "    y_train = costs[train_indices]\n",
    "    y_val = costs[val_indices]\n",
    "\n",
    "    train_dataset = FeatureRegressionDataset(X_train, y_train)\n",
    "    val_dataset   = FeatureRegressionDataset(X_val,   y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=False)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=512, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "    y_mean = y_train.mean()\n",
    "    y_std = y_train.std() + 1e-8  # 0 나누기 방지용 작은 값 추가\n",
    "    print(f\"y_train mean: {y_mean}, std: {y_std}\")\n",
    "\n",
    "    \n",
    "    return train_loader, val_loader, y_mean, y_std\n",
    "\n",
    "\n",
    "def make_xgb_datasets(inputs, results):\n",
    "    f_inputs = []\n",
    "    f_results = []\n",
    "    r_costs = []\n",
    "    for inp, res in zip(inputs, results):\n",
    "        cost = np.mean([c.value for c in res.costs])\n",
    "        if cost < 1e10:\n",
    "            f_inputs.append(inp)\n",
    "            f_results.append(res)\n",
    "            r_costs.append(cost)\n",
    "    r_costs = np.array(r_costs, dtype=np.float32)\n",
    "    \n",
    "    dataset = auto_scheduler.dataset.Dataset()\n",
    "    dataset.update_from_measure_pairs(f_inputs, f_results)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def split_xgb_datasets(dataset, train_indices, test_indices):\n",
    "\n",
    "    raw_features = list(dataset.features.values())[0]\n",
    "    raw_throughputs = list(dataset.throughputs.values())[0]\n",
    "\n",
    "    \n",
    "    train_set, test_set = dataset.random_split_within_task(train_set_ratio=0, \n",
    "                                                        train_idxs=train_indices.tolist(), \n",
    "                                                        test_idxs=test_indices.tolist())\n",
    "    return train_set, test_set, raw_throughputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "ac08a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vae_reg_model(vae, config, input_dim, latent_dim, hidden_dim, y_std, verbose=True):\n",
    "    \n",
    "    print(f\"lambda_reg={config['lambda_reg']}, lambda_pair={config['lambda_pair']}, margin_scale={config['margin_scale']}, epochs={config['epochs']}, gamma={config['gamma']}, beta={config['beta']}, noise_std={config['noise_std']}\",\n",
    "            f\"\\nscratch={config['scratch']}, encoder_freeze={config['encoder_freeze']}, encoder_lr={config['encoder_lr']}, cost_predictor_lr={config['cost_predictor_lr']}\")\n",
    "\n",
    "    vae_cost_model = VAECostPredictor(input_dim=input_dim, \n",
    "                                latent_dim=latent_dim, \n",
    "                                hidden_dim=hidden_dim, \n",
    "                                predictor_layers=2,\n",
    "                                dropout=0.1, use_feature=False).to(device)\n",
    "    if not config['scratch']:\n",
    "        vae_cost_model.load_pretrained_encoder(vae.state_dict())\n",
    "    \n",
    "    if config['encoder_freeze']:\n",
    "        for param in vae_cost_model.get_encoder_params():\n",
    "            param.requires_grad = False\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': vae_cost_model.get_cost_predictor_params(), 'lr': config['cost_predictor_lr']},\n",
    "        ], weight_decay=1e-5)\n",
    "    else:\n",
    "        for param in vae_cost_model.get_encoder_params():\n",
    "            param.requires_grad = True\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': vae_cost_model.get_encoder_params(), 'lr': config['encoder_lr']},\n",
    "            {'params': vae_cost_model.get_cost_predictor_params(), 'lr': config['cost_predictor_lr']},\n",
    "        ], weight_decay=1e-5)\n",
    "        \n",
    "    return vae_cost_model, optimizer, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "df4c5e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_pair_warmup(epoch: int, warmup_epochs: int, lambda_pair_max: float) -> float:\n",
    "    if warmup_epochs <= 0:\n",
    "        return lambda_pair_max\n",
    "    t = min(max(epoch, 0), warmup_epochs) / warmup_epochs  # 0~1\n",
    "    return lambda_pair_max * t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "47e0fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regression(vae_cost_model, optimizer, train_loader, val_loader, input_data_scaled, costs, config, top_k=10, use_rank=True, warmup_epochs=200):\n",
    "\n",
    "    # print(\"Train size :\", len(train_loader.dataset))\n",
    "\n",
    "    # all_reg_results = []\n",
    "\n",
    "    lambda_pair_max = config['lambda_pair']\n",
    "\n",
    "    for epoch in range(1, config['epochs']+1):\n",
    "        vae_cost_model.train()\n",
    "        for x_batch, labels in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            labels = labels.to(device).squeeze(-1)\n",
    "            \n",
    "        \n",
    "            cost_pred, mean, logvar, z = vae_cost_model(x_batch, use_mean=True)\n",
    "\n",
    "            config['lambda_pair'] = lambda_pair_warmup(epoch, warmup_epochs, lambda_pair_max)\n",
    "\n",
    "            \n",
    "            train_loss, train_components = compute_total_loss(vae_cost_model, \n",
    "                                                    cost_pred, mean, logvar, z, labels, None, config)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(vae_cost_model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "        \n",
    "\n",
    "        if epoch % config['epochs'] == 0:\n",
    "            vae_cost_model.eval()\n",
    "            with torch.no_grad():\n",
    "                all_preds = []\n",
    "                all_labels = []\n",
    "                for x_batch, labels in val_loader:\n",
    "                    x_batch = x_batch.to(device)\n",
    "                    labels = labels.to(device).squeeze(-1)\n",
    "\n",
    "                    cost_pred, mean, logvar, z = vae_cost_model(x_batch, use_mean=True)\n",
    "                    all_preds.append(cost_pred)\n",
    "                    all_labels.append(labels)\n",
    "\n",
    "                    val_loss, val_components = compute_total_loss(vae_cost_model, cost_pred, mean, logvar, z, labels, None, config)\n",
    "                val_reg_r2 = r2_score(torch.cat(all_labels).detach().cpu().numpy(), torch.cat(all_preds).detach().cpu().numpy())\n",
    "                val_reg_r2 = round(val_reg_r2, 4)\n",
    "                \n",
    "                print(f\"Train loss epoch {epoch} : reg={train_components['reg_loss']: .4f} rank={train_components['pair_loss']: .4f} kl={train_components['kld_loss']: .4f}\")\n",
    "                print(f\"Val loss epoch {epoch}: reg={val_components['reg_loss']: .4f} rank={val_components['pair_loss']: .4f} kl={val_components['kld_loss']: .4f}\")\n",
    "                \n",
    "                print(f\"Regression R2 : {val_reg_r2:.4f}, \")\n",
    "        \n",
    "        # rank r2 계산\n",
    "        vae_cost_model.eval()\n",
    "        with torch.no_grad():\n",
    "            if epoch % config['epochs'] == 0:\n",
    "                input_data_tensor = torch.from_numpy(input_data_scaled).float().to(device)\n",
    "                all_preds = vae_cost_model(input_data_tensor, use_mean=True)[0].detach().cpu().numpy()\n",
    "                if use_rank:\n",
    "                    val_rank_r2 = pair_accuracy(all_preds, costs)\n",
    "                    val_rank_r2 = round(val_rank_r2, 4)\n",
    "                    print(f\"Rank R2 : {val_rank_r2:.4f}\")\n",
    "                else:\n",
    "                    val_rank_r2 = None\n",
    "                recall_top_k = recall_at_k(torch.tensor(all_preds), torch.from_numpy(costs), k=top_k)\n",
    "                \n",
    "                print(f\"Recall@{top_k} : {recall_top_k}\")\n",
    "\n",
    "    return vae_cost_model, recall_top_k, val_reg_r2, val_rank_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "fbf8d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weight_grid(step=0.1):\n",
    "    m = int(round(1.0 / step))  # step=0.1 -> 10\n",
    "    weights = []\n",
    "    for i in range(m + 1):\n",
    "        for j in range(m + 1):\n",
    "            k = m - i - j\n",
    "            if k < 0:\n",
    "                continue\n",
    "            weights.append((i/m, j/m, k/m))\n",
    "    return weights\n",
    "weights = generate_weight_grid(step=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "18ffb652",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_weights = []\n",
    "for w in weights:\n",
    "    w_cost, w_unc, w_div = w\n",
    "    if w_cost < 0.3:\n",
    "        continue\n",
    "    # if w_unc == 0.0 and w_cost > 0.0 and w_div > 0.0:\n",
    "    #     f_weights.append(w)\n",
    "    #     continue\n",
    "    # if w_div == 0.0 and w_cost > 0.0 and w_unc > 0.0:\n",
    "    #     f_weights.append(w)\n",
    "        # continue\n",
    "    f_weights.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7144bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_already_measured(total_csv_path, sampling_hyper):\n",
    "\n",
    "    if total_csv_path is not None:\n",
    "        total_csv = pd.read_csv(total_csv_path)\n",
    "\n",
    "        measured_keys = {\n",
    "            (\n",
    "                row.measure_size,\n",
    "\n",
    "                # row.scratch,\n",
    "                # row.encoder_freeze,\n",
    "                \n",
    "                row.encoder_lr,\n",
    "                row.cost_predictor_lr,\n",
    "                row.weights,\n",
    "                row.sampling_seed,\n",
    "                row.rank_warmup_epochs,\n",
    "\n",
    "                # row.uncertainty_topk,\n",
    "                row.grad_num,\n",
    "                row.rand_num,\n",
    "            )\n",
    "            for row in total_csv.itertuples(index=False)\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        measured_keys = set()\n",
    "    to_measure_configs = []\n",
    "\n",
    "    for params in itertools.product(*sampling_hyper.values()):\n",
    "        hyper_config = dict(zip(sampling_hyper.keys(), params))\n",
    "\n",
    "        key = (\n",
    "            hyper_config[\"measure_size\"],\n",
    "            # hyper_config[\"scratch\"],\n",
    "            # hyper_config[\"encoder_freeze\"],\n",
    "\n",
    "            hyper_config[\"encoder_lr\"],\n",
    "            hyper_config[\"cost_predictor_lr\"],\n",
    "            str(hyper_config[\"weight\"]),\n",
    "            hyper_config[\"sampling_seed\"],\n",
    "            hyper_config[\"warmup_epochs\"],\n",
    "            \n",
    "            # hyper_config[\"uncertainty_topk\"],\n",
    "            hyper_config[\"grad_num\"],\n",
    "            hyper_config[\"rand_num\"],\n",
    "        )\n",
    "\n",
    "        if key in measured_keys:\n",
    "            continue\n",
    "\n",
    "        to_measure_configs.append(hyper_config)\n",
    "    \n",
    "    return to_measure_configs\n",
    "\n",
    "def save_avg_csv(df_results, filename, top_k):\n",
    "    group_cols = [\n",
    "        \"measure_size\",\n",
    "        # \"scratch\",\n",
    "        # \"encoder_freeze\",\n",
    "        \"encoder_lr\",\n",
    "        \"cost_predictor_lr\",\n",
    "        \"rank_warmup_epochs\",\n",
    "        \"weights\",\n",
    "        \"uncertainty_topk\",\n",
    "        \"grad_num\",\n",
    "        \"rand_num\",\n",
    "    ]\n",
    "\n",
    "    df_avg = (\n",
    "        df_results\n",
    "        .groupby(group_cols, as_index=False)\n",
    "        .agg(\n",
    "            phase=(\"phase\", \"mean\"),\n",
    "            train_size=(\"train_size\", \"mean\"),\n",
    "            used_time=(\"used_time\", \"mean\"),\n",
    "            **{f\"top-{top_k}\": (f\"top-{top_k}\", \"mean\")},\n",
    "            val_reg_r2=(\"val_reg_r2\", \"first\"),\n",
    "            val_rank_r2=(\"val_rank_r2\", \"first\"),\n",
    "            seed_n=(\"sampling_seed\", \"nunique\"),\n",
    "            sampling_seed=(\"sampling_seed\", list),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df_avg.to_csv(filename.replace(\".csv\", \"_avg.csv\"), index=False)\n",
    "    df_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26db8d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 -> 600개의 실험 남음\n",
      "########## 실험 1/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2000\n",
      "초기 랜덤 선택 샘플 인덱스: [   8  100  230  255  392  634  659  750  871  908  979  999 1227 1371\n",
      " 1524 1680 1734 1742 1879 1991 2015 2023 2130 2242 2402 2432 2768 3276\n",
      " 3348 3463 3572 3686]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.420133590698242, std: 1.7561746935708618\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3235 rank= 0.0057 kl= 0.5285\n",
      "Val loss epoch 1000: reg= 2.0443 rank= 0.3621 kl= 0.5472\n",
      "Regression R2 : 0.4159, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.271302700042725, std: 1.6699836354119872\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5388 rank= 0.0075 kl= 0.5408\n",
      "Val loss epoch 1000: reg= 1.9462 rank= 0.2864 kl= 0.5450\n",
      "Regression R2 : 0.3981, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.99 초\n",
      "=============================================\n",
      "########## 실험 2/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2001\n",
      "초기 랜덤 선택 샘플 인덱스: [  75   82  122  374  416  424  443  607  703  755  818  824  900 1217\n",
      " 1253 1569 1603 1617 1682 1761 1779 1918 1991 2132 2137 2229 2549 2845\n",
      " 2911 3569 3683 3710]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.486093044281006, std: 2.5365142922265624\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4466 rank= 0.0035 kl= 0.5563\n",
      "Val loss epoch 1000: reg= 1.8657 rank= 0.2718 kl= 0.5456\n",
      "Regression R2 : 0.5225, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.406013488769531, std: 2.053844461904297\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.7208 rank= 0.0098 kl= 0.5557\n",
      "Val loss epoch 1000: reg= 2.7328 rank= 0.3921 kl= 0.5401\n",
      "Regression R2 : 0.3093, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.7234649658203125, std: 1.845200787053833\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6392 rank= 0.0122 kl= 0.5622\n",
      "Val loss epoch 1000: reg= 2.6869 rank= 0.2668 kl= 0.5378\n",
      "Regression R2 : 0.3061, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 4 (128개) ================\n",
      "y_train mean: 7.890449523925781, std: 1.683975348935852\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4693 rank= 0.0138 kl= 0.5579\n",
      "Val loss epoch 1000: reg= 2.0542 rank= 0.1388 kl= 0.5171\n",
      "Regression R2 : 0.3310, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 5 (160개) ================\n",
      "y_train mean: 8.047894477844238, std: 1.549148688779602\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5707 rank= 0.0179 kl= 0.5448\n",
      "Val loss epoch 1000: reg= 2.9115 rank= 0.1998 kl= 0.5180\n",
      "Regression R2 : 0.2718, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 6 (192개) ================\n",
      "y_train mean: 8.171545028686523, std: 1.4482649664743041\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5719 rank= 0.0217 kl= 0.5402\n",
      "Val loss epoch 1000: reg= 2.7340 rank= 0.2062 kl= 0.5261\n",
      "Regression R2 : 0.3162, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 7 (224개) ================\n",
      "y_train mean: 8.245993614196777, std: 1.3617205719812011\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5494 rank= 0.0204 kl= 0.5379\n",
      "Val loss epoch 1000: reg= 2.5231 rank= 0.1851 kl= 0.5253\n",
      "Regression R2 : 0.3699, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 224\n",
      "총 측정 시간: 25.76 초\n",
      "=============================================\n",
      "########## 실험 3/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2002\n",
      "초기 랜덤 선택 샘플 인덱스: [ 136  395  402  409  532  561  564  638  885  923 1182 1208 1321 1525\n",
      " 1897 1919 1934 2197 2255 2440 2570 3093 3129 3291 3299 3334 3343 3425\n",
      " 3477 3495 3560 3626]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.467774868011475, std: 1.6519296269281005\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3532 rank= 0.0054 kl= 0.5238\n",
      "Val loss epoch 1000: reg= 1.5187 rank= 0.2522 kl= 0.5497\n",
      "Regression R2 : 0.5101, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.271766662597656, std: 1.5240259270532226\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4057 rank= 0.0127 kl= 0.5429\n",
      "Val loss epoch 1000: reg= 1.6604 rank= 0.2117 kl= 0.5365\n",
      "Regression R2 : 0.5259, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.692402362823486, std: 1.4128357272012328\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5843 rank= 0.0119 kl= 0.5477\n",
      "Val loss epoch 1000: reg= 2.3385 rank= 0.2298 kl= 0.5385\n",
      "Regression R2 : 0.4472, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 4 (128개) ================\n",
      "y_train mean: 7.946906089782715, std: 1.3321642975671386\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5545 rank= 0.0162 kl= 0.5503\n",
      "Val loss epoch 1000: reg= 1.5477 rank= 0.1719 kl= 0.5270\n",
      "Regression R2 : 0.3048, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 5 (160개) ================\n",
      "y_train mean: 7.980210781097412, std: 1.3154929976327514\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4598 rank= 0.0176 kl= 0.5673\n",
      "Val loss epoch 1000: reg= 2.0550 rank= 0.1849 kl= 0.5579\n",
      "Regression R2 : 0.4609, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 6 (192개) ================\n",
      "y_train mean: 8.115605354309082, std: 1.2479074101312255\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4357 rank= 0.0198 kl= 0.5611\n",
      "Val loss epoch 1000: reg= 2.1913 rank= 0.1777 kl= 0.5545\n",
      "Regression R2 : 0.4364, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 7 (224개) ================\n",
      "y_train mean: 8.215493202209473, std: 1.1886695723397827\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4307 rank= 0.0254 kl= 0.5546\n",
      "Val loss epoch 1000: reg= 2.7976 rank= 0.2171 kl= 0.5509\n",
      "Regression R2 : 0.3583, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 8 (256개) ================\n",
      "y_train mean: 8.266263961791992, std: 1.1327165465219116\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4284 rank= 0.0255 kl= 0.5435\n",
      "Val loss epoch 1000: reg= 2.7977 rank= 0.1994 kl= 0.5410\n",
      "Regression R2 : 0.3322, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 256\n",
      "총 측정 시간: 30.02 초\n",
      "=============================================\n",
      "########## 실험 4/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2003\n",
      "초기 랜덤 선택 샘플 인덱스: [  70  278  474  527  720  757  885  969  988 1006 1091 1309 1347 1449\n",
      " 1633 1868 2081 2112 2151 2415 2477 2489 2645 2950 3014 3217 3258 3265\n",
      " 3428 3509 3645 3704]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.318173408508301, std: 1.77249277638031\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4324 rank= 0.0076 kl= 0.5195\n",
      "Val loss epoch 1000: reg= 1.7852 rank= 0.2926 kl= 0.5469\n",
      "Regression R2 : 0.4739, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.158354759216309, std: 1.6136534314019775\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4020 rank= 0.0135 kl= 0.5546\n",
      "Val loss epoch 1000: reg= 1.5740 rank= 0.2497 kl= 0.5423\n",
      "Regression R2 : 0.6055, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.70 초\n",
      "=============================================\n",
      "########## 실험 5/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2004\n",
      "초기 랜덤 선택 샘플 인덱스: [  10  112  195  254  342  507  623  690 1078 1157 1302 1406 1700 1831\n",
      " 2036 2038 2156 2267 2300 2306 2371 2458 2570 2626 2630 2648 2662 2818\n",
      " 3037 3109 3397 3569]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.423959732055664, std: 2.109467039571533\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4601 rank= 0.0055 kl= 0.5341\n",
      "Val loss epoch 1000: reg= 2.5295 rank= 0.4202 kl= 0.5391\n",
      "Regression R2 : 0.5069, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.426758289337158, std: 1.8297971587045287\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.7022 rank= 0.0095 kl= 0.5689\n",
      "Val loss epoch 1000: reg= 2.7968 rank= 0.4718 kl= 0.5483\n",
      "Regression R2 : 0.3033, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.652252674102783, std: 1.6503440241677856\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4866 rank= 0.0125 kl= 0.5741\n",
      "Val loss epoch 1000: reg= 3.2583 rank= 0.3348 kl= 0.5304\n",
      "Regression R2 : 0.3294, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 96\n",
      "총 측정 시간: 10.49 초\n",
      "=============================================\n",
      "########## 실험 6/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2005\n",
      "초기 랜덤 선택 샘플 인덱스: [  12   33   62  109  111  489  793  846  971  990 1151 1320 1736 1793\n",
      " 1882 1884 2062 2197 2222 2420 2478 2503 2539 2658 2692 2761 2985 3170\n",
      " 3328 3379 3496 3598]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.756325721740723, std: 1.7418288092477416\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4401 rank= 0.0019 kl= 0.5537\n",
      "Val loss epoch 1000: reg= 2.2031 rank= 0.3710 kl= 0.5615\n",
      "Regression R2 : 0.4335, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.669419765472412, std: 1.5670029024942016\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6360 rank= 0.0088 kl= 0.5799\n",
      "Val loss epoch 1000: reg= 3.1221 rank= 0.4546 kl= 0.5638\n",
      "Regression R2 : 0.1959, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.718564987182617, std: 1.500195036397705\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4744 rank= 0.0144 kl= 0.5720\n",
      "Val loss epoch 1000: reg= 2.8841 rank= 0.2520 kl= 0.5387\n",
      "Regression R2 : 0.3448, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 4 (128개) ================\n",
      "y_train mean: 7.974491119384766, std: 1.3840192656381225\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5045 rank= 0.0130 kl= 0.5696\n",
      "Val loss epoch 1000: reg= 2.2847 rank= 0.2452 kl= 0.5262\n",
      "Regression R2 : 0.3944, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 128\n",
      "총 측정 시간: 14.03 초\n",
      "=============================================\n",
      "########## 실험 7/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2006\n",
      "초기 랜덤 선택 샘플 인덱스: [  47   78  204  215  279  353  565  816 1095 1122 1149 1296 1328 1432\n",
      " 1522 1762 2071 2176 2357 2466 2510 2531 2924 3036 3073 3229 3333 3368\n",
      " 3521 3564 3721 3731]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.419711112976074, std: 1.8596973519189453\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3070 rank= 0.0041 kl= 0.5293\n",
      "Val loss epoch 1000: reg= 1.7809 rank= 0.2738 kl= 0.5411\n",
      "Regression R2 : 0.5880, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.291566848754883, std: 1.6211214165551757\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5894 rank= 0.0090 kl= 0.5309\n",
      "Val loss epoch 1000: reg= 2.0892 rank= 0.2583 kl= 0.5248\n",
      "Regression R2 : 0.4293, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.556006908416748, std: 1.4737933974130248\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4287 rank= 0.0112 kl= 0.5415\n",
      "Val loss epoch 1000: reg= 2.8186 rank= 0.2273 kl= 0.5145\n",
      "Regression R2 : 0.4609, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 96\n",
      "총 측정 시간: 10.45 초\n",
      "=============================================\n",
      "########## 실험 8/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2007\n",
      "초기 랜덤 선택 샘플 인덱스: [  45  375  515  708  803  809  834 1015 1047 1083 1092 1150 1333 1455\n",
      " 1477 1689 1759 1787 1930 2051 2102 2259 2450 2496 2504 2669 2696 2814\n",
      " 3058 3283 3399 3438]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.840710639953613, std: 2.0724175076348876\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4997 rank= 0.0054 kl= 0.5306\n",
      "Val loss epoch 1000: reg= 2.8904 rank= 0.5332 kl= 0.5377\n",
      "Regression R2 : 0.3491, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.596524238586426, std: 1.7551726202828979\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6802 rank= 0.0105 kl= 0.5558\n",
      "Val loss epoch 1000: reg= 1.6624 rank= 0.2640 kl= 0.5264\n",
      "Regression R2 : 0.5304, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.64 초\n",
      "=============================================\n",
      "########## 실험 9/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2008\n",
      "초기 랜덤 선택 샘플 인덱스: [  53  112  160  174  213  231  945 1134 1193 1233 1267 1269 1487 2193\n",
      " 2294 2493 2542 2640 2713 2742 2803 2830 2857 2902 3140 3246 3288 3315\n",
      " 3360 3442 3471 3485]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.172525882720947, std: 1.5799144606454467\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3537 rank= 0.0047 kl= 0.5222\n",
      "Val loss epoch 1000: reg= 1.9354 rank= 0.2781 kl= 0.5382\n",
      "Regression R2 : 0.3065, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.246640205383301, std: 1.6165433029992675\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4284 rank= 0.0106 kl= 0.5590\n",
      "Val loss epoch 1000: reg= 2.1577 rank= 0.2812 kl= 0.5474\n",
      "Regression R2 : 0.2952, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.68 초\n",
      "=============================================\n",
      "########## 실험 10/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2009\n",
      "초기 랜덤 선택 샘플 인덱스: [   0  111  335  399  520  678  750  785  881  909 1097 1244 1434 1440\n",
      " 1580 1585 1713 1878 1963 2228 2249 2520 2691 2746 2856 2892 2972 3097\n",
      " 3216 3314 3349 3655]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.897634029388428, std: 1.8627023796899413\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5560 rank= 0.0058 kl= 0.5509\n",
      "Val loss epoch 1000: reg= 1.3827 rank= 0.2207 kl= 0.5476\n",
      "Regression R2 : 0.5423, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.669181823730469, std: 1.5670687060220336\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.8255 rank= 0.0101 kl= 0.5695\n",
      "Val loss epoch 1000: reg= 2.3119 rank= 0.3616 kl= 0.5472\n",
      "Regression R2 : 0.3279, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.78 초\n",
      "=============================================\n",
      "########## 실험 11/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2010\n",
      "초기 랜덤 선택 샘플 인덱스: [ 261  359  368  549  552  716  734  739  890  893  901 1012 1190 1368\n",
      " 1386 1395 1733 1820 1850 2040 2165 2428 2596 2650 2792 2805 2845 3137\n",
      " 3166 3182 3224 3417]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 5.9191484451293945, std: 1.6618188719613647\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3241 rank= 0.0040 kl= 0.5262\n",
      "Val loss epoch 1000: reg= 1.7836 rank= 0.2557 kl= 0.5321\n",
      "Regression R2 : 0.5203, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.087320327758789, std: 1.7149964670999145\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4944 rank= 0.0068 kl= 0.5454\n",
      "Val loss epoch 1000: reg= 2.4782 rank= 0.3322 kl= 0.5230\n",
      "Regression R2 : 0.2864, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.70 초\n",
      "=============================================\n",
      "########## 실험 12/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2011\n",
      "초기 랜덤 선택 샘플 인덱스: [  92  116  333  394  511  702  714  848  879  920  932  933 1309 1350\n",
      " 1370 1433 1728 1741 2152 2305 2429 2518 2531 2536 2571 2579 2619 2768\n",
      " 3170 3231 3347 3487]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.885871887207031, std: 1.8167252640588378\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6127 rank= 0.0037 kl= 0.5518\n",
      "Val loss epoch 1000: reg= 2.2918 rank= 0.3753 kl= 0.5612\n",
      "Regression R2 : 0.4020, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.594347953796387, std: 1.531169066892395\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6720 rank= 0.0117 kl= 0.5958\n",
      "Val loss epoch 1000: reg= 2.4337 rank= 0.3013 kl= 0.5385\n",
      "Regression R2 : 0.2661, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.913026809692383, std: 1.3480041127069091\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6532 rank= 0.0127 kl= 0.5939\n",
      "Val loss epoch 1000: reg= 2.8288 rank= 0.2967 kl= 0.5454\n",
      "Regression R2 : 0.2242, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 4 (128개) ================\n",
      "y_train mean: 8.048367500305176, std: 1.2537030081613159\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6031 rank= 0.0202 kl= 0.5924\n",
      "Val loss epoch 1000: reg= 2.1973 rank= 0.2389 kl= 0.5303\n",
      "Regression R2 : 0.2326, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 128\n",
      "총 측정 시간: 14.24 초\n",
      "=============================================\n",
      "########## 실험 13/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2012\n",
      "초기 랜덤 선택 샘플 인덱스: [ 245  271  401  535  567  880  900  917 1060 1229 1324 1441 1606 1654\n",
      " 1675 1683 1803 2019 2234 2294 2503 2741 2790 2809 2905 2944 3051 3119\n",
      " 3127 3187 3200 3306]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.339137077331543, std: 2.080928097234497\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3135 rank= 0.0027 kl= 0.5079\n",
      "Val loss epoch 1000: reg= 1.7144 rank= 0.2514 kl= 0.5235\n",
      "Regression R2 : 0.5289, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.291147232055664, std: 1.7891687254769897\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6002 rank= 0.0089 kl= 0.5347\n",
      "Val loss epoch 1000: reg= 2.0853 rank= 0.2979 kl= 0.5306\n",
      "Regression R2 : 0.4552, \n",
      "Recall@1 : 1\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.71 초\n",
      "=============================================\n",
      "########## 실험 14/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2013\n",
      "초기 랜덤 선택 샘플 인덱스: [ 151  728  730  762  893  970 1004 1010 1378 1477 1570 1612 1740 1759\n",
      " 1971 2035 2047 2087 2089 2122 2310 2336 2446 2694 3111 3389 3409 3487\n",
      " 3540 3550 3592 3717]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.386500835418701, std: 1.6640498738153076\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3420 rank= 0.0042 kl= 0.5492\n",
      "Val loss epoch 1000: reg= 1.4650 rank= 0.1794 kl= 0.5369\n",
      "Regression R2 : 0.5065, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.4453535079956055, std: 1.6502604584558105\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5437 rank= 0.0079 kl= 0.5543\n",
      "Val loss epoch 1000: reg= 2.3585 rank= 0.3447 kl= 0.5279\n",
      "Regression R2 : 0.4196, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.68 초\n",
      "=============================================\n",
      "########## 실험 15/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2014\n",
      "초기 랜덤 선택 샘플 인덱스: [ 180  239  243  400  440  736  892  933  984 1408 1710 1860 1954 2259\n",
      " 2340 2466 2491 2647 2720 2746 2780 2790 2840 2847 2941 3054 3097 3194\n",
      " 3364 3402 3469 3683]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.088900089263916, std: 2.1950976948602294\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3383 rank= 0.0064 kl= 0.5243\n",
      "Val loss epoch 1000: reg= 2.9969 rank= 0.4216 kl= 0.5250\n",
      "Regression R2 : 0.2478, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.1547722816467285, std: 1.958557377324829\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6287 rank= 0.0061 kl= 0.5541\n",
      "Val loss epoch 1000: reg= 2.3344 rank= 0.3750 kl= 0.5237\n",
      "Regression R2 : 0.3778, \n",
      "Recall@1 : 1\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.91 초\n",
      "=============================================\n",
      "########## 실험 16/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2015\n",
      "초기 랜덤 선택 샘플 인덱스: [  55  223  326  436  483  604  824  942 1062 1078 1132 1198 1330 1339\n",
      " 1488 1615 1672 1732 1867 2383 2494 2548 2592 2860 2934 2957 3215 3253\n",
      " 3374 3483 3531 3588]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 7.048605918884277, std: 1.5280425648553466\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4643 rank= 0.0057 kl= 0.5380\n",
      "Val loss epoch 1000: reg= 1.7080 rank= 0.2736 kl= 0.5386\n",
      "Regression R2 : 0.5854, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.753417491912842, std: 1.3458318810327148\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6302 rank= 0.0106 kl= 0.5318\n",
      "Val loss epoch 1000: reg= 2.1549 rank= 0.2605 kl= 0.5318\n",
      "Regression R2 : 0.4188, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.82 초\n",
      "=============================================\n",
      "########## 실험 17/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2016\n",
      "초기 랜덤 선택 샘플 인덱스: [ 186  307  358  379  584  783  868  948 1016 1120 1247 1258 1324 1496\n",
      " 1645 1721 1883 2097 2157 2258 2362 2392 2456 2594 2614 2870 2924 3343\n",
      " 3402 3407 3521 3582]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 7.144143581390381, std: 1.575487981305847\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6705 rank= 0.0071 kl= 0.5495\n",
      "Val loss epoch 1000: reg= 1.9914 rank= 0.3308 kl= 0.5465\n",
      "Regression R2 : 0.4674, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.55150842666626, std: 1.3750237326486205\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6365 rank= 0.0118 kl= 0.5532\n",
      "Val loss epoch 1000: reg= 3.2456 rank= 0.4825 kl= 0.5309\n",
      "Regression R2 : 0.3185, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.77618932723999, std: 1.2820478777749633\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5564 rank= 0.0164 kl= 0.5591\n",
      "Val loss epoch 1000: reg= 3.1543 rank= 0.3052 kl= 0.5278\n",
      "Regression R2 : 0.3427, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 96\n",
      "총 측정 시간: 10.21 초\n",
      "=============================================\n",
      "########## 실험 18/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2017\n",
      "초기 랜덤 선택 샘플 인덱스: [ 164  315  373  504  581  817 1219 1281 1347 1581 1684 1718 1873 1944\n",
      " 1981 2147 2163 2245 2335 2531 2611 2643 2809 3022 3123 3135 3194 3482\n",
      " 3488 3497 3665 3688]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 5.855408668518066, std: 2.019801626668701\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.2888 rank= 0.0032 kl= 0.5179\n",
      "Val loss epoch 1000: reg= 2.4701 rank= 0.3841 kl= 0.5293\n",
      "Regression R2 : 0.3696, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 32\n",
      "총 측정 시간: 3.29 초\n",
      "=============================================\n",
      "########## 실험 19/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2018\n",
      "초기 랜덤 선택 샘플 인덱스: [  13  230  342  421  515  516  572  664  679  890  946 1084 1133 1138\n",
      " 1248 1788 1805 1991 2114 2287 2289 2371 2510 2661 2930 3109 3268 3376\n",
      " 3465 3472 3548 3680]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.5333452224731445, std: 1.9987838368280029\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3781 rank= 0.0073 kl= 0.5178\n",
      "Val loss epoch 1000: reg= 1.9397 rank= 0.2899 kl= 0.5298\n",
      "Regression R2 : 0.4762, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.485151290893555, std: 1.7207969527108764\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.7004 rank= 0.0084 kl= 0.5375\n",
      "Val loss epoch 1000: reg= 1.8935 rank= 0.2393 kl= 0.5354\n",
      "Regression R2 : 0.4700, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.839837551116943, std: 1.5289914708001708\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.7092 rank= 0.0138 kl= 0.5561\n",
      "Val loss epoch 1000: reg= 2.2851 rank= 0.2231 kl= 0.5299\n",
      "Regression R2 : 0.3435, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 4 (128개) ================\n",
      "y_train mean: 8.030425071716309, std: 1.3757404188973998\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6443 rank= 0.0197 kl= 0.5509\n",
      "Val loss epoch 1000: reg= 1.5861 rank= 0.1597 kl= 0.5071\n",
      "Regression R2 : 0.2632, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 5 (160개) ================\n",
      "y_train mean: 8.046427726745605, std: 1.3441064457757568\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5592 rank= 0.0180 kl= 0.5550\n",
      "Val loss epoch 1000: reg= 3.0244 rank= 0.2528 kl= 0.5209\n",
      "Regression R2 : 0.2460, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 6 (192개) ================\n",
      "y_train mean: 8.172974586486816, std: 1.266322384343872\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6046 rank= 0.0178 kl= 0.5526\n",
      "Val loss epoch 1000: reg= 3.3673 rank= 0.2899 kl= 0.5272\n",
      "Regression R2 : 0.2046, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 7 (224개) ================\n",
      "y_train mean: 8.237719535827637, std: 1.2015714745385742\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6275 rank= 0.0234 kl= 0.5447\n",
      "Val loss epoch 1000: reg= 3.1359 rank= 0.2287 kl= 0.5273\n",
      "Regression R2 : 0.2289, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 8 (256개) ================\n",
      "y_train mean: 8.276174545288086, std: 1.155724058614502\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6437 rank= 0.0246 kl= 0.5231\n",
      "Val loss epoch 1000: reg= 3.0182 rank= 0.2225 kl= 0.5175\n",
      "Regression R2 : 0.2734, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 9 (288개) ================\n",
      "y_train mean: 8.334943771362305, std: 1.1094509463174438\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6489 rank= 0.0320 kl= 0.5208\n",
      "Val loss epoch 1000: reg= 2.7517 rank= 0.1982 kl= 0.5224\n",
      "Regression R2 : 0.2876, \n",
      "Recall@1 : 1\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 288\n",
      "총 측정 시간: 34.35 초\n",
      "=============================================\n",
      "########## 실험 20/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2019\n",
      "초기 랜덤 선택 샘플 인덱스: [ 417  535  676  735  742 1132 1197 1261 1292 1438 1440 1599 1638 1696\n",
      " 1785 1864 1969 2063 2105 2423 2554 2612 2715 2759 2851 3278 3398 3459\n",
      " 3480 3491 3521 3594]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.507327556610107, std: 1.7199821572167968\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4056 rank= 0.0081 kl= 0.5171\n",
      "Val loss epoch 1000: reg= 2.2060 rank= 0.3159 kl= 0.5411\n",
      "Regression R2 : 0.3492, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.4409637451171875, std: 1.5925490956170654\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5726 rank= 0.0129 kl= 0.5403\n",
      "Val loss epoch 1000: reg= 2.0403 rank= 0.2875 kl= 0.5299\n",
      "Regression R2 : 0.4386, \n",
      "Recall@1 : 1\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.77 초\n",
      "=============================================\n",
      "########## 실험 21/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2000\n",
      "초기 랜덤 선택 샘플 인덱스: [   8  100  230  255  392  634  659  750  871  908  979  999 1227 1371\n",
      " 1524 1680 1734 1742 1879 1991 2015 2023 2130 2242 2402 2432 2768 3276\n",
      " 3348 3463 3572 3686]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.420133590698242, std: 1.7561746935708618\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3043 rank= 0.0053 kl= 0.5313\n",
      "Val loss epoch 1000: reg= 2.0584 rank= 0.3640 kl= 0.5485\n",
      "Regression R2 : 0.4180, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 32\n",
      "총 측정 시간: 3.42 초\n",
      "=============================================\n",
      "########## 실험 22/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2001\n",
      "초기 랜덤 선택 샘플 인덱스: [  75   82  122  374  416  424  443  607  703  755  818  824  900 1217\n",
      " 1253 1569 1603 1617 1682 1761 1779 1918 1991 2132 2137 2229 2549 2845\n",
      " 2911 3569 3683 3710]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.486093044281006, std: 2.5365142922265624\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4234 rank= 0.0036 kl= 0.5520\n",
      "Val loss epoch 1000: reg= 1.8664 rank= 0.2745 kl= 0.5402\n",
      "Regression R2 : 0.5189, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.391078948974609, std: 2.044337521062622\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6501 rank= 0.0107 kl= 0.5506\n",
      "Val loss epoch 1000: reg= 2.7577 rank= 0.4114 kl= 0.5351\n",
      "Regression R2 : 0.3263, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.685632705688477, std: 1.828140387998352\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5933 rank= 0.0154 kl= 0.5418\n",
      "Val loss epoch 1000: reg= 2.5743 rank= 0.2562 kl= 0.5228\n",
      "Regression R2 : 0.3425, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 4 (128개) ================\n",
      "y_train mean: 7.799837112426758, std: 1.6852738957269287\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4539 rank= 0.0140 kl= 0.5431\n",
      "Val loss epoch 1000: reg= 2.3920 rank= 0.1966 kl= 0.5086\n",
      "Regression R2 : 0.3572, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 128\n",
      "총 측정 시간: 14.20 초\n",
      "=============================================\n",
      "########## 실험 23/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2002\n",
      "초기 랜덤 선택 샘플 인덱스: [ 136  395  402  409  532  561  564  638  885  923 1182 1208 1321 1525\n",
      " 1897 1919 1934 2197 2255 2440 2570 3093 3129 3291 3299 3334 3343 3425\n",
      " 3477 3495 3560 3626]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.467774868011475, std: 1.6519296269281005\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3454 rank= 0.0057 kl= 0.5219\n",
      "Val loss epoch 1000: reg= 1.5021 rank= 0.2497 kl= 0.5473\n",
      "Regression R2 : 0.5127, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.312918186187744, std: 1.5318399767739868\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5027 rank= 0.0087 kl= 0.5298\n",
      "Val loss epoch 1000: reg= 1.7060 rank= 0.2166 kl= 0.5197\n",
      "Regression R2 : 0.5045, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.663684368133545, std: 1.4137301545007324\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4786 rank= 0.0130 kl= 0.5391\n",
      "Val loss epoch 1000: reg= 2.0942 rank= 0.2384 kl= 0.5323\n",
      "Regression R2 : 0.5024, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 4 (128개) ================\n",
      "y_train mean: 7.918404579162598, std: 1.3212967018945312\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4990 rank= 0.0184 kl= 0.5518\n",
      "Val loss epoch 1000: reg= 1.1142 rank= 0.1153 kl= 0.5149\n",
      "Regression R2 : 0.4268, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 5 (160개) ================\n",
      "y_train mean: 8.05689525604248, std: 1.238709817395935\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4612 rank= 0.0215 kl= 0.5347\n",
      "Val loss epoch 1000: reg= 2.0552 rank= 0.2555 kl= 0.5183\n",
      "Regression R2 : 0.4626, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 160\n",
      "총 측정 시간: 17.93 초\n",
      "=============================================\n",
      "########## 실험 24/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2003\n",
      "초기 랜덤 선택 샘플 인덱스: [  70  278  474  527  720  757  885  969  988 1006 1091 1309 1347 1449\n",
      " 1633 1868 2081 2112 2151 2415 2477 2489 2645 2950 3014 3217 3258 3265\n",
      " 3428 3509 3645 3704]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.318173408508301, std: 1.77249277638031\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4152 rank= 0.0077 kl= 0.5195\n",
      "Val loss epoch 1000: reg= 1.8128 rank= 0.2980 kl= 0.5457\n",
      "Regression R2 : 0.4709, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.120508193969727, std: 1.6121398310525512\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3298 rank= 0.0097 kl= 0.5534\n",
      "Val loss epoch 1000: reg= 1.5609 rank= 0.2431 kl= 0.5341\n",
      "Regression R2 : 0.6003, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.70 초\n",
      "=============================================\n",
      "########## 실험 25/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2004\n",
      "초기 랜덤 선택 샘플 인덱스: [  10  112  195  254  342  507  623  690 1078 1157 1302 1406 1700 1831\n",
      " 2036 2038 2156 2267 2300 2306 2371 2458 2570 2626 2630 2648 2662 2818\n",
      " 3037 3109 3397 3569]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.423959732055664, std: 2.109467039571533\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4576 rank= 0.0046 kl= 0.5281\n",
      "Val loss epoch 1000: reg= 2.6110 rank= 0.4331 kl= 0.5329\n",
      "Regression R2 : 0.4961, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.4123029708862305, std: 1.8182571034295654\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5567 rank= 0.0090 kl= 0.5500\n",
      "Val loss epoch 1000: reg= 2.8452 rank= 0.4849 kl= 0.5326\n",
      "Regression R2 : 0.3191, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.619198322296143, std: 1.6726447443826293\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5090 rank= 0.0103 kl= 0.5665\n",
      "Val loss epoch 1000: reg= 3.6222 rank= 0.3968 kl= 0.5239\n",
      "Regression R2 : 0.3007, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 96\n",
      "총 측정 시간: 10.36 초\n",
      "=============================================\n",
      "########## 실험 26/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2005\n",
      "초기 랜덤 선택 샘플 인덱스: [  12   33   62  109  111  489  793  846  971  990 1151 1320 1736 1793\n",
      " 1882 1884 2062 2197 2222 2420 2478 2503 2539 2658 2692 2761 2985 3170\n",
      " 3328 3379 3496 3598]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.756325721740723, std: 1.7418288092477416\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4489 rank= 0.0024 kl= 0.5531\n",
      "Val loss epoch 1000: reg= 2.2423 rank= 0.3761 kl= 0.5568\n",
      "Regression R2 : 0.4271, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.692727088928223, std: 1.5730424027307128\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6302 rank= 0.0087 kl= 0.5680\n",
      "Val loss epoch 1000: reg= 2.9480 rank= 0.4204 kl= 0.5460\n",
      "Regression R2 : 0.2225, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.64 초\n",
      "=============================================\n",
      "########## 실험 27/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2006\n",
      "초기 랜덤 선택 샘플 인덱스: [  47   78  204  215  279  353  565  816 1095 1122 1149 1296 1328 1432\n",
      " 1522 1762 2071 2176 2357 2466 2510 2531 2924 3036 3073 3229 3333 3368\n",
      " 3521 3564 3721 3731]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.419711112976074, std: 1.8596973519189453\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.2976 rank= 0.0044 kl= 0.5267\n",
      "Val loss epoch 1000: reg= 1.8183 rank= 0.2763 kl= 0.5380\n",
      "Regression R2 : 0.5802, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.258119106292725, std: 1.6262649397714233\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4981 rank= 0.0088 kl= 0.5182\n",
      "Val loss epoch 1000: reg= 2.3722 rank= 0.3187 kl= 0.5049\n",
      "Regression R2 : 0.4566, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.74 초\n",
      "=============================================\n",
      "########## 실험 28/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2007\n",
      "초기 랜덤 선택 샘플 인덱스: [  45  375  515  708  803  809  834 1015 1047 1083 1092 1150 1333 1455\n",
      " 1477 1689 1759 1787 1930 2051 2102 2259 2450 2496 2504 2669 2696 2814\n",
      " 3058 3283 3399 3438]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.840710639953613, std: 2.0724175076348876\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4973 rank= 0.0049 kl= 0.5278\n",
      "Val loss epoch 1000: reg= 2.8043 rank= 0.5168 kl= 0.5350\n",
      "Regression R2 : 0.3613, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.581270217895508, std: 1.7589318852288818\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.7385 rank= 0.0103 kl= 0.5540\n",
      "Val loss epoch 1000: reg= 2.1470 rank= 0.3541 kl= 0.5244\n",
      "Regression R2 : 0.4198, \n",
      "Recall@1 : 1\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.74 초\n",
      "=============================================\n",
      "########## 실험 29/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2008\n",
      "초기 랜덤 선택 샘플 인덱스: [  53  112  160  174  213  231  945 1134 1193 1233 1267 1269 1487 2193\n",
      " 2294 2493 2542 2640 2713 2742 2803 2830 2857 2902 3140 3246 3288 3315\n",
      " 3360 3442 3471 3485]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.172525882720947, std: 1.5799144606454467\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3547 rank= 0.0049 kl= 0.5221\n",
      "Val loss epoch 1000: reg= 1.9633 rank= 0.2802 kl= 0.5364\n",
      "Regression R2 : 0.2926, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.248540878295898, std: 1.6188410620553588\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3371 rank= 0.0110 kl= 0.5544\n",
      "Val loss epoch 1000: reg= 2.2555 rank= 0.2809 kl= 0.5374\n",
      "Regression R2 : 0.1883, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.54257345199585, std: 1.5168719391687011\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3316 rank= 0.0100 kl= 0.5418\n",
      "Val loss epoch 1000: reg= 2.3537 rank= 0.2327 kl= 0.5151\n",
      "Regression R2 : 0.2097, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 96\n",
      "총 측정 시간: 10.35 초\n",
      "=============================================\n",
      "########## 실험 30/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2009\n",
      "초기 랜덤 선택 샘플 인덱스: [   0  111  335  399  520  678  750  785  881  909 1097 1244 1434 1440\n",
      " 1580 1585 1713 1878 1963 2228 2249 2520 2691 2746 2856 2892 2972 3097\n",
      " 3216 3314 3349 3655]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.897634029388428, std: 1.8627023796899413\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5494 rank= 0.0048 kl= 0.5475\n",
      "Val loss epoch 1000: reg= 1.3976 rank= 0.2199 kl= 0.5422\n",
      "Regression R2 : 0.5333, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.688762187957764, std: 1.578291903005371\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.7163 rank= 0.0093 kl= 0.5680\n",
      "Val loss epoch 1000: reg= 2.2972 rank= 0.3537 kl= 0.5433\n",
      "Regression R2 : 0.3412, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.68 초\n",
      "=============================================\n",
      "########## 실험 31/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2010\n",
      "초기 랜덤 선택 샘플 인덱스: [ 261  359  368  549  552  716  734  739  890  893  901 1012 1190 1368\n",
      " 1386 1395 1733 1820 1850 2040 2165 2428 2596 2650 2792 2805 2845 3137\n",
      " 3166 3182 3224 3417]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 5.9191484451293945, std: 1.6618188719613647\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3201 rank= 0.0041 kl= 0.5249\n",
      "Val loss epoch 1000: reg= 1.8244 rank= 0.2618 kl= 0.5305\n",
      "Regression R2 : 0.5108, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.049127578735352, std: 1.7269636492593383\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5369 rank= 0.0088 kl= 0.5258\n",
      "Val loss epoch 1000: reg= 2.8854 rank= 0.3873 kl= 0.5032\n",
      "Regression R2 : 0.2212, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.65 초\n",
      "=============================================\n",
      "########## 실험 32/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2011\n",
      "초기 랜덤 선택 샘플 인덱스: [  92  116  333  394  511  702  714  848  879  920  932  933 1309 1350\n",
      " 1370 1433 1728 1741 2152 2305 2429 2518 2531 2536 2571 2579 2619 2768\n",
      " 3170 3231 3347 3487]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.885871887207031, std: 1.8167252640588378\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5976 rank= 0.0043 kl= 0.5512\n",
      "Val loss epoch 1000: reg= 2.3141 rank= 0.3827 kl= 0.5577\n",
      "Regression R2 : 0.4055, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.574697494506836, std: 1.5336356263024902\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6243 rank= 0.0090 kl= 0.5984\n",
      "Val loss epoch 1000: reg= 2.5516 rank= 0.3311 kl= 0.5395\n",
      "Regression R2 : 0.2716, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.888014316558838, std: 1.372023592458496\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4841 rank= 0.0106 kl= 0.6038\n",
      "Val loss epoch 1000: reg= 2.9161 rank= 0.2579 kl= 0.5409\n",
      "Regression R2 : 0.2318, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 96\n",
      "총 측정 시간: 10.27 초\n",
      "=============================================\n",
      "########## 실험 33/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2012\n",
      "초기 랜덤 선택 샘플 인덱스: [ 245  271  401  535  567  880  900  917 1060 1229 1324 1441 1606 1654\n",
      " 1675 1683 1803 2019 2234 2294 2503 2741 2790 2809 2905 2944 3051 3119\n",
      " 3127 3187 3200 3306]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.339137077331543, std: 2.080928097234497\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.2934 rank= 0.0021 kl= 0.5000\n",
      "Val loss epoch 1000: reg= 1.7244 rank= 0.2558 kl= 0.5155\n",
      "Regression R2 : 0.5286, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.287907600402832, std: 1.7992708783013915\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5490 rank= 0.0080 kl= 0.5221\n",
      "Val loss epoch 1000: reg= 2.1718 rank= 0.3103 kl= 0.5147\n",
      "Regression R2 : 0.4518, \n",
      "Recall@1 : 1\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.65 초\n",
      "=============================================\n",
      "########## 실험 34/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2013\n",
      "초기 랜덤 선택 샘플 인덱스: [ 151  728  730  762  893  970 1004 1010 1378 1477 1570 1612 1740 1759\n",
      " 1971 2035 2047 2087 2089 2122 2310 2336 2446 2694 3111 3389 3409 3487\n",
      " 3540 3550 3592 3717]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.386500835418701, std: 1.6640498738153076\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3219 rank= 0.0047 kl= 0.5442\n",
      "Val loss epoch 1000: reg= 1.4416 rank= 0.1759 kl= 0.5326\n",
      "Regression R2 : 0.5116, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.497185707092285, std: 1.6588252882821655\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5231 rank= 0.0120 kl= 0.5577\n",
      "Val loss epoch 1000: reg= 2.0977 rank= 0.2799 kl= 0.5286\n",
      "Regression R2 : 0.3905, \n",
      "Recall@1 : 1\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.61 초\n",
      "=============================================\n",
      "########## 실험 35/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2014\n",
      "초기 랜덤 선택 샘플 인덱스: [ 180  239  243  400  440  736  892  933  984 1408 1710 1860 1954 2259\n",
      " 2340 2466 2491 2647 2720 2746 2780 2790 2840 2847 2941 3054 3097 3194\n",
      " 3364 3402 3469 3683]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.088900089263916, std: 2.1950976948602294\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3252 rank= 0.0064 kl= 0.5189\n",
      "Val loss epoch 1000: reg= 2.9659 rank= 0.4161 kl= 0.5196\n",
      "Regression R2 : 0.2483, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 32\n",
      "총 측정 시간: 3.53 초\n",
      "=============================================\n",
      "########## 실험 36/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2015\n",
      "초기 랜덤 선택 샘플 인덱스: [  55  223  326  436  483  604  824  942 1062 1078 1132 1198 1330 1339\n",
      " 1488 1615 1672 1732 1867 2383 2494 2548 2592 2860 2934 2957 3215 3253\n",
      " 3374 3483 3531 3588]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 7.048605918884277, std: 1.5280425648553466\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4512 rank= 0.0058 kl= 0.5331\n",
      "Val loss epoch 1000: reg= 1.7082 rank= 0.2772 kl= 0.5329\n",
      "Regression R2 : 0.5888, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.733736991882324, std: 1.3417906861169433\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6450 rank= 0.0108 kl= 0.5239\n",
      "Val loss epoch 1000: reg= 2.3232 rank= 0.2977 kl= 0.5169\n",
      "Regression R2 : 0.3743, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.62 초\n",
      "=============================================\n",
      "########## 실험 37/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2016\n",
      "초기 랜덤 선택 샘플 인덱스: [ 186  307  358  379  584  783  868  948 1016 1120 1247 1258 1324 1496\n",
      " 1645 1721 1883 2097 2157 2258 2362 2392 2456 2594 2614 2870 2924 3343\n",
      " 3402 3407 3521 3582]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 7.144143581390381, std: 1.575487981305847\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6430 rank= 0.0070 kl= 0.5435\n",
      "Val loss epoch 1000: reg= 1.9915 rank= 0.3325 kl= 0.5408\n",
      "Regression R2 : 0.4684, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.555819034576416, std: 1.3754524092538452\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4691 rank= 0.0121 kl= 0.5563\n",
      "Val loss epoch 1000: reg= 2.8308 rank= 0.4389 kl= 0.5300\n",
      "Regression R2 : 0.3567, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.774999618530273, std: 1.3224914173944091\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5446 rank= 0.0143 kl= 0.5561\n",
      "Val loss epoch 1000: reg= 2.7428 rank= 0.2615 kl= 0.5186\n",
      "Regression R2 : 0.3717, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 96\n",
      "총 측정 시간: 10.24 초\n",
      "=============================================\n",
      "########## 실험 38/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2017\n",
      "초기 랜덤 선택 샘플 인덱스: [ 164  315  373  504  581  817 1219 1281 1347 1581 1684 1718 1873 1944\n",
      " 1981 2147 2163 2245 2335 2531 2611 2643 2809 3022 3123 3135 3194 3482\n",
      " 3488 3497 3665 3688]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 5.855408668518066, std: 2.019801626668701\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.2836 rank= 0.0034 kl= 0.5115\n",
      "Val loss epoch 1000: reg= 2.4745 rank= 0.3845 kl= 0.5230\n",
      "Regression R2 : 0.3624, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 32\n",
      "총 측정 시간: 3.25 초\n",
      "=============================================\n",
      "########## 실험 39/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2018\n",
      "초기 랜덤 선택 샘플 인덱스: [  13  230  342  421  515  516  572  664  679  890  946 1084 1133 1138\n",
      " 1248 1788 1805 1991 2114 2287 2289 2371 2510 2661 2930 3109 3268 3376\n",
      " 3465 3472 3548 3680]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.5333452224731445, std: 1.9987838368280029\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3644 rank= 0.0069 kl= 0.5108\n",
      "Val loss epoch 1000: reg= 1.9704 rank= 0.2999 kl= 0.5251\n",
      "Regression R2 : 0.4774, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.451453685760498, std: 1.7079174618585204\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.7116 rank= 0.0124 kl= 0.5271\n",
      "Val loss epoch 1000: reg= 2.3595 rank= 0.2994 kl= 0.5220\n",
      "Regression R2 : 0.3741, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.829143524169922, std: 1.5308441023690795\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6675 rank= 0.0134 kl= 0.5212\n",
      "Val loss epoch 1000: reg= 2.4084 rank= 0.2050 kl= 0.5100\n",
      "Regression R2 : 0.2930, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 4 (128개) ================\n",
      "y_train mean: 8.04530143737793, std: 1.3882145981652831\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6422 rank= 0.0165 kl= 0.5380\n",
      "Val loss epoch 1000: reg= 1.7058 rank= 0.2204 kl= 0.5109\n",
      "Regression R2 : 0.3204, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 5 (160개) ================\n",
      "y_train mean: 8.011247634887695, std: 1.3810247282846069\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5598 rank= 0.0195 kl= 0.5352\n",
      "Val loss epoch 1000: reg= 2.7436 rank= 0.2451 kl= 0.5079\n",
      "Regression R2 : 0.3415, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 6 (192개) ================\n",
      "y_train mean: 8.133049964904785, std: 1.3007304768426513\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5296 rank= 0.0221 kl= 0.5437\n",
      "Val loss epoch 1000: reg= 2.7641 rank= 0.2047 kl= 0.5219\n",
      "Regression R2 : 0.3069, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 7 (224개) ================\n",
      "y_train mean: 8.208773612976074, std: 1.2327432732446288\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4692 rank= 0.0246 kl= 0.5330\n",
      "Val loss epoch 1000: reg= 2.3702 rank= 0.1622 kl= 0.5213\n",
      "Regression R2 : 0.3710, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 8 (256개) ================\n",
      "y_train mean: 8.259794235229492, std: 1.1800543169839477\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4885 rank= 0.0295 kl= 0.5266\n",
      "Val loss epoch 1000: reg= 2.4978 rank= 0.1668 kl= 0.5245\n",
      "Regression R2 : 0.3581, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 9 (288개) ================\n",
      "y_train mean: 8.30233097076416, std: 1.127860436902771\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5316 rank= 0.0247 kl= 0.5178\n",
      "Val loss epoch 1000: reg= 2.5634 rank= 0.1591 kl= 0.5225\n",
      "Regression R2 : 0.3306, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 288\n",
      "총 측정 시간: 34.11 초\n",
      "=============================================\n",
      "########## 실험 40/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2019\n",
      "초기 랜덤 선택 샘플 인덱스: [ 417  535  676  735  742 1132 1197 1261 1292 1438 1440 1599 1638 1696\n",
      " 1785 1864 1969 2063 2105 2423 2554 2612 2715 2759 2851 3278 3398 3459\n",
      " 3480 3491 3521 3594]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.507327556610107, std: 1.7199821572167968\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3934 rank= 0.0076 kl= 0.5154\n",
      "Val loss epoch 1000: reg= 2.2093 rank= 0.3162 kl= 0.5391\n",
      "Regression R2 : 0.3467, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 32\n",
      "총 측정 시간: 3.25 초\n",
      "=============================================\n",
      "########## 실험 41/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2000\n",
      "초기 랜덤 선택 샘플 인덱스: [   8  100  230  255  392  634  659  750  871  908  979  999 1227 1371\n",
      " 1524 1680 1734 1742 1879 1991 2015 2023 2130 2242 2402 2432 2768 3276\n",
      " 3348 3463 3572 3686]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.420133590698242, std: 1.7561746935708618\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.2973 rank= 0.0057 kl= 0.5289\n",
      "Val loss epoch 1000: reg= 2.0539 rank= 0.3634 kl= 0.5445\n",
      "Regression R2 : 0.4167, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 32\n",
      "총 측정 시간: 3.26 초\n",
      "=============================================\n",
      "########## 실험 42/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2001\n",
      "초기 랜덤 선택 샘플 인덱스: [  75   82  122  374  416  424  443  607  703  755  818  824  900 1217\n",
      " 1253 1569 1603 1617 1682 1761 1779 1918 1991 2132 2137 2229 2549 2845\n",
      " 2911 3569 3683 3710]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.486093044281006, std: 2.5365142922265624\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4048 rank= 0.0038 kl= 0.5451\n",
      "Val loss epoch 1000: reg= 1.8642 rank= 0.2761 kl= 0.5315\n",
      "Regression R2 : 0.5199, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.3973212242126465, std: 2.045804510579834\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5388 rank= 0.0100 kl= 0.5491\n",
      "Val loss epoch 1000: reg= 2.9981 rank= 0.4947 kl= 0.5328\n",
      "Regression R2 : 0.3670, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.61 초\n",
      "=============================================\n",
      "########## 실험 43/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2002\n",
      "초기 랜덤 선택 샘플 인덱스: [ 136  395  402  409  532  561  564  638  885  923 1182 1208 1321 1525\n",
      " 1897 1919 1934 2197 2255 2440 2570 3093 3129 3291 3299 3334 3343 3425\n",
      " 3477 3495 3560 3626]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.467774868011475, std: 1.6519296269281005\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3414 rank= 0.0052 kl= 0.5183\n",
      "Val loss epoch 1000: reg= 1.4964 rank= 0.2487 kl= 0.5439\n",
      "Regression R2 : 0.5087, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.303062438964844, std: 1.5335856776101684\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5458 rank= 0.0070 kl= 0.5284\n",
      "Val loss epoch 1000: reg= 1.7178 rank= 0.2252 kl= 0.5198\n",
      "Regression R2 : 0.5159, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.662703037261963, std: 1.3963528971536254\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5165 rank= 0.0126 kl= 0.5163\n",
      "Val loss epoch 1000: reg= 2.2061 rank= 0.2466 kl= 0.5098\n",
      "Regression R2 : 0.4737, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 4 (128개) ================\n",
      "y_train mean: 7.89984130859375, std: 1.296703825460205\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5295 rank= 0.0120 kl= 0.5155\n",
      "Val loss epoch 1000: reg= 0.9899 rank= 0.1028 kl= 0.4942\n",
      "Regression R2 : 0.3508, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 5 (160개) ================\n",
      "y_train mean: 8.02028751373291, std: 1.2613656620843505\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4470 rank= 0.0180 kl= 0.5270\n",
      "Val loss epoch 1000: reg= 2.2565 rank= 0.2477 kl= 0.5142\n",
      "Regression R2 : 0.4106, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 6 (192개) ================\n",
      "y_train mean: 8.122857093811035, std: 1.1939601998193359\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4673 rank= 0.0197 kl= 0.5316\n",
      "Val loss epoch 1000: reg= 2.4843 rank= 0.2870 kl= 0.5205\n",
      "Regression R2 : 0.3933, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 7 (224개) ================\n",
      "y_train mean: 8.146882057189941, std: 1.1776445011956787\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4218 rank= 0.0232 kl= 0.5272\n",
      "Val loss epoch 1000: reg= 2.0541 rank= 0.2242 kl= 0.5195\n",
      "Regression R2 : 0.4990, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 224\n",
      "총 측정 시간: 25.75 초\n",
      "=============================================\n",
      "########## 실험 44/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2003\n",
      "초기 랜덤 선택 샘플 인덱스: [  70  278  474  527  720  757  885  969  988 1006 1091 1309 1347 1449\n",
      " 1633 1868 2081 2112 2151 2415 2477 2489 2645 2950 3014 3217 3258 3265\n",
      " 3428 3509 3645 3704]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.318173408508301, std: 1.77249277638031\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4092 rank= 0.0076 kl= 0.5166\n",
      "Val loss epoch 1000: reg= 1.8093 rank= 0.2961 kl= 0.5431\n",
      "Regression R2 : 0.4716, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.155410289764404, std: 1.6118782858712768\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3753 rank= 0.0112 kl= 0.5468\n",
      "Val loss epoch 1000: reg= 1.5858 rank= 0.2451 kl= 0.5278\n",
      "Regression R2 : 0.5930, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.66 초\n",
      "=============================================\n",
      "########## 실험 45/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2004\n",
      "초기 랜덤 선택 샘플 인덱스: [  10  112  195  254  342  507  623  690 1078 1157 1302 1406 1700 1831\n",
      " 2036 2038 2156 2267 2300 2306 2371 2458 2570 2626 2630 2648 2662 2818\n",
      " 3037 3109 3397 3569]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.423959732055664, std: 2.109467039571533\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4588 rank= 0.0042 kl= 0.5217\n",
      "Val loss epoch 1000: reg= 2.6299 rank= 0.4333 kl= 0.5266\n",
      "Regression R2 : 0.4930, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.3647918701171875, std: 1.8129520516259765\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4681 rank= 0.0078 kl= 0.5505\n",
      "Val loss epoch 1000: reg= 3.2063 rank= 0.5216 kl= 0.5292\n",
      "Regression R2 : 0.2992, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.637262344360352, std: 1.6463780503137206\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4501 rank= 0.0102 kl= 0.5542\n",
      "Val loss epoch 1000: reg= 2.8810 rank= 0.3188 kl= 0.5222\n",
      "Regression R2 : 0.4250, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 96\n",
      "총 측정 시간: 10.16 초\n",
      "=============================================\n",
      "########## 실험 46/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2005\n",
      "초기 랜덤 선택 샘플 인덱스: [  12   33   62  109  111  489  793  846  971  990 1151 1320 1736 1793\n",
      " 1882 1884 2062 2197 2222 2420 2478 2503 2539 2658 2692 2761 2985 3170\n",
      " 3328 3379 3496 3598]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.756325721740723, std: 1.7418288092477416\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4466 rank= 0.0029 kl= 0.5503\n",
      "Val loss epoch 1000: reg= 2.2086 rank= 0.3695 kl= 0.5517\n",
      "Regression R2 : 0.4361, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 32\n",
      "총 측정 시간: 3.25 초\n",
      "=============================================\n",
      "########## 실험 47/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2006\n",
      "초기 랜덤 선택 샘플 인덱스: [  47   78  204  215  279  353  565  816 1095 1122 1149 1296 1328 1432\n",
      " 1522 1762 2071 2176 2357 2466 2510 2531 2924 3036 3073 3229 3333 3368\n",
      " 3521 3564 3721 3731]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.419711112976074, std: 1.8596973519189453\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.2842 rank= 0.0038 kl= 0.5188\n",
      "Val loss epoch 1000: reg= 1.8194 rank= 0.2768 kl= 0.5299\n",
      "Regression R2 : 0.5800, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.208494663238525, std: 1.620698104367981\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4234 rank= 0.0063 kl= 0.5009\n",
      "Val loss epoch 1000: reg= 2.6625 rank= 0.3277 kl= 0.4960\n",
      "Regression R2 : 0.3960, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.60 초\n",
      "=============================================\n",
      "########## 실험 48/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2007\n",
      "초기 랜덤 선택 샘플 인덱스: [  45  375  515  708  803  809  834 1015 1047 1083 1092 1150 1333 1455\n",
      " 1477 1689 1759 1787 1930 2051 2102 2259 2450 2496 2504 2669 2696 2814\n",
      " 3058 3283 3399 3438]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.840710639953613, std: 2.0724175076348876\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4926 rank= 0.0043 kl= 0.5183\n",
      "Val loss epoch 1000: reg= 2.7733 rank= 0.5097 kl= 0.5254\n",
      "Regression R2 : 0.3633, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.586145401000977, std: 1.7620402674539184\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6809 rank= 0.0106 kl= 0.5521\n",
      "Val loss epoch 1000: reg= 2.0356 rank= 0.3393 kl= 0.5223\n",
      "Regression R2 : 0.4272, \n",
      "Recall@1 : 1\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.62 초\n",
      "=============================================\n",
      "########## 실험 49/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2008\n",
      "초기 랜덤 선택 샘플 인덱스: [  53  112  160  174  213  231  945 1134 1193 1233 1267 1269 1487 2193\n",
      " 2294 2493 2542 2640 2713 2742 2803 2830 2857 2902 3140 3246 3288 3315\n",
      " 3360 3442 3471 3485]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.172525882720947, std: 1.5799144606454467\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3552 rank= 0.0047 kl= 0.5165\n",
      "Val loss epoch 1000: reg= 1.9580 rank= 0.2792 kl= 0.5304\n",
      "Regression R2 : 0.2951, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.264760971069336, std: 1.6293574671609496\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3465 rank= 0.0110 kl= 0.5500\n",
      "Val loss epoch 1000: reg= 2.4114 rank= 0.2948 kl= 0.5306\n",
      "Regression R2 : 0.1409, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.518272876739502, std: 1.5385927061898803\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3779 rank= 0.0113 kl= 0.5489\n",
      "Val loss epoch 1000: reg= 2.1751 rank= 0.2201 kl= 0.5195\n",
      "Regression R2 : 0.2657, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 4 (128개) ================\n",
      "y_train mean: 7.775663375854492, std: 1.4371730189187621\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4059 rank= 0.0136 kl= 0.5473\n",
      "Val loss epoch 1000: reg= 2.0760 rank= 0.2117 kl= 0.5035\n",
      "Regression R2 : 0.2971, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 128\n",
      "총 측정 시간: 14.10 초\n",
      "=============================================\n",
      "########## 실험 50/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2009\n",
      "초기 랜덤 선택 샘플 인덱스: [   0  111  335  399  520  678  750  785  881  909 1097 1244 1434 1440\n",
      " 1580 1585 1713 1878 1963 2228 2249 2520 2691 2746 2856 2892 2972 3097\n",
      " 3216 3314 3349 3655]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.897634029388428, std: 1.8627023796899413\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5505 rank= 0.0051 kl= 0.5401\n",
      "Val loss epoch 1000: reg= 1.4234 rank= 0.2195 kl= 0.5328\n",
      "Regression R2 : 0.5258, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.672824859619141, std: 1.5677790741784667\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.7098 rank= 0.0089 kl= 0.5600\n",
      "Val loss epoch 1000: reg= 2.3644 rank= 0.3537 kl= 0.5286\n",
      "Regression R2 : 0.3401, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.65 초\n",
      "=============================================\n",
      "########## 실험 51/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2010\n",
      "초기 랜덤 선택 샘플 인덱스: [ 261  359  368  549  552  716  734  739  890  893  901 1012 1190 1368\n",
      " 1386 1395 1733 1820 1850 2040 2165 2428 2596 2650 2792 2805 2845 3137\n",
      " 3166 3182 3224 3417]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 5.9191484451293945, std: 1.6618188719613647\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3098 rank= 0.0041 kl= 0.5169\n",
      "Val loss epoch 1000: reg= 1.8446 rank= 0.2636 kl= 0.5225\n",
      "Regression R2 : 0.5095, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.0281291007995605, std: 1.7144451241357421\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4537 rank= 0.0064 kl= 0.5282\n",
      "Val loss epoch 1000: reg= 2.9368 rank= 0.3883 kl= 0.5009\n",
      "Regression R2 : 0.2263, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.65 초\n",
      "=============================================\n",
      "########## 실험 52/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2011\n",
      "초기 랜덤 선택 샘플 인덱스: [  92  116  333  394  511  702  714  848  879  920  932  933 1309 1350\n",
      " 1370 1433 1728 1741 2152 2305 2429 2518 2531 2536 2571 2579 2619 2768\n",
      " 3170 3231 3347 3487]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.885871887207031, std: 1.8167252640588378\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5937 rank= 0.0043 kl= 0.5463\n",
      "Val loss epoch 1000: reg= 2.2793 rank= 0.3774 kl= 0.5516\n",
      "Regression R2 : 0.4140, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.597962856292725, std: 1.5370376210076904\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5204 rank= 0.0116 kl= 0.5938\n",
      "Val loss epoch 1000: reg= 2.4448 rank= 0.3225 kl= 0.5376\n",
      "Regression R2 : 0.2740, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.881275177001953, std: 1.3757739167077636\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5284 rank= 0.0138 kl= 0.5943\n",
      "Val loss epoch 1000: reg= 3.3519 rank= 0.3182 kl= 0.5335\n",
      "Regression R2 : 0.1787, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 96\n",
      "총 측정 시간: 10.32 초\n",
      "=============================================\n",
      "########## 실험 53/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2012\n",
      "초기 랜덤 선택 샘플 인덱스: [ 245  271  401  535  567  880  900  917 1060 1229 1324 1441 1606 1654\n",
      " 1675 1683 1803 2019 2234 2294 2503 2741 2790 2809 2905 2944 3051 3119\n",
      " 3127 3187 3200 3306]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.339137077331543, std: 2.080928097234497\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.2753 rank= 0.0027 kl= 0.4907\n",
      "Val loss epoch 1000: reg= 1.6946 rank= 0.2533 kl= 0.5062\n",
      "Regression R2 : 0.5398, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.296989917755127, std: 1.7900765042169189\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5203 rank= 0.0090 kl= 0.5114\n",
      "Val loss epoch 1000: reg= 2.1113 rank= 0.3205 kl= 0.5064\n",
      "Regression R2 : 0.4514, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.60 초\n",
      "=============================================\n",
      "########## 실험 54/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2013\n",
      "초기 랜덤 선택 샘플 인덱스: [ 151  728  730  762  893  970 1004 1010 1378 1477 1570 1612 1740 1759\n",
      " 1971 2035 2047 2087 2089 2122 2310 2336 2446 2694 3111 3389 3409 3487\n",
      " 3540 3550 3592 3717]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.386500835418701, std: 1.6640498738153076\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3154 rank= 0.0044 kl= 0.5377\n",
      "Val loss epoch 1000: reg= 1.4584 rank= 0.1762 kl= 0.5263\n",
      "Regression R2 : 0.5113, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.446397304534912, std: 1.653389821515808\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4319 rank= 0.0120 kl= 0.5558\n",
      "Val loss epoch 1000: reg= 2.3194 rank= 0.3347 kl= 0.5198\n",
      "Regression R2 : 0.3738, \n",
      "Recall@1 : 1\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.60 초\n",
      "=============================================\n",
      "########## 실험 55/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2014\n",
      "초기 랜덤 선택 샘플 인덱스: [ 180  239  243  400  440  736  892  933  984 1408 1710 1860 1954 2259\n",
      " 2340 2466 2491 2647 2720 2746 2780 2790 2840 2847 2941 3054 3097 3194\n",
      " 3364 3402 3469 3683]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.088900089263916, std: 2.1950976948602294\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3189 rank= 0.0068 kl= 0.5075\n",
      "Val loss epoch 1000: reg= 2.9737 rank= 0.4143 kl= 0.5086\n",
      "Regression R2 : 0.2444, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 32\n",
      "총 측정 시간: 3.25 초\n",
      "=============================================\n",
      "########## 실험 56/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2015\n",
      "초기 랜덤 선택 샘플 인덱스: [  55  223  326  436  483  604  824  942 1062 1078 1132 1198 1330 1339\n",
      " 1488 1615 1672 1732 1867 2383 2494 2548 2592 2860 2934 2957 3215 3253\n",
      " 3374 3483 3531 3588]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 7.048605918884277, std: 1.5280425648553466\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4619 rank= 0.0049 kl= 0.5282\n",
      "Val loss epoch 1000: reg= 1.7269 rank= 0.2798 kl= 0.5281\n",
      "Regression R2 : 0.5857, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.680107116699219, std: 1.319204817281494\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5555 rank= 0.0087 kl= 0.5124\n",
      "Val loss epoch 1000: reg= 2.4841 rank= 0.3230 kl= 0.5074\n",
      "Regression R2 : 0.4009, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.68 초\n",
      "=============================================\n",
      "########## 실험 57/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2016\n",
      "초기 랜덤 선택 샘플 인덱스: [ 186  307  358  379  584  783  868  948 1016 1120 1247 1258 1324 1496\n",
      " 1645 1721 1883 2097 2157 2258 2362 2392 2456 2594 2614 2870 2924 3343\n",
      " 3402 3407 3521 3582]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 7.144143581390381, std: 1.575487981305847\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6390 rank= 0.0076 kl= 0.5402\n",
      "Val loss epoch 1000: reg= 2.0245 rank= 0.3406 kl= 0.5387\n",
      "Regression R2 : 0.4639, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.48184871673584, std: 1.3486032585961913\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5382 rank= 0.0105 kl= 0.5548\n",
      "Val loss epoch 1000: reg= 3.0925 rank= 0.4817 kl= 0.5281\n",
      "Regression R2 : 0.3052, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.727710723876953, std: 1.3186310629708862\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4230 rank= 0.0175 kl= 0.5696\n",
      "Val loss epoch 1000: reg= 2.7496 rank= 0.2976 kl= 0.5249\n",
      "Regression R2 : 0.4009, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 96\n",
      "총 측정 시간: 10.24 초\n",
      "=============================================\n",
      "########## 실험 58/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2017\n",
      "초기 랜덤 선택 샘플 인덱스: [ 164  315  373  504  581  817 1219 1281 1347 1581 1684 1718 1873 1944\n",
      " 1981 2147 2163 2245 2335 2531 2611 2643 2809 3022 3123 3135 3194 3482\n",
      " 3488 3497 3665 3688]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 5.855408668518066, std: 2.019801626668701\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.2859 rank= 0.0036 kl= 0.5046\n",
      "Val loss epoch 1000: reg= 2.5386 rank= 0.3975 kl= 0.5166\n",
      "Regression R2 : 0.3536, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 32\n",
      "총 측정 시간: 3.27 초\n",
      "=============================================\n",
      "########## 실험 59/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2018\n",
      "초기 랜덤 선택 샘플 인덱스: [  13  230  342  421  515  516  572  664  679  890  946 1084 1133 1138\n",
      " 1248 1788 1805 1991 2114 2287 2289 2371 2510 2661 2930 3109 3268 3376\n",
      " 3465 3472 3548 3680]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.5333452224731445, std: 1.9987838368280029\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3645 rank= 0.0076 kl= 0.5016\n",
      "Val loss epoch 1000: reg= 1.9947 rank= 0.3036 kl= 0.5166\n",
      "Regression R2 : 0.4726, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.4397735595703125, std: 1.7030882935388183\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6507 rank= 0.0139 kl= 0.5104\n",
      "Val loss epoch 1000: reg= 1.8596 rank= 0.2488 kl= 0.5068\n",
      "Regression R2 : 0.5041, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.769306182861328, std: 1.559969673619995\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5768 rank= 0.0179 kl= 0.5254\n",
      "Val loss epoch 1000: reg= 2.2704 rank= 0.2311 kl= 0.5166\n",
      "Regression R2 : 0.4081, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 4 (128개) ================\n",
      "y_train mean: 7.918890476226807, std: 1.423183808789978\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5483 rank= 0.0190 kl= 0.5133\n",
      "Val loss epoch 1000: reg= 1.6068 rank= 0.1506 kl= 0.4976\n",
      "Regression R2 : 0.2598, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 5 (160개) ================\n",
      "y_train mean: 8.086767196655273, std: 1.3248008589608764\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5677 rank= 0.0189 kl= 0.5034\n",
      "Val loss epoch 1000: reg= 3.1458 rank= 0.2687 kl= 0.5014\n",
      "Regression R2 : 0.2515, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 6 (192개) ================\n",
      "y_train mean: 8.195710182189941, std: 1.2430754999978637\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5106 rank= 0.0200 kl= 0.5028\n",
      "Val loss epoch 1000: reg= 2.8441 rank= 0.2474 kl= 0.5046\n",
      "Regression R2 : 0.3063, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 7 (224개) ================\n",
      "y_train mean: 8.28318977355957, std: 1.1740864615304565\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5303 rank= 0.0259 kl= 0.4967\n",
      "Val loss epoch 1000: reg= 3.4342 rank= 0.2724 kl= 0.5047\n",
      "Regression R2 : 0.1773, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 8 (256개) ================\n",
      "y_train mean: 8.336650848388672, std: 1.1269333462579345\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4457 rank= 0.0278 kl= 0.4990\n",
      "Val loss epoch 1000: reg= 3.3145 rank= 0.2621 kl= 0.5153\n",
      "Regression R2 : 0.1823, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 9 (288개) ================\n",
      "y_train mean: 8.35612678527832, std: 1.0898019175393676\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4793 rank= 0.0286 kl= 0.4937\n",
      "Val loss epoch 1000: reg= 3.1121 rank= 0.2567 kl= 0.5120\n",
      "Regression R2 : 0.1932, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 10 (320개) ================\n",
      "y_train mean: 8.39560604095459, std: 1.046547780500183\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4573 rank= 0.0299 kl= 0.4916\n",
      "Val loss epoch 1000: reg= 2.8749 rank= 0.2349 kl= 0.5105\n",
      "Regression R2 : 0.2537, \n",
      "Recall@1 : 1\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 320\n",
      "총 측정 시간: 38.57 초\n",
      "=============================================\n",
      "########## 실험 60/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2019\n",
      "초기 랜덤 선택 샘플 인덱스: [ 417  535  676  735  742 1132 1197 1261 1292 1438 1440 1599 1638 1696\n",
      " 1785 1864 1969 2063 2105 2423 2554 2612 2715 2759 2851 3278 3398 3459\n",
      " 3480 3491 3521 3594]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.507327556610107, std: 1.7199821572167968\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3858 rank= 0.0077 kl= 0.5094\n",
      "Val loss epoch 1000: reg= 2.2350 rank= 0.3194 kl= 0.5332\n",
      "Regression R2 : 0.3359, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.431279182434082, std: 1.5843781332833862\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5139 rank= 0.0107 kl= 0.5416\n",
      "Val loss epoch 1000: reg= 2.2177 rank= 0.3146 kl= 0.5241\n",
      "Regression R2 : 0.4150, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.80 초\n",
      "=============================================\n",
      "########## 실험 61/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2000\n",
      "초기 랜덤 선택 샘플 인덱스: [   8  100  230  255  392  634  659  750  871  908  979  999 1227 1371\n",
      " 1524 1680 1734 1742 1879 1991 2015 2023 2130 2242 2402 2432 2768 3276\n",
      " 3348 3463 3572 3686]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.420133590698242, std: 1.7561746935708618\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.2979 rank= 0.0056 kl= 0.5201\n",
      "Val loss epoch 1000: reg= 2.0663 rank= 0.3646 kl= 0.5356\n",
      "Regression R2 : 0.4128, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 32\n",
      "총 측정 시간: 3.32 초\n",
      "=============================================\n",
      "########## 실험 62/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2001\n",
      "초기 랜덤 선택 샘플 인덱스: [  75   82  122  374  416  424  443  607  703  755  818  824  900 1217\n",
      " 1253 1569 1603 1617 1682 1761 1779 1918 1991 2132 2137 2229 2549 2845\n",
      " 2911 3569 3683 3710]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.486093044281006, std: 2.5365142922265624\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4122 rank= 0.0034 kl= 0.5381\n",
      "Val loss epoch 1000: reg= 1.8538 rank= 0.2737 kl= 0.5235\n",
      "Regression R2 : 0.5192, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.412630081176758, std: 2.046622047887573\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5286 rank= 0.0088 kl= 0.5453\n",
      "Val loss epoch 1000: reg= 2.5271 rank= 0.3762 kl= 0.5308\n",
      "Regression R2 : 0.4041, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.734208583831787, std: 1.8002958397729492\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5083 rank= 0.0142 kl= 0.5497\n",
      "Val loss epoch 1000: reg= 2.5284 rank= 0.2417 kl= 0.5250\n",
      "Regression R2 : 0.4062, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 4 (128개) ================\n",
      "y_train mean: 7.922677040100098, std: 1.6148802142007446\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4183 rank= 0.0174 kl= 0.5585\n",
      "Val loss epoch 1000: reg= 2.2303 rank= 0.1485 kl= 0.5155\n",
      "Regression R2 : 0.3680, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 5 (160개) ================\n",
      "y_train mean: 8.098173141479492, std: 1.4923497538430786\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5232 rank= 0.0200 kl= 0.5489\n",
      "Val loss epoch 1000: reg= 2.8183 rank= 0.2223 kl= 0.5181\n",
      "Regression R2 : 0.2201, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 6 (192개) ================\n",
      "y_train mean: 8.12779712677002, std: 1.3953855137689208\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4394 rank= 0.0238 kl= 0.5380\n",
      "Val loss epoch 1000: reg= 2.5786 rank= 0.1786 kl= 0.5134\n",
      "Regression R2 : 0.2891, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 7 (224개) ================\n",
      "y_train mean: 8.19562816619873, std: 1.3123691182000732\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4256 rank= 0.0253 kl= 0.5413\n",
      "Val loss epoch 1000: reg= 2.6402 rank= 0.1745 kl= 0.5220\n",
      "Regression R2 : 0.2945, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 8 (256개) ================\n",
      "y_train mean: 8.251399040222168, std: 1.2526066403253173\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3734 rank= 0.0262 kl= 0.5347\n",
      "Val loss epoch 1000: reg= 2.4032 rank= 0.1987 kl= 0.5195\n",
      "Regression R2 : 0.3901, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 9 (288개) ================\n",
      "y_train mean: 8.307974815368652, std: 1.196452508435974\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4047 rank= 0.0260 kl= 0.5207\n",
      "Val loss epoch 1000: reg= 2.9412 rank= 0.2055 kl= 0.5136\n",
      "Regression R2 : 0.2661, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 10 (320개) ================\n",
      "y_train mean: 8.352258682250977, std: 1.1496270995004272\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4275 rank= 0.0286 kl= 0.5059\n",
      "Val loss epoch 1000: reg= 2.6837 rank= 0.1842 kl= 0.5038\n",
      "Regression R2 : 0.2710, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 11 (352개) ================\n",
      "y_train mean: 8.375870704650879, std: 1.113472590909729\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3926 rank= 0.0317 kl= 0.5054\n",
      "Val loss epoch 1000: reg= 2.5323 rank= 0.2047 kl= 0.5033\n",
      "Regression R2 : 0.2576, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 12 (384개) ================\n",
      "y_train mean: 8.407584190368652, std: 1.0791870455606078\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4339 rank= 0.0289 kl= 0.5047\n",
      "Val loss epoch 1000: reg= 2.8087 rank= 0.2102 kl= 0.5061\n",
      "Regression R2 : 0.1394, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 13 (416개) ================\n",
      "y_train mean: 8.428174018859863, std: 1.0469954113824462\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4730 rank= 0.0326 kl= 0.5003\n",
      "Val loss epoch 1000: reg= 3.0738 rank= 0.2209 kl= 0.5024\n",
      "Regression R2 : 0.0888, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 14 (448개) ================\n",
      "y_train mean: 8.443143844604492, std: 1.0165021519525146\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4478 rank= 0.0305 kl= 0.5026\n",
      "Val loss epoch 1000: reg= 2.9161 rank= 0.2229 kl= 0.5090\n",
      "Regression R2 : 0.1784, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 15 (480개) ================\n",
      "y_train mean: 8.447430610656738, std: 0.9930933217866517\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4690 rank= 0.0337 kl= 0.4982\n",
      "Val loss epoch 1000: reg= 3.7283 rank= 0.2283 kl= 0.5072\n",
      "Regression R2 : -0.0166, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 16 (512개) ================\n",
      "y_train mean: 8.45969009399414, std: 0.9666503171784974\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5382 rank= 0.0361 kl= 0.4993\n",
      "Val loss epoch 1000: reg= 3.8447 rank= 0.2512 kl= 0.5088\n",
      "Regression R2 : -0.1025, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 17 (544개) ================\n",
      "y_train mean: 8.461780548095703, std: 0.9465311865670777\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5623 rank= 0.0139 kl= 0.3895\n",
      "Val loss epoch 1000: reg= 2.9089 rank= 0.2752 kl= 0.3893\n",
      "Regression R2 : 0.1453, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 18 (576개) ================\n",
      "y_train mean: 8.46497631072998, std: 0.9258154730661011\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5067 rank= 0.0207 kl= 0.3791\n",
      "Val loss epoch 1000: reg= 3.4050 rank= 0.2367 kl= 0.3830\n",
      "Regression R2 : -0.0264, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 19 (608개) ================\n",
      "y_train mean: 8.465130805969238, std: 0.9102696280343628\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4769 rank= 0.0176 kl= 0.3644\n",
      "Val loss epoch 1000: reg= 3.3666 rank= 0.1616 kl= 0.3710\n",
      "Regression R2 : -0.0502, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 20 (640개) ================\n",
      "y_train mean: 8.459236145019531, std: 0.8986137013299561\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4836 rank= 0.0216 kl= 0.3602\n",
      "Val loss epoch 1000: reg= 3.1863 rank= 0.1554 kl= 0.3592\n",
      "Regression R2 : -0.0213, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 21 (672개) ================\n",
      "y_train mean: 8.460453987121582, std: 0.8813027839524842\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4313 rank= 0.0219 kl= 0.3559\n",
      "Val loss epoch 1000: reg= 3.4377 rank= 0.1871 kl= 0.3597\n",
      "Regression R2 : 0.0348, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 22 (704개) ================\n",
      "y_train mean: 8.456915855407715, std: 0.8667873840196229\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4449 rank= 0.0226 kl= 0.3526\n",
      "Val loss epoch 1000: reg= 2.8529 rank= 0.1650 kl= 0.3592\n",
      "Regression R2 : 0.1546, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 704\n",
      "총 측정 시간: 120.43 초\n",
      "=============================================\n",
      "########## 실험 63/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2002\n",
      "초기 랜덤 선택 샘플 인덱스: [ 136  395  402  409  532  561  564  638  885  923 1182 1208 1321 1525\n",
      " 1897 1919 1934 2197 2255 2440 2570 3093 3129 3291 3299 3334 3343 3425\n",
      " 3477 3495 3560 3626]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.467774868011475, std: 1.6519296269281005\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3448 rank= 0.0051 kl= 0.5127\n",
      "Val loss epoch 1000: reg= 1.4964 rank= 0.2484 kl= 0.5382\n",
      "Regression R2 : 0.5078, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.329586982727051, std: 1.532952318654785\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4167 rank= 0.0101 kl= 0.5278\n",
      "Val loss epoch 1000: reg= 1.6874 rank= 0.2393 kl= 0.5177\n",
      "Regression R2 : 0.5056, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.6657023429870605, std: 1.4127814869744872\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4741 rank= 0.0146 kl= 0.5139\n",
      "Val loss epoch 1000: reg= 2.3334 rank= 0.2843 kl= 0.5159\n",
      "Regression R2 : 0.4804, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 4 (128개) ================\n",
      "y_train mean: 7.911494255065918, std: 1.331490169034729\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4732 rank= 0.0146 kl= 0.5255\n",
      "Val loss epoch 1000: reg= 0.9941 rank= 0.1167 kl= 0.4987\n",
      "Regression R2 : 0.4669, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 5 (160개) ================\n",
      "y_train mean: 7.918321132659912, std: 1.3339160780770873\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4495 rank= 0.0188 kl= 0.5241\n",
      "Val loss epoch 1000: reg= 2.0619 rank= 0.2423 kl= 0.5005\n",
      "Regression R2 : 0.4392, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 6 (192개) ================\n",
      "y_train mean: 8.028800010681152, std: 1.2558837036950683\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4734 rank= 0.0165 kl= 0.5195\n",
      "Val loss epoch 1000: reg= 2.2573 rank= 0.2476 kl= 0.4981\n",
      "Regression R2 : 0.4328, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 7 (224개) ================\n",
      "y_train mean: 8.107932090759277, std: 1.1952382426126098\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3889 rank= 0.0232 kl= 0.5235\n",
      "Val loss epoch 1000: reg= 2.1781 rank= 0.2226 kl= 0.5016\n",
      "Regression R2 : 0.4748, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 8 (256개) ================\n",
      "y_train mean: 8.191454887390137, std: 1.1447278361184692\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3707 rank= 0.0235 kl= 0.5204\n",
      "Val loss epoch 1000: reg= 2.2955 rank= 0.2230 kl= 0.5005\n",
      "Regression R2 : 0.4461, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 256\n",
      "총 측정 시간: 29.57 초\n",
      "=============================================\n",
      "########## 실험 64/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2003\n",
      "초기 랜덤 선택 샘플 인덱스: [  70  278  474  527  720  757  885  969  988 1006 1091 1309 1347 1449\n",
      " 1633 1868 2081 2112 2151 2415 2477 2489 2645 2950 3014 3217 3258 3265\n",
      " 3428 3509 3645 3704]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.318173408508301, std: 1.77249277638031\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4018 rank= 0.0082 kl= 0.5120\n",
      "Val loss epoch 1000: reg= 1.8334 rank= 0.2974 kl= 0.5386\n",
      "Regression R2 : 0.4644, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.187624931335449, std: 1.6058313946588134\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3312 rank= 0.0098 kl= 0.5476\n",
      "Val loss epoch 1000: reg= 1.5733 rank= 0.2527 kl= 0.5289\n",
      "Regression R2 : 0.6000, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.83 초\n",
      "=============================================\n",
      "########## 실험 65/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2004\n",
      "초기 랜덤 선택 샘플 인덱스: [  10  112  195  254  342  507  623  690 1078 1157 1302 1406 1700 1831\n",
      " 2036 2038 2156 2267 2300 2306 2371 2458 2570 2626 2630 2648 2662 2818\n",
      " 3037 3109 3397 3569]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.423959732055664, std: 2.109467039571533\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4598 rank= 0.0042 kl= 0.5112\n",
      "Val loss epoch 1000: reg= 2.6525 rank= 0.4334 kl= 0.5165\n",
      "Regression R2 : 0.4859, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.375130653381348, std: 1.8132656912667846\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5844 rank= 0.0076 kl= 0.5354\n",
      "Val loss epoch 1000: reg= 3.1651 rank= 0.5064 kl= 0.5096\n",
      "Regression R2 : 0.2485, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.623507022857666, std: 1.6399705510003662\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6051 rank= 0.0105 kl= 0.5485\n",
      "Val loss epoch 1000: reg= 4.1137 rank= 0.4064 kl= 0.5038\n",
      "Regression R2 : 0.2512, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 96\n",
      "총 측정 시간: 10.35 초\n",
      "=============================================\n",
      "########## 실험 66/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2005\n",
      "초기 랜덤 선택 샘플 인덱스: [  12   33   62  109  111  489  793  846  971  990 1151 1320 1736 1793\n",
      " 1882 1884 2062 2197 2222 2420 2478 2503 2539 2658 2692 2761 2985 3170\n",
      " 3328 3379 3496 3598]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.756325721740723, std: 1.7418288092477416\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4447 rank= 0.0030 kl= 0.5469\n",
      "Val loss epoch 1000: reg= 2.1659 rank= 0.3595 kl= 0.5462\n",
      "Regression R2 : 0.4472, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 32\n",
      "총 측정 시간: 3.32 초\n",
      "=============================================\n",
      "########## 실험 67/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2006\n",
      "초기 랜덤 선택 샘플 인덱스: [  47   78  204  215  279  353  565  816 1095 1122 1149 1296 1328 1432\n",
      " 1522 1762 2071 2176 2357 2466 2510 2531 2924 3036 3073 3229 3333 3368\n",
      " 3521 3564 3721 3731]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.419711112976074, std: 1.8596973519189453\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.2836 rank= 0.0041 kl= 0.5096\n",
      "Val loss epoch 1000: reg= 1.8263 rank= 0.2775 kl= 0.5215\n",
      "Regression R2 : 0.5779, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.255146503448486, std: 1.627840767369995\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4893 rank= 0.0063 kl= 0.5025\n",
      "Val loss epoch 1000: reg= 2.4445 rank= 0.3052 kl= 0.4937\n",
      "Regression R2 : 0.4547, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.642353057861328, std: 1.5392602782113647\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4981 rank= 0.0114 kl= 0.5108\n",
      "Val loss epoch 1000: reg= 3.6859 rank= 0.4280 kl= 0.4870\n",
      "Regression R2 : 0.3805, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 96\n",
      "총 측정 시간: 10.42 초\n",
      "=============================================\n",
      "########## 실험 68/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2007\n",
      "초기 랜덤 선택 샘플 인덱스: [  45  375  515  708  803  809  834 1015 1047 1083 1092 1150 1333 1455\n",
      " 1477 1689 1759 1787 1930 2051 2102 2259 2450 2496 2504 2669 2696 2814\n",
      " 3058 3283 3399 3438]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.840710639953613, std: 2.0724175076348876\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4870 rank= 0.0042 kl= 0.5099\n",
      "Val loss epoch 1000: reg= 2.7570 rank= 0.5064 kl= 0.5168\n",
      "Regression R2 : 0.3616, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.597539901733398, std: 1.7387237648828124\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6876 rank= 0.0087 kl= 0.5421\n",
      "Val loss epoch 1000: reg= 2.1618 rank= 0.3484 kl= 0.5080\n",
      "Regression R2 : 0.3748, \n",
      "Recall@1 : 1\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.86 초\n",
      "=============================================\n",
      "########## 실험 69/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2008\n",
      "초기 랜덤 선택 샘플 인덱스: [  53  112  160  174  213  231  945 1134 1193 1233 1267 1269 1487 2193\n",
      " 2294 2493 2542 2640 2713 2742 2803 2830 2857 2902 3140 3246 3288 3315\n",
      " 3360 3442 3471 3485]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.172525882720947, std: 1.5799144606454467\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3538 rank= 0.0049 kl= 0.5073\n",
      "Val loss epoch 1000: reg= 1.9763 rank= 0.2819 kl= 0.5209\n",
      "Regression R2 : 0.2858, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.294333457946777, std: 1.624828587041626\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4569 rank= 0.0096 kl= 0.5421\n",
      "Val loss epoch 1000: reg= 2.3992 rank= 0.2706 kl= 0.5201\n",
      "Regression R2 : 0.1707, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.564176082611084, std: 1.5178018908364868\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4293 rank= 0.0128 kl= 0.5330\n",
      "Val loss epoch 1000: reg= 2.3637 rank= 0.2249 kl= 0.5099\n",
      "Regression R2 : 0.2247, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 96\n",
      "총 측정 시간: 10.25 초\n",
      "=============================================\n",
      "########## 실험 70/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2009\n",
      "초기 랜덤 선택 샘플 인덱스: [   0  111  335  399  520  678  750  785  881  909 1097 1244 1434 1440\n",
      " 1580 1585 1713 1878 1963 2228 2249 2520 2691 2746 2856 2892 2972 3097\n",
      " 3216 3314 3349 3655]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.897634029388428, std: 1.8627023796899413\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5462 rank= 0.0046 kl= 0.5320\n",
      "Val loss epoch 1000: reg= 1.4351 rank= 0.2195 kl= 0.5246\n",
      "Regression R2 : 0.5233, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.657526016235352, std: 1.563490877614746\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.7443 rank= 0.0098 kl= 0.5496\n",
      "Val loss epoch 1000: reg= 2.1668 rank= 0.3198 kl= 0.5163\n",
      "Regression R2 : 0.3803, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.73 초\n",
      "=============================================\n",
      "########## 실험 71/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2010\n",
      "초기 랜덤 선택 샘플 인덱스: [ 261  359  368  549  552  716  734  739  890  893  901 1012 1190 1368\n",
      " 1386 1395 1733 1820 1850 2040 2165 2428 2596 2650 2792 2805 2845 3137\n",
      " 3166 3182 3224 3417]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 5.9191484451293945, std: 1.6618188719613647\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3113 rank= 0.0037 kl= 0.5075\n",
      "Val loss epoch 1000: reg= 1.8629 rank= 0.2661 kl= 0.5135\n",
      "Regression R2 : 0.5083, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.0163116455078125, std: 1.7056949238641357\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4382 rank= 0.0082 kl= 0.5193\n",
      "Val loss epoch 1000: reg= 3.0550 rank= 0.4022 kl= 0.4949\n",
      "Regression R2 : 0.1919, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.62 초\n",
      "=============================================\n",
      "########## 실험 72/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2011\n",
      "초기 랜덤 선택 샘플 인덱스: [  92  116  333  394  511  702  714  848  879  920  932  933 1309 1350\n",
      " 1370 1433 1728 1741 2152 2305 2429 2518 2531 2536 2571 2579 2619 2768\n",
      " 3170 3231 3347 3487]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.885871887207031, std: 1.8167252640588378\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5946 rank= 0.0044 kl= 0.5385\n",
      "Val loss epoch 1000: reg= 2.3003 rank= 0.3811 kl= 0.5442\n",
      "Regression R2 : 0.4153, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.601154327392578, std: 1.543608556257019\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5178 rank= 0.0126 kl= 0.5887\n",
      "Val loss epoch 1000: reg= 2.4413 rank= 0.3241 kl= 0.5310\n",
      "Regression R2 : 0.2955, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.8927764892578125, std: 1.387547502980957\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4375 rank= 0.0135 kl= 0.5876\n",
      "Val loss epoch 1000: reg= 2.9298 rank= 0.2793 kl= 0.5271\n",
      "Regression R2 : 0.2748, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 96\n",
      "총 측정 시간: 10.14 초\n",
      "=============================================\n",
      "########## 실험 73/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2012\n",
      "초기 랜덤 선택 샘플 인덱스: [ 245  271  401  535  567  880  900  917 1060 1229 1324 1441 1606 1654\n",
      " 1675 1683 1803 2019 2234 2294 2503 2741 2790 2809 2905 2944 3051 3119\n",
      " 3127 3187 3200 3306]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.339137077331543, std: 2.080928097234497\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.2884 rank= 0.0025 kl= 0.4828\n",
      "Val loss epoch 1000: reg= 1.6857 rank= 0.2548 kl= 0.4982\n",
      "Regression R2 : 0.5396, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.2774248123168945, std: 1.7836421828134155\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5814 rank= 0.0065 kl= 0.5007\n",
      "Val loss epoch 1000: reg= 2.0562 rank= 0.3021 kl= 0.4987\n",
      "Regression R2 : 0.4323, \n",
      "Recall@1 : 1\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.62 초\n",
      "=============================================\n",
      "########## 실험 74/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2013\n",
      "초기 랜덤 선택 샘플 인덱스: [ 151  728  730  762  893  970 1004 1010 1378 1477 1570 1612 1740 1759\n",
      " 1971 2035 2047 2087 2089 2122 2310 2336 2446 2694 3111 3389 3409 3487\n",
      " 3540 3550 3592 3717]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.386500835418701, std: 1.6640498738153076\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3195 rank= 0.0048 kl= 0.5303\n",
      "Val loss epoch 1000: reg= 1.4708 rank= 0.1767 kl= 0.5191\n",
      "Regression R2 : 0.5107, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.4209818840026855, std: 1.6417679886682128\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4545 rank= 0.0098 kl= 0.5468\n",
      "Val loss epoch 1000: reg= 2.2755 rank= 0.3231 kl= 0.5101\n",
      "Regression R2 : 0.3683, \n",
      "Recall@1 : 1\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.78 초\n",
      "=============================================\n",
      "########## 실험 75/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2014\n",
      "초기 랜덤 선택 샘플 인덱스: [ 180  239  243  400  440  736  892  933  984 1408 1710 1860 1954 2259\n",
      " 2340 2466 2491 2647 2720 2746 2780 2790 2840 2847 2941 3054 3097 3194\n",
      " 3364 3402 3469 3683]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.088900089263916, std: 2.1950976948602294\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3115 rank= 0.0062 kl= 0.4982\n",
      "Val loss epoch 1000: reg= 3.0036 rank= 0.4166 kl= 0.5001\n",
      "Regression R2 : 0.2398, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 32\n",
      "총 측정 시간: 3.32 초\n",
      "=============================================\n",
      "########## 실험 76/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2015\n",
      "초기 랜덤 선택 샘플 인덱스: [  55  223  326  436  483  604  824  942 1062 1078 1132 1198 1330 1339\n",
      " 1488 1615 1672 1732 1867 2383 2494 2548 2592 2860 2934 2957 3215 3253\n",
      " 3374 3483 3531 3588]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 7.048605918884277, std: 1.5280425648553466\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4499 rank= 0.0048 kl= 0.5215\n",
      "Val loss epoch 1000: reg= 1.7211 rank= 0.2793 kl= 0.5219\n",
      "Regression R2 : 0.5861, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.69644832611084, std: 1.3128726582391357\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6305 rank= 0.0083 kl= 0.5017\n",
      "Val loss epoch 1000: reg= 2.4821 rank= 0.3441 kl= 0.4967\n",
      "Regression R2 : 0.3492, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.68 초\n",
      "=============================================\n",
      "########## 실험 77/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2016\n",
      "초기 랜덤 선택 샘플 인덱스: [ 186  307  358  379  584  783  868  948 1016 1120 1247 1258 1324 1496\n",
      " 1645 1721 1883 2097 2157 2258 2362 2392 2456 2594 2614 2870 2924 3343\n",
      " 3402 3407 3521 3582]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 7.144143581390381, std: 1.575487981305847\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6145 rank= 0.0068 kl= 0.5330\n",
      "Val loss epoch 1000: reg= 2.0641 rank= 0.3485 kl= 0.5330\n",
      "Regression R2 : 0.4557, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.501492023468018, std: 1.351190457807312\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4979 rank= 0.0092 kl= 0.5538\n",
      "Val loss epoch 1000: reg= 3.3848 rank= 0.5456 kl= 0.5298\n",
      "Regression R2 : 0.2443, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.639362335205078, std: 1.3426135878427123\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3754 rank= 0.0140 kl= 0.5591\n",
      "Val loss epoch 1000: reg= 3.0413 rank= 0.2892 kl= 0.5208\n",
      "Regression R2 : 0.3558, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 96\n",
      "총 측정 시간: 10.35 초\n",
      "=============================================\n",
      "########## 실험 78/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2017\n",
      "초기 랜덤 선택 샘플 인덱스: [ 164  315  373  504  581  817 1219 1281 1347 1581 1684 1718 1873 1944\n",
      " 1981 2147 2163 2245 2335 2531 2611 2643 2809 3022 3123 3135 3194 3482\n",
      " 3488 3497 3665 3688]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 5.855408668518066, std: 2.019801626668701\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.2873 rank= 0.0037 kl= 0.4942\n",
      "Val loss epoch 1000: reg= 2.5609 rank= 0.3981 kl= 0.5067\n",
      "Regression R2 : 0.3449, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 32\n",
      "총 측정 시간: 3.43 초\n",
      "=============================================\n",
      "########## 실험 79/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2018\n",
      "초기 랜덤 선택 샘플 인덱스: [  13  230  342  421  515  516  572  664  679  890  946 1084 1133 1138\n",
      " 1248 1788 1805 1991 2114 2287 2289 2371 2510 2661 2930 3109 3268 3376\n",
      " 3465 3472 3548 3680]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.5333452224731445, std: 1.9987838368280029\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3642 rank= 0.0074 kl= 0.4909\n",
      "Val loss epoch 1000: reg= 1.9971 rank= 0.3015 kl= 0.5060\n",
      "Regression R2 : 0.4680, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.425851821899414, std: 1.6899939875466918\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5925 rank= 0.0097 kl= 0.5111\n",
      "Val loss epoch 1000: reg= 2.3105 rank= 0.3324 kl= 0.5079\n",
      "Regression R2 : 0.4468, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.756715297698975, std: 1.5330104927880859\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6676 rank= 0.0149 kl= 0.5241\n",
      "Val loss epoch 1000: reg= 2.6741 rank= 0.2783 kl= 0.4998\n",
      "Regression R2 : 0.2772, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 4 (128개) ================\n",
      "y_train mean: 7.9723663330078125, std: 1.3988407950265502\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6691 rank= 0.0181 kl= 0.5205\n",
      "Val loss epoch 1000: reg= 2.2001 rank= 0.2244 kl= 0.4885\n",
      "Regression R2 : 0.1304, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 5 (160개) ================\n",
      "y_train mean: 8.058281898498535, std: 1.3016880850656127\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4329 rank= 0.0215 kl= 0.5247\n",
      "Val loss epoch 1000: reg= 3.0167 rank= 0.2681 kl= 0.5014\n",
      "Regression R2 : 0.2905, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 6 (192개) ================\n",
      "y_train mean: 8.150623321533203, std: 1.2243940930230712\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4104 rank= 0.0212 kl= 0.5118\n",
      "Val loss epoch 1000: reg= 2.9258 rank= 0.2299 kl= 0.4954\n",
      "Regression R2 : 0.2974, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 7 (224개) ================\n",
      "y_train mean: 8.225455284118652, std: 1.1577802996499633\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4809 rank= 0.0229 kl= 0.4929\n",
      "Val loss epoch 1000: reg= 3.1581 rank= 0.2337 kl= 0.4892\n",
      "Regression R2 : 0.2415, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 8 (256개) ================\n",
      "y_train mean: 8.297100067138672, std: 1.1089178423745727\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4569 rank= 0.0289 kl= 0.4788\n",
      "Val loss epoch 1000: reg= 2.8022 rank= 0.1942 kl= 0.4886\n",
      "Regression R2 : 0.2988, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 9 (288개) ================\n",
      "y_train mean: 8.343941688537598, std: 1.0650705199105834\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4787 rank= 0.0292 kl= 0.4763\n",
      "Val loss epoch 1000: reg= 3.0191 rank= 0.1925 kl= 0.4940\n",
      "Regression R2 : 0.2008, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 10 (320개) ================\n",
      "y_train mean: 8.380374908447266, std: 1.031672487722168\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4551 rank= 0.0259 kl= 0.4758\n",
      "Val loss epoch 1000: reg= 2.8038 rank= 0.1876 kl= 0.4947\n",
      "Regression R2 : 0.2363, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 320\n",
      "총 측정 시간: 38.09 초\n",
      "=============================================\n",
      "########## 실험 80/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2019\n",
      "초기 랜덤 선택 샘플 인덱스: [ 417  535  676  735  742 1132 1197 1261 1292 1438 1440 1599 1638 1696\n",
      " 1785 1864 1969 2063 2105 2423 2554 2612 2715 2759 2851 3278 3398 3459\n",
      " 3480 3491 3521 3594]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.507327556610107, std: 1.7199821572167968\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3855 rank= 0.0073 kl= 0.5013\n",
      "Val loss epoch 1000: reg= 2.2615 rank= 0.3226 kl= 0.5248\n",
      "Regression R2 : 0.3275, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.431279182434082, std: 1.5843781332833862\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5065 rank= 0.0099 kl= 0.5370\n",
      "Val loss epoch 1000: reg= 2.2304 rank= 0.3181 kl= 0.5175\n",
      "Regression R2 : 0.4133, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.60 초\n",
      "=============================================\n",
      "########## 실험 81/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2000\n",
      "초기 랜덤 선택 샘플 인덱스: [   8  100  230  255  392  634  659  750  871  908  979  999 1227 1371\n",
      " 1524 1680 1734 1742 1879 1991 2015 2023 2130 2242 2402 2432 2768 3276\n",
      " 3348 3463 3572 3686]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.420133590698242, std: 1.7561746935708618\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.2989 rank= 0.0054 kl= 0.5103\n",
      "Val loss epoch 1000: reg= 2.0559 rank= 0.3625 kl= 0.5256\n",
      "Regression R2 : 0.4154, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 32\n",
      "총 측정 시간: 3.24 초\n",
      "=============================================\n",
      "########## 실험 82/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2001\n",
      "초기 랜덤 선택 샘플 인덱스: [  75   82  122  374  416  424  443  607  703  755  818  824  900 1217\n",
      " 1253 1569 1603 1617 1682 1761 1779 1918 1991 2132 2137 2229 2549 2845\n",
      " 2911 3569 3683 3710]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.486093044281006, std: 2.5365142922265624\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4158 rank= 0.0033 kl= 0.5270\n",
      "Val loss epoch 1000: reg= 1.8524 rank= 0.2729 kl= 0.5134\n",
      "Regression R2 : 0.5195, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.3744587898254395, std: 2.030856142507324\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5615 rank= 0.0105 kl= 0.5481\n",
      "Val loss epoch 1000: reg= 2.5993 rank= 0.3877 kl= 0.5307\n",
      "Regression R2 : 0.3853, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.663119792938232, std: 1.8191760878427123\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4755 rank= 0.0127 kl= 0.5475\n",
      "Val loss epoch 1000: reg= 2.1443 rank= 0.2505 kl= 0.5273\n",
      "Regression R2 : 0.5329, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 4 (128개) ================\n",
      "y_train mean: 7.903137683868408, std: 1.6463550429208373\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4733 rank= 0.0207 kl= 0.5490\n",
      "Val loss epoch 1000: reg= 1.9438 rank= 0.1716 kl= 0.5073\n",
      "Regression R2 : 0.4762, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 5 (160개) ================\n",
      "y_train mean: 7.98516845703125, std: 1.5394021372659301\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4520 rank= 0.0171 kl= 0.5363\n",
      "Val loss epoch 1000: reg= 2.0013 rank= 0.1504 kl= 0.5048\n",
      "Regression R2 : 0.4379, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 6 (192개) ================\n",
      "y_train mean: 8.08999252319336, std: 1.4475796322686767\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4186 rank= 0.0228 kl= 0.5283\n",
      "Val loss epoch 1000: reg= 2.0946 rank= 0.1454 kl= 0.5042\n",
      "Regression R2 : 0.4510, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 192\n",
      "총 측정 시간: 21.27 초\n",
      "=============================================\n",
      "########## 실험 83/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2002\n",
      "초기 랜덤 선택 샘플 인덱스: [ 136  395  402  409  532  561  564  638  885  923 1182 1208 1321 1525\n",
      " 1897 1919 1934 2197 2255 2440 2570 3093 3129 3291 3299 3334 3343 3425\n",
      " 3477 3495 3560 3626]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.467774868011475, std: 1.6519296269281005\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3498 rank= 0.0048 kl= 0.5047\n",
      "Val loss epoch 1000: reg= 1.5127 rank= 0.2500 kl= 0.5300\n",
      "Regression R2 : 0.5073, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.313525199890137, std: 1.5398681263787841\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5067 rank= 0.0063 kl= 0.5230\n",
      "Val loss epoch 1000: reg= 1.8543 rank= 0.2338 kl= 0.5090\n",
      "Regression R2 : 0.4498, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.693683624267578, std: 1.4052169422967529\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4851 rank= 0.0133 kl= 0.5111\n",
      "Val loss epoch 1000: reg= 2.2322 rank= 0.2611 kl= 0.4998\n",
      "Regression R2 : 0.4100, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 4 (128개) ================\n",
      "y_train mean: 7.934490203857422, std: 1.3467134337289428\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4530 rank= 0.0163 kl= 0.5175\n",
      "Val loss epoch 1000: reg= 0.9819 rank= 0.1203 kl= 0.4883\n",
      "Regression R2 : 0.3453, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 5 (160개) ================\n",
      "y_train mean: 8.002902030944824, std: 1.261186967359314\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3834 rank= 0.0204 kl= 0.5202\n",
      "Val loss epoch 1000: reg= 2.3654 rank= 0.2845 kl= 0.5045\n",
      "Regression R2 : 0.4057, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 6 (192개) ================\n",
      "y_train mean: 8.11984920501709, std: 1.1954352955682372\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3446 rank= 0.0210 kl= 0.5256\n",
      "Val loss epoch 1000: reg= 2.2676 rank= 0.2570 kl= 0.5128\n",
      "Regression R2 : 0.4373, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 7 (224개) ================\n",
      "y_train mean: 8.194456100463867, std: 1.137912402616272\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3502 rank= 0.0258 kl= 0.5098\n",
      "Val loss epoch 1000: reg= 2.3721 rank= 0.2584 kl= 0.5029\n",
      "Regression R2 : 0.4519, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 8 (256개) ================\n",
      "y_train mean: 8.24565315246582, std: 1.0851847033364868\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3483 rank= 0.0245 kl= 0.5041\n",
      "Val loss epoch 1000: reg= 2.7479 rank= 0.2752 kl= 0.5031\n",
      "Regression R2 : 0.3715, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 9 (288개) ================\n",
      "y_train mean: 8.282337188720703, std: 1.0437451701028442\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3455 rank= 0.0220 kl= 0.4984\n",
      "Val loss epoch 1000: reg= 2.6939 rank= 0.2516 kl= 0.5033\n",
      "Regression R2 : 0.3842, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 10 (320개) ================\n",
      "y_train mean: 8.328702926635742, std: 1.0084941487176513\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3757 rank= 0.0273 kl= 0.4993\n",
      "Val loss epoch 1000: reg= 2.7582 rank= 0.2504 kl= 0.5034\n",
      "Regression R2 : 0.3409, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 11 (352개) ================\n",
      "y_train mean: 8.368639945983887, std: 0.9754007558686829\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4246 rank= 0.0279 kl= 0.4979\n",
      "Val loss epoch 1000: reg= 3.0699 rank= 0.2562 kl= 0.5010\n",
      "Regression R2 : 0.2962, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 12 (384개) ================\n",
      "y_train mean: 8.40322494506836, std: 0.945705006585846\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4233 rank= 0.0274 kl= 0.4946\n",
      "Val loss epoch 1000: reg= 2.6662 rank= 0.2362 kl= 0.5016\n",
      "Regression R2 : 0.3551, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 13 (416개) ================\n",
      "y_train mean: 8.429309844970703, std: 0.9215706686837769\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4358 rank= 0.0279 kl= 0.4887\n",
      "Val loss epoch 1000: reg= 2.4177 rank= 0.2370 kl= 0.4965\n",
      "Regression R2 : 0.3644, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 14 (448개) ================\n",
      "y_train mean: 8.434347152709961, std: 0.9028204183442688\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4185 rank= 0.0315 kl= 0.4863\n",
      "Val loss epoch 1000: reg= 2.8067 rank= 0.2538 kl= 0.4978\n",
      "Regression R2 : 0.3074, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 15 (480개) ================\n",
      "y_train mean: 8.439594268798828, std: 0.8847867946488953\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4114 rank= 0.0304 kl= 0.4856\n",
      "Val loss epoch 1000: reg= 2.7395 rank= 0.2594 kl= 0.5011\n",
      "Regression R2 : 0.3264, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 16 (512개) ================\n",
      "y_train mean: 8.453887939453125, std: 0.8629165987832642\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4443 rank= 0.0314 kl= 0.4903\n",
      "Val loss epoch 1000: reg= 2.6694 rank= 0.2716 kl= 0.5053\n",
      "Regression R2 : 0.2977, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 17 (544개) ================\n",
      "y_train mean: 8.45991039276123, std: 0.8426649074418641\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5200 rank= 0.0118 kl= 0.3814\n",
      "Val loss epoch 1000: reg= 2.0706 rank= 0.2095 kl= 0.3843\n",
      "Regression R2 : 0.3569, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 18 (576개) ================\n",
      "y_train mean: 8.459239959716797, std: 0.8293863634927369\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4736 rank= 0.0205 kl= 0.3714\n",
      "Val loss epoch 1000: reg= 1.9341 rank= 0.1911 kl= 0.3785\n",
      "Regression R2 : 0.3113, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 19 (608개) ================\n",
      "y_train mean: 8.462897300720215, std: 0.8101145725114441\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4902 rank= 0.0207 kl= 0.3591\n",
      "Val loss epoch 1000: reg= 2.1722 rank= 0.2011 kl= 0.3643\n",
      "Regression R2 : 0.2594, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 20 (640개) ================\n",
      "y_train mean: 8.461060523986816, std: 0.7950556974275208\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4969 rank= 0.0267 kl= 0.3535\n",
      "Val loss epoch 1000: reg= 2.0711 rank= 0.2196 kl= 0.3469\n",
      "Regression R2 : 0.2601, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 21 (672개) ================\n",
      "y_train mean: 8.460593223571777, std: 0.7801157336099244\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4377 rank= 0.0215 kl= 0.3499\n",
      "Val loss epoch 1000: reg= 3.1322 rank= 0.2665 kl= 0.3518\n",
      "Regression R2 : 0.1847, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 22 (704개) ================\n",
      "y_train mean: 8.454402923583984, std: 0.7714707355363465\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4765 rank= 0.0264 kl= 0.3528\n",
      "Val loss epoch 1000: reg= 3.0109 rank= 0.2573 kl= 0.3555\n",
      "Regression R2 : 0.2063, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 23 (736개) ================\n",
      "y_train mean: 8.446222305297852, std: 0.7618319492204285\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3753 rank= 0.0239 kl= 0.3509\n",
      "Val loss epoch 1000: reg= 2.9432 rank= 0.2698 kl= 0.3547\n",
      "Regression R2 : 0.2253, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 24 (768개) ================\n",
      "y_train mean: 8.437387466430664, std: 0.7528897027833558\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3779 rank= 0.0252 kl= 0.3451\n",
      "Val loss epoch 1000: reg= 3.1393 rank= 0.2794 kl= 0.3518\n",
      "Regression R2 : 0.2019, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 25 (800개) ================\n",
      "y_train mean: 8.427528381347656, std: 0.7444519500596619\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3670 rank= 0.0255 kl= 0.3462\n",
      "Val loss epoch 1000: reg= 3.2093 rank= 0.2846 kl= 0.3519\n",
      "Regression R2 : 0.1806, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 26 (832개) ================\n",
      "y_train mean: 8.414499282836914, std: 0.7401600580079651\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3796 rank= 0.0264 kl= 0.3420\n",
      "Val loss epoch 1000: reg= 3.1797 rank= 0.2848 kl= 0.3506\n",
      "Regression R2 : 0.1651, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 27 (864개) ================\n",
      "y_train mean: 8.401729583740234, std: 0.7342310051782227\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3078 rank= 0.0246 kl= 0.3418\n",
      "Val loss epoch 1000: reg= 3.0081 rank= 0.2759 kl= 0.3503\n",
      "Regression R2 : 0.2019, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 28 (896개) ================\n",
      "y_train mean: 8.391512870788574, std: 0.7309552531106568\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3440 rank= 0.0293 kl= 0.3387\n",
      "Val loss epoch 1000: reg= 2.5025 rank= 0.2515 kl= 0.3465\n",
      "Regression R2 : 0.3355, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 896\n",
      "총 측정 시간: 177.62 초\n",
      "=============================================\n",
      "########## 실험 84/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2003\n",
      "초기 랜덤 선택 샘플 인덱스: [  70  278  474  527  720  757  885  969  988 1006 1091 1309 1347 1449\n",
      " 1633 1868 2081 2112 2151 2415 2477 2489 2645 2950 3014 3217 3258 3265\n",
      " 3428 3509 3645 3704]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.318173408508301, std: 1.77249277638031\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4026 rank= 0.0082 kl= 0.5070\n",
      "Val loss epoch 1000: reg= 1.8287 rank= 0.2961 kl= 0.5334\n",
      "Regression R2 : 0.4644, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.1629462242126465, std: 1.614287267194519\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3521 rank= 0.0108 kl= 0.5404\n",
      "Val loss epoch 1000: reg= 1.7476 rank= 0.2754 kl= 0.5214\n",
      "Regression R2 : 0.5837, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.61 초\n",
      "=============================================\n",
      "########## 실험 85/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2004\n",
      "초기 랜덤 선택 샘플 인덱스: [  10  112  195  254  342  507  623  690 1078 1157 1302 1406 1700 1831\n",
      " 2036 2038 2156 2267 2300 2306 2371 2458 2570 2626 2630 2648 2662 2818\n",
      " 3037 3109 3397 3569]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.423959732055664, std: 2.109467039571533\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4586 rank= 0.0046 kl= 0.4996\n",
      "Val loss epoch 1000: reg= 2.6970 rank= 0.4375 kl= 0.5049\n",
      "Regression R2 : 0.4789, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.360374927520752, std: 1.8129922251565551\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5370 rank= 0.0088 kl= 0.5385\n",
      "Val loss epoch 1000: reg= 3.4114 rank= 0.5494 kl= 0.5110\n",
      "Regression R2 : 0.2372, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.602818965911865, std: 1.691549787984619\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4580 rank= 0.0129 kl= 0.5333\n",
      "Val loss epoch 1000: reg= 4.5898 rank= 0.4442 kl= 0.5037\n",
      "Regression R2 : 0.2056, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 4 (128개) ================\n",
      "y_train mean: 7.890404224395752, std: 1.5646301607949828\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3472 rank= 0.0171 kl= 0.5318\n",
      "Val loss epoch 1000: reg= 2.4629 rank= 0.3314 kl= 0.5046\n",
      "Regression R2 : 0.3184, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 5 (160개) ================\n",
      "y_train mean: 8.017151832580566, std: 1.447274933324585\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3454 rank= 0.0165 kl= 0.5253\n",
      "Val loss epoch 1000: reg= 3.1501 rank= 0.2990 kl= 0.5038\n",
      "Regression R2 : 0.2311, \n",
      "Recall@1 : 1\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 160\n",
      "총 측정 시간: 18.22 초\n",
      "=============================================\n",
      "########## 실험 86/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2005\n",
      "초기 랜덤 선택 샘플 인덱스: [  12   33   62  109  111  489  793  846  971  990 1151 1320 1736 1793\n",
      " 1882 1884 2062 2197 2222 2420 2478 2503 2539 2658 2692 2761 2985 3170\n",
      " 3328 3379 3496 3598]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.756325721740723, std: 1.7418288092477416\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4454 rank= 0.0030 kl= 0.5407\n",
      "Val loss epoch 1000: reg= 2.1746 rank= 0.3597 kl= 0.5397\n",
      "Regression R2 : 0.4493, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.668529033660889, std: 1.5765471558435058\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5753 rank= 0.0092 kl= 0.5543\n",
      "Val loss epoch 1000: reg= 3.1782 rank= 0.4387 kl= 0.5196\n",
      "Regression R2 : 0.2235, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.71 초\n",
      "=============================================\n",
      "########## 실험 87/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2006\n",
      "초기 랜덤 선택 샘플 인덱스: [  47   78  204  215  279  353  565  816 1095 1122 1149 1296 1328 1432\n",
      " 1522 1762 2071 2176 2357 2466 2510 2531 2924 3036 3073 3229 3333 3368\n",
      " 3521 3564 3721 3731]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.419711112976074, std: 1.8596973519189453\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.2837 rank= 0.0041 kl= 0.4991\n",
      "Val loss epoch 1000: reg= 1.8087 rank= 0.2742 kl= 0.5107\n",
      "Regression R2 : 0.5781, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.245265007019043, std: 1.6249057154519653\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4851 rank= 0.0079 kl= 0.4922\n",
      "Val loss epoch 1000: reg= 2.4631 rank= 0.2958 kl= 0.4836\n",
      "Regression R2 : 0.4231, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.635911464691162, std: 1.54055918263031\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4248 rank= 0.0102 kl= 0.5086\n",
      "Val loss epoch 1000: reg= 3.4339 rank= 0.3700 kl= 0.4868\n",
      "Regression R2 : 0.3959, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 96\n",
      "총 측정 시간: 10.26 초\n",
      "=============================================\n",
      "########## 실험 88/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2007\n",
      "초기 랜덤 선택 샘플 인덱스: [  45  375  515  708  803  809  834 1015 1047 1083 1092 1150 1333 1455\n",
      " 1477 1689 1759 1787 1930 2051 2102 2259 2450 2496 2504 2669 2696 2814\n",
      " 3058 3283 3399 3438]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.840710639953613, std: 2.0724175076348876\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4789 rank= 0.0044 kl= 0.5011\n",
      "Val loss epoch 1000: reg= 2.7585 rank= 0.5056 kl= 0.5083\n",
      "Regression R2 : 0.3610, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.580652236938477, std: 1.7611152033670043\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5965 rank= 0.0097 kl= 0.5306\n",
      "Val loss epoch 1000: reg= 2.2019 rank= 0.3596 kl= 0.4985\n",
      "Regression R2 : 0.3815, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.68 초\n",
      "=============================================\n",
      "########## 실험 89/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2008\n",
      "초기 랜덤 선택 샘플 인덱스: [  53  112  160  174  213  231  945 1134 1193 1233 1267 1269 1487 2193\n",
      " 2294 2493 2542 2640 2713 2742 2803 2830 2857 2902 3140 3246 3288 3315\n",
      " 3360 3442 3471 3485]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.172525882720947, std: 1.5799144606454467\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3481 rank= 0.0051 kl= 0.4978\n",
      "Val loss epoch 1000: reg= 1.9947 rank= 0.2847 kl= 0.5111\n",
      "Regression R2 : 0.2802, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.294429302215576, std: 1.6248908142907714\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4243 rank= 0.0103 kl= 0.5356\n",
      "Val loss epoch 1000: reg= 2.4051 rank= 0.2734 kl= 0.5141\n",
      "Regression R2 : 0.1631, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.5947747230529785, std: 1.5260244707925414\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3516 rank= 0.0136 kl= 0.5233\n",
      "Val loss epoch 1000: reg= 2.4864 rank= 0.2392 kl= 0.4948\n",
      "Regression R2 : 0.1965, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 96\n",
      "총 측정 시간: 10.35 초\n",
      "=============================================\n",
      "########## 실험 90/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2009\n",
      "초기 랜덤 선택 샘플 인덱스: [   0  111  335  399  520  678  750  785  881  909 1097 1244 1434 1440\n",
      " 1580 1585 1713 1878 1963 2228 2249 2520 2691 2746 2856 2892 2972 3097\n",
      " 3216 3314 3349 3655]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.897634029388428, std: 1.8627023796899413\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5405 rank= 0.0046 kl= 0.5247\n",
      "Val loss epoch 1000: reg= 1.4167 rank= 0.2146 kl= 0.5169\n",
      "Regression R2 : 0.5240, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.658543586730957, std: 1.5644017557962036\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.7086 rank= 0.0093 kl= 0.5438\n",
      "Val loss epoch 1000: reg= 2.3064 rank= 0.3299 kl= 0.5115\n",
      "Regression R2 : 0.3576, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.67 초\n",
      "=============================================\n",
      "########## 실험 91/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2010\n",
      "초기 랜덤 선택 샘플 인덱스: [ 261  359  368  549  552  716  734  739  890  893  901 1012 1190 1368\n",
      " 1386 1395 1733 1820 1850 2040 2165 2428 2596 2650 2792 2805 2845 3137\n",
      " 3166 3182 3224 3417]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 5.9191484451293945, std: 1.6618188719613647\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3026 rank= 0.0038 kl= 0.4966\n",
      "Val loss epoch 1000: reg= 1.8569 rank= 0.2646 kl= 0.5026\n",
      "Regression R2 : 0.5087, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 6.993738174438477, std: 1.6960294346673583\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4104 rank= 0.0085 kl= 0.5168\n",
      "Val loss epoch 1000: reg= 2.9715 rank= 0.3901 kl= 0.4903\n",
      "Regression R2 : 0.2215, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.69 초\n",
      "=============================================\n",
      "########## 실험 92/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2011\n",
      "초기 랜덤 선택 샘플 인덱스: [  92  116  333  394  511  702  714  848  879  920  932  933 1309 1350\n",
      " 1370 1433 1728 1741 2152 2305 2429 2518 2531 2536 2571 2579 2619 2768\n",
      " 3170 3231 3347 3487]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.885871887207031, std: 1.8167252640588378\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5855 rank= 0.0042 kl= 0.5307\n",
      "Val loss epoch 1000: reg= 2.2648 rank= 0.3765 kl= 0.5362\n",
      "Regression R2 : 0.4204, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.620649814605713, std: 1.5337458948953246\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5115 rank= 0.0118 kl= 0.5839\n",
      "Val loss epoch 1000: reg= 2.3382 rank= 0.2930 kl= 0.5277\n",
      "Regression R2 : 0.2989, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.89874792098999, std: 1.3688942293984985\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5046 rank= 0.0134 kl= 0.5883\n",
      "Val loss epoch 1000: reg= 3.2580 rank= 0.3207 kl= 0.5283\n",
      "Regression R2 : 0.2147, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 96\n",
      "총 측정 시간: 10.43 초\n",
      "=============================================\n",
      "########## 실험 93/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2012\n",
      "초기 랜덤 선택 샘플 인덱스: [ 245  271  401  535  567  880  900  917 1060 1229 1324 1441 1606 1654\n",
      " 1675 1683 1803 2019 2234 2294 2503 2741 2790 2809 2905 2944 3051 3119\n",
      " 3127 3187 3200 3306]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.339137077331543, std: 2.080928097234497\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.2783 rank= 0.0026 kl= 0.4748\n",
      "Val loss epoch 1000: reg= 1.6805 rank= 0.2547 kl= 0.4902\n",
      "Regression R2 : 0.5428, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.313441753387451, std: 1.8012298445565795\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5896 rank= 0.0067 kl= 0.4939\n",
      "Val loss epoch 1000: reg= 2.1311 rank= 0.3158 kl= 0.4868\n",
      "Regression R2 : 0.4502, \n",
      "Recall@1 : 1\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.88 초\n",
      "=============================================\n",
      "########## 실험 94/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2013\n",
      "초기 랜덤 선택 샘플 인덱스: [ 151  728  730  762  893  970 1004 1010 1378 1477 1570 1612 1740 1759\n",
      " 1971 2035 2047 2087 2089 2122 2310 2336 2446 2694 3111 3389 3409 3487\n",
      " 3540 3550 3592 3717]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.386500835418701, std: 1.6640498738153076\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3136 rank= 0.0047 kl= 0.5218\n",
      "Val loss epoch 1000: reg= 1.4734 rank= 0.1772 kl= 0.5112\n",
      "Regression R2 : 0.5103, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.444720268249512, std: 1.651436696515808\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4297 rank= 0.0102 kl= 0.5343\n",
      "Val loss epoch 1000: reg= 2.1972 rank= 0.3094 kl= 0.4980\n",
      "Regression R2 : 0.3887, \n",
      "Recall@1 : 1\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.99 초\n",
      "=============================================\n",
      "########## 실험 95/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2014\n",
      "초기 랜덤 선택 샘플 인덱스: [ 180  239  243  400  440  736  892  933  984 1408 1710 1860 1954 2259\n",
      " 2340 2466 2491 2647 2720 2746 2780 2790 2840 2847 2941 3054 3097 3194\n",
      " 3364 3402 3469 3683]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.088900089263916, std: 2.1950976948602294\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3131 rank= 0.0063 kl= 0.4894\n",
      "Val loss epoch 1000: reg= 3.0217 rank= 0.4177 kl= 0.4913\n",
      "Regression R2 : 0.2351, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 32\n",
      "총 측정 시간: 3.48 초\n",
      "=============================================\n",
      "########## 실험 96/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2015\n",
      "초기 랜덤 선택 샘플 인덱스: [  55  223  326  436  483  604  824  942 1062 1078 1132 1198 1330 1339\n",
      " 1488 1615 1672 1732 1867 2383 2494 2548 2592 2860 2934 2957 3215 3253\n",
      " 3374 3483 3531 3588]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 7.048605918884277, std: 1.5280425648553466\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4445 rank= 0.0042 kl= 0.5129\n",
      "Val loss epoch 1000: reg= 1.7170 rank= 0.2790 kl= 0.5137\n",
      "Regression R2 : 0.5892, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.680524826049805, std: 1.3112964730126953\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6179 rank= 0.0066 kl= 0.5000\n",
      "Val loss epoch 1000: reg= 2.5028 rank= 0.3473 kl= 0.4979\n",
      "Regression R2 : 0.3387, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.938027858734131, std: 1.2036984066827392\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6083 rank= 0.0127 kl= 0.4977\n",
      "Val loss epoch 1000: reg= 2.9068 rank= 0.2578 kl= 0.4964\n",
      "Regression R2 : 0.3297, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 4 (128개) ================\n",
      "y_train mean: 8.125186920166016, std: 1.1316970686776733\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4640 rank= 0.0172 kl= 0.5040\n",
      "Val loss epoch 1000: reg= 1.8285 rank= 0.2094 kl= 0.4850\n",
      "Regression R2 : 0.2575, \n",
      "Recall@1 : 1\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 128\n",
      "총 측정 시간: 14.10 초\n",
      "=============================================\n",
      "########## 실험 97/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2016\n",
      "초기 랜덤 선택 샘플 인덱스: [ 186  307  358  379  584  783  868  948 1016 1120 1247 1258 1324 1496\n",
      " 1645 1721 1883 2097 2157 2258 2362 2392 2456 2594 2614 2870 2924 3343\n",
      " 3402 3407 3521 3582]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 7.144143581390381, std: 1.575487981305847\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5999 rank= 0.0066 kl= 0.5256\n",
      "Val loss epoch 1000: reg= 2.0670 rank= 0.3494 kl= 0.5254\n",
      "Regression R2 : 0.4594, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.528773307800293, std: 1.35831619309021\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5059 rank= 0.0094 kl= 0.5502\n",
      "Val loss epoch 1000: reg= 3.5407 rank= 0.5719 kl= 0.5274\n",
      "Regression R2 : 0.2076, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.548058986663818, std: 1.3922351698739623\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3679 rank= 0.0137 kl= 0.5486\n",
      "Val loss epoch 1000: reg= 2.6544 rank= 0.2530 kl= 0.5127\n",
      "Regression R2 : 0.3947, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 4 (128개) ================\n",
      "y_train mean: 7.829996585845947, std: 1.3383758168084716\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3803 rank= 0.0174 kl= 0.5452\n",
      "Val loss epoch 1000: reg= 1.8848 rank= 0.1414 kl= 0.4910\n",
      "Regression R2 : 0.3650, \n",
      "Recall@1 : 1\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 128\n",
      "총 측정 시간: 14.00 초\n",
      "=============================================\n",
      "########## 실험 98/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2017\n",
      "초기 랜덤 선택 샘플 인덱스: [ 164  315  373  504  581  817 1219 1281 1347 1581 1684 1718 1873 1944\n",
      " 1981 2147 2163 2245 2335 2531 2611 2643 2809 3022 3123 3135 3194 3482\n",
      " 3488 3497 3665 3688]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 5.855408668518066, std: 2.019801626668701\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.2874 rank= 0.0039 kl= 0.4852\n",
      "Val loss epoch 1000: reg= 2.5725 rank= 0.4002 kl= 0.4978\n",
      "Regression R2 : 0.3430, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 32\n",
      "총 측정 시간: 3.28 초\n",
      "=============================================\n",
      "########## 실험 99/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2018\n",
      "초기 랜덤 선택 샘플 인덱스: [  13  230  342  421  515  516  572  664  679  890  946 1084 1133 1138\n",
      " 1248 1788 1805 1991 2114 2287 2289 2371 2510 2661 2930 3109 3268 3376\n",
      " 3465 3472 3548 3680]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.5333452224731445, std: 1.9987838368280029\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3620 rank= 0.0071 kl= 0.4821\n",
      "Val loss epoch 1000: reg= 1.9902 rank= 0.2985 kl= 0.4970\n",
      "Regression R2 : 0.4680, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.467068672180176, std: 1.7167849640710449\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6445 rank= 0.0080 kl= 0.4955\n",
      "Val loss epoch 1000: reg= 1.8243 rank= 0.2361 kl= 0.4912\n",
      "Regression R2 : 0.4761, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.7888312339782715, std: 1.5575618843896484\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.6149 rank= 0.0129 kl= 0.5132\n",
      "Val loss epoch 1000: reg= 2.6117 rank= 0.2371 kl= 0.4991\n",
      "Regression R2 : 0.4223, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 4 (128개) ================\n",
      "y_train mean: 7.987709045410156, std: 1.4219074349267578\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5078 rank= 0.0192 kl= 0.5101\n",
      "Val loss epoch 1000: reg= 0.8029 rank= 0.0961 kl= 0.4863\n",
      "Regression R2 : 0.4378, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 5 (160개) ================\n",
      "y_train mean: 8.114392280578613, std: 1.3156489233834838\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5093 rank= 0.0183 kl= 0.4902\n",
      "Val loss epoch 1000: reg= 2.3519 rank= 0.2216 kl= 0.4827\n",
      "Regression R2 : 0.4201, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 6 (192개) ================\n",
      "y_train mean: 8.205395698547363, std: 1.2335004906518554\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4685 rank= 0.0203 kl= 0.4925\n",
      "Val loss epoch 1000: reg= 2.7264 rank= 0.2435 kl= 0.4883\n",
      "Regression R2 : 0.3249, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 7 (224개) ================\n",
      "y_train mean: 8.275251388549805, std: 1.163119087682495\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4466 rank= 0.0209 kl= 0.4863\n",
      "Val loss epoch 1000: reg= 2.7864 rank= 0.2289 kl= 0.4914\n",
      "Regression R2 : 0.2954, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 8 (256개) ================\n",
      "y_train mean: 8.330775260925293, std: 1.1087205510003662\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5131 rank= 0.0254 kl= 0.4853\n",
      "Val loss epoch 1000: reg= 2.9727 rank= 0.2263 kl= 0.5024\n",
      "Regression R2 : 0.2433, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 9 (288개) ================\n",
      "y_train mean: 8.376626014709473, std: 1.0589090685708618\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.5012 rank= 0.0265 kl= 0.4826\n",
      "Val loss epoch 1000: reg= 3.0161 rank= 0.2063 kl= 0.5016\n",
      "Regression R2 : 0.2035, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 10 (320개) ================\n",
      "y_train mean: 8.388425827026367, std: 1.026862631307373\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4094 rank= 0.0260 kl= 0.4858\n",
      "Val loss epoch 1000: reg= 2.9265 rank= 0.1891 kl= 0.5097\n",
      "Regression R2 : 0.2100, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 11 (352개) ================\n",
      "y_train mean: 8.408492088317871, std: 0.9982838134629822\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4302 rank= 0.0285 kl= 0.4800\n",
      "Val loss epoch 1000: reg= 3.0657 rank= 0.2499 kl= 0.5006\n",
      "Regression R2 : 0.2118, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 12 (384개) ================\n",
      "y_train mean: 8.405182838439941, std: 0.9817881088121033\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4478 rank= 0.0298 kl= 0.4792\n",
      "Val loss epoch 1000: reg= 2.7813 rank= 0.2249 kl= 0.5006\n",
      "Regression R2 : 0.2883, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 13 (416개) ================\n",
      "y_train mean: 8.426243782043457, std: 0.9520177345140076\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4463 rank= 0.0319 kl= 0.4763\n",
      "Val loss epoch 1000: reg= 2.6081 rank= 0.2150 kl= 0.4956\n",
      "Regression R2 : 0.3553, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 416\n",
      "총 측정 시간: 53.13 초\n",
      "=============================================\n",
      "########## 실험 100/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2019\n",
      "초기 랜덤 선택 샘플 인덱스: [ 417  535  676  735  742 1132 1197 1261 1292 1438 1440 1599 1638 1696\n",
      " 1785 1864 1969 2063 2105 2423 2554 2612 2715 2759 2851 3278 3398 3459\n",
      " 3480 3491 3521 3594]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.507327556610107, std: 1.7199821572167968\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3906 rank= 0.0075 kl= 0.4945\n",
      "Val loss epoch 1000: reg= 2.2780 rank= 0.3240 kl= 0.5177\n",
      "Regression R2 : 0.3216, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.4401535987854, std: 1.5867011647088622\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.4722 rank= 0.0094 kl= 0.5230\n",
      "Val loss epoch 1000: reg= 2.1881 rank= 0.3156 kl= 0.5031\n",
      "Regression R2 : 0.4268, \n",
      "Recall@1 : 1\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 6.65 초\n",
      "=============================================\n",
      "########## 실험 101/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2000\n",
      "초기 랜덤 선택 샘플 인덱스: [   8  100  230  255  392  634  659  750  871  908  979  999 1227 1371\n",
      " 1524 1680 1734 1742 1879 1991 2015 2023 2130 2242 2402 2432 2768 3276\n",
      " 3348 3463 3572 3686]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.420133590698242, std: 1.7561746935708618\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.3478 rank= 0.0102 kl= 1.3664\n",
      "Val loss epoch 1000: reg= 1.7853 rank= 0.2708 kl= 1.2824\n",
      "Regression R2 : 0.4545, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 32\n",
      "총 측정 시간: 3.32 초\n",
      "=============================================\n",
      "########## 실험 102/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2001\n",
      "초기 랜덤 선택 샘플 인덱스: [  75   82  122  374  416  424  443  607  703  755  818  824  900 1217\n",
      " 1253 1569 1603 1617 1682 1761 1779 1918 1991 2132 2137 2229 2549 2845\n",
      " 2911 3569 3683 3710]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.486093044281006, std: 2.5365142922265624\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.4025 rank= 0.0131 kl= 1.4618\n",
      "Val loss epoch 1000: reg= 1.5427 rank= 0.2118 kl= 1.2480\n",
      "Regression R2 : 0.5774, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.340588569641113, std: 2.0450754265649413\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.3021 rank= 0.0259 kl= 1.4563\n",
      "Val loss epoch 1000: reg= 3.4298 rank= 0.4234 kl= 1.1653\n",
      "Regression R2 : -0.0046, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.711961269378662, std: 1.8435182671411132\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.3120 rank= 0.0343 kl= 1.6070\n",
      "Val loss epoch 1000: reg= 3.1712 rank= 0.2886 kl= 1.2404\n",
      "Regression R2 : 0.2064, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 4 (128개) ================\n",
      "y_train mean: 7.837285995483398, std: 1.6847238640649413\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.3759 rank= 0.0430 kl= 1.5536\n",
      "Val loss epoch 1000: reg= 2.8151 rank= 0.2527 kl= 1.1755\n",
      "Regression R2 : 0.1245, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 5 (160개) ================\n",
      "y_train mean: 7.930935859680176, std: 1.5815619330270385\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.4240 rank= 0.0397 kl= 1.4562\n",
      "Val loss epoch 1000: reg= 3.1165 rank= 0.1883 kl= 1.0643\n",
      "Regression R2 : 0.1209, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 6 (192개) ================\n",
      "y_train mean: 8.059900283813477, std: 1.4819920163018798\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.4510 rank= 0.0445 kl= 1.4347\n",
      "Val loss epoch 1000: reg= 3.4782 rank= 0.2192 kl= 1.0603\n",
      "Regression R2 : 0.0366, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 7 (224개) ================\n",
      "y_train mean: 8.159708023071289, std: 1.399484644399414\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.7063 rank= 0.0454 kl= 1.4351\n",
      "Val loss epoch 1000: reg= 3.9361 rank= 0.2152 kl= 1.0481\n",
      "Regression R2 : -0.0560, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 8 (256개) ================\n",
      "y_train mean: 8.21700668334961, std: 1.3375935654504394\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.9171 rank= 0.0554 kl= 1.4162\n",
      "Val loss epoch 1000: reg= 3.6763 rank= 0.2254 kl= 1.0241\n",
      "Regression R2 : -0.0149, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 9 (288개) ================\n",
      "y_train mean: 8.270026206970215, std: 1.280439029203186\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 2.0660 rank= 0.0530 kl= 1.3782\n",
      "Val loss epoch 1000: reg= 3.7842 rank= 0.2549 kl= 1.0053\n",
      "Regression R2 : -0.0456, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 10 (320개) ================\n",
      "y_train mean: 8.31104564666748, std: 1.2371116976602172\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.9504 rank= 0.0561 kl= 1.3418\n",
      "Val loss epoch 1000: reg= 4.2998 rank= 0.2801 kl= 0.9608\n",
      "Regression R2 : -0.1342, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 11 (352개) ================\n",
      "y_train mean: 8.34891414642334, std: 1.1911919216973876\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 2.0664 rank= 0.0607 kl= 1.3538\n",
      "Val loss epoch 1000: reg= 4.6925 rank= 0.2985 kl= 0.9830\n",
      "Regression R2 : -0.2161, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 12 (384개) ================\n",
      "y_train mean: 8.380805015563965, std: 1.1479208569390869\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 2.0267 rank= 0.0607 kl= 1.3567\n",
      "Val loss epoch 1000: reg= 4.6385 rank= 0.3025 kl= 0.9884\n",
      "Regression R2 : -0.2358, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 13 (416개) ================\n",
      "y_train mean: 8.393661499023438, std: 1.1138105492456054\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 2.2007 rank= 0.0607 kl= 1.3403\n",
      "Val loss epoch 1000: reg= 5.1700 rank= 0.3418 kl= 0.9722\n",
      "Regression R2 : -0.4462, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 14 (448개) ================\n",
      "y_train mean: 8.417241096496582, std: 1.0802058081491088\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 2.0804 rank= 0.0566 kl= 1.3157\n",
      "Val loss epoch 1000: reg= 5.6040 rank= 0.3922 kl= 0.9503\n",
      "Regression R2 : -0.5369, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 15 (480개) ================\n",
      "y_train mean: 8.42239761352539, std: 1.056656251416931\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 2.0298 rank= 0.0652 kl= 1.3186\n",
      "Val loss epoch 1000: reg= 5.8494 rank= 0.4042 kl= 0.9698\n",
      "Regression R2 : -0.5393, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 16 (512개) ================\n",
      "y_train mean: 8.432161331176758, std: 1.0280546049935912\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 2.0724 rank= 0.0619 kl= 1.2959\n",
      "Val loss epoch 1000: reg= 5.5820 rank= 0.4099 kl= 0.9697\n",
      "Regression R2 : -0.5725, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 17 (544개) ================\n",
      "y_train mean: 8.438331604003906, std: 1.0021368365151977\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.5114 rank= 0.0299 kl= 0.9677\n",
      "Val loss epoch 1000: reg= 4.3618 rank= 0.3931 kl= 0.6531\n",
      "Regression R2 : -0.2859, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 18 (576개) ================\n",
      "y_train mean: 8.440656661987305, std: 0.9844343762261963\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.0104 rank= 0.0341 kl= 0.9010\n",
      "Val loss epoch 1000: reg= 4.8670 rank= 0.4325 kl= 0.6188\n",
      "Regression R2 : -0.3835, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 19 (608개) ================\n",
      "y_train mean: 8.439393043518066, std: 0.9636954765184021\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.9360 rank= 0.0326 kl= 0.8643\n",
      "Val loss epoch 1000: reg= 3.7254 rank= 0.3316 kl= 0.6299\n",
      "Regression R2 : -0.1497, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 20 (640개) ================\n",
      "y_train mean: 8.438934326171875, std: 0.9442229966981507\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.9341 rank= 0.0321 kl= 0.8807\n",
      "Val loss epoch 1000: reg= 3.1896 rank= 0.2756 kl= 0.6485\n",
      "Regression R2 : -0.3364, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 21 (672개) ================\n",
      "y_train mean: 8.435043334960938, std: 0.9280036191804505\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.9333 rank= 0.0363 kl= 0.8747\n",
      "Val loss epoch 1000: reg= 5.4328 rank= 0.3536 kl= 0.6157\n",
      "Regression R2 : -0.4138, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 22 (704개) ================\n",
      "y_train mean: 8.430846214294434, std: 0.9152738551957703\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.0387 rank= 0.0385 kl= 0.8546\n",
      "Val loss epoch 1000: reg= 6.0137 rank= 0.3756 kl= 0.6053\n",
      "Regression R2 : -0.5554, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 23 (736개) ================\n",
      "y_train mean: 8.425525665283203, std: 0.9010264973504639\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.9620 rank= 0.0407 kl= 0.8674\n",
      "Val loss epoch 1000: reg= 6.1272 rank= 0.3745 kl= 0.6172\n",
      "Regression R2 : -0.6170, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 24 (768개) ================\n",
      "y_train mean: 8.417490005493164, std: 0.8880362014634705\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.9348 rank= 0.0411 kl= 0.8469\n",
      "Val loss epoch 1000: reg= 6.3040 rank= 0.3919 kl= 0.5977\n",
      "Regression R2 : -0.6991, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 25 (800개) ================\n",
      "y_train mean: 8.409908294677734, std: 0.8748461704118348\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.9519 rank= 0.0462 kl= 0.8354\n",
      "Val loss epoch 1000: reg= 6.4514 rank= 0.3952 kl= 0.5972\n",
      "Regression R2 : -0.7461, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 26 (832개) ================\n",
      "y_train mean: 8.397713661193848, std: 0.8672806124551392\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.9146 rank= 0.0426 kl= 0.8202\n",
      "Val loss epoch 1000: reg= 6.3746 rank= 0.4152 kl= 0.5902\n",
      "Regression R2 : -0.7495, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 27 (864개) ================\n",
      "y_train mean: 8.389406204223633, std: 0.85629142330719\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.9881 rank= 0.0496 kl= 0.8251\n",
      "Val loss epoch 1000: reg= 6.3869 rank= 0.4181 kl= 0.5917\n",
      "Regression R2 : -0.7735, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 28 (896개) ================\n",
      "y_train mean: 8.378692626953125, std: 0.8477002482278443\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.9183 rank= 0.0447 kl= 0.8353\n",
      "Val loss epoch 1000: reg= 5.9752 rank= 0.4008 kl= 0.6041\n",
      "Regression R2 : -0.7165, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 29 (928개) ================\n",
      "y_train mean: 8.359698295593262, std: 0.8461181025369263\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.9305 rank= 0.0553 kl= 0.8230\n",
      "Val loss epoch 1000: reg= 5.6485 rank= 0.3665 kl= 0.5963\n",
      "Regression R2 : -0.7335, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 30 (960개) ================\n",
      "y_train mean: 8.350098609924316, std: 0.8361942868096924\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.9055 rank= 0.0504 kl= 0.8112\n",
      "Val loss epoch 1000: reg= 5.7021 rank= 0.3701 kl= 0.5891\n",
      "Regression R2 : -0.7794, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 31 (992개) ================\n",
      "y_train mean: 8.341546058654785, std: 0.826367209420929\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.8765 rank= 0.0495 kl= 0.8126\n",
      "Val loss epoch 1000: reg= 5.7138 rank= 0.3709 kl= 0.5916\n",
      "Regression R2 : -0.7572, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 32 (1024개) ================\n",
      "y_train mean: 8.32485580444336, std: 0.8237001995904542\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.8602 rank= 0.0525 kl= 0.7985\n",
      "Val loss epoch 1000: reg= 5.4216 rank= 0.3839 kl= 0.5935\n",
      "Regression R2 : -0.7498, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 33 (1056개) ================\n",
      "y_train mean: 8.308469772338867, std: 0.820919702516327\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.7587 rank= 0.0242 kl= 0.6723\n",
      "Val loss epoch 1000: reg= 5.0533 rank= 0.4269 kl= 0.4622\n",
      "Regression R2 : -0.6419, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 34 (1088개) ================\n",
      "y_train mean: 8.290059089660645, std: 0.8260223369462586\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.6253 rank= 0.0248 kl= 0.6259\n",
      "Val loss epoch 1000: reg= 4.6709 rank= 0.4232 kl= 0.4484\n",
      "Regression R2 : -0.5330, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 35 (1120개) ================\n",
      "y_train mean: 8.272704124450684, std: 0.824144731031189\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.5445 rank= 0.0311 kl= 0.6208\n",
      "Val loss epoch 1000: reg= 4.2994 rank= 0.3599 kl= 0.4464\n",
      "Regression R2 : -0.5298, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 36 (1152개) ================\n",
      "y_train mean: 8.256757736206055, std: 0.8210433225495911\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.5824 rank= 0.0318 kl= 0.5996\n",
      "Val loss epoch 1000: reg= 3.5110 rank= 0.3094 kl= 0.4218\n",
      "Regression R2 : -0.5835, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 37 (1184개) ================\n",
      "y_train mean: 8.240466117858887, std: 0.8190955023629761\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.5593 rank= 0.0361 kl= 0.5915\n",
      "Val loss epoch 1000: reg= 5.1551 rank= 0.3505 kl= 0.4174\n",
      "Regression R2 : -0.5365, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 38 (1216개) ================\n",
      "y_train mean: 8.223814010620117, std: 0.8171032767160035\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.5489 rank= 0.0330 kl= 0.5719\n",
      "Val loss epoch 1000: reg= 3.9744 rank= 0.2611 kl= 0.4123\n",
      "Regression R2 : -0.2121, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 39 (1248개) ================\n",
      "y_train mean: 8.217741966247559, std: 0.8193869690759278\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.5658 rank= 0.0344 kl= 0.5728\n",
      "Val loss epoch 1000: reg= 1.8930 rank= 0.1416 kl= 0.4369\n",
      "Regression R2 : 0.4214, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 1248\n",
      "총 측정 시간: 330.46 초\n",
      "=============================================\n",
      "########## 실험 103/600 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 32, T_mc: 20, sampling_seed: 2002\n",
      "초기 랜덤 선택 샘플 인덱스: [ 136  395  402  409  532  561  564  638  885  923 1182 1208 1321 1525\n",
      " 1897 1919 1934 2197 2255 2440 2570 3093 3129 3291 3299 3334 3343 3425\n",
      " 3477 3495 3560 3626]\n",
      "=============== 측정 Phase 1 (32개) ================\n",
      "y_train mean: 6.467774868011475, std: 1.6519296269281005\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.2175 rank= 0.0208 kl= 1.3056\n",
      "Val loss epoch 1000: reg= 1.7853 rank= 0.1987 kl= 1.2723\n",
      "Regression R2 : 0.3615, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (64개) ================\n",
      "y_train mean: 7.314189910888672, std: 1.5560612778527831\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.6855 rank= 0.0156 kl= 1.3985\n",
      "Val loss epoch 1000: reg= 1.9594 rank= 0.2181 kl= 1.1688\n",
      "Regression R2 : 0.3307, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 (96개) ================\n",
      "y_train mean: 7.318471431732178, std: 1.4308163027627563\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.2380 rank= 0.0285 kl= 1.4202\n",
      "Val loss epoch 1000: reg= 2.6691 rank= 0.1551 kl= 1.1436\n",
      "Regression R2 : 0.2329, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 4 (128개) ================\n",
      "y_train mean: 7.571151256561279, std: 1.3455331425531005\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.6013 rank= 0.0334 kl= 1.4763\n",
      "Val loss epoch 1000: reg= 2.4390 rank= 0.1397 kl= 1.1301\n",
      "Regression R2 : 0.1628, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 5 (160개) ================\n",
      "y_train mean: 7.792524814605713, std: 1.3041924338204955\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.3428 rank= 0.0385 kl= 1.5020\n",
      "Val loss epoch 1000: reg= 2.7438 rank= 0.1932 kl= 1.1519\n",
      "Regression R2 : 0.2248, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 6 (192개) ================\n",
      "y_train mean: 7.923684597015381, std: 1.2807998757226562\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.4787 rank= 0.0408 kl= 1.5059\n",
      "Val loss epoch 1000: reg= 2.8002 rank= 0.2090 kl= 1.1601\n",
      "Regression R2 : 0.2085, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 7 (224개) ================\n",
      "y_train mean: 7.984576225280762, std: 1.231515894399414\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.5132 rank= 0.0387 kl= 1.4639\n",
      "Val loss epoch 1000: reg= 3.0975 rank= 0.2521 kl= 1.1351\n",
      "Regression R2 : 0.1436, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 8 (256개) ================\n",
      "y_train mean: 8.040132522583008, std: 1.1989028553826904\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.6207 rank= 0.0452 kl= 1.4211\n",
      "Val loss epoch 1000: reg= 3.2085 rank= 0.2464 kl= 1.0713\n",
      "Regression R2 : 0.1193, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 9 (288개) ================\n",
      "y_train mean: 8.11606216430664, std: 1.155423889623413\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.6439 rank= 0.0536 kl= 1.4230\n",
      "Val loss epoch 1000: reg= 3.1781 rank= 0.2545 kl= 1.0794\n",
      "Regression R2 : 0.1372, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 10 (320개) ================\n",
      "y_train mean: 8.182672500610352, std: 1.1181676487786865\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.6482 rank= 0.0532 kl= 1.3922\n",
      "Val loss epoch 1000: reg= 3.4420 rank= 0.2934 kl= 1.0419\n",
      "Regression R2 : 0.0945, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 11 (352개) ================\n",
      "y_train mean: 8.217902183532715, std: 1.0868289570672607\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.7173 rank= 0.0581 kl= 1.3763\n",
      "Val loss epoch 1000: reg= 3.9747 rank= 0.3358 kl= 1.0153\n",
      "Regression R2 : -0.0230, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 12 (384개) ================\n",
      "y_train mean: 8.257418632507324, std: 1.0571316580636596\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.7247 rank= 0.0574 kl= 1.3379\n",
      "Val loss epoch 1000: reg= 3.8987 rank= 0.3305 kl= 0.9783\n",
      "Regression R2 : -0.0298, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 13 (416개) ================\n",
      "y_train mean: 8.293831825256348, std: 1.0293389658792114\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.6982 rank= 0.0542 kl= 1.3424\n",
      "Val loss epoch 1000: reg= 3.8010 rank= 0.3278 kl= 0.9814\n",
      "Regression R2 : -0.1214, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 14 (448개) ================\n",
      "y_train mean: 8.327610969543457, std: 1.0038298468453979\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.7557 rank= 0.0577 kl= 1.3542\n",
      "Val loss epoch 1000: reg= 4.2876 rank= 0.3769 kl= 0.9862\n",
      "Regression R2 : -0.2065, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 15 (480개) ================\n",
      "y_train mean: 8.34910774230957, std: 0.9797754387719727\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.7996 rank= 0.0592 kl= 1.3189\n",
      "Val loss epoch 1000: reg= 4.4398 rank= 0.3851 kl= 0.9928\n",
      "Regression R2 : -0.2118, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 16 (512개) ================\n",
      "y_train mean: 8.36561107635498, std: 0.9563317398889161\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.8521 rank= 0.0635 kl= 1.3133\n",
      "Val loss epoch 1000: reg= 4.3181 rank= 0.3833 kl= 0.9933\n",
      "Regression R2 : -0.2919, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 17 (544개) ================\n",
      "y_train mean: 8.383831977844238, std: 0.9351777534349061\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.2893 rank= 0.0225 kl= 0.9422\n",
      "Val loss epoch 1000: reg= 3.4778 rank= 0.3547 kl= 0.6928\n",
      "Regression R2 : -0.0166, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 18 (576개) ================\n",
      "y_train mean: 8.386425018310547, std: 0.9196121792657471\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 1.0267 rank= 0.0249 kl= 0.9542\n",
      "Val loss epoch 1000: reg= 3.4330 rank= 0.3246 kl= 0.6900\n",
      "Regression R2 : -0.0632, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 19 (608개) ================\n",
      "y_train mean: 8.393406867980957, std: 0.9036954145295716\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.9391 rank= 0.0329 kl= 0.8827\n",
      "Val loss epoch 1000: reg= 2.7334 rank= 0.2616 kl= 0.6785\n",
      "Regression R2 : 0.1660, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 20 (640개) ================\n",
      "y_train mean: 8.389994621276855, std: 0.8904070954187012\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.9222 rank= 0.0335 kl= 0.8703\n",
      "Val loss epoch 1000: reg= 2.5981 rank= 0.2816 kl= 0.6754\n",
      "Regression R2 : 0.0268, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 21 (672개) ================\n",
      "y_train mean: 8.395675659179688, std: 0.8744861583573914\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.9599 rank= 0.0346 kl= 0.8630\n",
      "Val loss epoch 1000: reg= 4.4550 rank= 0.3525 kl= 0.6224\n",
      "Regression R2 : -0.1328, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 22 (704개) ================\n",
      "y_train mean: 8.398273468017578, std: 0.8577706317765809\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.9948 rank= 0.0375 kl= 0.8468\n",
      "Val loss epoch 1000: reg= 5.2632 rank= 0.3879 kl= 0.6116\n",
      "Regression R2 : -0.3135, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 23 (736개) ================\n",
      "y_train mean: 8.401473045349121, std: 0.8413896660668946\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.9854 rank= 0.0420 kl= 0.8447\n",
      "Val loss epoch 1000: reg= 5.6711 rank= 0.4170 kl= 0.6059\n",
      "Regression R2 : -0.4423, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 24 (768개) ================\n",
      "y_train mean: 8.39442253112793, std: 0.8322846393449402\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.9789 rank= 0.0392 kl= 0.8389\n",
      "Val loss epoch 1000: reg= 5.9313 rank= 0.4246 kl= 0.6055\n",
      "Regression R2 : -0.5408, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 25 (800개) ================\n",
      "y_train mean: 8.384098052978516, std: 0.8258078794343567\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.9315 rank= 0.0407 kl= 0.8063\n",
      "Val loss epoch 1000: reg= 5.9808 rank= 0.4073 kl= 0.5882\n",
      "Regression R2 : -0.5590, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 26 (832개) ================\n",
      "y_train mean: 8.375675201416016, std: 0.8170161943299866\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.9188 rank= 0.0425 kl= 0.8020\n",
      "Val loss epoch 1000: reg= 5.8871 rank= 0.4292 kl= 0.5832\n",
      "Regression R2 : -0.5647, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 27 (864개) ================\n",
      "y_train mean: 8.366029739379883, std: 0.8074863653047181\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.9248 rank= 0.0456 kl= 0.8120\n",
      "Val loss epoch 1000: reg= 6.0082 rank= 0.4158 kl= 0.5885\n",
      "Regression R2 : -0.6279, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 28 (896개) ================\n",
      "y_train mean: 8.359831809997559, std: 0.7981279592378235\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.9619 rank= 0.0442 kl= 0.7985\n",
      "Val loss epoch 1000: reg= 5.8698 rank= 0.4192 kl= 0.5906\n",
      "Regression R2 : -0.6391, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 29 (928개) ================\n",
      "y_train mean: 8.347970008850098, std: 0.7922735910279847\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.8985 rank= 0.0529 kl= 0.8024\n",
      "Val loss epoch 1000: reg= 5.3823 rank= 0.3789 kl= 0.5949\n",
      "Regression R2 : -0.6093, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 30 (960개) ================\n",
      "y_train mean: 8.334609985351562, std: 0.7870580057962037\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.8946 rank= 0.0475 kl= 0.8027\n",
      "Val loss epoch 1000: reg= 5.1671 rank= 0.3758 kl= 0.6003\n",
      "Regression R2 : -0.6440, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 31 (992개) ================\n",
      "y_train mean: 8.323140144348145, std: 0.7809215884072876\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.8658 rank= 0.0479 kl= 0.7998\n",
      "Val loss epoch 1000: reg= 5.6882 rank= 0.3997 kl= 0.5942\n",
      "Regression R2 : -0.6837, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 32 (1024개) ================\n",
      "y_train mean: 8.314041137695312, std: 0.7734216551644898\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.8844 rank= 0.0534 kl= 0.7901\n",
      "Val loss epoch 1000: reg= 5.2936 rank= 0.4056 kl= 0.5971\n",
      "Regression R2 : -0.6810, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 33 (1056개) ================\n",
      "y_train mean: 8.300677299499512, std: 0.7701497177941895\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.6979 rank= 0.0230 kl= 0.6809\n",
      "Val loss epoch 1000: reg= 4.4521 rank= 0.4012 kl= 0.4850\n",
      "Regression R2 : -0.4557, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 34 (1088개) ================\n",
      "y_train mean: 8.287508010864258, std: 0.7648895482881165\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.5457 rank= 0.0238 kl= 0.6089\n",
      "Val loss epoch 1000: reg= 4.5504 rank= 0.4226 kl= 0.4454\n",
      "Regression R2 : -0.5312, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 35 (1120개) ================\n",
      "y_train mean: 8.271135330200195, std: 0.7653523783547974\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n",
      "Train loss epoch 1000 : reg= 0.4938 rank= 0.0275 kl= 0.6046\n",
      "Val loss epoch 1000: reg= 4.0602 rank= 0.3732 kl= 0.4498\n",
      "Regression R2 : -0.5654, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 36 (1152개) ================\n",
      "y_train mean: 8.254817008972168, std: 0.7650625805718995\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=1e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[215], line 138\u001b[0m\n\u001b[1;32m    135\u001b[0m vae_cost_model, optimizer, config \u001b[38;5;241m=\u001b[39m make_vae_reg_model(vae, hyperparameter, input_dim, latent_dim, hidden_dim, y_std, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    137\u001b[0m seed_everything(train_seed)\n\u001b[0;32m--> 138\u001b[0m vae_cost_model, topk_recall_signal, val_reg_r2, val_rank_r2 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvae_cost_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m                                                                               \u001b[49m\u001b[43minput_data_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcosts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyper_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwarmup_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m reg_history\u001b[38;5;241m.\u001b[39mappend(val_reg_r2)\n\u001b[1;32m    142\u001b[0m rank_history\u001b[38;5;241m.\u001b[39mappend(val_rank_r2)\n",
      "Cell \u001b[0;32mIn[81], line 11\u001b[0m, in \u001b[0;36mtrain_regression\u001b[0;34m(vae_cost_model, optimizer, train_loader, val_loader, input_data_scaled, costs, config, top_k, use_rank, warmup_epochs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     10\u001b[0m     vae_cost_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x_batch, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     12\u001b[0m         x_batch \u001b[38;5;241m=\u001b[39m x_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/work/tenset/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/work/tenset/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/work/tenset/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/work/tenset/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[32], line 31\u001b[0m, in \u001b[0;36mFeatureRegressionDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[idx]\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX[idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature[idx]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "# 데이터셋 길이만큼의 인덱스 numpy 배열 생성\n",
    "all_indices = np.arange(len(input_data_scaled))\n",
    "costs = np.array(records[\"costs\"], dtype=np.float32)\n",
    "\n",
    "real_optimum_index = np.argmax(costs)\n",
    "\n",
    "top_k = 1\n",
    "\n",
    "train_seed = 2023\n",
    "\n",
    "\n",
    "sampling_hyper = {\n",
    "    \"measure_size\": [32, 48],\n",
    "    \"weight\" : [\n",
    "            # (1.0, 0.0, 0.0),\n",
    "            # (0.7, 0.0, 0.3),\n",
    "            # (0.7, 0.3, 0.0),\n",
    "            # (0.6, 0.1, 0.3),\n",
    "            # (0.3, 0.4, 0.3),\n",
    "            (0.4, 0.3, 0.3),\n",
    "            # (0.3, 0.3, 0.4),\n",
    "            # (0.5, 0.2, 0.3),\n",
    "            ],\n",
    "    \n",
    "    \"uncertainty_topk\": [0, 64],    # 몇 개부터 불확실성 기반 선택을 할지\n",
    "    # \"weight\" : f_weights,\n",
    "    \"grad_num\": [2, 4],\n",
    "    \"rand_num\": [0],\n",
    "    \n",
    "    \"T_mc\": [20],\n",
    "    # \"encoder_freeze\": [False, True],\n",
    "    \"encoder_freeze\": [False],\n",
    "    \"scratch\": [False],\n",
    "    \"encoder_lr\": [1e-5],\n",
    "    \"cost_predictor_lr\": [1e-4, 1e-5],\n",
    "    \"warmup_epochs\" : [100, 200, 300, 400, 500],\n",
    "\n",
    "    \"sampling_seed\" : range(2000, 2020),\n",
    "    # \"seed\" : [2023,2025],\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "       \n",
    "now = datetime.datetime.now().strftime(\"%m%d_%H%M\")\n",
    "filename = f\"result/{os.path.basename(json_file)}/vae_extent_{now}.csv\"\n",
    "\n",
    "# 아래 경로의 CSV 파일에 이미 측정된 설정들은 건너뛰기\n",
    "total_csv_path = f\"result/{os.path.basename(json_file)}/hyper/vae_extent_1220_2013.csv\"\n",
    "to_measure_configs = filter_already_measured(total_csv_path, sampling_hyper)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"{len(list(itertools.product(*sampling_hyper.values())))} -> {len(to_measure_configs)}개의 실험 남음\")\n",
    "\n",
    "random_indices_list = []\n",
    "all_results = []\n",
    "\n",
    "cnt = 0\n",
    "for hyper_config in to_measure_configs:\n",
    "\n",
    "    # hyper_config = dict(zip(sampling_hyper.keys(), params))\n",
    "\n",
    "    cnt += 1\n",
    "    print(f\"########## 실험 {cnt}/{len(to_measure_configs)} ##########\")\n",
    "\n",
    "    tic = time.time()\n",
    "    # used_indices : 이미 측정된 인덱스 집합. train_indices와 동일\n",
    "    # remaining_indices : 아직 측정되지 않은 인덱스 집합. val_indices와 동일\n",
    "    used_indices = set()\n",
    "    remaining_indices = set(all_indices)\n",
    "    \n",
    "    measure_size = hyper_config[\"measure_size\"]\n",
    "    sampling_seed = hyper_config[\"sampling_seed\"]\n",
    "    w_cost, w_unc, w_div = hyper_config[\"weight\"]\n",
    "    print(f\"weights: {hyper_config['weight']}\")\n",
    "    print(f\"measure_size: {hyper_config['measure_size']}, T_mc: {hyper_config['T_mc']}, sampling_seed: {hyper_config['sampling_seed']}\")\n",
    "\n",
    "    sampling_rng = np.random.default_rng(sampling_seed)\n",
    "\n",
    "    hyperparameter = {\n",
    "\n",
    "        'lambda_reg' : 0.01,\n",
    "        'lambda_pair': 3.0,\n",
    "        'margin_scale': 0.3,\n",
    "        'gamma': 0.01,\n",
    "        'beta': 0.01,\n",
    "        'noise_std': 0.001,\n",
    "\n",
    "        'encoder_lr': hyper_config[\"encoder_lr\"],\n",
    "        'encoder_freeze' : hyper_config[\"encoder_freeze\"],\n",
    "        'scratch': hyper_config[\"scratch\"],\n",
    "        'feature_predictor_lr': 0,\n",
    "        'cost_predictor_lr': hyper_config[\"cost_predictor_lr\"],\n",
    "        'epochs': 1000,\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    seed_everything(sampling_seed)\n",
    "\n",
    "    init_indices, remaining_indices = random_select_indices(remaining_indices, select_size=sampling_hyper[\"measure_size\"][0], rng=sampling_rng)\n",
    "    random_indices_list.append(init_indices)\n",
    "\n",
    "    # random_num = int(sampling_hyper[\"measure_size\"][0] * (3/4))\n",
    "    # diverse_num = int(sampling_hyper[\"measure_size\"][0] - random_num)\n",
    "    # random_indices, remaining_indices = random_select_indices(remaining_indices, select_size=random_num, rng=sampling_rng)\n",
    "    # diverse_indices, remaining_indices = select_init_latent_diversity(vae, input_data_scaled, remaining_indices, diverse_num, device)\n",
    "    # init_indices = np.concatenate([random_indices, diverse_indices])\n",
    "\n",
    "    # init_indices, remaining_indices = select_representative_kmeans(vae, input_data_scaled, remaining_indices, sampling_hyper[\"measure_size\"][0], device, iters=5)\n",
    "\n",
    "\n",
    "    print(f\"초기 랜덤 선택 샘플 인덱스: {np.sort(init_indices)}\")\n",
    "    used_indices.update(init_indices)\n",
    "    \n",
    "\n",
    "    reg_history = []\n",
    "    rank_history = []\n",
    "\n",
    "    for phase in range(1, len(input_data_scaled) // measure_size + 1):\n",
    "\n",
    "        print(f\"=============== 측정 Phase {phase} ({len(used_indices)}개) ================\")\n",
    "\n",
    "\n",
    "        # DataLoader 갱신\n",
    "        seed_everything(train_seed)\n",
    "        train_loader, val_loader, y_mean, y_std = make_vae_reg_dataloaders(input_data_scaled, costs, used_indices, remaining_indices)\n",
    "\n",
    "        \n",
    "        vae_cost_model, optimizer, config = make_vae_reg_model(vae, hyperparameter, input_dim, latent_dim, hidden_dim, y_std, verbose=False)\n",
    "        \n",
    "        seed_everything(train_seed)\n",
    "        vae_cost_model, topk_recall_signal, val_reg_r2, val_rank_r2 = train_regression(vae_cost_model, optimizer, train_loader, val_loader, \n",
    "                                                                                       input_data_scaled, costs, config, top_k=top_k, use_rank=False, warmup_epochs=hyper_config[\"warmup_epochs\"])\n",
    "\n",
    "        reg_history.append(val_reg_r2)\n",
    "        rank_history.append(val_rank_r2)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # 다음 측정할 샘플 선택\n",
    "        selected_indices, remaining_indices = select_programs(\n",
    "            model=vae_cost_model,\n",
    "            input_data_scaled=input_data_scaled,\n",
    "            remaining_indices=remaining_indices,\n",
    "            used_indices=used_indices,\n",
    "            num_select=measure_size,\n",
    "            T_mc=hyper_config[\"T_mc\"],\n",
    "            w_cost=w_cost,\n",
    "            w_unc=w_unc,\n",
    "            w_div=w_div,\n",
    "            # w_cost=0.3,\n",
    "            # w_unc=0.35,\n",
    "            # w_div=0.35,\n",
    "            uncertainty_topk=hyper_config[\"uncertainty_topk\"],\n",
    "            grad_num=hyper_config[\"grad_num\"],\n",
    "            rand_num=hyper_config[\"rand_num\"],\n",
    "            device=device,\n",
    "            rng=sampling_rng,\n",
    "            \n",
    "            topk_factor=5\n",
    "        )\n",
    "        # w_cost += 0.03\n",
    "        # w_unc -= 0.02\n",
    "        # w_div -= 0.01\n",
    "\n",
    "        # selected_indices: numpy 배열\n",
    "        used_indices.update(selected_indices.tolist())\n",
    "\n",
    "        measured_optimum = True if real_optimum_index in used_indices else False\n",
    "\n",
    "\n",
    "        use_topk = False\n",
    "        \n",
    "\n",
    "        break_signal = False\n",
    "        if not use_topk and measured_optimum:\n",
    "            break_signal = True\n",
    "        elif use_topk and topk_recall_signal:\n",
    "            break_signal = True\n",
    "            filename= filename.replace(\"result/\", \"result_topk/\")\n",
    "\n",
    "\n",
    "        if break_signal:\n",
    "            print(\"최적화 종료\")\n",
    "            print(\"학습한 데이터 수 :\", len(used_indices)-measure_size)\n",
    "            used_time = time.time() - tic\n",
    "            print(f\"총 측정 시간: {used_time:.2f} 초\")\n",
    "            print(\"=============================================\")\n",
    "            all_results.append({\n",
    "                \"measure_size\": measure_size,\n",
    "                # \"scratch\": hyper_config[\"scratch\"],\n",
    "                # \"encoder_freeze\": hyper_config[\"encoder_freeze\"],\n",
    "                \"encoder_lr\": hyper_config[\"encoder_lr\"],\n",
    "                \"cost_predictor_lr\": hyper_config[\"cost_predictor_lr\"],\n",
    "                \"rank_warmup_epochs\": hyper_config[\"warmup_epochs\"],\n",
    "                \n",
    "                \"weights\": hyper_config[\"weight\"],\n",
    "                \"uncertainty_topk\": hyper_config[\"uncertainty_topk\"],\n",
    "                \"grad_num\": hyper_config[\"grad_num\"],\n",
    "                \"rand_num\": hyper_config[\"rand_num\"],\n",
    "                \"phase\" : phase,\n",
    "                \"used_time\": round(used_time, 2),\n",
    "                \"train_size\" : len(used_indices)-measure_size,\n",
    "                f\"top-{top_k}\" : topk_recall_signal,\n",
    "                \"val_reg_r2\": reg_history,\n",
    "                \"val_rank_r2\": rank_history,\n",
    "                \"sampling_seed\": sampling_seed,\n",
    "                \n",
    "                \n",
    "            })\n",
    "            if use_topk:\n",
    "                all_results[-1][\"top_k\"] = top_k\n",
    "\n",
    "            df_results = pd.DataFrame(all_results)\n",
    "            \n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            df_results.to_csv(filename, index=False)\n",
    "            \n",
    "            break\n",
    "\n",
    "if len(all_results) > 0:\n",
    "    save_avg_csv(df_results, filename, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84b5dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure_size</th>\n",
       "      <th>encoder_lr</th>\n",
       "      <th>cost_predictor_lr</th>\n",
       "      <th>weights</th>\n",
       "      <th>uncertainty_topk</th>\n",
       "      <th>grad_num</th>\n",
       "      <th>rand_num</th>\n",
       "      <th>phase</th>\n",
       "      <th>used_time</th>\n",
       "      <th>train_size</th>\n",
       "      <th>val_reg_r2</th>\n",
       "      <th>sampling_seed</th>\n",
       "      <th>rank_warmup_epochs</th>\n",
       "      <th>top-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.44</td>\n",
       "      <td>64</td>\n",
       "      <td>[0.8153]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.44</td>\n",
       "      <td>64</td>\n",
       "      <td>[0.713]</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.41</td>\n",
       "      <td>64</td>\n",
       "      <td>[0.7737]</td>\n",
       "      <td>2002</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11.19</td>\n",
       "      <td>192</td>\n",
       "      <td>[0.7897, 0.7438, 0.8066]</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10.98</td>\n",
       "      <td>192</td>\n",
       "      <td>[0.7887, 0.8399, 0.8845]</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11.01</td>\n",
       "      <td>192</td>\n",
       "      <td>[0.6095, 0.4503, 0.42]</td>\n",
       "      <td>2015</td>\n",
       "      <td>200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7.07</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.5551, 0.3795]</td>\n",
       "      <td>2016</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.40</td>\n",
       "      <td>64</td>\n",
       "      <td>[0.6697]</td>\n",
       "      <td>2017</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7.33</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.5625, 0.4166]</td>\n",
       "      <td>2018</td>\n",
       "      <td>200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.40</td>\n",
       "      <td>64</td>\n",
       "      <td>[0.5933]</td>\n",
       "      <td>2019</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>810 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     measure_size  encoder_lr  cost_predictor_lr          weights  \\\n",
       "0              64      0.0001            0.01000  (0.4, 0.3, 0.3)   \n",
       "1              64      0.0001            0.01000  (0.4, 0.3, 0.3)   \n",
       "2              64      0.0001            0.01000  (0.4, 0.3, 0.3)   \n",
       "3              64      0.0001            0.01000  (0.4, 0.3, 0.3)   \n",
       "4              64      0.0001            0.01000  (0.4, 0.3, 0.3)   \n",
       "..            ...         ...                ...              ...   \n",
       "805            64      0.0001            0.00001  (0.4, 0.3, 0.3)   \n",
       "806            64      0.0001            0.00001  (0.4, 0.3, 0.3)   \n",
       "807            64      0.0001            0.00001  (0.4, 0.3, 0.3)   \n",
       "808            64      0.0001            0.00001  (0.4, 0.3, 0.3)   \n",
       "809            64      0.0001            0.00001  (0.4, 0.3, 0.3)   \n",
       "\n",
       "     uncertainty_topk  grad_num  rand_num  phase  used_time  train_size  \\\n",
       "0                  64         4         0      1       3.44          64   \n",
       "1                  64         4         0      1       3.44          64   \n",
       "2                  64         4         0      1       3.41          64   \n",
       "3                  64         4         0      3      11.19         192   \n",
       "4                  64         4         0      3      10.98         192   \n",
       "..                ...       ...       ...    ...        ...         ...   \n",
       "805                64         4         0      3      11.01         192   \n",
       "806                64         4         0      2       7.07         128   \n",
       "807                64         4         0      1       3.40          64   \n",
       "808                64         4         0      2       7.33         128   \n",
       "809                64         4         0      1       3.40          64   \n",
       "\n",
       "                   val_reg_r2  sampling_seed  rank_warmup_epochs  top-1  \n",
       "0                    [0.8153]           2000                   0    NaN  \n",
       "1                     [0.713]           2001                   0    NaN  \n",
       "2                    [0.7737]           2002                   0    NaN  \n",
       "3    [0.7897, 0.7438, 0.8066]           2003                   0    NaN  \n",
       "4    [0.7887, 0.8399, 0.8845]           2004                   0    NaN  \n",
       "..                        ...            ...                 ...    ...  \n",
       "805    [0.6095, 0.4503, 0.42]           2015                 200    1.0  \n",
       "806          [0.5551, 0.3795]           2016                 200    0.0  \n",
       "807                  [0.6697]           2017                 200    0.0  \n",
       "808          [0.5625, 0.4166]           2018                 200    1.0  \n",
       "809                  [0.5933]           2019                 200    0.0  \n",
       "\n",
       "[810 rows x 14 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "csv_dir = glob(\"/root/work/tenset/scripts/pre_experiments/model_myself/result/([0c9a5ba46ffc5e1a9e5641018527117f,4,7,7,160,1,1,160,960,1,1,1,960,4,7,7,960],cuda).json/vae_extent_*.csv\")\n",
    "csv_dir = [p for p in csv_dir if not p.endswith(\"avg.csv\")]\n",
    "\n",
    "dfs = []\n",
    "for p in csv_dir:\n",
    "    sub_df = pd.read_csv(p)\n",
    "\n",
    "    if \"rank_warmup_epochs\" not in sub_df.columns:\n",
    "        sub_df[\"rank_warmup_epochs\"] = 0\n",
    "    if \"measure_size\" not in sub_df.columns:\n",
    "        sub_df[\"measure_size\"] = 64\n",
    "    \n",
    "    dfs.append(sub_df)\n",
    "\n",
    "    \n",
    "df_total = pd.concat(dfs, ignore_index=True)\n",
    "df_total.drop(columns=[\"scratch\", \"encoder_freeze\", \"val_rank_r2\"], inplace=True)\n",
    "\n",
    "# measure_size 컬럼을 맨 앞으로 이동\n",
    "cols = df_total.columns.tolist()\n",
    "df_total = df_total[[\"measure_size\"] + [c for c in cols if c != \"measure_size\"]]\n",
    "\n",
    "df_total.to_csv(os.path.dirname(filename)+\"/vae_extent_total.csv\", index=False)\n",
    "df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "7582a685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure_size</th>\n",
       "      <th>encoder_lr</th>\n",
       "      <th>cost_predictor_lr</th>\n",
       "      <th>rank_warmup_epochs</th>\n",
       "      <th>weights</th>\n",
       "      <th>uncertainty_topk</th>\n",
       "      <th>grad_num</th>\n",
       "      <th>rand_num</th>\n",
       "      <th>phase</th>\n",
       "      <th>train_size</th>\n",
       "      <th>used_time</th>\n",
       "      <th>val_reg_r2</th>\n",
       "      <th>seed_n</th>\n",
       "      <th>sampling_seed</th>\n",
       "      <th>top-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>163.200000</td>\n",
       "      <td>9.323500</td>\n",
       "      <td>[-10.9241, -10.8547, -10.8065]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>124.800000</td>\n",
       "      <td>6.986500</td>\n",
       "      <td>[-8.103, -8.1111]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>108.800000</td>\n",
       "      <td>6.079500</td>\n",
       "      <td>[0.7073]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>7.318500</td>\n",
       "      <td>[0.7496, 0.7937]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.450000</td>\n",
       "      <td>156.800000</td>\n",
       "      <td>9.345500</td>\n",
       "      <td>[0.8061, 0.794]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>102.400000</td>\n",
       "      <td>5.899500</td>\n",
       "      <td>[-7.0339, -7.3333]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.433333</td>\n",
       "      <td>91.733333</td>\n",
       "      <td>5.152333</td>\n",
       "      <td>[0.6151]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>100</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>4.396500</td>\n",
       "      <td>[0.6717]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>200</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>83.200000</td>\n",
       "      <td>4.565500</td>\n",
       "      <td>[0.6877]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>300</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>86.400000</td>\n",
       "      <td>4.747000</td>\n",
       "      <td>[0.6945]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>400</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>4.433500</td>\n",
       "      <td>[0.697]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>500</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>83.200000</td>\n",
       "      <td>4.647500</td>\n",
       "      <td>[0.7001]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>89.600000</td>\n",
       "      <td>5.066800</td>\n",
       "      <td>[0.6977]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>100</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>86.400000</td>\n",
       "      <td>4.804000</td>\n",
       "      <td>[0.6966]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>200</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>83.200000</td>\n",
       "      <td>4.555000</td>\n",
       "      <td>[0.7004]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>300</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>4.337500</td>\n",
       "      <td>[0.6978]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>400</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>83.200000</td>\n",
       "      <td>4.848000</td>\n",
       "      <td>[0.6965]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>500</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>4.420000</td>\n",
       "      <td>[0.6931]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>121.600000</td>\n",
       "      <td>7.096500</td>\n",
       "      <td>[0.7552, 0.7861]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.650000</td>\n",
       "      <td>169.600000</td>\n",
       "      <td>10.196500</td>\n",
       "      <td>[0.7916, 0.8094]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>6.441500</td>\n",
       "      <td>[-0.0166, -0.6613]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>5.296500</td>\n",
       "      <td>[0.621]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>100</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>89.600000</td>\n",
       "      <td>4.979000</td>\n",
       "      <td>[0.6387]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>200</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.466667</td>\n",
       "      <td>93.866667</td>\n",
       "      <td>5.210667</td>\n",
       "      <td>[0.6293]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>300</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>92.800000</td>\n",
       "      <td>5.122000</td>\n",
       "      <td>[0.6235]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>400</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>5.374000</td>\n",
       "      <td>[0.6248]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>500</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>102.400000</td>\n",
       "      <td>5.825500</td>\n",
       "      <td>[0.6256]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>87.040000</td>\n",
       "      <td>4.826600</td>\n",
       "      <td>[0.7052]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>100</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>89.600000</td>\n",
       "      <td>4.948500</td>\n",
       "      <td>[0.7117]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>200</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.550000</td>\n",
       "      <td>99.200000</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>[0.7106]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>300</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>89.600000</td>\n",
       "      <td>5.079500</td>\n",
       "      <td>[0.71]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>400</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>92.800000</td>\n",
       "      <td>5.442500</td>\n",
       "      <td>[0.7098]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>500</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.550000</td>\n",
       "      <td>99.200000</td>\n",
       "      <td>5.723000</td>\n",
       "      <td>[0.7091]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>102.400000</td>\n",
       "      <td>5.653000</td>\n",
       "      <td>[0.7838]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.050000</td>\n",
       "      <td>131.200000</td>\n",
       "      <td>7.462500</td>\n",
       "      <td>[0.8153]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    measure_size  encoder_lr  cost_predictor_lr  rank_warmup_epochs  \\\n",
       "0             64    0.000001           0.000001                   0   \n",
       "1             64    0.000001           0.000010                   0   \n",
       "2             64    0.000001           0.000100                   0   \n",
       "3             64    0.000001           0.001000                   0   \n",
       "4             64    0.000001           0.010000                   0   \n",
       "5             64    0.000010           0.000001                   0   \n",
       "6             64    0.000010           0.000010                   0   \n",
       "7             64    0.000010           0.000010                 100   \n",
       "8             64    0.000010           0.000010                 200   \n",
       "9             64    0.000010           0.000010                 300   \n",
       "10            64    0.000010           0.000010                 400   \n",
       "11            64    0.000010           0.000010                 500   \n",
       "12            64    0.000010           0.000100                   0   \n",
       "13            64    0.000010           0.000100                 100   \n",
       "14            64    0.000010           0.000100                 200   \n",
       "15            64    0.000010           0.000100                 300   \n",
       "16            64    0.000010           0.000100                 400   \n",
       "17            64    0.000010           0.000100                 500   \n",
       "18            64    0.000010           0.001000                   0   \n",
       "19            64    0.000010           0.010000                   0   \n",
       "20            64    0.000100           0.000001                   0   \n",
       "21            64    0.000100           0.000010                   0   \n",
       "22            64    0.000100           0.000010                 100   \n",
       "23            64    0.000100           0.000010                 200   \n",
       "24            64    0.000100           0.000010                 300   \n",
       "25            64    0.000100           0.000010                 400   \n",
       "26            64    0.000100           0.000010                 500   \n",
       "27            64    0.000100           0.000100                   0   \n",
       "28            64    0.000100           0.000100                 100   \n",
       "29            64    0.000100           0.000100                 200   \n",
       "30            64    0.000100           0.000100                 300   \n",
       "31            64    0.000100           0.000100                 400   \n",
       "32            64    0.000100           0.000100                 500   \n",
       "33            64    0.000100           0.001000                   0   \n",
       "34            64    0.000100           0.010000                   0   \n",
       "\n",
       "            weights  uncertainty_topk  grad_num  rand_num     phase  \\\n",
       "0   (0.4, 0.3, 0.3)                64         4         0  2.550000   \n",
       "1   (0.4, 0.3, 0.3)                64         4         0  1.950000   \n",
       "2   (0.4, 0.3, 0.3)                64         4         0  1.700000   \n",
       "3   (0.4, 0.3, 0.3)                64         4         0  2.000000   \n",
       "4   (0.4, 0.3, 0.3)                64         4         0  2.450000   \n",
       "5   (0.4, 0.3, 0.3)                64         4         0  1.600000   \n",
       "6   (0.4, 0.3, 0.3)                64         4         0  1.433333   \n",
       "7   (0.4, 0.3, 0.3)                64         4         0  1.250000   \n",
       "8   (0.4, 0.3, 0.3)                64         4         0  1.300000   \n",
       "9   (0.4, 0.3, 0.3)                64         4         0  1.350000   \n",
       "10  (0.4, 0.3, 0.3)                64         4         0  1.250000   \n",
       "11  (0.4, 0.3, 0.3)                64         4         0  1.300000   \n",
       "12  (0.4, 0.3, 0.3)                64         4         0  1.400000   \n",
       "13  (0.4, 0.3, 0.3)                64         4         0  1.350000   \n",
       "14  (0.4, 0.3, 0.3)                64         4         0  1.300000   \n",
       "15  (0.4, 0.3, 0.3)                64         4         0  1.250000   \n",
       "16  (0.4, 0.3, 0.3)                64         4         0  1.300000   \n",
       "17  (0.4, 0.3, 0.3)                64         4         0  1.250000   \n",
       "18  (0.4, 0.3, 0.3)                64         4         0  1.900000   \n",
       "19  (0.4, 0.3, 0.3)                64         4         0  2.650000   \n",
       "20  (0.4, 0.3, 0.3)                64         4         0  1.750000   \n",
       "21  (0.4, 0.3, 0.3)                64         4         0  1.500000   \n",
       "22  (0.4, 0.3, 0.3)                64         4         0  1.400000   \n",
       "23  (0.4, 0.3, 0.3)                64         4         0  1.466667   \n",
       "24  (0.4, 0.3, 0.3)                64         4         0  1.450000   \n",
       "25  (0.4, 0.3, 0.3)                64         4         0  1.500000   \n",
       "26  (0.4, 0.3, 0.3)                64         4         0  1.600000   \n",
       "27  (0.4, 0.3, 0.3)                64         4         0  1.360000   \n",
       "28  (0.4, 0.3, 0.3)                64         4         0  1.400000   \n",
       "29  (0.4, 0.3, 0.3)                64         4         0  1.550000   \n",
       "30  (0.4, 0.3, 0.3)                64         4         0  1.400000   \n",
       "31  (0.4, 0.3, 0.3)                64         4         0  1.450000   \n",
       "32  (0.4, 0.3, 0.3)                64         4         0  1.550000   \n",
       "33  (0.4, 0.3, 0.3)                64         4         0  1.600000   \n",
       "34  (0.4, 0.3, 0.3)                64         4         0  2.050000   \n",
       "\n",
       "    train_size  used_time                      val_reg_r2  seed_n  \\\n",
       "0   163.200000   9.323500  [-10.9241, -10.8547, -10.8065]      20   \n",
       "1   124.800000   6.986500               [-8.103, -8.1111]      20   \n",
       "2   108.800000   6.079500                        [0.7073]      20   \n",
       "3   128.000000   7.318500                [0.7496, 0.7937]      20   \n",
       "4   156.800000   9.345500                 [0.8061, 0.794]      20   \n",
       "5   102.400000   5.899500              [-7.0339, -7.3333]      20   \n",
       "6    91.733333   5.152333                        [0.6151]      20   \n",
       "7    80.000000   4.396500                        [0.6717]      20   \n",
       "8    83.200000   4.565500                        [0.6877]      20   \n",
       "9    86.400000   4.747000                        [0.6945]      20   \n",
       "10   80.000000   4.433500                         [0.697]      20   \n",
       "11   83.200000   4.647500                        [0.7001]      20   \n",
       "12   89.600000   5.066800                        [0.6977]      20   \n",
       "13   86.400000   4.804000                        [0.6966]      20   \n",
       "14   83.200000   4.555000                        [0.7004]      20   \n",
       "15   80.000000   4.337500                        [0.6978]      20   \n",
       "16   83.200000   4.848000                        [0.6965]      20   \n",
       "17   80.000000   4.420000                        [0.6931]      20   \n",
       "18  121.600000   7.096500                [0.7552, 0.7861]      20   \n",
       "19  169.600000  10.196500                [0.7916, 0.8094]      20   \n",
       "20  112.000000   6.441500              [-0.0166, -0.6613]      20   \n",
       "21   96.000000   5.296500                         [0.621]      20   \n",
       "22   89.600000   4.979000                        [0.6387]      20   \n",
       "23   93.866667   5.210667                        [0.6293]      20   \n",
       "24   92.800000   5.122000                        [0.6235]      20   \n",
       "25   96.000000   5.374000                        [0.6248]      20   \n",
       "26  102.400000   5.825500                        [0.6256]      20   \n",
       "27   87.040000   4.826600                        [0.7052]      20   \n",
       "28   89.600000   4.948500                        [0.7117]      20   \n",
       "29   99.200000   5.900000                        [0.7106]      20   \n",
       "30   89.600000   5.079500                          [0.71]      20   \n",
       "31   92.800000   5.442500                        [0.7098]      20   \n",
       "32   99.200000   5.723000                        [0.7091]      20   \n",
       "33  102.400000   5.653000                        [0.7838]      20   \n",
       "34  131.200000   7.462500                        [0.8153]      20   \n",
       "\n",
       "                                        sampling_seed     top-1  \n",
       "0   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...       NaN  \n",
       "1   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...       NaN  \n",
       "2   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...       NaN  \n",
       "3   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...       NaN  \n",
       "4   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...       NaN  \n",
       "5   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...       NaN  \n",
       "6   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.400000  \n",
       "7   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.300000  \n",
       "8   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.350000  \n",
       "9   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.350000  \n",
       "10  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.250000  \n",
       "11  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.300000  \n",
       "12  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.300000  \n",
       "13  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.250000  \n",
       "14  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.150000  \n",
       "15  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.200000  \n",
       "16  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.200000  \n",
       "17  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.250000  \n",
       "18  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...       NaN  \n",
       "19  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...       NaN  \n",
       "20  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...       NaN  \n",
       "21  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...       NaN  \n",
       "22  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.166667  \n",
       "23  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.266667  \n",
       "24  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.200000  \n",
       "25  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.300000  \n",
       "26  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.250000  \n",
       "27  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.100000  \n",
       "28  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.250000  \n",
       "29  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.300000  \n",
       "30  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.250000  \n",
       "31  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.250000  \n",
       "32  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.300000  \n",
       "33  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...       NaN  \n",
       "34  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...       NaN  "
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_kwargs = {\n",
    "    \"phase\": (\"phase\", \"mean\"),\n",
    "    \"train_size\": (\"train_size\", \"mean\"),\n",
    "    \"used_time\": (\"used_time\", \"mean\"),\n",
    "    \"val_reg_r2\": (\"val_reg_r2\", \"first\"),\n",
    "    \"seed_n\": (\"sampling_seed\", \"nunique\"),\n",
    "    \"sampling_seed\": (\"sampling_seed\", list),\n",
    "}\n",
    "\n",
    "topk_col = f\"top-{top_k}\"\n",
    "if topk_col in df_total.columns:\n",
    "    agg_kwargs[topk_col] = (topk_col, \"mean\")\n",
    "\n",
    "group_cols = [\n",
    "    \"measure_size\",\n",
    "    # \"scratch\",\n",
    "    # \"encoder_freeze\",\n",
    "    \"encoder_lr\",\n",
    "    \"cost_predictor_lr\",\n",
    "    \"rank_warmup_epochs\",\n",
    "    \"weights\",\n",
    "    \"uncertainty_topk\",\n",
    "    \"grad_num\",\n",
    "    \"rand_num\",\n",
    "]\n",
    "\n",
    "df_total_avg = (\n",
    "    df_total\n",
    "    .groupby(group_cols, as_index=False)\n",
    "    .agg(**agg_kwargs)\n",
    ")\n",
    "df_total_avg.to_csv(os.path.dirname(filename)+\"/vae_extent_total_avg.csv\", index=False)\n",
    "df_total_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "83ab0ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoder_lr</th>\n",
       "      <th>cost_predictor_lr</th>\n",
       "      <th>rank_warmup_epochs</th>\n",
       "      <th>weights</th>\n",
       "      <th>uncertainty_topk</th>\n",
       "      <th>grad_num</th>\n",
       "      <th>rand_num</th>\n",
       "      <th>phase</th>\n",
       "      <th>train_size</th>\n",
       "      <th>used_time</th>\n",
       "      <th>val_reg_r2</th>\n",
       "      <th>seed_n</th>\n",
       "      <th>sampling_seed</th>\n",
       "      <th>top-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>163.200000</td>\n",
       "      <td>9.323500</td>\n",
       "      <td>[-10.9241, -10.8547, -10.8065]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>124.800000</td>\n",
       "      <td>6.986500</td>\n",
       "      <td>[-8.103, -8.1111]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>108.800000</td>\n",
       "      <td>6.026500</td>\n",
       "      <td>[0.7073]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>102.400000</td>\n",
       "      <td>5.899500</td>\n",
       "      <td>[-7.0339, -7.3333]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.433333</td>\n",
       "      <td>91.733333</td>\n",
       "      <td>5.152333</td>\n",
       "      <td>[0.6151]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>100</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>4.396500</td>\n",
       "      <td>[0.6717]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>200</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>83.200000</td>\n",
       "      <td>4.565500</td>\n",
       "      <td>[0.6877]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>300</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>86.400000</td>\n",
       "      <td>4.747000</td>\n",
       "      <td>[0.6945]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.433333</td>\n",
       "      <td>91.733333</td>\n",
       "      <td>5.202667</td>\n",
       "      <td>[0.6977]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>100</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>86.400000</td>\n",
       "      <td>4.804000</td>\n",
       "      <td>[0.6966]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>200</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>83.200000</td>\n",
       "      <td>4.555000</td>\n",
       "      <td>[0.7004]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>300</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>4.337500</td>\n",
       "      <td>[0.6978]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>6.441500</td>\n",
       "      <td>[-0.0166, -0.6613]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>5.296500</td>\n",
       "      <td>[0.621]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>100</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>76.800000</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>[0.6387]</td>\n",
       "      <td>10</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>200</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>76.800000</td>\n",
       "      <td>4.099000</td>\n",
       "      <td>[0.6293]</td>\n",
       "      <td>10</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>300</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>89.600000</td>\n",
       "      <td>4.870000</td>\n",
       "      <td>[0.6235]</td>\n",
       "      <td>10</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>89.600000</td>\n",
       "      <td>5.005000</td>\n",
       "      <td>[0.7052]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>100</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>89.600000</td>\n",
       "      <td>4.948500</td>\n",
       "      <td>[0.7117]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>200</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.550000</td>\n",
       "      <td>99.200000</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>[0.7106]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>300</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>89.600000</td>\n",
       "      <td>5.079500</td>\n",
       "      <td>[0.71]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    encoder_lr  cost_predictor_lr  rank_warmup_epochs          weights  \\\n",
       "0     0.000001           0.000001                   0  (0.4, 0.3, 0.3)   \n",
       "1     0.000001           0.000010                   0  (0.4, 0.3, 0.3)   \n",
       "2     0.000001           0.000100                   0  (0.4, 0.3, 0.3)   \n",
       "3     0.000010           0.000001                   0  (0.4, 0.3, 0.3)   \n",
       "4     0.000010           0.000010                   0  (0.4, 0.3, 0.3)   \n",
       "5     0.000010           0.000010                 100  (0.4, 0.3, 0.3)   \n",
       "6     0.000010           0.000010                 200  (0.4, 0.3, 0.3)   \n",
       "7     0.000010           0.000010                 300  (0.4, 0.3, 0.3)   \n",
       "8     0.000010           0.000100                   0  (0.4, 0.3, 0.3)   \n",
       "9     0.000010           0.000100                 100  (0.4, 0.3, 0.3)   \n",
       "10    0.000010           0.000100                 200  (0.4, 0.3, 0.3)   \n",
       "11    0.000010           0.000100                 300  (0.4, 0.3, 0.3)   \n",
       "12    0.000100           0.000001                   0  (0.4, 0.3, 0.3)   \n",
       "13    0.000100           0.000010                   0  (0.4, 0.3, 0.3)   \n",
       "14    0.000100           0.000010                 100  (0.4, 0.3, 0.3)   \n",
       "15    0.000100           0.000010                 200  (0.4, 0.3, 0.3)   \n",
       "16    0.000100           0.000010                 300  (0.4, 0.3, 0.3)   \n",
       "17    0.000100           0.000100                   0  (0.4, 0.3, 0.3)   \n",
       "18    0.000100           0.000100                 100  (0.4, 0.3, 0.3)   \n",
       "19    0.000100           0.000100                 200  (0.4, 0.3, 0.3)   \n",
       "20    0.000100           0.000100                 300  (0.4, 0.3, 0.3)   \n",
       "\n",
       "    uncertainty_topk  grad_num  rand_num     phase  train_size  used_time  \\\n",
       "0                 64         4         0  2.550000  163.200000   9.323500   \n",
       "1                 64         4         0  1.950000  124.800000   6.986500   \n",
       "2                 64         4         0  1.700000  108.800000   6.026500   \n",
       "3                 64         4         0  1.600000  102.400000   5.899500   \n",
       "4                 64         4         0  1.433333   91.733333   5.152333   \n",
       "5                 64         4         0  1.250000   80.000000   4.396500   \n",
       "6                 64         4         0  1.300000   83.200000   4.565500   \n",
       "7                 64         4         0  1.350000   86.400000   4.747000   \n",
       "8                 64         4         0  1.433333   91.733333   5.202667   \n",
       "9                 64         4         0  1.350000   86.400000   4.804000   \n",
       "10                64         4         0  1.300000   83.200000   4.555000   \n",
       "11                64         4         0  1.250000   80.000000   4.337500   \n",
       "12                64         4         0  1.750000  112.000000   6.441500   \n",
       "13                64         4         0  1.500000   96.000000   5.296500   \n",
       "14                64         4         0  1.200000   76.800000   4.125000   \n",
       "15                64         4         0  1.200000   76.800000   4.099000   \n",
       "16                64         4         0  1.400000   89.600000   4.870000   \n",
       "17                64         4         0  1.400000   89.600000   5.005000   \n",
       "18                64         4         0  1.400000   89.600000   4.948500   \n",
       "19                64         4         0  1.550000   99.200000   5.900000   \n",
       "20                64         4         0  1.400000   89.600000   5.079500   \n",
       "\n",
       "                        val_reg_r2  seed_n  \\\n",
       "0   [-10.9241, -10.8547, -10.8065]      20   \n",
       "1                [-8.103, -8.1111]      20   \n",
       "2                         [0.7073]      20   \n",
       "3               [-7.0339, -7.3333]      20   \n",
       "4                         [0.6151]      20   \n",
       "5                         [0.6717]      20   \n",
       "6                         [0.6877]      20   \n",
       "7                         [0.6945]      20   \n",
       "8                         [0.6977]      20   \n",
       "9                         [0.6966]      20   \n",
       "10                        [0.7004]      20   \n",
       "11                        [0.6978]      20   \n",
       "12              [-0.0166, -0.6613]      20   \n",
       "13                         [0.621]      20   \n",
       "14                        [0.6387]      10   \n",
       "15                        [0.6293]      10   \n",
       "16                        [0.6235]      10   \n",
       "17                        [0.7052]      20   \n",
       "18                        [0.7117]      20   \n",
       "19                        [0.7106]      20   \n",
       "20                          [0.71]      20   \n",
       "\n",
       "                                        sampling_seed  top-1  \n",
       "0   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...    NaN  \n",
       "1   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...    NaN  \n",
       "2   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...    NaN  \n",
       "3   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...    NaN  \n",
       "4   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.40  \n",
       "5   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.30  \n",
       "6   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.35  \n",
       "7   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.35  \n",
       "8   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.30  \n",
       "9   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.25  \n",
       "10  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.15  \n",
       "11  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.20  \n",
       "12  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...    NaN  \n",
       "13  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...    NaN  \n",
       "14  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.30  \n",
       "15  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.20  \n",
       "16  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.20  \n",
       "17  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.10  \n",
       "18  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.25  \n",
       "19  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.30  \n",
       "20  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.25  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# group_cols = [\n",
    "#     # \"scratch\",\n",
    "#     # \"encoder_freeze\",\n",
    "#     \"encoder_lr\",\n",
    "#     \"cost_predictor_lr\",\n",
    "#     \"rank_warmup_epochs\",\n",
    "#     \"weights\",\n",
    "#     \"uncertainty_topk\",\n",
    "#     \"grad_num\",\n",
    "#     \"rand_num\",\n",
    "# ]\n",
    "\n",
    "# df_total_avg = (\n",
    "#     df_total\n",
    "#     .groupby(group_cols, as_index=False)\n",
    "#     .agg(\n",
    "#         phase=(\"phase\", \"mean\"),\n",
    "#         train_size=(\"train_size\", \"mean\"),\n",
    "#         used_time=(\"used_time\", \"mean\"),\n",
    "#         **{f\"top-{top_k}\": (f\"top-{top_k}\", \"mean\")},\n",
    "#         val_reg_r2=(\"val_reg_r2\", \"first\"),\n",
    "#         # val_rank_r2=(\"val_rank_r2\", \"first\"),\n",
    "#         seed_n=(\"sampling_seed\", \"nunique\"),\n",
    "#         sampling_seed=(\"sampling_seed\", list),\n",
    "#     )\n",
    "# )\n",
    "df_total_avg.to_csv(os.path.dirname(filename)+\"/vae_extent_avg_total.csv\", index=False)\n",
    "df_total_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074df23e",
   "metadata": {},
   "source": [
    "## XGB test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f06d5436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=UserWarning,\n",
    "    message=\".*Old style callback is deprecated.*\"\n",
    ")\n",
    "\n",
    "from tvm.auto_scheduler.cost_model.xgb_model import XGBModelInternal\n",
    "\n",
    "\n",
    "inputs, results = auto_scheduler.RecordReader(json_file).read_lines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfa4fafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   8   41   99  108  228  236  253  389  415  435  563  629  639  654\n",
      "  709  743  788  864  900  947  961  971  990  991 1121 1217 1239 1359\n",
      " 1511 1581 1665 1696 1719 1727 1828 1838 1841 1851 1863 1949 1974 1998\n",
      " 2006 2111 2124 2129 2223 2241 2256 2381 2411 2471 2585 2745 2885 3225\n",
      " 3248 3300 3320 3434 3542 3552 3654 3703]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.4698\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.5222\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "Fit a xgb booster. Train size: 192\n",
      "XGB Reg R2 : 0.5323\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 4.03 초\n",
      "=============================================\n",
      "[  26   74   81  121  217  371  395  412  420  440  579  602  697  714\n",
      "  745  748  809  811  817  892  945 1024 1104 1206 1210 1242 1474 1493\n",
      " 1555 1562 1589 1603 1620 1637 1667 1746 1752 1764 1811 1827 1901 1974\n",
      " 2066 2069 2082 2114 2119 2189 2210 2331 2527 2656 2713 2820 2886 3122\n",
      " 3130 3151 3168 3222 3462 3538 3651 3678]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.5438\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.5810\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "Fit a xgb booster. Train size: 192\n",
      "XGB Reg R2 : 0.6565\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 4 ================\n",
      "Fit a xgb booster. Train size: 256\n",
      "XGB Reg R2 : 0.6160\n",
      "XGB Recall@1 : 1\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 4.85 초\n",
      "=============================================\n",
      "[  15  135  166  294  337  392  399  406  465  490  507  528  556  560\n",
      "  632  711  728  730  831  860  877  915  921  928 1171 1198 1309 1385\n",
      " 1512 1535 1643 1675 1685 1768 1793 1881 1902 1918 2029 2124 2136 2178\n",
      " 2235 2276 2419 2538 2548 2842 3066 3102 3110 3263 3271 3306 3314 3354\n",
      " 3395 3396 3447 3465 3520 3529 3553 3595]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.6159\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.6019\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "Fit a xgb booster. Train size: 192\n",
      "XGB Reg R2 : 0.6477\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 4 ================\n",
      "Fit a xgb booster. Train size: 256\n",
      "XGB Reg R2 : 0.6020\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 5 ================\n",
      "Fit a xgb booster. Train size: 320\n",
      "XGB Reg R2 : 0.6259\n",
      "XGB Recall@1 : 1\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 5.82 초\n",
      "=============================================\n",
      "[  19   57   69  146  187  276  304  396  470  522  627  714  750  878\n",
      "  961  979  997 1048 1081 1115 1168 1181 1269 1297 1336 1432 1436 1558\n",
      " 1619 1712 1852 1933 2043 2063 2094 2132 2202 2223 2394 2402 2428 2449\n",
      " 2456 2467 2473 2622 2680 2920 2924 2934 2988 3171 3190 3230 3234 3237\n",
      " 3398 3409 3417 3418 3479 3614 3672 3711]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.5736\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.5157\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "Fit a xgb booster. Train size: 192\n",
      "XGB Reg R2 : 0.6200\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 4 ================\n",
      "Fit a xgb booster. Train size: 256\n",
      "XGB Reg R2 : 0.6610\n",
      "XGB Recall@1 : 1\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 4.81 초\n",
      "=============================================\n",
      "[  10  111  113  116  194  252  340  497  502  543  618  684 1068 1084\n",
      " 1095 1133 1147 1177 1291 1297 1305 1394 1404 1427 1474 1553 1563 1580\n",
      " 1685 1815 1971 2019 2020 2063 2073 2137 2172 2248 2280 2286 2293 2350\n",
      " 2437 2492 2547 2597 2603 2607 2613 2625 2639 2784 2794 2954 3011 3055\n",
      " 3082 3111 3236 3368 3372 3420 3514 3538]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.4630\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.5990\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.28 초\n",
      "=============================================\n",
      "[  12   33   61  108  110  360  420  485  567  675  701  715  720  737\n",
      "  770  786  838  963  982 1141 1173 1274 1307 1309 1389 1420 1445 1473\n",
      " 1722 1777 1866 1868 2044 2083 2103 2156 2178 2203 2266 2298 2345 2399\n",
      " 2457 2481 2517 2531 2635 2648 2669 2684 2737 2739 2788 2944 2959 2981\n",
      " 3143 3300 3320 3350 3466 3503 3517 3567]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.3881\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.5376\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.34 초\n",
      "=============================================\n",
      "[  18   46   77  203  211  213  276  350  462  535  541  560  700  730\n",
      "  809 1086 1112 1113 1139 1284 1317 1324 1420 1509 1538 1571 1671 1675\n",
      " 1747 1840 2050 2054 2157 2185 2259 2336 2433 2445 2453 2489 2509 2555\n",
      " 2689 2829 2830 2899 3009 3037 3047 3152 3160 3183 3193 3201 3239 3305\n",
      " 3339 3342 3425 3491 3533 3563 3658 3689]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.5962\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.5675\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "Fit a xgb booster. Train size: 192\n",
      "XGB Reg R2 : 0.4604\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 4 ================\n",
      "Fit a xgb booster. Train size: 256\n",
      "XGB Reg R2 : 0.5493\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 4.93 초\n",
      "=============================================\n",
      "[  45   95  113  192  215  372  382  510  516  633  670  702  735  796\n",
      "  802  827  902 1005 1006 1038 1054 1057 1074 1082 1125 1140 1271 1322\n",
      " 1375 1378 1440 1442 1464 1674 1744 1771 1776 1913 2034 2084 2145 2186\n",
      " 2230 2240 2428 2475 2479 2482 2646 2673 2784 2790 2818 2894 2971 3032\n",
      " 3177 3225 3255 3260 3370 3383 3408 3663]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.4937\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.4923\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.33 초\n",
      "=============================================\n",
      "[  17   52  111  159  172  182  185  193  211  229  482  664  930  937\n",
      " 1046 1125 1183 1222 1256 1258 1276 1298 1350 1360 1433 1475 1519 1589\n",
      " 1685 1805 1842 1996 2102 2127 2174 2274 2419 2472 2520 2618 2664 2689\n",
      " 2718 2779 2806 2833 2877 2878 2971 2990 3039 3068 3069 3112 3126 3164\n",
      " 3218 3260 3287 3331 3412 3441 3455 3694]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.5632\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.6486\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "Fit a xgb booster. Train size: 192\n",
      "XGB Reg R2 : 0.6080\n",
      "XGB Recall@1 : 1\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 4.06 초\n",
      "=============================================\n",
      "[   0  110  270  332  342  396  482  516  672  744  778  873  901  952\n",
      "  970  972 1088 1173 1233 1324 1343 1382 1422 1428 1566 1572 1605 1698\n",
      " 1747 1804 1862 1928 1946 2023 2102 2209 2230 2423 2444 2460 2498 2641\n",
      " 2668 2676 2708 2722 2747 2831 2867 2947 3042 3071 3076 3182 3188 3242\n",
      " 3285 3320 3514 3523 3623 3658 3659 3720]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.3922\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.5490\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.37 초\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "topk_size = int(measure_size * 0.95)\n",
    "eps_greedy_size = measure_size - topk_size\n",
    "\n",
    "\n",
    "seeds = sampling_hyper[\"seed\"]\n",
    "random_indices = random_indices_list[:len(seeds)]\n",
    "\n",
    "xgb_results = []\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%m%d_%H%M\")\n",
    "xgb_filename = f\"result_xgb/{os.path.basename(json_file)}/xgb_search_{now}.csv\"\n",
    "\n",
    "for i, seed in enumerate(seeds):\n",
    "\n",
    "    tic = time.time()\n",
    "    sample_rng = np.random.default_rng(seed)\n",
    "\n",
    "    \n",
    "    \n",
    "    tenset_model = XGBModelInternal(use_workload_embedding=False, seed=train_seed)\n",
    "\n",
    "    seed_everything(train_seed)\n",
    "    dataset = make_xgb_datasets(inputs, results)\n",
    "\n",
    "    \n",
    "    used_indices = set(random_indices[i])\n",
    "    remaining_indices = set(all_indices)\n",
    "    remaining_indices.difference_update(used_indices)\n",
    "\n",
    "    train_indices = np.array(sorted(used_indices), dtype=np.int64)\n",
    "    test_indices = np.array(sorted(remaining_indices), dtype=np.int64)\n",
    "    print(train_indices)\n",
    "\n",
    "    reg_history = []\n",
    "    rank_history = []\n",
    "\n",
    "    for phase in range(1,  len(input_data_scaled) // measure_size + 1):\n",
    "\n",
    "        print(f\"=============== 측정 Phase {phase} ================\")\n",
    "\n",
    "        seed_everything(train_seed)\n",
    "        train_set, test_set, dataset_costs = split_xgb_datasets(dataset, train_indices, test_indices)\n",
    "        real_optimum_idx = np.argmax(dataset_costs)\n",
    "        seed_everything(train_seed)\n",
    "        tenset_model.fit_base(train_set=train_set)\n",
    "        xgb_all_preds = tenset_model.predict(dataset)\n",
    "        xgb_all_preds = np.array(list(xgb_all_preds.values())[0], dtype=np.float32)\n",
    "        \n",
    "        \n",
    "        xgb_reg_r2 = r2_score(dataset_costs, xgb_all_preds)\n",
    "        reg_history.append(round(xgb_reg_r2, 4))\n",
    "        print(f\"XGB Reg R2 : {xgb_reg_r2:.4f}\")\n",
    "\n",
    "        # xgb_rank_r2 = pair_accuracy(xgb_all_preds, dataset_costs)\n",
    "        # rank_history.append(round(xgb_rank_r2, 4))\n",
    "        # print(f\"XGB Rank R2 : {xgb_rank_r2:.4f}\")\n",
    "\n",
    "        recall_score = recall_at_k(torch.tensor(xgb_all_preds), torch.tensor(dataset_costs), k=10)        \n",
    "        print(f\"XGB Recall@{top_k} : {recall_score}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # 다음 측정할 샘플 선택\n",
    "        train_indices, test_indices = xgb_select_indices(xgb_all_preds, \n",
    "                            train_indices, test_indices, topk_size=topk_size, eps_greedy_size=eps_greedy_size, rng=sample_rng)\n",
    "        measured_optimum = True if real_optimum_idx in train_indices else False\n",
    "\n",
    "        use_topk = False\n",
    "        \n",
    "\n",
    "        break_signal = False\n",
    "        if not use_topk and measured_optimum:\n",
    "            break_signal = True\n",
    "            \n",
    "        elif use_topk and recall_score:\n",
    "            break_signal = True\n",
    "            xgb_filename= xgb_filename.replace(\"result_xgb/\", \"result_xgb_topk/\")\n",
    "\n",
    "\n",
    "        if break_signal:\n",
    "        # if recall_score:\n",
    "            print(\"XGB 최적화 종료 신호 감지\")\n",
    "            print(f\"총 측정 시간: {time.time() - tic:.2f} 초\")\n",
    "            print(\"=============================================\")\n",
    "            xgb_results.append({\n",
    "                \"measure_size\": measure_size,\n",
    "                \"phase\" : phase,\n",
    "                \"used_time\": round(time.time() - tic, 2),\n",
    "                \"train_size\" : len(train_indices) - measure_size,\n",
    "                \"val_reg_r2\": reg_history,\n",
    "                \"val_rank_r2\": rank_history,\n",
    "                \"sampling_seed\": seed,\n",
    "                \n",
    "            })\n",
    "            df_xgb_results = pd.DataFrame(xgb_results)\n",
    "            os.makedirs(os.path.dirname(xgb_filename), exist_ok=True)\n",
    "            df_xgb_results.to_csv(xgb_filename, index=False)\n",
    "            # raise KeyboardInterrupt\n",
    "            break\n",
    "        \n",
    "        if test_indices.shape[0] < measure_size:\n",
    "            print(\"측정할 샘플이 더 이상 남아있지 않음\")\n",
    "            xgb_results.append({\n",
    "                \"measure_size\": measure_size,\n",
    "                \"phase\" : \"all but not found\",\n",
    "                \"used_time\": round(time.time() - tic, 2),\n",
    "                \"train_size\" : len(train_indices) - measure_size,\n",
    "                \"val_reg_r2\": reg_history,\n",
    "                \"val_rank_r2\": rank_history,\n",
    "                \"sampling_seed\": seed,\n",
    "                \n",
    "            })\n",
    "            df_xgb_results = pd.DataFrame(xgb_results)\n",
    "            os.makedirs(os.path.dirname(xgb_filename), exist_ok=True)\n",
    "            df_xgb_results.to_csv(xgb_filename, index=False)\n",
    "            break\n",
    "            # raise KeyboardInterrupt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d6f6988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure_size</th>\n",
       "      <th>train_size</th>\n",
       "      <th>used_time</th>\n",
       "      <th>val_reg_r2</th>\n",
       "      <th>val_rank_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>198.4</td>\n",
       "      <td>4.182</td>\n",
       "      <td>[0.4698, 0.5222, 0.5323]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   measure_size  train_size  used_time                val_reg_r2 val_rank_r2\n",
       "0            64       198.4      4.182  [0.4698, 0.5222, 0.5323]          []"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_cols = [\n",
    "    \"measure_size\",\n",
    "]\n",
    "\n",
    "agg_dict = {\n",
    "    # \"phase\": \"mean\",\n",
    "    \"train_size\": \"mean\",\n",
    "    \"used_time\": \"mean\",\n",
    "    \"val_reg_r2\": \"first\",\n",
    "    \"val_rank_r2\": \"first\",\n",
    "}\n",
    "\n",
    "df_avg = (\n",
    "    df_xgb_results\n",
    "    .groupby(group_cols, as_index=False)\n",
    "    .agg(agg_dict)\n",
    ")\n",
    "df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf23a3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   8   41   99  108  228  236  253  389  415  435  563  629  639  654\n",
      "  709  743  788  864  900  947  961  971  990  991 1121 1217 1239 1359\n",
      " 1511 1581 1665 1696 1719 1727 1828 1838 1841 1851 1863 1949 1974 1998\n",
      " 2006 2111 2124 2129 2223 2241 2256 2381 2411 2471 2585 2745 2885 3225\n",
      " 3248 3300 3320 3434 3542 3552 3654 3703]\n",
      "=============== 측정 Phase 1 ================\n",
      "XGB Reg R2 : 0.6295\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.15 초\n",
      "=============================================\n",
      "[  26   74   81  121  217  371  395  412  420  440  579  602  697  714\n",
      "  745  748  809  811  817  892  945 1024 1104 1206 1210 1242 1474 1493\n",
      " 1555 1562 1589 1603 1620 1637 1667 1746 1752 1764 1811 1827 1901 1974\n",
      " 2066 2069 2082 2114 2119 2189 2210 2331 2527 2656 2713 2820 2886 3122\n",
      " 3130 3151 3168 3222 3462 3538 3651 3678]\n",
      "=============== 측정 Phase 1 ================\n",
      "XGB Reg R2 : 0.5064\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "XGB Reg R2 : 0.6101\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "XGB Reg R2 : 0.6527\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 4 ================\n",
      "XGB Reg R2 : 0.6519\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 5 ================\n",
      "XGB Reg R2 : 0.6354\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 6.34 초\n",
      "=============================================\n",
      "[  15  135  166  294  337  392  399  406  465  490  507  528  556  560\n",
      "  632  711  728  730  831  860  877  915  921  928 1171 1198 1309 1385\n",
      " 1512 1535 1643 1675 1685 1768 1793 1881 1902 1918 2029 2124 2136 2178\n",
      " 2235 2276 2419 2538 2548 2842 3066 3102 3110 3263 3271 3306 3314 3354\n",
      " 3395 3396 3447 3465 3520 3529 3553 3595]\n",
      "=============== 측정 Phase 1 ================\n",
      "XGB Reg R2 : 0.5497\n",
      "XGB Recall@1 : 1\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.13 초\n",
      "=============================================\n",
      "[  19   57   69  146  187  276  304  396  470  522  627  714  750  878\n",
      "  961  979  997 1048 1081 1115 1168 1181 1269 1297 1336 1432 1436 1558\n",
      " 1619 1712 1852 1933 2043 2063 2094 2132 2202 2223 2394 2402 2428 2449\n",
      " 2456 2467 2473 2622 2680 2920 2924 2934 2988 3171 3190 3230 3234 3237\n",
      " 3398 3409 3417 3418 3479 3614 3672 3711]\n",
      "=============== 측정 Phase 1 ================\n",
      "XGB Reg R2 : 0.4898\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "XGB Reg R2 : 0.6130\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.89 초\n",
      "=============================================\n",
      "[  10  111  113  116  194  252  340  497  502  543  618  684 1068 1084\n",
      " 1095 1133 1147 1177 1291 1297 1305 1394 1404 1427 1474 1553 1563 1580\n",
      " 1685 1815 1971 2019 2020 2063 2073 2137 2172 2248 2280 2286 2293 2350\n",
      " 2437 2492 2547 2597 2603 2607 2613 2625 2639 2784 2794 2954 3011 3055\n",
      " 3082 3111 3236 3368 3372 3420 3514 3538]\n",
      "=============== 측정 Phase 1 ================\n",
      "XGB Reg R2 : 0.5044\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "XGB Reg R2 : 0.3218\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "XGB Reg R2 : 0.7177\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 4 ================\n",
      "XGB Reg R2 : 0.7214\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 5 ================\n",
      "XGB Reg R2 : 0.7610\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 6 ================\n",
      "XGB Reg R2 : 0.7626\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 7 ================\n",
      "XGB Reg R2 : 0.7528\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 7.88 초\n",
      "=============================================\n",
      "[  12   33   61  108  110  360  420  485  567  675  701  715  720  737\n",
      "  770  786  838  963  982 1141 1173 1274 1307 1309 1389 1420 1445 1473\n",
      " 1722 1777 1866 1868 2044 2083 2103 2156 2178 2203 2266 2298 2345 2399\n",
      " 2457 2481 2517 2531 2635 2648 2669 2684 2737 2739 2788 2944 2959 2981\n",
      " 3143 3300 3320 3350 3466 3503 3517 3567]\n",
      "=============== 측정 Phase 1 ================\n",
      "XGB Reg R2 : 0.5193\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.12 초\n",
      "=============================================\n",
      "[  18   46   77  203  211  213  276  350  462  535  541  560  700  730\n",
      "  809 1086 1112 1113 1139 1284 1317 1324 1420 1509 1538 1571 1671 1675\n",
      " 1747 1840 2050 2054 2157 2185 2259 2336 2433 2445 2453 2489 2509 2555\n",
      " 2689 2829 2830 2899 3009 3037 3047 3152 3160 3183 3193 3201 3239 3305\n",
      " 3339 3342 3425 3491 3533 3563 3658 3689]\n",
      "=============== 측정 Phase 1 ================\n",
      "XGB Reg R2 : 0.5668\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "XGB Reg R2 : 0.6046\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "XGB Reg R2 : 0.6341\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 4.65 초\n",
      "=============================================\n",
      "[  45   95  113  192  215  372  382  510  516  633  670  702  735  796\n",
      "  802  827  902 1005 1006 1038 1054 1057 1074 1082 1125 1140 1271 1322\n",
      " 1375 1378 1440 1442 1464 1674 1744 1771 1776 1913 2034 2084 2145 2186\n",
      " 2230 2240 2428 2475 2479 2482 2646 2673 2784 2790 2818 2894 2971 3032\n",
      " 3177 3225 3255 3260 3370 3383 3408 3663]\n",
      "=============== 측정 Phase 1 ================\n",
      "XGB Reg R2 : 0.3925\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "XGB Reg R2 : 0.5128\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.88 초\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import multiprocessing\n",
    "\n",
    "topk_size = int(measure_size * 0.95)\n",
    "eps_greedy_size = measure_size - topk_size\n",
    "\n",
    "\n",
    "seeds = sampling_hyper[\"seed\"]\n",
    "random_indices = random_indices_list[:len(seeds)]\n",
    "\n",
    "xgb_results = []\n",
    "\n",
    "# XGBModelInternal과 동일한 xgb_params 설정\n",
    "xgb_params = {\n",
    "    \"max_depth\": 6,\n",
    "    \"gamma\": 0.003,\n",
    "    \"min_child_weight\": 2,\n",
    "    \"eta\": 0.2,\n",
    "    \"n_gpus\": 0,\n",
    "    \"nthread\": multiprocessing.cpu_count() // 2,\n",
    "    \"verbosity\": 0,\n",
    "    \"seed\": train_seed or 43,\n",
    "    \"disable_default_eval_metric\": 1,\n",
    "}\n",
    "\n",
    "for i, seed in enumerate(seeds):\n",
    "\n",
    "    tic = time.time()\n",
    "    sample_rng = np.random.default_rng(seed)\n",
    "\n",
    "    dataset = make_xgb_datasets(inputs, results)\n",
    "    \n",
    "    used_indices = set(random_indices[i])\n",
    "    remaining_indices = set(all_indices)\n",
    "    remaining_indices.difference_update(used_indices)\n",
    "\n",
    "    train_indices = np.array(sorted(used_indices), dtype=np.int64)\n",
    "    test_indices = np.array(sorted(remaining_indices), dtype=np.int64)\n",
    "    print(train_indices)\n",
    "\n",
    "    reg_history = []\n",
    "    rank_history = []\n",
    "\n",
    "    for phase in range(1,  len(input_data_scaled) // measure_size + 1):\n",
    "\n",
    "        print(f\"=============== 측정 Phase {phase} ================\")\n",
    "\n",
    "        seed_everything(train_seed)\n",
    "        _, _, dataset_costs = split_xgb_datasets(dataset, train_indices, test_indices)\n",
    "        input_train = input_data_scaled[train_indices]\n",
    "        label_train = dataset_costs[train_indices]\n",
    "        input_test = input_data_scaled[test_indices]\n",
    "        label_test = dataset_costs[test_indices]\n",
    "        \n",
    "        real_optimum_idx = np.argmax(dataset_costs)\n",
    "        \n",
    "        # XGB 모델 학습 - input_train, label_train 사용\n",
    "        seed_everything(train_seed)\n",
    "        dtrain = xgb.DMatrix(input_train, label=label_train)\n",
    "        dtest = xgb.DMatrix(input_test, label=label_test)\n",
    "        \n",
    "        # 학습 (XGBModelInternal과 유사하게 num_boost_round=300, early stopping 없이 단순화)\n",
    "        bst = xgb.train(\n",
    "            params=xgb_params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=300,\n",
    "            evals=[(dtrain, \"train\"), (dtest, \"test\")],\n",
    "            verbose_eval=50,\n",
    "        )\n",
    "        \n",
    "        # input_data_scaled 전체로 predict\n",
    "        dmatrix_all = xgb.DMatrix(input_data_scaled)\n",
    "        xgb_all_preds = bst.predict(dmatrix_all)\n",
    "        xgb_all_preds = np.array(xgb_all_preds, dtype=np.float32)\n",
    "        \n",
    "        \n",
    "        xgb_reg_r2 = r2_score(dataset_costs, xgb_all_preds)\n",
    "        reg_history.append(round(xgb_reg_r2, 4))\n",
    "        print(f\"XGB Reg R2 : {xgb_reg_r2:.4f}\")\n",
    "\n",
    "        # xgb_rank_r2 = pair_accuracy(xgb_all_preds, dataset_costs)\n",
    "        # rank_history.append(round(xgb_rank_r2, 4))\n",
    "        # print(f\"XGB Rank R2 : {xgb_rank_r2:.4f}\")\n",
    "\n",
    "        recall_score = recall_at_k(torch.tensor(xgb_all_preds), torch.tensor(dataset_costs), k=10)        \n",
    "        print(f\"XGB Recall@{top_k} : {recall_score}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # 다음 측정할 샘플 선택\n",
    "        train_indices, test_indices = xgb_select_indices(xgb_all_preds, \n",
    "                            train_indices, test_indices, topk_size=topk_size, eps_greedy_size=eps_greedy_size, rng=sample_rng)\n",
    "        measured_optimum = True if real_optimum_idx in train_indices else False\n",
    "\n",
    "        use_topk = False\n",
    "        \n",
    "\n",
    "        break_signal = False\n",
    "        if not use_topk and measured_optimum:\n",
    "            break_signal = True\n",
    "            \n",
    "        elif use_topk and recall_score:\n",
    "            break_signal = True\n",
    "            xgb_filename= xgb_filename.replace(\"result_xgb/\", \"result_xgb_topk/topk_\")\n",
    "\n",
    "\n",
    "        if break_signal:\n",
    "            print(\"XGB 최적화 종료 신호 감지\")\n",
    "            print(f\"총 측정 시간: {time.time() - tic:.2f} 초\")\n",
    "            print(\"=============================================\")\n",
    "            xgb_results.append({\n",
    "                \"measure_size\": measure_size,\n",
    "                \"phase\" : phase,\n",
    "                \"used_time\": round(time.time() - tic, 2),\n",
    "                \"train_size\" : len(train_indices) - measure_size,\n",
    "                \"val_reg_r2\": reg_history,\n",
    "                \"val_rank_r2\": rank_history,\n",
    "                \"sampling_seed\": seed,\n",
    "                \n",
    "            })\n",
    "            df_xgb_results = pd.DataFrame(xgb_results)\n",
    "            # df_xgb_results.to_csv(xgb_filename, index=False)\n",
    "            break\n",
    "        \n",
    "        if test_indices.shape[0] < measure_size:\n",
    "            print(\"측정할 샘플이 더 이상 남아있지 않음\")\n",
    "            xgb_results.append({\n",
    "                \"measure_size\": measure_size,\n",
    "                \"phase\" : \"all but not found\",\n",
    "                \"used_time\": round(time.time() - tic, 2),\n",
    "                \"train_size\" : len(train_indices) - measure_size,\n",
    "                \"val_reg_r2\": reg_history,\n",
    "                \"val_rank_r2\": rank_history,\n",
    "                \"sampling_seed\": seed,\n",
    "                \n",
    "            })\n",
    "            df_xgb_results = pd.DataFrame(xgb_results)\n",
    "            # df_xgb_results.to_csv(xgb_filename, index=False)\n",
    "            break\n",
    "            # raise KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "83a274fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure_size</th>\n",
       "      <th>phase</th>\n",
       "      <th>used_time</th>\n",
       "      <th>train_size</th>\n",
       "      <th>val_reg_r2</th>\n",
       "      <th>val_rank_r2</th>\n",
       "      <th>sampling_seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>3.16</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.455, 0.4711]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>4.44</td>\n",
       "      <td>320</td>\n",
       "      <td>[0.5689, 0.5475, 0.5938, 0.5836, 0.6436]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>4.01</td>\n",
       "      <td>256</td>\n",
       "      <td>[0.588, 0.5441, 0.5812, 0.6114]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>3.09</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.5876, 0.5477]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>3.24</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.4325, 0.608]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>3.09</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.4016, 0.5381]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>3.09</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.5755, 0.333]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>5.60</td>\n",
       "      <td>448</td>\n",
       "      <td>[0.4894, 0.5619, 0.478, 0.5676, 0.5699, 0.5365...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>3.11</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.5457, 0.6573]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3.92</td>\n",
       "      <td>256</td>\n",
       "      <td>[0.3686, 0.5665, 0.5754, 0.6717]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   measure_size  phase  used_time  train_size  \\\n",
       "0            64      2       3.16         128   \n",
       "1            64      5       4.44         320   \n",
       "2            64      4       4.01         256   \n",
       "3            64      2       3.09         128   \n",
       "4            64      2       3.24         128   \n",
       "5            64      2       3.09         128   \n",
       "6            64      2       3.09         128   \n",
       "7            64      7       5.60         448   \n",
       "8            64      2       3.11         128   \n",
       "9            64      4       3.92         256   \n",
       "\n",
       "                                          val_reg_r2 val_rank_r2  \\\n",
       "0                                    [0.455, 0.4711]          []   \n",
       "1           [0.5689, 0.5475, 0.5938, 0.5836, 0.6436]          []   \n",
       "2                    [0.588, 0.5441, 0.5812, 0.6114]          []   \n",
       "3                                   [0.5876, 0.5477]          []   \n",
       "4                                    [0.4325, 0.608]          []   \n",
       "5                                   [0.4016, 0.5381]          []   \n",
       "6                                    [0.5755, 0.333]          []   \n",
       "7  [0.4894, 0.5619, 0.478, 0.5676, 0.5699, 0.5365...          []   \n",
       "8                                   [0.5457, 0.6573]          []   \n",
       "9                   [0.3686, 0.5665, 0.5754, 0.6717]          []   \n",
       "\n",
       "   sampling_seed  \n",
       "0           2000  \n",
       "1           2001  \n",
       "2           2002  \n",
       "3           2003  \n",
       "4           2004  \n",
       "5           2005  \n",
       "6           2006  \n",
       "7           2007  \n",
       "8           2008  \n",
       "9           2009  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xgb_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d57c684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure_size</th>\n",
       "      <th>train_size</th>\n",
       "      <th>used_time</th>\n",
       "      <th>val_reg_r2</th>\n",
       "      <th>val_rank_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>176.0</td>\n",
       "      <td>4.505</td>\n",
       "      <td>[0.6295]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   measure_size  train_size  used_time val_reg_r2 val_rank_r2\n",
       "0            64       176.0      4.505   [0.6295]          []"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_cols = [\n",
    "    \"measure_size\",\n",
    "]\n",
    "\n",
    "agg_dict = {\n",
    "    # \"phase\": \"mean\",\n",
    "    \"train_size\": \"mean\",\n",
    "    \"used_time\": \"mean\",\n",
    "    \"val_reg_r2\": \"first\",\n",
    "    \"val_rank_r2\": \"first\",\n",
    "}\n",
    "\n",
    "df_avg = (\n",
    "    df_xgb_results\n",
    "    .groupby(group_cols, as_index=False)\n",
    "    .agg(agg_dict)\n",
    ")\n",
    "df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336e266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit a xgb booster. Train size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenset 모델 Rank Accuracy: 0.8091\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tvm.auto_scheduler.cost_model.xgb_model import XGBModelInternal\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    tenset_model = XGBModelInternal()\n",
    "    tenset_model.fit_base(train_set, valid_set=test_set)\n",
    "    throughputs = np.array(list(test_set.throughputs.values()))\n",
    "\n",
    "    pred = tenset_model.predict(test_set)\n",
    "\n",
    "    true_biggest_index = np.argsort(throughputs[0])[-1]\n",
    "    biggest_indices_64 = np.argsort(list(pred.values())[0])[-64:]\n",
    "\n",
    "    # list(pred.values())[0]\n",
    "    if true_biggest_index in biggest_indices_64:\n",
    "        print(\"✓ Tenset 모델이 실제 가장 높은 throughput 정확히 예측했습니다!\")\n",
    "        break\n",
    "    break\n",
    "\n",
    "\n",
    "# pred, throughputs rank accuracy\n",
    "correct_pairs = 0\n",
    "total_pairs = 0\n",
    "n_samples = min(2000, throughputs.shape[-1])\n",
    "sample_indices = np.random.choice(throughputs.shape[-1], n_samples, replace=False)\n",
    "pred_values = list(pred.values())[0]\n",
    "throughput_values = throughputs.squeeze()\n",
    "rank_accuracy = pair_accuracy(pred_values, throughput_values)\n",
    "print(f\"Tenset 모델 Rank Accuracy: {rank_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
