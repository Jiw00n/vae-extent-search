{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d665df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "project_root = \"/root/work/tenset\"\n",
    "os.environ[\"TVM_HOME\"] = f\"{project_root}\"\n",
    "os.environ[\"TVM_LIBRARY_PATH\"] = f\"{project_root}/build\"\n",
    "if f\"{project_root}/python\" not in sys.path:\n",
    "    sys.path.insert(0, f\"{project_root}/python\")\n",
    "    \n",
    "\n",
    "sys.path = [p for p in sys.path if not p.startswith(f\"{project_root}/build\")]\n",
    "sys.path.append(f\"{project_root}/build\")\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = f\"{project_root}/build:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb3f4c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sys.path.append(\"/root/work/tenset/scripts\")\n",
    "from print_programs import return_all_states\n",
    "from make_dataset import load_and_register_tasks\n",
    "from tvm import auto_scheduler\n",
    "from tvm.auto_scheduler.dataset import Dataset, make_dataset_from_log_file\n",
    "from glob import glob\n",
    "\n",
    "# json_file = \"/root/work/tenset/dataset/measure_records_tenset/k80/([0c9a5ba46ffc5e1a9e5641018527117f,4,7,7,160,1,1,160,960,1,1,1,960,4,7,7,960],cuda).json\"\n",
    "json_files = glob(\"/root/work/tenset/dataset/measure_records_tenset/k80/([0c9a5ba46ffc5e1a9e5641018527117f,4*.json\")\n",
    "load_and_register_tasks()\n",
    "\n",
    "inputs, results = auto_scheduler.RecordReader(json_files[0]).read_lines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62a81865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm.auto_scheduler.measure import recover_measure_input\n",
    "inp = recover_measure_input(inputs[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "328075fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[auto_scheduler.Stage(0x58d6f9e0: placeholder), auto_scheduler.Stage(0x5a7537d0: PaddedInput), auto_scheduler.Stage(0x5ab0d490: PaddedInput.shared), auto_scheduler.Stage(0x5ef02770: placeholder), auto_scheduler.Stage(0x5a752a00: placeholder.shared), auto_scheduler.Stage(0x5a752b00: Conv2dOutput.local), auto_scheduler.Stage(0x5afafbc0: Conv2dOutput), auto_scheduler.Stage(0x5ef01480: placeholder), auto_scheduler.Stage(0x5a752370: T_add), auto_scheduler.Stage(0x5a751d70: compile_engine_const), auto_scheduler.Stage(0x5a751ed0: T_add), auto_scheduler.Stage(0x5a750400: compute), auto_scheduler.Stage(0x5a7503c0: compile_engine_const), auto_scheduler.Stage(0x5a750010: T_divide), auto_scheduler.Stage(0x5a751d30: T_multiply)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.state.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "947647eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d82deb4e66439ba029229551d1fe4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ([0c9a5ba46ffc5e1a9e5641018527117f,4,16,16,112,1,1,112,672,1,1,1,672,4,16,16,672],cuda).json\n",
      "Processing file: ([0c9a5ba46ffc5e1a9e5641018527117f,4,15,15,112,1,1,112,672,1,1,1,672,4,15,15,672],cuda).json\n",
      "Processing file: ([0c9a5ba46ffc5e1a9e5641018527117f,4,14,14,80,1,1,80,184,1,1,1,184,4,14,14,184],cuda).json\n",
      "Processing file: ([0c9a5ba46ffc5e1a9e5641018527117f,4,15,15,80,1,1,80,480,1,1,1,480,4,15,15,480],cuda).json\n",
      "Processing file: ([0c9a5ba46ffc5e1a9e5641018527117f,4,14,14,80,1,1,80,480,1,1,1,480,4,14,14,480],cuda).json\n",
      "Processing file: ([0c9a5ba46ffc5e1a9e5641018527117f,4,7,7,160,1,1,160,960,1,1,1,960,4,7,7,960],cuda).json\n",
      "Processing file: ([0c9a5ba46ffc5e1a9e5641018527117f,4,32,32,40,1,1,40,240,1,1,1,240,4,32,32,240],cuda).json\n",
      "Processing file: ([0c9a5ba46ffc5e1a9e5641018527117f,4,14,14,80,1,1,80,200,1,1,1,200,4,14,14,200],cuda).json\n",
      "Processing file: ([0c9a5ba46ffc5e1a9e5641018527117f,4,30,30,40,1,1,40,240,1,1,1,240,4,30,30,240],cuda).json\n",
      "Processing file: ([0c9a5ba46ffc5e1a9e5641018527117f,4,16,16,80,1,1,80,200,1,1,1,200,4,16,16,200],cuda).json\n",
      "Processing file: ([0c9a5ba46ffc5e1a9e5641018527117f,4,15,15,80,1,1,80,200,1,1,1,200,4,15,15,200],cuda).json\n",
      "Processing file: ([0c9a5ba46ffc5e1a9e5641018527117f,4,28,28,40,1,1,40,240,1,1,1,240,4,28,28,240],cuda).json\n",
      "Processing file: ([0c9a5ba46ffc5e1a9e5641018527117f,4,16,16,80,1,1,80,184,1,1,1,184,4,16,16,184],cuda).json\n",
      "Processing file: ([0c9a5ba46ffc5e1a9e5641018527117f,4,14,14,112,1,1,112,672,1,1,1,672,4,14,14,672],cuda).json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m json_file \u001b[38;5;129;01min\u001b[39;00m tqdm(json_files):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing file:\u001b[39m\u001b[38;5;124m\"\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(json_file))\n\u001b[0;32m----> 5\u001b[0m     states, costs \u001b[38;5;241m=\u001b[39m \u001b[43mreturn_all_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     records_raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mstr\u001b[39m(x)\u001b[38;5;241m.\u001b[39mstrip(), states))\n\u001b[1;32m      8\u001b[0m     records \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mschedules\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextents\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosts\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munroll\u001b[39m\u001b[38;5;124m\"\u001b[39m : [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m: []}\n",
      "File \u001b[0;32m~/work/tenset/scripts/print_programs.py:45\u001b[0m, in \u001b[0;36mreturn_all_states\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     43\u001b[0m inputs, results \u001b[38;5;241m=\u001b[39m auto_scheduler\u001b[38;5;241m.\u001b[39mRecordReader(filename)\u001b[38;5;241m.\u001b[39mread_lines()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs)):\n\u001b[0;32m---> 45\u001b[0m     state, cost \u001b[38;5;241m=\u001b[39m \u001b[43mreturn_program\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m         states\u001b[38;5;241m.\u001b[39mappend(state)\n",
      "File \u001b[0;32m~/work/tenset/scripts/print_programs.py:36\u001b[0m, in \u001b[0;36mreturn_program\u001b[0;34m(inp, res)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res\u001b[38;5;241m.\u001b[39mcosts[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1000000\u001b[39m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m inp \u001b[38;5;241m=\u001b[39m \u001b[43mrecover_measure_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inp\u001b[38;5;241m.\u001b[39mstate, res\u001b[38;5;241m.\u001b[39mcosts\n",
      "File \u001b[0;32m~/work/tenset/python/tvm/auto_scheduler/measure.py:236\u001b[0m, in \u001b[0;36mrecover_measure_input\u001b[0;34m(inp, rebuild_state)\u001b[0m\n\u001b[1;32m    226\u001b[0m new_task \u001b[38;5;241m=\u001b[39m SearchTask(\n\u001b[1;32m    227\u001b[0m     workload_key\u001b[38;5;241m=\u001b[39mtask\u001b[38;5;241m.\u001b[39mworkload_key,\n\u001b[1;32m    228\u001b[0m     target\u001b[38;5;241m=\u001b[39mtask\u001b[38;5;241m.\u001b[39mtarget,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m     task_inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(task\u001b[38;5;241m.\u001b[39mtask_input_names),\n\u001b[1;32m    233\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rebuild_state:\n\u001b[0;32m--> 236\u001b[0m     new_state \u001b[38;5;241m=\u001b[39m \u001b[43mnew_task\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_bound_from_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m     new_state \u001b[38;5;241m=\u001b[39m inp\u001b[38;5;241m.\u001b[39mstate\n",
      "File \u001b[0;32m~/work/tenset/python/tvm/auto_scheduler/compute_dag.py:198\u001b[0m, in \u001b[0;36mComputeDAG.infer_bound_from_state\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03mInfer and fill the bound of all iterators of a state.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    The State with complete bound information.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m state_obj \u001b[38;5;241m=\u001b[39m state \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state, StateObject) \u001b[38;5;28;01melse\u001b[39;00m state\u001b[38;5;241m.\u001b[39mstate_object\n\u001b[0;32m--> 198\u001b[0m updated_state \u001b[38;5;241m=\u001b[39m State(\u001b[43m_ffi_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mComputeDAGInferBoundFromState\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_obj\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Copy the stage_id_map from the original state to make sure the old indices are still\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# valid\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state, State):\n",
      "File \u001b[0;32m~/work/tenset/python/tvm/_ffi/_ctypes/packed_func.py:227\u001b[0m, in \u001b[0;36mPackedFuncBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    224\u001b[0m ret_val \u001b[38;5;241m=\u001b[39m TVMValue()\n\u001b[1;32m    225\u001b[0m ret_tcode \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int()\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m--> 227\u001b[0m     \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTVMFuncCall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtcodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret_tcode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    236\u001b[0m ):\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m get_last_ffi_error()\n\u001b[1;32m    238\u001b[0m _ \u001b[38;5;241m=\u001b[39m temp_args\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "records_all = []\n",
    "for json_file in tqdm(json_files):\n",
    "    print(\"Processing file:\", os.path.basename(json_file))\n",
    "    states, costs = return_all_states(json_file)\n",
    "    records_raw = list(map(lambda x: str(x).strip(), states))\n",
    "\n",
    "    records = {\"schedules\": [], \"extents\": [], \"costs\": [], \"unroll\" : [], \"all\": []}\n",
    "\n",
    "    for rec, cost in zip(records_raw, costs):\n",
    "        cost = np.array([c.value for c in cost])\n",
    "        cost = -np.log(np.mean(cost) + 1e-8)\n",
    "        schedule = rec.split(\"Placeholder\")[-1][2:]\n",
    "        \n",
    "        records[\"schedules\"].append(schedule)\n",
    "        records[\"costs\"].append(cost)\n",
    "    records_all.append(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f47597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'placeholder, placeholder, placeholder\\nblockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,4)\\n  vthread nn.1@yy.1@xx.1@ff.1@ (0,8)\\n    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,14)\\n      Conv2dOutput.local auto_unroll: 64\\n      for nn_c.0 (0,1)\\n        for yy_c.0 (0,1)\\n          for xx_c.0 (0,1)\\n            for ff_c.0 (0,1)\\n              for nn_c.1 (0,1)\\n                for yy_c.1 (0,1)\\n                  for xx_c.1 (0,1)\\n                    for ff_c.1 (0,1)\\n                      for nn_c.2 (0,1)\\n                        for yy_c.2 (0,1)\\n                          for xx_c.2 (0,1)\\n                            for ff_c.2 (0,1)\\n                              for ry.0 (0,1)\\n                                for rx.0 (0,1)\\n                                  for rc.0 (0,14)\\n                                    for ax0@ax1@ax2@ax3@.0.0 (0,192)\\n                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,14)\\n                                        vectorize ax0@ax1@ax2@ax3@.1 (0,2)\\n                                          placeholder.shared = ...\\n                                    for ax0@ax1@ax2@ax3@.0.0 (0,37)\\n                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,14)\\n                                        vectorize ax0@ax1@ax2@ax3@.1 (0,4)\\n                                          PaddedInput.shared = ...\\n                                    for ry.1 (0,1)\\n                                      for rx.1 (0,1)\\n                                        for rc.1 (0,4)\\n                                          for nn_c.3 (0,1)\\n                                            for yy_c.3 (0,1)\\n                                              for xx_c.3 (0,1)\\n                                                for ff_c.3 (0,4)\\n                                                  for ry.2 (0,1)\\n                                                    for rx.2 (0,1)\\n                                                      for rc.2 (0,2)\\n                                                        for nn_c.4 (0,1)\\n                                                          for yy_c.4 (0,4)\\n                                                            for xx_c.4 (0,8)\\n                                                              for ff_c.4 (0,12)\\n                                                                Conv2dOutput.local = ...\\n      for nn.3 (0,1)\\n        for yy.3 (0,4)\\n          for xx.3 (0,8)\\n            for ff.3 (0,48)\\n              Conv2dOutput = ...\\nblockIdx.x ax0@ax1@ax2@ax3@.0 (0,10752)\\n  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,64)\\n    T_multiply = ...'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff08adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "발견된 공통 (0,1) for문 변수: {'yy_c.0', 'rx.2', 'ry.0', 'ry.1', 'xx_c.1', 'rx.1', 'xx_c.0', 'nn_c.1', 'ff_c.2', 'nn_c.0', 'ff_c.1', 'xx_c.2', 'nn_c.2', 'rx.0', 'ff_c.0', 'yy_c.2', 'ry.2', 'yy_c.1'}\n",
      "제거된 줄 수: 18.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def find_common_for_loops(schedules):\n",
    "    \"\"\"\n",
    "    모든 스케줄에서 공통으로 나타나는 (0,1) for문 변수명을 찾음\n",
    "    \"\"\"\n",
    "    common_vars = None\n",
    "    \n",
    "    for schedule in schedules:\n",
    "        lines = schedule.split('\\n')\n",
    "        vars_in_schedule = set()\n",
    "        \n",
    "        for line in lines:\n",
    "            stripped = line.lstrip()\n",
    "            match = re.match(r'for\\s+(\\S+)\\s+\\(0,\\s*1\\)', stripped)\n",
    "            if match:\n",
    "                vars_in_schedule.add(match.group(1))\n",
    "        \n",
    "        if common_vars is None:\n",
    "            common_vars = vars_in_schedule\n",
    "        else:\n",
    "            common_vars &= vars_in_schedule  # 교집합\n",
    "    \n",
    "    return common_vars if common_vars is not None else set()\n",
    "\n",
    "\n",
    "def remove_common_for_loops_(schedule, common_vars):\n",
    "    \"\"\"\n",
    "    스케줄 코드에서 공통으로 나타나는 (0,1) for문을 제거하고 들여쓰기를 정리\n",
    "    \"\"\"\n",
    "    lines = schedule.split('\\n')\n",
    "    result_lines = []\n",
    "    \n",
    "    # 제거할 for문의 인덱스들을 먼저 찾기\n",
    "    remove_indices = set()\n",
    "    for_loop_indents = {}  # 제거될 for문의 인덱스 -> 들여쓰기 레벨\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        stripped = line.lstrip()\n",
    "        indent_level = len(line) - len(stripped)\n",
    "        \n",
    "        # (0,1) for문인지 확인\n",
    "        match = re.match(r'for\\s+(\\S+)\\s+\\(0,\\s*1\\)', stripped)\n",
    "        if match and match.group(1) in common_vars:\n",
    "            remove_indices.add(i)\n",
    "            for_loop_indents[i] = indent_level\n",
    "    \n",
    "    # 각 줄에 대해 들여쓰기를 얼마나 줄여야 하는지 계산\n",
    "    indent_reduction = [0] * len(lines)\n",
    "    \n",
    "    for idx in sorted(remove_indices):\n",
    "        base_indent = for_loop_indents[idx]\n",
    "        # 이 for문 다음부터 같거나 작은 들여쓰기가 나올 때까지 2칸씩 줄이기\n",
    "        for j in range(idx + 1, len(lines)):\n",
    "            if j in remove_indices:\n",
    "                continue\n",
    "            line = lines[j]\n",
    "            stripped = line.lstrip()\n",
    "            if not stripped:  # 빈 줄\n",
    "                continue\n",
    "            current_indent = len(line) - len(stripped)\n",
    "            \n",
    "            # 이 for문의 body인 경우 (들여쓰기가 더 큰 경우)\n",
    "            if current_indent > base_indent:\n",
    "                indent_reduction[j] += 2\n",
    "            else:\n",
    "                # 같거나 작은 들여쓰기 레벨이 나오면 이 for문 블록 종료\n",
    "                break\n",
    "    \n",
    "    # 제거하지 않는 줄들에 대해 들여쓰기를 조정하여 결과 생성\n",
    "    for i, line in enumerate(lines):\n",
    "        if i in remove_indices:\n",
    "            continue\n",
    "        \n",
    "        if not line.strip():  # 빈 줄\n",
    "            result_lines.append(line)\n",
    "            continue\n",
    "        \n",
    "        stripped = line.lstrip()\n",
    "        original_indent = len(line) - len(stripped)\n",
    "        new_indent = max(0, original_indent - indent_reduction[i])\n",
    "        result_lines.append(' ' * new_indent + stripped)\n",
    "    \n",
    "    return '\\n'.join(result_lines)\n",
    "\n",
    "\n",
    "def remove_commons(records):\n",
    "\n",
    "    common_for_loops = find_common_for_loops(records[\"schedules\"])\n",
    "    print(f\"발견된 공통 (0,1) for문 변수: {common_for_loops}\")\n",
    "\n",
    "\n",
    "    # 모든 스케줄에 적용\n",
    "    cleaned_schedules = []\n",
    "    records[\"extents\"] = []\n",
    "    records[\"unroll\"] = []\n",
    "    records[\"all\"] = []\n",
    "    for i, schedule in enumerate(records[\"schedules\"]):\n",
    "        extents = [float(x) for x in re.findall(r'\\(0,\\s*(\\d+)\\)', schedule)]\n",
    "\n",
    "    for i, schedule in enumerate(records[\"schedules\"]):\n",
    "        extents = [float(x) for x in re.findall(r'\\(0,\\s*(\\d+)\\)', schedule)]\n",
    "        unrolls = [float(x) for x in re.findall(r'auto_unroll:\\s*(\\d+)', schedule)]\n",
    "        records[\"extents\"].append(extents)\n",
    "        if unrolls == []:\n",
    "            unrolls = [0.0]\n",
    "        records[\"unroll\"].append(unrolls)\n",
    "        feature = extents+unrolls\n",
    "        records[\"all\"].append(np.array(feature, dtype=np.float32))\n",
    "        \n",
    "        cleaned = remove_common_for_loops_(schedule, common_for_loops)\n",
    "        cleaned_schedules.append(cleaned)\n",
    "    records[\"cleaned_schedules\"] = cleaned_schedules\n",
    "\n",
    "\n",
    "    total_removed = sum(len(orig.split('\\n')) - len(clean.split('\\n')) \n",
    "                        for orig, clean in zip(records['schedules'], cleaned_schedules))\n",
    "    avg_removed = total_removed / len(cleaned_schedules)\n",
    "    print(f\"제거된 줄 수: {avg_removed:.1f}\")\n",
    "    return records\n",
    "\n",
    "# records_merged = remove_commons(records_merged)\n",
    "records = remove_commons(records_all[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1663eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class FeatureRegressionDataset(Dataset):\n",
    "    def __init__(self, X, y, feature=None):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            self.X = torch.from_numpy(X).float()\n",
    "        else:\n",
    "            self.X = X\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        if self.y.ndim == 1:\n",
    "            self.y = self.y.unsqueeze(1)\n",
    "\n",
    "        self.feature = feature\n",
    "        if feature is not None:\n",
    "            if isinstance(feature, np.ndarray):\n",
    "                self.feature = torch.from_numpy(feature).float()\n",
    "            else:\n",
    "                self.feature = feature\n",
    "            \n",
    "            if self.feature.ndim == 1:\n",
    "                self.feature = self.feature.unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.feature is None:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        return self.X[idx], self.y[idx], self.feature[idx]\n",
    "\n",
    "\n",
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, X, feature=None):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            self.X = torch.from_numpy(X).float()\n",
    "        else:\n",
    "            self.X = X\n",
    "        \n",
    "        if isinstance(feature, np.ndarray):\n",
    "            self.feature = torch.from_numpy(feature).float()\n",
    "        else:\n",
    "            self.feature = feature\n",
    "        # feature shape이 (N,)이면 (N,1)로 바꿔주는 게 편할 때가 많음\n",
    "        if self.feature is not None and self.feature.ndim == 1:\n",
    "            self.feature = self.feature.unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.feature is None:\n",
    "            return self.X[idx]\n",
    "        return self.X[idx], self.feature[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd8864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class VAE_feature_head(nn.Module):\n",
    "    def __init__(self, input_dim, feature_dim=None, latent_dim=16, hidden_dim=128):\n",
    "        \"\"\"\n",
    "        input_dim: 2 * D (v_norm + is_zero concat한 차원)\n",
    "        latent_dim: latent space 차원\n",
    "        hidden_dim: MLP hidden 크기\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            \n",
    "            # 출력은 연속값이니까 activation 없이 그대로\n",
    "        )\n",
    "\n",
    "        if feature_dim is None:\n",
    "            self.use_feature = False\n",
    "        else:\n",
    "            self.use_feature = True\n",
    "            self.feature_predictor = nn.Sequential(\n",
    "                nn.Linear(latent_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, feature_dim),  # features.shape[1]는 feature 차원\n",
    "            )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def predict_feature(self, z):\n",
    "        return self.feature_predictor(z)\n",
    "\n",
    "    def forward(self, x, use_mean=True):\n",
    "        mu, logvar = self.encode(x)\n",
    "        if use_mean:\n",
    "            z = mu\n",
    "        else:\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        \n",
    "        if self.use_feature:\n",
    "            feature_pred = self.predict_feature(z)\n",
    "        else:\n",
    "            feature_pred = None\n",
    "        return x_recon, mu, logvar, z, feature_pred\n",
    "\n",
    "class L3Loss(torch.nn.Module):\n",
    "    def forward(self, pred, target):\n",
    "        return torch.mean(torch.abs(pred - target) ** 4)\n",
    "\n",
    "def vae_feature_loss(x_recon, x, mu, logvar, feature_pred, feature, alpha_recon=0, alpha_feature=0, beta=1.0):\n",
    "    \"\"\"\n",
    "    x, x_recon: (B, input_dim)\n",
    "    mu, logvar: (B, latent_dim)\n",
    "\n",
    "    beta: KL 가중치 (β-VAE 스타일로 조절)\n",
    "    \"\"\"\n",
    "    # reconstruction loss: MSE\n",
    "    recon_loss = F.mse_loss(x_recon, x, reduction=\"mean\")\n",
    "    # \n",
    "    # recon_loss = L3Loss()(x_recon, x)\n",
    "\n",
    "    feature_loss = F.mse_loss(feature_pred, feature, reduction=\"mean\") if feature_pred is not None else 0.0\n",
    "\n",
    "    # KL divergence: D_KL(q(z|x) || N(0, I))\n",
    "    kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    loss = alpha_recon * recon_loss + beta * kl + alpha_feature * feature_loss\n",
    "    return loss, recon_loss, kl, feature_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08123a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b057bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "input_data = np.log1p(np.array(records[\"all\"], dtype=np.float32))\n",
    "\n",
    "costs = np.array(records[\"costs\"], dtype=np.float32)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "input_data_scaled = scaler.fit_transform(input_data)\n",
    "\n",
    "\n",
    "X_train, X_val = train_test_split(\n",
    "    input_data_scaled,  test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# feature 없음\n",
    "train_dataset = FeatureDataset(X_train)\n",
    "val_dataset   = FeatureDataset(X_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=False)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=512, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17270272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "Experiment 1/1\n",
      "beta=0.01, alpha_recon=1.0, alpha_feature=1.0,\n",
      "epochs=300, latent_dim=64, hidden_dim=256, lr=0.001\n",
      "Early stopping at epoch 209\n",
      "epoch 209: loss=0.0217, recon=0.0110, kl=1.0667\n",
      "epoch 209: val loss=0.0171, val recon=0.0062, val kl=1.0904\n",
      "Recon R2 : 0.5937040065107005, Feature R2 : None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import itertools\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[-1]\n",
    "latent_dim = 64\n",
    "hidden_dim = 256\n",
    "\n",
    "\n",
    "hyperparameter = {\n",
    "    'beta': [0.01],\n",
    "    'alpha_recon': [1.0],\n",
    "    'alpha_feature': [1.0],\n",
    "    'latent_dim': [64],\n",
    "    'lr': [1e-3],\n",
    "}\n",
    "\n",
    "cnt = 0\n",
    "epochs = 300\n",
    "\n",
    "\n",
    "for vals in itertools.product(*hyperparameter.values()):\n",
    "    (beta, alpha_recon, alpha_feature, latent_dim, lr) = vals\n",
    "    cnt += 1\n",
    "    print(\"=============================================\")\n",
    "    print(f\"Experiment {cnt}/{len(list(itertools.product(*hyperparameter.values())))}\")\n",
    "    print(f\"beta={beta}, alpha_recon={alpha_recon}, alpha_feature={alpha_feature},\\nepochs={epochs}, latent_dim={latent_dim}, hidden_dim={hidden_dim}, lr={lr}\")\n",
    "\n",
    "    seed_everything(42)\n",
    "\n",
    "    vae = VAE_feature_head(input_dim=input_dim, latent_dim=latent_dim, hidden_dim=hidden_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(vae.parameters(), lr=lr)\n",
    "\n",
    "    # early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 30\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        vae.train()\n",
    "        for x_batch in train_loader:\n",
    "            if len(x_batch) == 2:\n",
    "                x_batch, feature_batch = x_batch\n",
    "                feature_batch = feature_batch.to(device)\n",
    "            else:\n",
    "                feature_batch = None\n",
    "            x_batch = x_batch.to(device)  # (N, D)\n",
    "            \n",
    "            \n",
    "\n",
    "            x_recon, mu, logvar, z, feature_pred = vae(x_batch, use_mean=False)\n",
    "\n",
    "            loss, recon_loss, kl, feature_loss = vae_feature_loss(x_recon, x_batch, mu, logvar, feature_pred, feature_batch, alpha_recon=alpha_recon, alpha_feature=alpha_feature, beta=beta)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        vae.eval()\n",
    "        for x_batch in val_loader:\n",
    "            if len(x_batch) == 2:\n",
    "                x_batch, feature_batch = x_batch\n",
    "                feature_batch = feature_batch.to(device)\n",
    "            else:\n",
    "                feature_batch = None\n",
    "            x_batch = x_batch.to(device)\n",
    "            if feature_batch is not None:\n",
    "                feature_batch = feature_batch.to(device)\n",
    "            x_recon, mu, logvar, z, feature_pred = vae(x_batch, use_mean=True)\n",
    "            val_loss, val_recon_loss, val_kl, val_feature_loss = vae_feature_loss(x_recon, x_batch, mu, logvar, feature_pred, feature_batch, alpha_recon=alpha_recon, alpha_feature=alpha_feature, beta=beta)\n",
    "            val_recon_r2 = r2_score(x_batch.detach().cpu().numpy(), x_recon.detach().cpu().numpy())\n",
    "            if feature_batch is not None:\n",
    "                val_feature_r2 = r2_score(feature_batch.detach().cpu().numpy(), feature_pred.detach().cpu().numpy())\n",
    "            else:\n",
    "                val_feature_r2 = None\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss.item()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "    print(f\"epoch {epoch}: loss={loss.item():.4f}, recon={recon_loss.item():.4f}, kl={kl.item():.4f}\")\n",
    "    print(f\"epoch {epoch}: val loss={val_loss.item():.4f}, val recon={val_recon_loss.item():.4f}, val kl={val_kl.item():.4f}\")\n",
    "\n",
    "    print(f\"Recon R2 : {val_recon_r2}, Feature R2 : {val_feature_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6310053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAECostPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    VAE 기반 Cost Regression 모델\n",
    "    \n",
    "    구조:\n",
    "    - input → segment_encoder → segment_sum → VAE encoder → z → cost_predictor → cost\n",
    "    \n",
    "    특징:\n",
    "    - Pretrained VAE encoder를 finetune (작은 learning rate)\n",
    "    - Cost predictor는 더 큰 learning rate로 학습\n",
    "    - 전체 forward 경로가 완전히 미분 가능 (detach, stop_grad 없음)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, feature_dim=None, hidden_dim=256, latent_dim=64, \n",
    "                 predictor_hidden=256, predictor_layers=2, dropout=0.1, use_feature=False):\n",
    "        super(VAECostPredictor, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # ========== Cost Predictor (새로 학습) ==========\n",
    "        predictor_modules = []\n",
    "        current_dim = latent_dim\n",
    "        for i in range(predictor_layers):\n",
    "            predictor_modules.extend([\n",
    "                nn.Linear(current_dim, predictor_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout) if i < predictor_layers - 1 else nn.Identity(),\n",
    "            ])\n",
    "            current_dim = predictor_hidden\n",
    "        predictor_modules.append(nn.Linear(predictor_hidden, 1))\n",
    "        \n",
    "        self.cost_predictor = nn.Sequential(*predictor_modules)\n",
    "\n",
    "        self.use_feature = use_feature\n",
    "        if self.use_feature:\n",
    "            pass\n",
    "            self.feature_predictor = nn.Sequential(\n",
    "                nn.Linear(latent_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, feature_dim),  # feature_dim는 feature 차원\n",
    "            )\n",
    "        \n",
    "    \n",
    "    def encode(self, input_data):\n",
    "        \"\"\"\n",
    "        Full encoding path: features → z\n",
    "        완전히 미분 가능\n",
    "        \"\"\"\n",
    "                \n",
    "        # VAE Encoder\n",
    "        h = self.encoder(input_data)\n",
    "        \n",
    "        mean = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        \n",
    "        return mean, logvar, input_data\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        \"\"\"Reparameterization trick - 미분 가능\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "    \n",
    "    def predict_cost(self, z):\n",
    "        \"\"\"z → cost prediction - 완전히 미분 가능\"\"\"\n",
    "        return self.cost_predictor(z).squeeze(-1)\n",
    "    \n",
    "    def predict_feature(self, z):\n",
    "        return self.feature_predictor(z)\n",
    "    \n",
    "    def forward(self, input_data, use_mean=True):\n",
    "        \"\"\"\n",
    "        Forward pass: input → z → cost\n",
    "        \n",
    "        Args:\n",
    "            use_mean: True면 reparameterize 대신 mean 사용 (inference용)\n",
    "        \n",
    "        Returns:\n",
    "            cost_pred: 예측된 cost\n",
    "            mean: latent mean\n",
    "            logvar: latent log-variance\n",
    "            z: sampled/mean latent vector\n",
    "        \"\"\"\n",
    "        mean, logvar, input_data = self.encode(input_data)\n",
    "        \n",
    "        if use_mean:\n",
    "            z = mean  # Inference시 deterministic\n",
    "        else:\n",
    "            z = self.reparameterize(mean, logvar)  # Training시 stochastic\n",
    "        \n",
    "        cost_pred = self.predict_cost(z)\n",
    "        \n",
    "        return cost_pred, mean, logvar, z\n",
    "    \n",
    "    def get_encoder_params(self):\n",
    "        \"\"\"Encoder 파라미터 (작은 lr)\"\"\"\n",
    "        encoder_params = []\n",
    "        encoder_params.extend(self.encoder.parameters())\n",
    "        encoder_params.extend(self.fc_mu.parameters())\n",
    "        encoder_params.extend(self.fc_logvar.parameters())\n",
    "        return encoder_params\n",
    "    \n",
    "    def get_cost_predictor_params(self):\n",
    "        \"\"\"Predictor 파라미터 (큰 lr)\"\"\"\n",
    "        return self.cost_predictor.parameters()\n",
    "    \n",
    "    def get_feature_predictor_params(self):\n",
    "        \"\"\"Feature Predictor 파라미터\"\"\"\n",
    "        return self.feature_predictor.parameters()\n",
    "\n",
    "    def load_pretrained_encoder(self, checkpoint):\n",
    "        \"\"\"Pretrained VAE encoder 가중치 로드\"\"\"\n",
    "        \n",
    "\n",
    "        vae_state = checkpoint\n",
    "        \n",
    "        # 매칭되는 키만 로드\n",
    "        encoder_keys = ['encoder', 'fc_mu', 'fc_logvar']\n",
    "        own_state = self.state_dict()\n",
    "        \n",
    "        loaded_keys = []\n",
    "        for name, param in vae_state.items():\n",
    "            if any(name.startswith(k) for k in encoder_keys):\n",
    "                if name in own_state and own_state[name].shape == param.shape:\n",
    "                    own_state[name].copy_(param)\n",
    "                    loaded_keys.append(name)\n",
    "        \n",
    "        # print(f\"Loaded {len(loaded_keys)} parameters from pretrained VAE\")\n",
    "        # return loaded_keys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b584ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_loss_fn(cost_pred, cost_true, loss_type='mse'):\n",
    "    \"\"\"\n",
    "    기본 회귀 손실 (MSE 또는 MAE)\n",
    "    \"\"\"\n",
    "    if loss_type == 'mse':\n",
    "        return F.mse_loss(cost_pred, cost_true)\n",
    "    else:  # mae\n",
    "        return F.l1_loss(cost_pred, cost_true)\n",
    "\n",
    "\n",
    "def pair_loss_fn(cost_pred, cost_true, margin=0.1):\n",
    "    \"\"\"\n",
    "    Pairwise ranking loss: 실제 cost 순서를 예측이 유지하도록.\n",
    "    cost_true[i] < cost_true[j] 이면 cost_pred[i] < cost_pred[j] + margin\n",
    "    \"\"\"\n",
    "    batch_size = cost_pred.size(0)\n",
    "    if batch_size < 2:\n",
    "        return torch.tensor(0.0, device=cost_pred.device)\n",
    "    \n",
    "    # 모든 쌍에 대해 ranking loss 계산\n",
    "    idx = torch.arange(batch_size, device=cost_pred.device)\n",
    "    i_idx, j_idx = torch.meshgrid(idx, idx, indexing='ij')\n",
    "    mask = i_idx < j_idx  # upper triangular only\n",
    "    \n",
    "    pred_i = cost_pred[i_idx[mask]]\n",
    "    pred_j = cost_pred[j_idx[mask]]\n",
    "    true_i = cost_true[i_idx[mask]]\n",
    "    true_j = cost_true[j_idx[mask]]\n",
    "    \n",
    "    # label: 1 if true_i < true_j, -1 otherwise\n",
    "    labels = torch.sign(true_j - true_i).float()\n",
    "    \n",
    "    # Margin ranking loss\n",
    "    loss = F.margin_ranking_loss(pred_j.view(-1), pred_i.view(-1), labels.view(-1), margin=margin)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def smooth_loss_fn(model, z, noise_std=0.1):\n",
    "    \"\"\"\n",
    "    Smoothness loss: z에 작은 노이즈를 더했을 때 예측이 크게 변하지 않도록.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z_noisy = z + noise_std * torch.randn_like(z)\n",
    "    \n",
    "    cost_original = model.predict_cost(z)\n",
    "    cost_noisy = model.predict_cost(z_noisy)\n",
    "    \n",
    "    smooth_loss = F.mse_loss(cost_original, cost_noisy)\n",
    "    return smooth_loss\n",
    "\n",
    "\n",
    "def kld_loss_fn(mean, logvar):\n",
    "    \"\"\"\n",
    "    KL Divergence: q(z|x) || N(0, I)\n",
    "    \"\"\"\n",
    "    kld = -0.5 * torch.mean(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "    return kld\n",
    "\n",
    "def feature_loss_fn(use_feature, feature_pred, feature_true, coef=0.1):\n",
    "    \"\"\"\n",
    "    Feature 예측 손실 (MSE)\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if not use_feature:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "    return F.mse_loss(feature_pred, feature_true) * coef\n",
    "\n",
    "\n",
    "def compute_total_loss(model, cost_pred, mean, logvar, z, labels, feature, config, return_components=True):\n",
    "    \"\"\"\n",
    "    Total loss 계산 (Segment 기반 데이터용).\n",
    "    total_loss = reg_loss + λ_pair * pair_loss + γ * smooth_loss + β * kld_loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # Individual losses\n",
    "    reg = reg_loss_fn(cost_pred, labels, loss_type=config.get('loss_type', 'mse'))\n",
    "    pair = pair_loss_fn(cost_pred.view(-1), labels.view(-1), margin=config.get('margin', 0.1))\n",
    "    smooth = smooth_loss_fn(model, z, noise_std=config.get('noise_std', 0.1))\n",
    "    kld = kld_loss_fn(mean, logvar)\n",
    "    feature_loss = feature_loss_fn(model.use_feature, None, feature, coef=0)\n",
    "    \n",
    "    # Weighted sum\n",
    "    total = config['lambda_reg'] * reg + config['lambda_pair'] * pair + config['gamma'] * smooth + config['beta'] * kld + feature_loss\n",
    "    \n",
    "    if return_components:\n",
    "        return total, {\n",
    "            'reg_loss': reg.item(),\n",
    "            'pair_loss': pair.item(),\n",
    "            'smooth_loss': smooth.item(),\n",
    "            'kld_loss': kld.item(),\n",
    "            'feature_loss': feature_loss.item(),\n",
    "        }\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ef7144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_accuracy(cost_pred, labels):\n",
    "    \"\"\"\n",
    "    cost_pred, labels: (B,) 텐서\n",
    "    \"\"\"\n",
    "    seed_everything(42)\n",
    "    n_samples = min(2000, len(cost_pred))\n",
    "    sample_indices = np.random.choice(len(cost_pred), n_samples, replace=False)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i + 1, n_samples):\n",
    "            idx_i = sample_indices[i]\n",
    "            idx_j = sample_indices[j]\n",
    "            pred_diff = cost_pred[idx_i] - cost_pred[idx_j]\n",
    "            true_diff = labels[idx_i] - labels[idx_j]\n",
    "            if (pred_diff * true_diff) > 0:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy\n",
    "\n",
    "def recall_at_k(pred, labels, k=1):\n",
    "    true_best_idx = torch.argmax(labels)\n",
    "    topk_pred_idx = torch.topk(pred, k=k, largest=True).indices\n",
    "\n",
    "    return int((topk_pred_idx == true_best_idx).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fe4fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(records, train_size):\n",
    "    input_data = np.log1p(np.array(records[\"all\"], dtype=np.float32))\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    input_data_scaled = scaler.fit_transform(input_data)\n",
    "\n",
    "    costs = np.array(records[\"costs\"], dtype=np.float32)\n",
    "\n",
    "    random_indices = np.random.permutation(len(input_data))\n",
    "\n",
    "\n",
    "    train_indices = random_indices[:train_size]\n",
    "    val_indices   = random_indices[train_size:]\n",
    "\n",
    "    X_train = input_data_scaled[train_indices]\n",
    "    X_val = input_data_scaled[val_indices]\n",
    "    y_train = costs[train_indices]\n",
    "    y_val = costs[val_indices]\n",
    "\n",
    "    costs = np.array(records[\"costs\"])\n",
    "\n",
    "    # X_train, X_val, y_train, y_val = train_test_split(\n",
    "    #     input_data_scaled, costs, test_size=0.2, random_state=42\n",
    "    # )\n",
    "\n",
    "    train_dataset = FeatureRegressionDataset(X_train, y_train)\n",
    "    val_dataset   = FeatureRegressionDataset(X_val,   y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=False)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=512, shuffle=False)\n",
    "\n",
    "\n",
    "    y_mean = y_train.mean()\n",
    "    y_std = y_train.std() + 1e-8  # 0 나누기 방지용 작은 값 추가\n",
    "    print(f\"y_train mean: {y_mean}, std: {y_std}\")\n",
    "    return train_loader, val_loader, y_mean, y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47e0fe3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size : 3257\n",
      "=============================================\n",
      "Experiment 1/1\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, gamma=0.01, beta=0.01\n",
      "noise_std=0.001\n",
      "encoder_lr=1e-05, feature_predictor_lr=0, cost_predictor_lr=1e-05\n",
      "epochs=1000, seed=2024\n",
      "y_train mean: 5.647425651550293, std: 1.6238420109613037\n",
      "Train loss epoch 1000 : reg= 2.1718 rank= 0.0761 kl= 1.4551\n",
      "Val loss epoch 1000: reg= 2.7296 rank= 0.1572 kl= 1.4124\n",
      "Regression R2 : 0.4349, Rank R2 : 0.8844\n",
      "Recall@1 : 0\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "input_dim = X_train.shape[-1]\n",
    "latent_dim = 64\n",
    "hidden_dim = 256\n",
    "\n",
    "print(\"Train size :\", len(train_dataset))\n",
    "\n",
    "hyperparameter = {\n",
    "    # 'alpha': [1e-4, 1e-3],\n",
    "    \n",
    "    \n",
    "    'lambda_reg' : [0.01],\n",
    "    'lambda_pair': [3.0],\n",
    "    'margin_scale': [0.3],\n",
    "    'gamma': [0.01],\n",
    "    'beta': [0.01],\n",
    "    'noise_std': [0.001],\n",
    "    # 'alpha': [1e-5],\n",
    "\n",
    "    'encoder_lr': [1e-5],\n",
    "    'feature_predictor_lr': [0],\n",
    "    'cost_predictor_lr': [1e-5],\n",
    "    \n",
    "    'epochs' : [1000], \n",
    "    # 'seed': range(2023, 2028),\n",
    "    'seed': [2024],\n",
    "    'use_feature': [False],\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "all_reg_results = []\n",
    "cnt = 0\n",
    "\n",
    "for vals in itertools.product(*hyperparameter.values()):\n",
    "    \n",
    "    \n",
    "    (lambda_reg, lambda_pair, margin_scale, gamma, beta, noise_std, \n",
    "     encoder_lr, feature_predictor_lr, cost_predictor_lr, epochs, seed, use_feature) = vals\n",
    "    \n",
    "    cnt += 1\n",
    "    print(\"=============================================\")\n",
    "    print(f\"Experiment {cnt}/{len(list(itertools.product(*hyperparameter.values())))}\")\n",
    "    print(f\"lambda_reg={lambda_reg}, lambda_pair={lambda_pair}, margin_scale={margin_scale}, gamma={gamma}, beta={beta}\")\n",
    "    print(f\"noise_std={noise_std}\\nencoder_lr={encoder_lr}, feature_predictor_lr={feature_predictor_lr}, cost_predictor_lr={cost_predictor_lr}\")\n",
    "    print(f\"epochs={epochs}, seed={seed}\")\n",
    "    \n",
    "    seed_everything(seed)\n",
    "    \n",
    "\n",
    "    train_loader, val_loader, y_mean, y_std = make_dataset(records, train_size=256)\n",
    "\n",
    "    config = {\n",
    "                'encoder_lr': encoder_lr,\n",
    "                'feature_predictor_lr': feature_predictor_lr,\n",
    "                'cost_predictor_lr': cost_predictor_lr,\n",
    "                'lambda_reg' : lambda_reg,\n",
    "                'lambda_pair': lambda_pair,\n",
    "                'gamma': gamma,\n",
    "                'beta': beta,\n",
    "                'margin': margin_scale * y_std,\n",
    "                'noise_std': noise_std,\n",
    "                'loss_type': 'mse'\n",
    "            }\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    vae_cost_model = VAECostPredictor(input_dim=input_dim, \n",
    "                                    latent_dim=latent_dim, \n",
    "                                    hidden_dim=hidden_dim, \n",
    "                                    predictor_layers=2,\n",
    "                                    dropout=0.1, use_feature=use_feature).to(device)\n",
    "    vae_cost_model.load_pretrained_encoder(vae.state_dict())\n",
    "\n",
    "    # for param in vae_cost_model.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    optimizer = torch.optim.AdamW([\n",
    "            {'params': vae_cost_model.get_encoder_params(), 'lr': config['encoder_lr']},\n",
    "            # {'params': vae_cost_model.get_feature_predictor_params(), 'lr': config['feature_predictor_lr']},\n",
    "            {'params': vae_cost_model.get_cost_predictor_params(), 'lr': config['cost_predictor_lr']}\n",
    "        ], weight_decay=1e-5)\n",
    "    \n",
    "\n",
    "\n",
    "    epochs = epochs\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 20\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        vae_cost_model.train()\n",
    "        for x_batch, labels in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            labels = labels.to(device).squeeze(-1)\n",
    "            \n",
    "        \n",
    "            cost_pred, mean, logvar, z = vae_cost_model(x_batch, use_mean=True)\n",
    "\n",
    "            train_loss, train_components = compute_total_loss(vae_cost_model, \n",
    "                                                    cost_pred, mean, logvar, z, labels, None, config)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(vae_cost_model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "        \n",
    "\n",
    "        if epoch % epochs == 0:\n",
    "            vae_cost_model.eval()\n",
    "            with torch.no_grad():\n",
    "                all_preds = []\n",
    "                all_labels = []\n",
    "                for x_batch, labels in val_loader:\n",
    "                    x_batch = x_batch.to(device)\n",
    "                    labels = labels.to(device).squeeze(-1)\n",
    "\n",
    "                    cost_pred, mean, logvar, z = vae_cost_model(x_batch, use_mean=True)\n",
    "\n",
    "                    val_loss, val_components = compute_total_loss(vae_cost_model, cost_pred, mean, logvar, z, labels, None, config)\n",
    "                val_reg_r2 = r2_score(cost_pred.detach().cpu().numpy(), labels.detach().cpu().numpy())\n",
    "                \n",
    "                print(f\"Train loss epoch {epoch} : reg={train_components['reg_loss']: .4f} rank={train_components['pair_loss']: .4f} kl={train_components['kld_loss']: .4f}\")\n",
    "                print(f\"Val loss epoch {epoch}: reg={val_components['reg_loss']: .4f} rank={val_components['pair_loss']: .4f} kl={val_components['kld_loss']: .4f}\")\n",
    "                \n",
    "                print(f\"Regression R2 : {val_reg_r2:.4f}, \", end='')\n",
    "        \n",
    "        # rank r2 계산\n",
    "        vae_cost_model.eval()\n",
    "        with torch.no_grad():\n",
    "            if epoch % epochs == 0:\n",
    "                input_data_tensor = torch.from_numpy(input_data_scaled).float().to(device)\n",
    "                all_preds = vae_cost_model(input_data_tensor, use_mean=True)[0].detach().cpu().numpy()\n",
    "                val_rank_r2 = pair_accuracy(all_preds, costs)\n",
    "                recall_top_k = recall_at_k(torch.tensor(all_preds), torch.from_numpy(costs), k=1)\n",
    "                print(f\"Rank R2 : {val_rank_r2:.4f}\")\n",
    "                print(f\"Recall@1 : {recall_top_k}\")\n",
    "                \n",
    "    all_reg_results.append({\n",
    "        \"lambda_reg\": lambda_reg,\n",
    "        \"lambda_pair\": lambda_pair,\n",
    "        \"margin_scale\": margin_scale,\n",
    "        \"gamma\": gamma,\n",
    "        \"beta\": beta,\n",
    "        \"noise_std\": noise_std,\n",
    "        \"encoder_lr\": encoder_lr,\n",
    "        \"feature_predictor_lr\": feature_predictor_lr,\n",
    "        \"cost_predictor_lr\": cost_predictor_lr,\n",
    "        \"use_feature\": use_feature,\n",
    "        \"seed\": seed,\n",
    "        \"reg_r2\": val_reg_r2,\n",
    "        \"rank_r2\": val_rank_r2,\n",
    "        \"recall@64\": recall_top_k\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae06a36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "발견된 공통 (0,1) for문 변수: {'yy_c.0', 'ry.1', 'rx.1', 'yy_c.2', 'ff_c.2', 'xx_c.2', 'ff_c.0', 'yy_c.1', 'ff_c.1', 'nn_c.2', 'rx.0', 'ry.0', 'nn_c.0', 'ry.2', 'rx.2', 'xx_c.1', 'nn_c.1', 'xx_c.0'}\n",
      "제거된 줄 수: 18.0\n",
      "Regression R2 : 0.8400, Rank R2 : 0.8941\n",
      "Recall@1 : 0\n",
      "발견된 공통 (0,1) for문 변수: {'yy_c.0', 'ry.1', 'rx.1', 'yy_c.2', 'ff_c.2', 'xx_c.2', 'ff_c.0', 'yy_c.1', 'ff_c.1', 'nn_c.2', 'rx.0', 'ry.0', 'nn_c.0', 'ry.2', 'rx.2', 'xx_c.1', 'nn_c.1', 'xx_c.0'}\n",
      "제거된 줄 수: 18.0\n",
      "Regression R2 : 0.3273, Rank R2 : 0.8836\n",
      "Recall@1 : 0\n",
      "발견된 공통 (0,1) for문 변수: {'yy_c.0', 'ry.1', 'rx.1', 'yy_c.2', 'ff_c.2', 'xx_c.2', 'ff_c.0', 'yy_c.1', 'ff_c.1', 'nn_c.2', 'rx.0', 'ry.0', 'nn_c.0', 'ry.2', 'rx.2', 'xx_c.1', 'nn_c.1', 'xx_c.0'}\n",
      "제거된 줄 수: 18.0\n",
      "Regression R2 : 0.8626, Rank R2 : 0.8968\n",
      "Recall@1 : 0\n",
      "발견된 공통 (0,1) for문 변수: {'yy_c.0', 'ry.1', 'rx.1', 'yy_c.2', 'ff_c.2', 'xx_c.2', 'ff_c.0', 'yy_c.1', 'ff_c.1', 'nn_c.2', 'rx.0', 'ry.0', 'nn_c.0', 'ry.2', 'rx.2', 'xx_c.1', 'nn_c.1', 'xx_c.0'}\n",
      "제거된 줄 수: 18.0\n",
      "Regression R2 : 0.8001, Rank R2 : 0.8951\n",
      "Recall@1 : 0\n",
      "발견된 공통 (0,1) for문 변수: {'yy_c.0', 'ry.1', 'rx.1', 'yy_c.2', 'ff_c.2', 'xx_c.2', 'ff_c.0', 'yy_c.1', 'ff_c.1', 'nn_c.2', 'rx.0', 'ry.0', 'nn_c.0', 'ry.2', 'rx.2', 'xx_c.1', 'nn_c.1', 'xx_c.0'}\n",
      "제거된 줄 수: 18.0\n",
      "Regression R2 : 0.7953, Rank R2 : 0.8891\n",
      "Recall@1 : 0\n",
      "발견된 공통 (0,1) for문 변수: {'yy_c.0', 'ry.1', 'rx.1', 'yy_c.2', 'ff_c.2', 'xx_c.2', 'ff_c.0', 'yy_c.1', 'ff_c.1', 'nn_c.2', 'rx.0', 'ry.0', 'nn_c.0', 'ry.2', 'rx.2', 'xx_c.1', 'nn_c.1', 'xx_c.0'}\n",
      "제거된 줄 수: 18.0\n",
      "Regression R2 : 0.7153, Rank R2 : 0.8640\n",
      "Recall@1 : 0\n",
      "발견된 공통 (0,1) for문 변수: {'yy_c.0', 'ry.1', 'rx.1', 'yy_c.2', 'ff_c.2', 'xx_c.2', 'ff_c.0', 'yy_c.1', 'ff_c.1', 'nn_c.2', 'rx.0', 'ry.0', 'nn_c.0', 'ry.2', 'rx.2', 'xx_c.1', 'nn_c.1', 'xx_c.0'}\n",
      "제거된 줄 수: 18.0\n",
      "Regression R2 : 0.3860, Rank R2 : 0.8783\n",
      "Recall@1 : 0\n",
      "발견된 공통 (0,1) for문 변수: {'yy_c.0', 'ry.1', 'rx.1', 'yy_c.2', 'ff_c.2', 'xx_c.2', 'ff_c.0', 'yy_c.1', 'ff_c.1', 'nn_c.2', 'rx.0', 'ry.0', 'nn_c.0', 'ry.2', 'rx.2', 'xx_c.1', 'nn_c.1', 'xx_c.0'}\n",
      "제거된 줄 수: 18.0\n",
      "Regression R2 : 0.6628, Rank R2 : 0.8603\n",
      "Recall@1 : 0\n",
      "발견된 공통 (0,1) for문 변수: {'yy_c.0', 'ry.1', 'rx.1', 'yy_c.2', 'ff_c.2', 'xx_c.2', 'ff_c.0', 'yy_c.1', 'ff_c.1', 'nn_c.2', 'ry.0', 'rx.0', 'nn_c.0', 'ry.2', 'rx.2', 'xx_c.1', 'nn_c.1', 'xx_c.0'}\n",
      "제거된 줄 수: 18.0\n",
      "Regression R2 : 0.5705, Rank R2 : 0.8964\n",
      "Recall@1 : 0\n",
      "발견된 공통 (0,1) for문 변수: {'yy_c.0', 'ry.1', 'rx.1', 'yy_c.2', 'ff_c.2', 'xx_c.2', 'ff_c.0', 'yy_c.1', 'ff_c.1', 'nn_c.2', 'rx.0', 'ry.0', 'nn_c.0', 'ry.2', 'rx.2', 'xx_c.1', 'nn_c.1', 'xx_c.0'}\n",
      "제거된 줄 수: 18.0\n",
      "Regression R2 : 0.2282, Rank R2 : 0.8465\n",
      "Recall@1 : 0\n",
      "발견된 공통 (0,1) for문 변수: {'yy_c.0', 'ry.1', 'rx.1', 'yy_c.2', 'ff_c.2', 'xx_c.2', 'ff_c.0', 'yy_c.1', 'ff_c.1', 'nn_c.2', 'rx.0', 'ry.0', 'nn_c.0', 'ry.2', 'rx.2', 'xx_c.1', 'nn_c.1', 'xx_c.0'}\n",
      "제거된 줄 수: 18.0\n",
      "Regression R2 : 0.6264, Rank R2 : 0.8678\n",
      "Recall@1 : 0\n",
      "발견된 공통 (0,1) for문 변수: {'yy_c.0', 'ry.1', 'rx.1', 'yy_c.2', 'ff_c.2', 'xx_c.2', 'ff_c.0', 'yy_c.1', 'ff_c.1', 'nn_c.2', 'rx.0', 'ry.0', 'nn_c.0', 'ry.2', 'rx.2', 'xx_c.1', 'nn_c.1', 'xx_c.0'}\n",
      "제거된 줄 수: 18.0\n",
      "Regression R2 : 0.4705, Rank R2 : 0.8905\n",
      "Recall@1 : 1\n",
      "발견된 공통 (0,1) for문 변수: {'yy_c.0', 'ry.1', 'rx.1', 'yy_c.2', 'ff_c.2', 'xx_c.2', 'ff_c.0', 'yy_c.1', 'ff_c.1', 'nn_c.2', 'rx.0', 'ry.0', 'nn_c.0', 'ry.2', 'rx.2', 'xx_c.1', 'nn_c.1', 'xx_c.0'}\n",
      "제거된 줄 수: 18.0\n",
      "Regression R2 : 0.8160, Rank R2 : 0.8859\n",
      "Recall@1 : 0\n",
      "발견된 공통 (0,1) for문 변수: {'yy_c.0', 'ry.1', 'rx.1', 'yy_c.2', 'ff_c.2', 'xx_c.2', 'ff_c.0', 'yy_c.1', 'ff_c.1', 'nn_c.2', 'rx.0', 'ry.0', 'nn_c.0', 'ry.2', 'rx.2', 'xx_c.1', 'nn_c.1', 'xx_c.0'}\n",
      "제거된 줄 수: 18.0\n",
      "Regression R2 : 0.4493, Rank R2 : 0.8832\n",
      "Recall@1 : 0\n",
      "발견된 공통 (0,1) for문 변수: {'yy_c.0', 'ry.1', 'rx.1', 'yy_c.2', 'ff_c.2', 'xx_c.2', 'ff_c.0', 'yy_c.1', 'ff_c.1', 'nn_c.2', 'rx.0', 'ry.0', 'nn_c.0', 'ry.2', 'rx.2', 'xx_c.1', 'nn_c.1', 'xx_c.0'}\n",
      "제거된 줄 수: 18.0\n",
      "Regression R2 : 0.6894, Rank R2 : 0.8847\n",
      "Recall@1 : 0\n",
      "발견된 공통 (0,1) for문 변수: {'yy_c.0', 'ry.1', 'rx.1', 'yy_c.2', 'ff_c.2', 'xx_c.2', 'ff_c.0', 'yy_c.1', 'ff_c.1', 'nn_c.2', 'rx.0', 'ry.0', 'nn_c.0', 'ry.2', 'rx.2', 'xx_c.1', 'nn_c.1', 'xx_c.0'}\n",
      "제거된 줄 수: 18.0\n",
      "Regression R2 : 0.8141, Rank R2 : 0.8947\n",
      "Recall@1 : 1\n"
     ]
    }
   ],
   "source": [
    "reg_results = []\n",
    "rank_results = []\n",
    "topk_results = []\n",
    "for r in records_all[1:]:\n",
    "    record_other = remove_commons(r)\n",
    "    input_data_other = np.log1p(np.array(record_other[\"all\"], dtype=np.float32))\n",
    "    costs_other = np.array(record_other[\"costs\"], dtype=np.float32)\n",
    "    input_data_scaled_other = scaler.fit_transform(input_data_other)\n",
    "\n",
    "    vae_cost_model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_data_tensor = torch.from_numpy(input_data_scaled_other).float().to(device)\n",
    "        all_preds = vae_cost_model(input_data_tensor, use_mean=True)[0].detach().cpu().numpy()\n",
    "        val_reg_r2 = r2_score(all_preds, costs_other)\n",
    "        val_rank_r2 = pair_accuracy(all_preds, costs_other)\n",
    "        recall_top_k = recall_at_k(torch.tensor(all_preds), torch.from_numpy(costs_other), k=1)\n",
    "\n",
    "        reg_results.append(val_reg_r2)\n",
    "        rank_results.append(val_rank_r2)\n",
    "        topk_results.append(recall_top_k)\n",
    "\n",
    "        \n",
    "        print(f\"Regression R2 : {val_reg_r2:.4f}, \", end='')\n",
    "        print(f\"Rank R2 : {val_rank_r2:.4f}\")\n",
    "        print(f\"Recall@1 : {recall_top_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "839517a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_names = [os.path.basename(f) for f in json_files[1:]]\n",
    "shapes = []\n",
    "for name in json_file_names:\n",
    "    name_ = name.replace(\"([0c9a5ba46ffc5e1a9e5641018527117f,\", \"\")\n",
    "    name_ = name_.replace(\"],cuda).json\", \"\")\n",
    "    shapes.append(name_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b24a35db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>shape</th>\n",
       "      <th>regression_r2</th>\n",
       "      <th>ranking_r2</th>\n",
       "      <th>recall@1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4,15,15,112,1,1,112,672,1,1,1,672,4,15,15,672</td>\n",
       "      <td>0.8400</td>\n",
       "      <td>0.8941</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4,14,14,80,1,1,80,184,1,1,1,184,4,14,14,184</td>\n",
       "      <td>0.3273</td>\n",
       "      <td>0.8836</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4,15,15,80,1,1,80,480,1,1,1,480,4,15,15,480</td>\n",
       "      <td>0.8626</td>\n",
       "      <td>0.8968</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4,14,14,80,1,1,80,480,1,1,1,480,4,14,14,480</td>\n",
       "      <td>0.8001</td>\n",
       "      <td>0.8951</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4,7,7,160,1,1,160,960,1,1,1,960,4,7,7,960</td>\n",
       "      <td>0.7953</td>\n",
       "      <td>0.8891</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>4,32,32,40,1,1,40,240,1,1,1,240,4,32,32,240</td>\n",
       "      <td>0.7153</td>\n",
       "      <td>0.8640</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>4,14,14,80,1,1,80,200,1,1,1,200,4,14,14,200</td>\n",
       "      <td>0.3860</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>4,30,30,40,1,1,40,240,1,1,1,240,4,30,30,240</td>\n",
       "      <td>0.6628</td>\n",
       "      <td>0.8603</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>4,16,16,80,1,1,80,200,1,1,1,200,4,16,16,200</td>\n",
       "      <td>0.5705</td>\n",
       "      <td>0.8964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>4,15,15,80,1,1,80,200,1,1,1,200,4,15,15,200</td>\n",
       "      <td>0.2282</td>\n",
       "      <td>0.8465</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>4,28,28,40,1,1,40,240,1,1,1,240,4,28,28,240</td>\n",
       "      <td>0.6264</td>\n",
       "      <td>0.8678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>4,16,16,80,1,1,80,184,1,1,1,184,4,16,16,184</td>\n",
       "      <td>0.4705</td>\n",
       "      <td>0.8905</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>4,14,14,112,1,1,112,672,1,1,1,672,4,14,14,672</td>\n",
       "      <td>0.8160</td>\n",
       "      <td>0.8859</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>4,15,15,80,1,1,80,184,1,1,1,184,4,15,15,184</td>\n",
       "      <td>0.4493</td>\n",
       "      <td>0.8832</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>4,8,8,160,1,1,160,960,1,1,1,960,4,8,8,960</td>\n",
       "      <td>0.6894</td>\n",
       "      <td>0.8847</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>4,16,16,80,1,1,80,480,1,1,1,480,4,16,16,480</td>\n",
       "      <td>0.8141</td>\n",
       "      <td>0.8947</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    experiment_id                                          shape  \\\n",
       "0               0  4,15,15,112,1,1,112,672,1,1,1,672,4,15,15,672   \n",
       "1               1    4,14,14,80,1,1,80,184,1,1,1,184,4,14,14,184   \n",
       "2               2    4,15,15,80,1,1,80,480,1,1,1,480,4,15,15,480   \n",
       "3               3    4,14,14,80,1,1,80,480,1,1,1,480,4,14,14,480   \n",
       "4               4      4,7,7,160,1,1,160,960,1,1,1,960,4,7,7,960   \n",
       "5               5    4,32,32,40,1,1,40,240,1,1,1,240,4,32,32,240   \n",
       "6               6    4,14,14,80,1,1,80,200,1,1,1,200,4,14,14,200   \n",
       "7               7    4,30,30,40,1,1,40,240,1,1,1,240,4,30,30,240   \n",
       "8               8    4,16,16,80,1,1,80,200,1,1,1,200,4,16,16,200   \n",
       "9               9    4,15,15,80,1,1,80,200,1,1,1,200,4,15,15,200   \n",
       "10             10    4,28,28,40,1,1,40,240,1,1,1,240,4,28,28,240   \n",
       "11             11    4,16,16,80,1,1,80,184,1,1,1,184,4,16,16,184   \n",
       "12             12  4,14,14,112,1,1,112,672,1,1,1,672,4,14,14,672   \n",
       "13             13    4,15,15,80,1,1,80,184,1,1,1,184,4,15,15,184   \n",
       "14             14      4,8,8,160,1,1,160,960,1,1,1,960,4,8,8,960   \n",
       "15             15    4,16,16,80,1,1,80,480,1,1,1,480,4,16,16,480   \n",
       "\n",
       "    regression_r2  ranking_r2  recall@1  \n",
       "0          0.8400      0.8941         0  \n",
       "1          0.3273      0.8836         0  \n",
       "2          0.8626      0.8968         0  \n",
       "3          0.8001      0.8951         0  \n",
       "4          0.7953      0.8891         0  \n",
       "5          0.7153      0.8640         0  \n",
       "6          0.3860      0.8783         0  \n",
       "7          0.6628      0.8603         0  \n",
       "8          0.5705      0.8964         0  \n",
       "9          0.2282      0.8465         0  \n",
       "10         0.6264      0.8678         0  \n",
       "11         0.4705      0.8905         1  \n",
       "12         0.8160      0.8859         0  \n",
       "13         0.4493      0.8832         0  \n",
       "14         0.6894      0.8847         0  \n",
       "15         0.8141      0.8947         1  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# row는 json file 이름, column은 metric\n",
    "# 소수점 셋째 자리까지 반올림\n",
    "# index column 이름 추가\n",
    "reg_results_df = pd.DataFrame({\n",
    "    \"shape\": shapes,\n",
    "    \"regression_r2\": reg_results,\n",
    "    \"ranking_r2\": rank_results,\n",
    "    \"recall@1\": topk_results\n",
    "}).round(4)\n",
    "reg_results_df = reg_results_df.reset_index().rename(columns={\"index\": \"experiment_id\"})\n",
    "\n",
    "reg_results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
