{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d665df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "\n",
    "project_root = \"/root/work/tenset\"\n",
    "os.environ[\"TVM_HOME\"] = f\"{project_root}\"\n",
    "os.environ[\"TVM_LIBRARY_PATH\"] = f\"{project_root}/build\"\n",
    "if f\"{project_root}/python\" not in sys.path:\n",
    "    sys.path.insert(0, f\"{project_root}/python\")\n",
    "    \n",
    "\n",
    "sys.path = [p for p in sys.path if not p.startswith(f\"{project_root}/build\")]\n",
    "sys.path.append(f\"{project_root}/build\")\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = f\"{project_root}/build:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb3f4c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from ['/root/work/tenset/dataset/measure_records_tenset/k80/([0c9a5ba46ffc5e1a9e5641018527117f,4,16,16,112,1,1,112,672,1,1,1,672,4,16,16,672],cuda).json', '/root/work/tenset/dataset/measure_records_tenset/k80/([0c9a5ba46ffc5e1a9e5641018527117f,4,15,15,112,1,1,112,672,1,1,1,672,4,15,15,672],cuda).json', '/root/work/tenset/dataset/measure_records_tenset/k80/([0c9a5ba46ffc5e1a9e5641018527117f,4,14,14,80,1,1,80,184,1,1,1,184,4,14,14,184],cuda).json', '/root/work/tenset/dataset/measure_records_tenset/k80/([0c9a5ba46ffc5e1a9e5641018527117f,4,15,15,80,1,1,80,480,1,1,1,480,4,15,15,480],cuda).json', '/root/work/tenset/dataset/measure_records_tenset/k80/([0c9a5ba46ffc5e1a9e5641018527117f,4,14,14,80,1,1,80,480,1,1,1,480,4,14,14,480],cuda).json', '/root/work/tenset/dataset/measure_records_tenset/k80/([0c9a5ba46ffc5e1a9e5641018527117f,4,7,7,160,1,1,160,960,1,1,1,960,4,7,7,960],cuda).json', '/root/work/tenset/dataset/measure_records_tenset/k80/([0c9a5ba46ffc5e1a9e5641018527117f,4,32,32,40,1,1,40,240,1,1,1,240,4,32,32,240],cuda).json', '/root/work/tenset/dataset/measure_records_tenset/k80/([0c9a5ba46ffc5e1a9e5641018527117f,4,14,14,80,1,1,80,200,1,1,1,200,4,14,14,200],cuda).json', '/root/work/tenset/dataset/measure_records_tenset/k80/([0c9a5ba46ffc5e1a9e5641018527117f,4,30,30,40,1,1,40,240,1,1,1,240,4,30,30,240],cuda).json', '/root/work/tenset/dataset/measure_records_tenset/k80/([0c9a5ba46ffc5e1a9e5641018527117f,4,16,16,80,1,1,80,200,1,1,1,200,4,16,16,200],cuda).json', '/root/work/tenset/dataset/measure_records_tenset/k80/([0c9a5ba46ffc5e1a9e5641018527117f,4,15,15,80,1,1,80,200,1,1,1,200,4,15,15,200],cuda).json', '/root/work/tenset/dataset/measure_records_tenset/k80/([0c9a5ba46ffc5e1a9e5641018527117f,4,28,28,40,1,1,40,240,1,1,1,240,4,28,28,240],cuda).json', '/root/work/tenset/dataset/measure_records_tenset/k80/([0c9a5ba46ffc5e1a9e5641018527117f,4,16,16,80,1,1,80,184,1,1,1,184,4,16,16,184],cuda).json', '/root/work/tenset/dataset/measure_records_tenset/k80/([0c9a5ba46ffc5e1a9e5641018527117f,4,14,14,112,1,1,112,672,1,1,1,672,4,14,14,672],cuda).json', '/root/work/tenset/dataset/measure_records_tenset/k80/([0c9a5ba46ffc5e1a9e5641018527117f,4,15,15,80,1,1,80,184,1,1,1,184,4,15,15,184],cuda).json', '/root/work/tenset/dataset/measure_records_tenset/k80/([0c9a5ba46ffc5e1a9e5641018527117f,4,8,8,160,1,1,160,960,1,1,1,960,4,8,8,960],cuda).json', '/root/work/tenset/dataset/measure_records_tenset/k80/([0c9a5ba46ffc5e1a9e5641018527117f,4,16,16,80,1,1,80,480,1,1,1,480,4,16,16,480],cuda).json']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sys.path.append(\"/root/work/tenset/scripts\")\n",
    "from print_programs import return_all_states\n",
    "from make_dataset import load_and_register_tasks\n",
    "from tvm import auto_scheduler\n",
    "from tvm.auto_scheduler.dataset import Dataset, make_dataset_from_log_file\n",
    "from glob import glob\n",
    "# json_file = \"/root/work/tenset/dataset/measure_records_tenset/k80/([0bcb8746286db050cd088f375c85372d,1,64,64,128,6,6,32,128,1,64,64,32],cuda).json\"\n",
    "# json_file = \"/root/work/tenset/dataset/measure_records_tenset/k80/([0c9a5ba46ffc5e1a9e5641018527117f,4,7,7,160,1,1,160,960,1,1,1,960,4,7,7,960],cuda).json\"\n",
    "json_files = glob(\"/root/work/tenset/dataset/measure_records_tenset/k80/([0c9a5ba46ffc5e1a9e5641018527117f,4*.json\")\n",
    "# json_file = \"/root/work/tenset/dataset/measure_records_tenset/k80/([3eb184d18885126bd13d564ef260c820,4,16,16,256,6,6,256,256,1,1,1,256,4,16,16,256,4,16,16,256],cuda).json\"\n",
    "# json_file = \"/root/work/tenset/dataset/measure_records_tenset/k80/([8c674f26f66543069d1e1c56cda249f9,4,60,60,256,1,1,256,512,1,1,1,512,4,30,30,512],cuda).json\"\n",
    "load_and_register_tasks()\n",
    "print(\"Loading dataset from\", json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "947647eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26735412d5044f969f2acd91fe3e88bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ([0c9a5ba46ffc5e1a9e5641018527117f,4,16,16,112,1,1,112,672,1,1,1,672,4,16,16,672],cuda).json\n",
      "Processing file: ([0c9a5ba46ffc5e1a9e5641018527117f,4,15,15,112,1,1,112,672,1,1,1,672,4,15,15,672],cuda).json\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "records_all = []\n",
    "for json_file in tqdm(json_files[:2]):\n",
    "    print(\"Processing file:\", os.path.basename(json_file))\n",
    "    states, costs = return_all_states(json_file)\n",
    "    records_raw = list(map(lambda x: str(x).strip(), states))\n",
    "\n",
    "    records = {\"schedules\": [], \"extents\": [], \"costs\": [], \"unroll\" : [], \"all\": []}\n",
    "\n",
    "    for rec, cost in zip(records_raw, costs):\n",
    "        cost = np.array([c.value for c in cost])\n",
    "        cost = -np.log(np.mean(cost) + 1e-8)\n",
    "        schedule = rec.split(\"Placeholder\")[-1][2:]\n",
    "        \n",
    "        records[\"schedules\"].append(schedule)\n",
    "        records[\"costs\"].append(cost)\n",
    "    records_all.append(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f5ee376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# records_all 모두 합치기\n",
    "records_merged = {\"schedules\": [], \"extents\": [], \"costs\": [], \"unroll\" : [], \"all\": []}\n",
    "records = records_all[0]\n",
    "for rec in records_all:\n",
    "    records_merged[\"schedules\"].extend(rec[\"schedules\"])\n",
    "    records_merged[\"costs\"].extend(rec[\"costs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5e40e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "placeholder, placeholder, placeholder\n",
      "blockIdx.x nn.0@yy.0@xx.0@ff.0@ (0,28)\n",
      "  vthread nn.1@yy.1@xx.1@ff.1@ (0,4)\n",
      "    threadIdx.x nn.2@yy.2@xx.2@ff.2@ (0,96)\n",
      "      Conv2dOutput.local auto_unroll: 64\n",
      "      for nn_c.0 (0,1)\n",
      "        for yy_c.0 (0,1)\n",
      "          for xx_c.0 (0,1)\n",
      "            for ff_c.0 (0,1)\n",
      "              for nn_c.1 (0,1)\n",
      "                for yy_c.1 (0,1)\n",
      "                  for xx_c.1 (0,1)\n",
      "                    for ff_c.1 (0,1)\n",
      "                      for nn_c.2 (0,1)\n",
      "                        for yy_c.2 (0,1)\n",
      "                          for xx_c.2 (0,1)\n",
      "                            for ff_c.2 (0,1)\n",
      "                              for ry.0 (0,1)\n",
      "                                for rx.0 (0,1)\n",
      "                                  for rc.0 (0,14)\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,8)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,96)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          placeholder.shared = ...\n",
      "                                    for ax0@ax1@ax2@ax3@.0.0 (0,22)\n",
      "                                      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,96)\n",
      "                                        vectorize ax0@ax1@ax2@ax3@.1 (0,1)\n",
      "                                          PaddedInput.shared = ...\n",
      "                                    for ry.1 (0,1)\n",
      "                                      for rx.1 (0,1)\n",
      "                                        for rc.1 (0,8)\n",
      "                                          for nn_c.3 (0,1)\n",
      "                                            for yy_c.3 (0,1)\n",
      "                                              for xx_c.3 (0,8)\n",
      "                                                for ff_c.3 (0,1)\n",
      "                                                  for ry.2 (0,1)\n",
      "                                                    for rx.2 (0,1)\n",
      "                                                      for rc.2 (0,1)\n",
      "                                                        for nn_c.4 (0,2)\n",
      "                                                          for yy_c.4 (0,2)\n",
      "                                                            for xx_c.4 (0,1)\n",
      "                                                              for ff_c.4 (0,2)\n",
      "                                                                Conv2dOutput.local = ...\n",
      "      for nn.3 (0,2)\n",
      "        for yy.3 (0,2)\n",
      "          for xx.3 (0,8)\n",
      "            for ff.3 (0,2)\n",
      "              Conv2dOutput = ...\n",
      "blockIdx.x ax0@ax1@ax2@ax3@.0 (0,21504)\n",
      "  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)\n",
      "    T_multiply = ...\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for a in records[\"schedules\"][:1]:\n",
    "    print(a)\n",
    "    print(\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bff08adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "발견된 공통 (0,1) for문 변수: {'yy_c.1', 'ff_c.0', 'nn_c.2', 'ry.0', 'ry.1', 'rx.2', 'ff_c.2', 'rx.1', 'rx.0', 'ff_c.1', 'yy_c.0', 'xx_c.1', 'ry.2', 'xx_c.0', 'nn_c.1', 'yy_c.2', 'nn_c.0', 'xx_c.2'}\n",
      "제거된 줄 수: 18.0\n",
      "발견된 공통 (0,1) for문 변수: {'yy_c.1', 'ff_c.0', 'nn_c.2', 'ry.0', 'ry.1', 'rx.2', 'ff_c.2', 'rx.1', 'rx.0', 'ff_c.1', 'yy_c.0', 'xx_c.1', 'ry.2', 'xx_c.0', 'nn_c.1', 'yy_c.2', 'nn_c.0', 'xx_c.2'}\n",
      "제거된 줄 수: 18.0\n",
      "발견된 공통 (0,1) for문 변수: {'yy_c.1', 'ff_c.0', 'nn_c.2', 'ry.1', 'ry.0', 'rx.2', 'ff_c.2', 'rx.1', 'rx.0', 'ff_c.1', 'yy_c.0', 'xx_c.1', 'ry.2', 'xx_c.0', 'nn_c.1', 'yy_c.2', 'nn_c.0', 'xx_c.2'}\n",
      "제거된 줄 수: 18.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def find_common_for_loops(schedules):\n",
    "    \"\"\"\n",
    "    모든 스케줄에서 공통으로 나타나는 (0,1) for문 변수명을 찾음\n",
    "    \"\"\"\n",
    "    common_vars = None\n",
    "    \n",
    "    for schedule in schedules:\n",
    "        lines = schedule.split('\\n')\n",
    "        vars_in_schedule = set()\n",
    "        \n",
    "        for line in lines:\n",
    "            stripped = line.lstrip()\n",
    "            match = re.match(r'for\\s+(\\S+)\\s+\\(0,\\s*1\\)', stripped)\n",
    "            if match:\n",
    "                vars_in_schedule.add(match.group(1))\n",
    "        \n",
    "        if common_vars is None:\n",
    "            common_vars = vars_in_schedule\n",
    "        else:\n",
    "            common_vars &= vars_in_schedule  # 교집합\n",
    "    \n",
    "    return common_vars if common_vars is not None else set()\n",
    "\n",
    "\n",
    "def remove_common_for_loops_(schedule, common_vars):\n",
    "    \"\"\"\n",
    "    스케줄 코드에서 공통으로 나타나는 (0,1) for문을 제거하고 들여쓰기를 정리\n",
    "    \"\"\"\n",
    "    lines = schedule.split('\\n')\n",
    "    result_lines = []\n",
    "    \n",
    "    # 제거할 for문의 인덱스들을 먼저 찾기\n",
    "    remove_indices = set()\n",
    "    for_loop_indents = {}  # 제거될 for문의 인덱스 -> 들여쓰기 레벨\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        stripped = line.lstrip()\n",
    "        indent_level = len(line) - len(stripped)\n",
    "        \n",
    "        # (0,1) for문인지 확인\n",
    "        match = re.match(r'for\\s+(\\S+)\\s+\\(0,\\s*1\\)', stripped)\n",
    "        if match and match.group(1) in common_vars:\n",
    "            remove_indices.add(i)\n",
    "            for_loop_indents[i] = indent_level\n",
    "    \n",
    "    # 각 줄에 대해 들여쓰기를 얼마나 줄여야 하는지 계산\n",
    "    indent_reduction = [0] * len(lines)\n",
    "    \n",
    "    for idx in sorted(remove_indices):\n",
    "        base_indent = for_loop_indents[idx]\n",
    "        # 이 for문 다음부터 같거나 작은 들여쓰기가 나올 때까지 2칸씩 줄이기\n",
    "        for j in range(idx + 1, len(lines)):\n",
    "            if j in remove_indices:\n",
    "                continue\n",
    "            line = lines[j]\n",
    "            stripped = line.lstrip()\n",
    "            if not stripped:  # 빈 줄\n",
    "                continue\n",
    "            current_indent = len(line) - len(stripped)\n",
    "            \n",
    "            # 이 for문의 body인 경우 (들여쓰기가 더 큰 경우)\n",
    "            if current_indent > base_indent:\n",
    "                indent_reduction[j] += 2\n",
    "            else:\n",
    "                # 같거나 작은 들여쓰기 레벨이 나오면 이 for문 블록 종료\n",
    "                break\n",
    "    \n",
    "    # 제거하지 않는 줄들에 대해 들여쓰기를 조정하여 결과 생성\n",
    "    for i, line in enumerate(lines):\n",
    "        if i in remove_indices:\n",
    "            continue\n",
    "        \n",
    "        if not line.strip():  # 빈 줄\n",
    "            result_lines.append(line)\n",
    "            continue\n",
    "        \n",
    "        stripped = line.lstrip()\n",
    "        original_indent = len(line) - len(stripped)\n",
    "        new_indent = max(0, original_indent - indent_reduction[i])\n",
    "        result_lines.append(' ' * new_indent + stripped)\n",
    "    \n",
    "    return '\\n'.join(result_lines)\n",
    "\n",
    "\n",
    "def remove_commons(records):\n",
    "\n",
    "    common_for_loops = find_common_for_loops(records[\"schedules\"])\n",
    "    print(f\"발견된 공통 (0,1) for문 변수: {common_for_loops}\")\n",
    "\n",
    "\n",
    "    # 모든 스케줄에 적용\n",
    "    cleaned_schedules = []\n",
    "    records[\"extents\"] = []\n",
    "    records[\"unroll\"] = []\n",
    "    records[\"all\"] = []\n",
    "    for i, schedule in enumerate(records[\"schedules\"]):\n",
    "        extents = [float(x) for x in re.findall(r'\\(0,\\s*(\\d+)\\)', schedule)]\n",
    "\n",
    "    for i, schedule in enumerate(records[\"schedules\"]):\n",
    "        extents = [float(x) for x in re.findall(r'\\(0,\\s*(\\d+)\\)', schedule)]\n",
    "        unrolls = [float(x) for x in re.findall(r'auto_unroll:\\s*(\\d+)', schedule)]\n",
    "        records[\"extents\"].append(extents)\n",
    "        if unrolls == []:\n",
    "            unrolls = [0.0]\n",
    "        records[\"unroll\"].append(unrolls)\n",
    "        feature = extents+unrolls\n",
    "        records[\"all\"].append(np.array(feature, dtype=np.float32))\n",
    "        \n",
    "        cleaned = remove_common_for_loops_(schedule, common_for_loops)\n",
    "        cleaned_schedules.append(cleaned)\n",
    "    records[\"cleaned_schedules\"] = cleaned_schedules\n",
    "\n",
    "\n",
    "    total_removed = sum(len(orig.split('\\n')) - len(clean.split('\\n')) \n",
    "                        for orig, clean in zip(records['schedules'], cleaned_schedules))\n",
    "    avg_removed = total_removed / len(cleaned_schedules)\n",
    "    print(f\"제거된 줄 수: {avg_removed:.1f}\")\n",
    "    return records\n",
    "\n",
    "records_merged = remove_commons(records_merged)\n",
    "records = remove_commons(records)\n",
    "records_1 = remove_commons(records_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ed1663eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class FeatureRegressionDataset(Dataset):\n",
    "    def __init__(self, X, y, feature=None):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            self.X = torch.from_numpy(X).float()\n",
    "        else:\n",
    "            self.X = X\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        if self.y.ndim == 1:\n",
    "            self.y = self.y.unsqueeze(1)\n",
    "\n",
    "        self.feature = feature\n",
    "        if feature is not None:\n",
    "            if isinstance(feature, np.ndarray):\n",
    "                self.feature = torch.from_numpy(feature).float()\n",
    "            else:\n",
    "                self.feature = feature\n",
    "            \n",
    "            if self.feature.ndim == 1:\n",
    "                self.feature = self.feature.unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.feature is None:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        return self.X[idx], self.y[idx], self.feature[idx]\n",
    "\n",
    "\n",
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, X, feature=None):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            self.X = torch.from_numpy(X).float()\n",
    "        else:\n",
    "            self.X = X\n",
    "        \n",
    "        if isinstance(feature, np.ndarray):\n",
    "            self.feature = torch.from_numpy(feature).float()\n",
    "        else:\n",
    "            self.feature = feature\n",
    "        # feature shape이 (N,)이면 (N,1)로 바꿔주는 게 편할 때가 많음\n",
    "        if self.feature is not None and self.feature.ndim == 1:\n",
    "            self.feature = self.feature.unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.feature is None:\n",
    "            return self.X[idx]\n",
    "        return self.X[idx], self.feature[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4cd8864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class VAE_feature_head(nn.Module):\n",
    "    def __init__(self, input_dim, feature_dim=None, latent_dim=16, hidden_dim=128):\n",
    "        \"\"\"\n",
    "        input_dim: 2 * D (v_norm + is_zero concat한 차원)\n",
    "        latent_dim: latent space 차원\n",
    "        hidden_dim: MLP hidden 크기\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            \n",
    "            # 출력은 연속값이니까 activation 없이 그대로\n",
    "        )\n",
    "\n",
    "        if feature_dim is None:\n",
    "            self.use_feature = False\n",
    "        else:\n",
    "            self.use_feature = True\n",
    "            self.feature_predictor = nn.Sequential(\n",
    "                nn.Linear(latent_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, feature_dim),  # features.shape[1]는 feature 차원\n",
    "            )\n",
    "\n",
    "    def encode(self, x, use_mean=False):\n",
    "\n",
    "        h = self.encoder(x)\n",
    "        mean = self.fc_mu(h)\n",
    "        if not use_mean:\n",
    "            logvar = self.fc_logvar(h)\n",
    "        else:\n",
    "            return mean\n",
    "        \n",
    "        return mean, logvar\n",
    "\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def predict_feature(self, z):\n",
    "        return self.feature_predictor(z)\n",
    "\n",
    "    def forward(self, x, use_mean=True):\n",
    "        mu, logvar = self.encode(x)\n",
    "        if use_mean:\n",
    "            z = mu\n",
    "        else:\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        \n",
    "        if self.use_feature:\n",
    "            feature_pred = self.predict_feature(z)\n",
    "        else:\n",
    "            feature_pred = None\n",
    "        return x_recon, mu, logvar, z, feature_pred\n",
    "\n",
    "class L3Loss(torch.nn.Module):\n",
    "    def forward(self, pred, target):\n",
    "        return torch.mean(torch.abs(pred - target) ** 4)\n",
    "\n",
    "def vae_feature_loss(x_recon, x, mu, logvar, feature_pred, feature, alpha_recon=0, alpha_feature=0, beta=1.0):\n",
    "    \"\"\"\n",
    "    x, x_recon: (B, input_dim)\n",
    "    mu, logvar: (B, latent_dim)\n",
    "\n",
    "    beta: KL 가중치 (β-VAE 스타일로 조절)\n",
    "    \"\"\"\n",
    "    # reconstruction loss: MSE\n",
    "    recon_loss = F.mse_loss(x_recon, x, reduction=\"mean\")\n",
    "    # \n",
    "    # recon_loss = L3Loss()(x_recon, x)\n",
    "\n",
    "    feature_loss = F.mse_loss(feature_pred, feature, reduction=\"mean\") if feature_pred is not None else 0.0\n",
    "\n",
    "    # KL divergence: D_KL(q(z|x) || N(0, I))\n",
    "    kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    loss = alpha_recon * recon_loss + beta * kl + alpha_feature * feature_loss\n",
    "    return loss, recon_loss, kl, feature_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a08123a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6b057bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_seed = 2023\n",
    "seed_everything(train_seed)\n",
    "\n",
    "\n",
    "input_data = np.log1p(np.array(records[\"all\"], dtype=np.float32))\n",
    "input_data_1 = np.log1p(np.array(records_1[\"all\"], dtype=np.float32))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "input_data_scaled = scaler.fit_transform(input_data)\n",
    "input_data_scaled_1 = scaler.transform(input_data_1)\n",
    "\n",
    "X_train, X_val = train_test_split(\n",
    "    input_data_scaled,  test_size=0.2, random_state=train_seed\n",
    ")\n",
    "\n",
    "\n",
    "# feature 없음\n",
    "train_dataset = FeatureDataset(X_train)\n",
    "val_dataset   = FeatureDataset(X_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=False)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=512, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "17270272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "Experiment 1/1\n",
      "beta=0.01, alpha_recon=1.0, alpha_feature=0.0,\n",
      "epochs=500, latent_dim=64, hidden_dim=256, lr=0.001\n",
      "Early stopping at epoch 428\n",
      "epoch 428: loss=0.0130, recon=0.0043, kl=0.8697\n",
      "epoch 428: val loss=0.0156, val recon=0.0069, val kl=0.8681\n",
      "Recon R2 : 0.592185834803539, Feature R2 : None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import itertools\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[-1]\n",
    "latent_dim = 64\n",
    "hidden_dim = 256\n",
    "\n",
    "\n",
    "hyperparameter = {\n",
    "    'beta': [0.01],\n",
    "    'alpha_recon': [1.0],\n",
    "    'alpha_feature': [0.0],\n",
    "    'latent_dim': [64],\n",
    "    'lr': [1e-3],\n",
    "}\n",
    "\n",
    "cnt = 0\n",
    "epochs = 500\n",
    "\n",
    "\n",
    "for vals in itertools.product(*hyperparameter.values()):\n",
    "    (beta, alpha_recon, alpha_feature, latent_dim, lr) = vals\n",
    "    cnt += 1\n",
    "    print(\"=============================================\")\n",
    "    print(f\"Experiment {cnt}/{len(list(itertools.product(*hyperparameter.values())))}\")\n",
    "    print(f\"beta={beta}, alpha_recon={alpha_recon}, alpha_feature={alpha_feature},\\nepochs={epochs}, latent_dim={latent_dim}, hidden_dim={hidden_dim}, lr={lr}\")\n",
    "\n",
    "    seed_everything(train_seed)\n",
    "\n",
    "    vae = VAE_feature_head(input_dim=input_dim, latent_dim=latent_dim, hidden_dim=hidden_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(vae.parameters(), lr=lr)\n",
    "\n",
    "    # early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 30\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        vae.train()\n",
    "        for x_batch in train_loader:\n",
    "            if len(x_batch) == 2:\n",
    "                x_batch, feature_batch = x_batch\n",
    "                feature_batch = feature_batch.to(device)\n",
    "            else:\n",
    "                feature_batch = None\n",
    "            x_batch = x_batch.to(device)  # (N, D)\n",
    "            \n",
    "            \n",
    "\n",
    "            x_recon, mu, logvar, z, feature_pred = vae(x_batch, use_mean=False)\n",
    "\n",
    "            loss, recon_loss, kl, feature_loss = vae_feature_loss(x_recon, x_batch, mu, logvar, feature_pred, feature_batch, alpha_recon=alpha_recon, alpha_feature=alpha_feature, beta=beta)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        vae.eval()\n",
    "        for x_batch in val_loader:\n",
    "            if len(x_batch) == 2:\n",
    "                x_batch, feature_batch = x_batch\n",
    "                feature_batch = feature_batch.to(device)\n",
    "            else:\n",
    "                feature_batch = None\n",
    "            x_batch = x_batch.to(device)\n",
    "            if feature_batch is not None:\n",
    "                feature_batch = feature_batch.to(device)\n",
    "            x_recon, mu, logvar, z, feature_pred = vae(x_batch, use_mean=True)\n",
    "            val_loss, val_recon_loss, val_kl, val_feature_loss = vae_feature_loss(x_recon, x_batch, mu, logvar, feature_pred, feature_batch, alpha_recon=alpha_recon, alpha_feature=alpha_feature, beta=beta)\n",
    "            val_recon_r2 = r2_score(x_batch.detach().cpu().numpy(), x_recon.detach().cpu().numpy())\n",
    "            if feature_batch is not None:\n",
    "                val_feature_r2 = r2_score(feature_batch.detach().cpu().numpy(), feature_pred.detach().cpu().numpy())\n",
    "            else:\n",
    "                val_feature_r2 = None\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss.item()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "    print(f\"epoch {epoch}: loss={loss.item():.4f}, recon={recon_loss.item():.4f}, kl={kl.item():.4f}\")\n",
    "    print(f\"epoch {epoch}: val loss={val_loss.item():.4f}, val recon={val_recon_loss.item():.4f}, val kl={val_kl.item():.4f}\")\n",
    "\n",
    "    print(f\"Recon R2 : {val_recon_r2}, Feature R2 : {val_feature_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6310053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAECostPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    VAE 기반 Cost Regression 모델\n",
    "    \n",
    "    구조:\n",
    "    - input → segment_encoder → segment_sum → VAE encoder → z → cost_predictor → cost\n",
    "    \n",
    "    특징:\n",
    "    - Pretrained VAE encoder를 finetune (작은 learning rate)\n",
    "    - Cost predictor는 더 큰 learning rate로 학습\n",
    "    - 전체 forward 경로가 완전히 미분 가능 (detach, stop_grad 없음)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, feature_dim=None, hidden_dim=256, latent_dim=64, \n",
    "                 predictor_hidden=256, predictor_layers=2, dropout=0.1, use_feature=False):\n",
    "        super(VAECostPredictor, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # ========== Cost Predictor (새로 학습) ==========\n",
    "        predictor_modules = []\n",
    "        current_dim = latent_dim\n",
    "        for i in range(predictor_layers):\n",
    "            predictor_modules.extend([\n",
    "                nn.Linear(current_dim, predictor_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout) if i < predictor_layers - 1 else nn.Identity(),\n",
    "            ])\n",
    "            current_dim = predictor_hidden\n",
    "        predictor_modules.append(nn.Linear(predictor_hidden, 1))\n",
    "        \n",
    "        self.cost_predictor = nn.Sequential(*predictor_modules)\n",
    "\n",
    "        self.use_feature = use_feature\n",
    "        if self.use_feature:\n",
    "            pass\n",
    "            self.feature_predictor = nn.Sequential(\n",
    "                nn.Linear(latent_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, feature_dim),  # feature_dim는 feature 차원\n",
    "            )\n",
    "        \n",
    "    \n",
    "    def encode(self, input_data, use_mean=False):\n",
    "        \"\"\"\n",
    "        Full encoding path: features → z\n",
    "        완전히 미분 가능\n",
    "        \"\"\"\n",
    "                \n",
    "        # VAE Encoder\n",
    "        h = self.encoder(input_data)\n",
    "        \n",
    "        mean = self.fc_mu(h)\n",
    "        if not use_mean:\n",
    "            logvar = self.fc_logvar(h)\n",
    "        else:\n",
    "            return mean\n",
    "        \n",
    "        return mean, logvar\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        \"\"\"Reparameterization trick - 미분 가능\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "    \n",
    "    def predict_cost(self, z):\n",
    "        \"\"\"z → cost prediction - 완전히 미분 가능\"\"\"\n",
    "        return self.cost_predictor(z).squeeze(-1)\n",
    "    \n",
    "    def predict_feature(self, z):\n",
    "        return self.feature_predictor(z)\n",
    "    \n",
    "    def forward(self, input_data, use_mean=True):\n",
    "        \"\"\"\n",
    "        Forward pass: input → z → cost\n",
    "        \n",
    "        Args:\n",
    "            use_mean: True면 reparameterize 대신 mean 사용 (inference용)\n",
    "        \n",
    "        Returns:\n",
    "            cost_pred: 예측된 cost\n",
    "            mean: latent mean\n",
    "            logvar: latent log-variance\n",
    "            z: sampled/mean latent vector\n",
    "        \"\"\"\n",
    "        mean, logvar = self.encode(input_data)\n",
    "        \n",
    "        if use_mean:\n",
    "            z = mean  # Inference시 deterministic\n",
    "        else:\n",
    "            z = self.reparameterize(mean, logvar)  # Training시 stochastic\n",
    "        \n",
    "        cost_pred = self.predict_cost(z)\n",
    "        \n",
    "        return cost_pred, mean, logvar, z\n",
    "    \n",
    "    def get_encoder_params(self):\n",
    "        \"\"\"Encoder 파라미터 (작은 lr)\"\"\"\n",
    "        encoder_params = []\n",
    "        encoder_params.extend(self.encoder.parameters())\n",
    "        encoder_params.extend(self.fc_mu.parameters())\n",
    "        encoder_params.extend(self.fc_logvar.parameters())\n",
    "        return encoder_params\n",
    "    \n",
    "    def get_cost_predictor_params(self):\n",
    "        \"\"\"Predictor 파라미터 (큰 lr)\"\"\"\n",
    "        return self.cost_predictor.parameters()\n",
    "    \n",
    "    def get_feature_predictor_params(self):\n",
    "        \"\"\"Feature Predictor 파라미터\"\"\"\n",
    "        return self.feature_predictor.parameters()\n",
    "\n",
    "    def load_pretrained_encoder(self, checkpoint):\n",
    "        \"\"\"Pretrained VAE encoder 가중치 로드\"\"\"\n",
    "        \n",
    "\n",
    "        vae_state = checkpoint\n",
    "        \n",
    "        # 매칭되는 키만 로드\n",
    "        encoder_keys = ['encoder', 'fc_mu', 'fc_logvar']\n",
    "        own_state = self.state_dict()\n",
    "        \n",
    "        loaded_keys = []\n",
    "        for name, param in vae_state.items():\n",
    "            if any(name.startswith(k) for k in encoder_keys):\n",
    "                if name in own_state and own_state[name].shape == param.shape:\n",
    "                    own_state[name].copy_(param)\n",
    "                    loaded_keys.append(name)\n",
    "        \n",
    "        # print(f\"Loaded {len(loaded_keys)} parameters from pretrained VAE\")\n",
    "        # return loaded_keys\n",
    "\n",
    "    def _enable_dropout(self):\n",
    "        \"\"\"모든 Dropout 모듈을 train 모드로 강제 활성화\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.train()\n",
    "\n",
    "    def mc_predict(self, input_tensor, T=20):\n",
    "        \"\"\"\n",
    "        MC Dropout 기반 불확실성 추정\n",
    "        \n",
    "        Args:\n",
    "            input_tensor: 입력 텐서 (shape [N, input_dim])\n",
    "            T: MC 샘플 수\n",
    "        \n",
    "        Returns:\n",
    "            mean: epistemic 평균 cost (shape [N])\n",
    "            var: epistemic 분산 (shape [N])\n",
    "        \"\"\"\n",
    "\n",
    "        self.eval()  # 전체 모델을 eval 모드로\n",
    "        self._enable_dropout()  # Dropout만 train 모드로 활성화\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            \n",
    "            for _ in range(T):\n",
    "                # Encode\n",
    "                z, logvar = self.encode(input_tensor)\n",
    "                cost_pred = self.predict_cost(z)\n",
    "                predictions.append(cost_pred)\n",
    "            \n",
    "            predictions = torch.stack(predictions, dim=0)\n",
    "            \n",
    "            # epistemic mean & variance\n",
    "            mc_mean = predictions.mean(dim=0)\n",
    "            mc_var = predictions.var(dim=0)\n",
    "\n",
    "        return mc_mean, mc_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1b584ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_loss_fn(cost_pred, cost_true, loss_type='mse'):\n",
    "    \"\"\"\n",
    "    기본 회귀 손실 (MSE 또는 MAE)\n",
    "    \"\"\"\n",
    "    if loss_type == 'mse':\n",
    "        return F.mse_loss(cost_pred, cost_true)\n",
    "    else:  # mae\n",
    "        return F.l1_loss(cost_pred, cost_true)\n",
    "\n",
    "\n",
    "def pair_loss_fn(cost_pred, cost_true, margin=0.1):\n",
    "    \"\"\"\n",
    "    Pairwise ranking loss: 실제 cost 순서를 예측이 유지하도록.\n",
    "    cost_true[i] < cost_true[j] 이면 cost_pred[i] < cost_pred[j] + margin\n",
    "    \"\"\"\n",
    "    batch_size = cost_pred.size(0)\n",
    "    if batch_size < 2:\n",
    "        return torch.tensor(0.0, device=cost_pred.device)\n",
    "    \n",
    "    # 모든 쌍에 대해 ranking loss 계산\n",
    "    idx = torch.arange(batch_size, device=cost_pred.device)\n",
    "    i_idx, j_idx = torch.meshgrid(idx, idx, indexing='ij')\n",
    "    mask = i_idx < j_idx  # upper triangular only\n",
    "    \n",
    "    pred_i = cost_pred[i_idx[mask]]\n",
    "    pred_j = cost_pred[j_idx[mask]]\n",
    "    true_i = cost_true[i_idx[mask]]\n",
    "    true_j = cost_true[j_idx[mask]]\n",
    "    \n",
    "    # label: 1 if true_i < true_j, -1 otherwise\n",
    "    labels = torch.sign(true_j - true_i).float()\n",
    "    \n",
    "    # Margin ranking loss\n",
    "    loss = F.margin_ranking_loss(pred_j.view(-1), pred_i.view(-1), labels.view(-1), margin=margin)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def smooth_loss_fn(model, z, noise_std=0.1):\n",
    "    \"\"\"\n",
    "    Smoothness loss: z에 작은 노이즈를 더했을 때 예측이 크게 변하지 않도록.\n",
    "    \"\"\"\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    \n",
    "    z_noisy = z + noise_std * torch.randn_like(z)\n",
    "    \n",
    "    cost_original = model.predict_cost(z)\n",
    "    cost_noisy = model.predict_cost(z_noisy)\n",
    "    \n",
    "    smooth_loss = F.mse_loss(cost_original, cost_noisy)\n",
    "    \n",
    "    if was_training:\n",
    "        model.train()\n",
    "    \n",
    "    return smooth_loss\n",
    "\n",
    "\n",
    "def kld_loss_fn(mean, logvar):\n",
    "    \"\"\"\n",
    "    KL Divergence: q(z|x) || N(0, I)\n",
    "    \"\"\"\n",
    "    kld = -0.5 * torch.mean(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "    return kld\n",
    "\n",
    "def feature_loss_fn(use_feature, feature_pred, feature_true, coef=0.1):\n",
    "    \"\"\"\n",
    "    Feature 예측 손실 (MSE)\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if not use_feature:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "    return F.mse_loss(feature_pred, feature_true) * coef\n",
    "\n",
    "\n",
    "def compute_total_loss(model, cost_pred, mean, logvar, z, labels, feature, config, return_components=True):\n",
    "    \"\"\"\n",
    "    Total loss 계산 (Segment 기반 데이터용).\n",
    "    total_loss = reg_loss + λ_pair * pair_loss + γ * smooth_loss + β * kld_loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # Individual losses\n",
    "    reg = reg_loss_fn(cost_pred, labels, loss_type=config.get('loss_type', 'mse'))\n",
    "    pair = pair_loss_fn(cost_pred.view(-1), labels.view(-1), margin=config.get('margin', 0.1))\n",
    "    smooth = smooth_loss_fn(model, z, noise_std=config.get('noise_std', 0.1))\n",
    "    kld = kld_loss_fn(mean, logvar)\n",
    "    feature_loss = feature_loss_fn(model.use_feature, None, feature, coef=0)\n",
    "    \n",
    "    # Weighted sum\n",
    "    total = config['lambda_reg'] * reg + config['lambda_pair'] * pair + config['gamma'] * smooth + config['beta'] * kld + feature_loss\n",
    "    \n",
    "    if return_components:\n",
    "        return total, {\n",
    "            'reg_loss': reg.item(),\n",
    "            'pair_loss': pair.item(),\n",
    "            'smooth_loss': smooth.item(),\n",
    "            'kld_loss': kld.item(),\n",
    "            'feature_loss': feature_loss.item(),\n",
    "        }\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6ef7144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_accuracy(cost_pred, labels, rng=np.random.default_rng(42)):\n",
    "    \"\"\"\n",
    "    cost_pred, labels: (B,) 텐서\n",
    "    \"\"\"\n",
    "    n_samples = min(1000, len(cost_pred))\n",
    "    sample_indices = rng.choice(len(cost_pred), n_samples, replace=False)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i + 1, n_samples):\n",
    "            idx_i = sample_indices[i]\n",
    "            idx_j = sample_indices[j]\n",
    "            pred_diff = cost_pred[idx_i] - cost_pred[idx_j]\n",
    "            true_diff = labels[idx_i] - labels[idx_j]\n",
    "            if (pred_diff * true_diff) > 0:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy\n",
    "\n",
    "def recall_at_k(pred, labels, k=1):\n",
    "    true_best_idx = torch.argmax(labels)\n",
    "    topk_pred_idx = torch.topk(pred, k=k, largest=True).indices\n",
    "\n",
    "    return int((topk_pred_idx == true_best_idx).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a03fd72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_select_indices(xgb_all_preds, train_indices, test_indices, topk_size, eps_greedy_size, rng):\n",
    "    \"\"\"\n",
    "    랜덤으로 2개, xgb 모델로 상위 62개 선택\n",
    "    \"\"\"\n",
    "    # 남은 인덱스 중에서 무작위로 random_select_size개 선택\n",
    "\n",
    "    remaining_indices = set(test_indices)\n",
    "\n",
    "    if topk_size + eps_greedy_size > test_indices.shape[0]:\n",
    "        remaining_indices.update(train_indices.tolist())\n",
    "        train_indices = np.array(list(remaining_indices), dtype=np.int64)\n",
    "        return train_indices, np.array([], dtype=np.int64)\n",
    "\n",
    "\n",
    "    top_indices, remaining_indices = select_topk_cost(xgb_all_preds, remaining_indices, topk_size)\n",
    "    random_indices, remaining_indices = random_select_indices(remaining_indices, eps_greedy_size, rng=rng)\n",
    "    test_indices = np.array(list(remaining_indices), dtype=np.int64)\n",
    "\n",
    "    selected_indices = np.concatenate([top_indices, random_indices])\n",
    "\n",
    "    train_indices = np.concatenate([train_indices, selected_indices])\n",
    "\n",
    "    return train_indices, test_indices\n",
    "\n",
    "\n",
    "\n",
    "def random_select_indices(remaining_indices, select_size, rng=np.random.default_rng(42)):\n",
    "    if select_size == 0:\n",
    "        return np.array([], dtype=np.int64), remaining_indices\n",
    "    \n",
    "    random_indices = rng.choice(list(remaining_indices), size=select_size, replace=False)\n",
    "\n",
    "    remaining_indices = util_update_remaining_indices(remaining_indices, random_indices)\n",
    "\n",
    "    return random_indices, remaining_indices\n",
    "\n",
    "\n",
    "\n",
    "def util_update_remaining_indices(remaining_indices, selected_indices):\n",
    "    \"\"\"\n",
    "    남은 인덱스 집합 업데이트\n",
    "    util_update_remaining_indices에서 selected_indices 제거\n",
    "    \"\"\"\n",
    "    selected_indices = set(selected_indices)\n",
    "    remaining_indices.difference_update(selected_indices)\n",
    "\n",
    "    return remaining_indices\n",
    "\n",
    "\n",
    "\n",
    "def util_select_topk(predictions, remaining_indices, num_select):\n",
    "    \"\"\"\n",
    "    예측값 기반 다음 측정할 샘플 선택\n",
    "    \n",
    "    Args:\n",
    "        predictions: 전체 예측값 리스트 ([N, ] 형태)\n",
    "        remaining_indices: 아직 측정되지 않은 인덱스 집합 (set)\n",
    "        num_select: 선택할 샘플 수\n",
    "    \n",
    "    Returns:\n",
    "        selected_indices: 선택된 샘플의 인덱스 numpy 배열\n",
    "        remaining_indices: 업데이트된 남은 인덱스 집합 (set)\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction = np.asarray(predictions)  # [N]\n",
    "\n",
    "    remaining_np = np.array(list(remaining_indices), dtype=np.int64)\n",
    "    remaining_pred = prediction[remaining_np]\n",
    "\n",
    "    k = min(num_select, len(remaining_np))\n",
    "\n",
    "    topk_local = np.argsort(remaining_pred)[-k:]\n",
    "    selected_indices = remaining_np[topk_local]\n",
    "\n",
    "    # remaining 업데이트\n",
    "    remaining_indices.difference_update(selected_indices.tolist())\n",
    "\n",
    "    return selected_indices, remaining_indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_topk_cost(cost_pred, remaining_indices, num_select):\n",
    "    \"\"\"\n",
    "    예측된 cost 기반 다음 측정할 샘플 선택\n",
    "    \n",
    "    Args:\n",
    "        model: VAECostPredictor 모델\n",
    "        input_data_scaled: 전체 input 리스트 ([N, input_dim] 형태)\n",
    "        remaining_indices: 아직 측정되지 않은 인덱스 집합 (set)\n",
    "        num_select: 선택할 샘플 수\n",
    "    \n",
    "    \"\"\"\n",
    "    if num_select == 0:\n",
    "        return np.array([], dtype=np.int64), remaining_indices\n",
    "\n",
    "    if isinstance(cost_pred, torch.Tensor):\n",
    "        cost_pred = cost_pred.detach().cpu().numpy()  # [N]\n",
    "\n",
    "    topk_cost_indices, remaining_indices = util_select_topk(cost_pred, remaining_indices, num_select)\n",
    "    \n",
    "\n",
    "    return topk_cost_indices, remaining_indices\n",
    "\n",
    "\n",
    "def select_topk_z_grad(z, cost_pred, remaining_indices, num_select):\n",
    "    \"\"\"\n",
    "    z에 대한 cost gradient 기반 다음 측정할 샘플 선택\n",
    "    \n",
    "    Args:\n",
    "        model: VAECostPredictor 모델\n",
    "        input_tensor: 전체 input numpy 배열 ([N, input_dim] 형태)\n",
    "        remaining_indices: 아직 측정되지 않은 인덱스 집합 (set)\n",
    "        num_select: 선택할 샘플 수\n",
    "    \n",
    "    \"\"\"\n",
    "    if num_select == 0:\n",
    "        return np.array([], dtype=np.int64), remaining_indices\n",
    "\n",
    "    candidate_indices = np.array(list(remaining_indices), dtype=np.int64)\n",
    "\n",
    "    # z-gradient 계산\n",
    "    z_grad = torch.autograd.grad(\n",
    "        outputs=cost_pred.sum(),\n",
    "        inputs=z,\n",
    "        retain_graph=False,\n",
    "        create_graph=False\n",
    "    )[0]  # [N, latent_dim]\n",
    "\n",
    "    z_grad_norm = torch.norm(z_grad, dim=1).detach().cpu().numpy()  # [N]\n",
    "\n",
    "    # 후보 중 grad-norm top-k\n",
    "    candidate_grad = z_grad_norm[candidate_indices]\n",
    "    k = min(num_select, len(candidate_indices))\n",
    "\n",
    "    topk_local = np.argsort(candidate_grad)[-k:]\n",
    "    selected_indices = candidate_indices[topk_local]\n",
    "\n",
    "    # remaining 업데이트\n",
    "    remaining_indices = set(remaining_indices)\n",
    "    remaining_indices.difference_update(selected_indices.tolist())\n",
    "\n",
    "    return selected_indices, remaining_indices\n",
    "\n",
    "\n",
    "def select_topk_uncertainty(model, input_tensor, remaining_indices, num_select, T_mc=10):\n",
    "    \"\"\"\n",
    "    MC Dropout 기반 불확실성 추정으로 다음 측정할 샘플 선택\n",
    "    \n",
    "    Args:\n",
    "        model: VAECostPredictor 모델\n",
    "        input_data_scaled: 전체 input 리스트 ([N, input_dim] 형태)\n",
    "        remaining_indices: 아직 측정되지 않은 인덱스 집합 (set)\n",
    "        num_select: 선택할 샘플 수\n",
    "        T_mc: MC Dropout 샘플 수\n",
    "    \n",
    "    Returns:\n",
    "        selected_indices: 선택된 샘플의 인덱스 리스트\n",
    "    \"\"\"\n",
    "    if num_select == 0:\n",
    "        return np.array([], dtype=np.int64), remaining_indices\n",
    "\n",
    "\n",
    "    was_training = model.training\n",
    "    model.train()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, mc_var = model.mc_predict(input_tensor, T=T_mc)\n",
    "\n",
    "    if not was_training:\n",
    "        model.eval()  # 원복\n",
    "\n",
    "    var_np = mc_var.detach().cpu().numpy()  # [N]\n",
    "\n",
    "    topk_uncertainty_indices, remaining_indices = util_select_topk(var_np, remaining_indices, num_select)\n",
    "\n",
    "    return topk_uncertainty_indices, remaining_indices\n",
    "\n",
    "\n",
    "def select_topk_latent_diversity(z, candidate_indices, used_indices, select_n_div, chunk_size=1024, eps=1e-12):\n",
    "    \"\"\"\n",
    "    먼저 candidates 320개를 뽑았다고 치자.\n",
    "    이후 앞에서 topk_cost, topk_z_grad로 40개 정도를 뽑았다고 치자.\n",
    "    latent diversity는 40개 + used_indices로부터 가장 멀리 떨어진 24개를 280개에서 뽑는다.\n",
    "\n",
    "    z를 L2 정규화한 뒤, k-center greedy(farthest-first)로 diversity 선택.\n",
    "    초기 센터는 used_indices (이미 측정된 점들).\n",
    "    매 스텝마다 \"센터 집합까지의 최소거리\"가 최대인 candidate를 하나씩 추가.\n",
    "    \n",
    "    Args:\n",
    "        z: torch.Tensor [N, latent_dim]\n",
    "        candidate_indices: set(int)\n",
    "        used_indices: set(int)\n",
    "        select_n_div: int\n",
    "        chunk_size: int\n",
    "    Returns:\n",
    "        diverse_indices: np.ndarray (int64)\n",
    "        candidate_indices: set (선택된 인덱스 제거된 상태)\n",
    "    \"\"\"\n",
    "    if select_n_div == 0 or len(candidate_indices) == 0:\n",
    "        return np.array([], dtype=np.int64), candidate_indices\n",
    "\n",
    "\n",
    "    device = z.device\n",
    "\n",
    "    # 1) L2 normalize z  (각 벡터를 단위벡터로)\n",
    "    with torch.no_grad():\n",
    "        z_norm = z / (z.norm(dim=1, keepdim=True) + eps)\n",
    "\n",
    "    cand = np.array(list(candidate_indices), dtype=np.int64)\n",
    "    k = min(select_n_div, len(cand))\n",
    "\n",
    "    cand_t = torch.from_numpy(cand).to(device=device)\n",
    "    z_cand = z_norm[cand_t]  # [M, D], M=len(cand)\n",
    "\n",
    "    # 초기 센터: used_indices (비어있을 수도 있음)\n",
    "    used = np.array(list(used_indices), dtype=np.int64)\n",
    "    selected = []\n",
    "\n",
    "    # 2) 각 candidate의 \"현재 센터 집합까지 최소거리\" 벡터 init\n",
    "    #    used가 비어있으면 +inf로 시작해서 임의 첫 점을 뽑게(가장 큰 값) 만들기\n",
    "    if len(used) > 0:\n",
    "        used_t = torch.from_numpy(used).to(device=device)\n",
    "        z_used = z_norm[used_t]  # [U, D]\n",
    "\n",
    "        # min_dists[j] = min_{u in used} ||z_cand[j] - z_used[u]||\n",
    "        min_dists = torch.empty(len(cand), device=device, dtype=torch.float32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for s in range(0, len(cand), chunk_size):\n",
    "                e = min(s + chunk_size, len(cand))\n",
    "                d = torch.cdist(z_cand[s:e], z_used, p=2)  # [B, U]\n",
    "                min_dists[s:e] = d.min(dim=1).values\n",
    "    else:\n",
    "        # 센터가 없으면 모두 동일하게 시작 → 첫 선택은 아래 argmax가 0번째로 갈 수 있음\n",
    "        # 다양성 목적이면 랜덤/최대 norm 등으로 첫 점을 정할 수도 있지만,\n",
    "        # 여기서는 \"가장 큰 min_dists\"를 위해 +inf로 둔다.\n",
    "        min_dists = torch.full((len(cand),), float(\"inf\"), device=device, dtype=torch.float32)\n",
    "\n",
    "    # 3) k-center greedy 반복\n",
    "    #    매번 argmax(min_dists) 하나 선택 -> 그 점을 센터에 추가 -> min_dists 갱신\n",
    "    with torch.no_grad():\n",
    "        for _ in range(k):\n",
    "            j = torch.argmax(min_dists).item()     # cand 내부 위치\n",
    "            sel_idx = cand[j]                      # 원본 인덱스\n",
    "            selected.append(sel_idx)\n",
    "\n",
    "            # 선택된 점을 \"센터\"로 추가: 모든 candidate에 대해 dist_to_new_center 계산 후 min 갱신\n",
    "            new_center = z_cand[j:j+1]  # [1, D]\n",
    "\n",
    "            # 방금 뽑은 점은 다시 뽑히지 않게 min_dists를 -inf로\n",
    "            min_dists[j] = -float(\"inf\")\n",
    "\n",
    "            # 나머지 후보들의 min 거리 업데이트\n",
    "            for s in range(0, len(cand), chunk_size):\n",
    "                e = min(s + chunk_size, len(cand))\n",
    "                d_new = torch.cdist(z_cand[s:e], new_center, p=2).squeeze(1)  # [B]\n",
    "                min_dists[s:e] = torch.minimum(min_dists[s:e], d_new)\n",
    "\n",
    "    diverse_indices = np.array(selected, dtype=np.int64)\n",
    "\n",
    "    candidate_indices = set(candidate_indices)\n",
    "    candidate_indices.difference_update(diverse_indices.tolist())\n",
    "\n",
    "    return diverse_indices, candidate_indices\n",
    "\n",
    "\n",
    "def select_init_latent_diversity(model, input_data_scaled, remaining_indices, select_num, device):\n",
    "    model.eval()\n",
    "\n",
    "    # remaining indices를 리스트로 고정\n",
    "    rem_idx = np.array(list(remaining_indices), dtype=np.int64)\n",
    "    select_num = min(select_num, len(rem_idx))\n",
    "\n",
    "    input_tensor = torch.tensor(\n",
    "        input_data_scaled[rem_idx],\n",
    "        dtype=torch.float32,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(input_tensor, use_mean=True)  # (M, D)\n",
    "\n",
    "    z = z.detach()\n",
    "    M = z.size(0)\n",
    "\n",
    "    selected_local = []\n",
    "\n",
    "    # 1) 첫 점 랜덤 (remaining 내부)\n",
    "    first = torch.randint(0, M, (1,), device=device).item()\n",
    "    selected_local.append(first)\n",
    "\n",
    "    # 2) farthest-point greedy (remaining 내부)\n",
    "    dist = torch.cdist(z, z[[first]])[:, 0]  # (M,)\n",
    "\n",
    "    for _ in range(1, select_num):\n",
    "        idx = torch.argmax(dist).item()\n",
    "        selected_local.append(idx)\n",
    "\n",
    "        new_dist = torch.cdist(z, z[[idx]])[:, 0]\n",
    "        dist = torch.minimum(dist, new_dist)\n",
    "\n",
    "    # local index → global index\n",
    "    selected_global = rem_idx[selected_local]\n",
    "\n",
    "    # remaining 업데이트\n",
    "    remaining_indices.difference_update(selected_global.tolist())\n",
    "\n",
    "    return selected_global, remaining_indices\n",
    "\n",
    "\n",
    "def select_representative_kmeans(model, input_data_scaled, remaining_indices, select_num, device, iters=10):\n",
    "    model.eval()\n",
    "    x = torch.tensor(input_data_scaled, dtype=torch.float32, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(x, use_mean=True)  # (N, D)\n",
    "    z = z.detach()\n",
    "    N, D = z.shape\n",
    "    k = min(select_num, N)\n",
    "\n",
    "    # --- kmeans++ 초기화 (center는 실제 데이터 포인트로 잡음) ---\n",
    "    centers_idx = []\n",
    "    first = torch.randint(0, N, (1,), device=z.device).item()\n",
    "    centers_idx.append(first)\n",
    "\n",
    "    dist = torch.cdist(z, z[[first]])[:, 0]  # (N,)\n",
    "    for _ in range(1, k):\n",
    "        probs = (dist ** 2)\n",
    "        probs = probs / probs.sum().clamp_min(1e-12)\n",
    "        idx = torch.multinomial(probs, 1).item()\n",
    "        centers_idx.append(idx)\n",
    "        dist = torch.minimum(dist, torch.cdist(z, z[[idx]])[:, 0])\n",
    "\n",
    "    centers = z[centers_idx].clone()  # (k, D)\n",
    "\n",
    "    # --- Lloyd iterations ---\n",
    "    for _ in range(iters):\n",
    "        d = torch.cdist(z, centers)          # (N, k)\n",
    "        assign = torch.argmin(d, dim=1)      # (N,)\n",
    "        new_centers = centers.clone()\n",
    "        for j in range(k):\n",
    "            mask = (assign == j)\n",
    "            if mask.any():\n",
    "                new_centers[j] = z[mask].mean(dim=0)\n",
    "        centers = new_centers\n",
    "\n",
    "    # --- 각 중심에 가장 가까운 실제 데이터 인덱스 선택 ---\n",
    "    d = torch.cdist(z, centers)  # (N, k)\n",
    "    selected = []\n",
    "    used = set()\n",
    "    for j in range(k):\n",
    "        # 중심 j에 가장 가까운 점부터 시도하되, 중복 방지\n",
    "        order = torch.argsort(d[:, j])\n",
    "        for idx in order.tolist():\n",
    "            if idx not in used:\n",
    "                used.add(idx)\n",
    "                selected.append(idx)\n",
    "                break\n",
    "\n",
    "    remaining_indices.difference_update(selected)\n",
    "    k_means_indices = np.array(selected, dtype=np.int64)\n",
    "\n",
    "    return k_means_indices, remaining_indices\n",
    "\n",
    "def select_programs(model, input_data_scaled, used_indices, remaining_indices, num_select=64, T_mc=10, uncertainty_topk=128,\n",
    "                    w_cost=0.5, w_unc=0.3, w_div=0.2, grad_num=2, rand_num=0, rng=np.random.default_rng(42), device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), topk_factor=5):\n",
    "    \"\"\"\n",
    "    Active learning 기반 다음 측정할 샘플 선택\n",
    "    \n",
    "    Args:\n",
    "        model: VAECostPredictor 모델\n",
    "        input_data_scaled: 전체 input 리스트 ([N, input_dim] 형태)\n",
    "        used_indices: 이미 측정된 인덱스 집합(set)\n",
    "        remaining_indices: 아직 측정되지 않은 인덱스 집합 (set)\n",
    "        num_select: 선택할 샘플 수\n",
    "        T_mc: MC Dropout 샘플 수\n",
    "        w_cost: 예측값이 큰 샘플 가중치\n",
    "        w_unc: epistemic 불확실성이 높은 샘플 가중치\n",
    "        w_div: latent 다양성이 높은 샘플 가중치\n",
    "        grad_num: z에 대한 cost의 gradient가 큰 샘플 수\n",
    "        rand_num: 무작위로 선택할 샘플 수\n",
    "    \n",
    "    Returns:\n",
    "        selected_indices: 선택된 샘플의 인덱스 리스트\n",
    "    \"\"\"\n",
    "\n",
    "    # 합쳐서 64개 선택\n",
    "    total = num_select\n",
    "    budget = total - grad_num - rand_num\n",
    "\n",
    "    # 랜덤 선택만 할 경우\n",
    "    if num_select == 0 and rand_num > 0:\n",
    "        rand_indices, remaining_indices = random_select_indices(remaining_indices, rand_num, rng=rng)\n",
    "        return rand_indices, remaining_indices\n",
    "    \n",
    "\n",
    "    select_n_cost = int(budget * w_cost)\n",
    "    select_n_unc  = int(budget * w_unc)\n",
    "    select_n_div  = int(budget * w_div)\n",
    "    select_n_grad = grad_num\n",
    "    s = select_n_cost + select_n_unc + select_n_div\n",
    "    if s < budget:\n",
    "        select_n_cost += budget - s\n",
    "\n",
    "    input_tensor = torch.tensor(input_data_scaled, dtype=torch.float32, device=device)\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z, _ = model.encode(input_tensor)\n",
    "    z = z.detach().requires_grad_(True)\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    cost_pred = model.predict_cost(z)\n",
    "    cost_pred = cost_pred.view(-1)\n",
    "    cost_np = cost_pred.detach().cpu().numpy()\n",
    "\n",
    "    remaining_np = np.array(list(remaining_indices), dtype=np.int64)\n",
    "    remaining_cost = cost_np[remaining_np]\n",
    "\n",
    "    k_pref = min(len(remaining_np), total * topk_factor)\n",
    "    top_local = np.argsort(remaining_cost)[-k_pref:]\n",
    "    candidate_indices = set(remaining_np[top_local].tolist())  # 작업용 remaining\n",
    "\n",
    "    # print(f\"Candidate pool size: {len(candidate_indices)}\")\n",
    "\n",
    "\n",
    "    # 중복 방지용\n",
    "    currently_used = set()\n",
    "    topk_cost_indices, candidate_indices = select_topk_cost(cost_pred, candidate_indices, select_n_cost)\n",
    "    currently_used.update(topk_cost_indices.tolist())\n",
    "    z_grad_indices, candidate_indices = select_topk_z_grad(z, cost_pred, candidate_indices, select_n_grad)\n",
    "    currently_used.update(z_grad_indices.tolist())\n",
    "\n",
    "    # if len(used_indices) / len(input_data_scaled) >= 0.1:\n",
    "    if len(used_indices) >= uncertainty_topk:\n",
    "        uncertainty_indices, candidate_indices = select_topk_uncertainty(model, input_tensor, candidate_indices, select_n_unc, T_mc=T_mc)\n",
    "    else:\n",
    "        pool_for_uncertainty = set(remaining_indices)\n",
    "        pool_for_uncertainty.difference_update(currently_used)\n",
    "        uncertainty_indices, _ = select_topk_uncertainty(model, input_tensor, pool_for_uncertainty, select_n_unc, T_mc=T_mc)\n",
    "        candidate_indices.difference_update(uncertainty_indices.tolist())\n",
    "\n",
    "\n",
    "    currently_used.update(uncertainty_indices.tolist())\n",
    "    used_local = set(used_indices)\n",
    "    used_local.update(currently_used)\n",
    "\n",
    "    diverse_indices, _ = select_topk_latent_diversity(z, candidate_indices, used_local, select_n_div)\n",
    "    currently_used.update(diverse_indices.tolist())\n",
    "\n",
    "\n",
    "    remaining_indices.difference_update(currently_used)\n",
    "\n",
    "\n",
    "    rand_indices, remaining_indices = random_select_indices(remaining_indices, rand_num, rng=rng)\n",
    "    currently_used.update(rand_indices.tolist())\n",
    "\n",
    "    \n",
    "\n",
    "    all_selected_indices = np.array(sorted(currently_used), dtype=np.int64)\n",
    "\n",
    "\n",
    "\n",
    "    return all_selected_indices, remaining_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3fe4fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vae_reg_dataloaders(input_data_scaled, costs, used_indices, remaining_indices):\n",
    "\n",
    "    train_indices = np.array(list(used_indices), dtype=np.int64)\n",
    "    val_indices = np.array(list(remaining_indices), dtype=np.int64)\n",
    "\n",
    "    X_train = input_data_scaled[train_indices]\n",
    "    X_val = input_data_scaled[val_indices]\n",
    "    y_train = costs[train_indices]\n",
    "    y_val = costs[val_indices]\n",
    "\n",
    "    train_dataset = FeatureRegressionDataset(X_train, y_train)\n",
    "    val_dataset   = FeatureRegressionDataset(X_val,   y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=False)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=512, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "    y_mean = y_train.mean()\n",
    "    y_std = y_train.std() + 1e-8  # 0 나누기 방지용 작은 값 추가\n",
    "    print(f\"y_train mean: {y_mean}, std: {y_std}\")\n",
    "\n",
    "    \n",
    "    return train_loader, val_loader, y_mean, y_std\n",
    "\n",
    "\n",
    "def make_xgb_datasets(inputs, results):\n",
    "    f_inputs = []\n",
    "    f_results = []\n",
    "    r_costs = []\n",
    "    for inp, res in zip(inputs, results):\n",
    "        cost = np.mean([c.value for c in res.costs])\n",
    "        if cost < 1e10:\n",
    "            f_inputs.append(inp)\n",
    "            f_results.append(res)\n",
    "            r_costs.append(cost)\n",
    "    r_costs = np.array(r_costs, dtype=np.float32)\n",
    "    \n",
    "    dataset = auto_scheduler.dataset.Dataset()\n",
    "    dataset.update_from_measure_pairs(f_inputs, f_results)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def split_xgb_datasets(dataset, train_indices, test_indices):\n",
    "\n",
    "    raw_features = list(dataset.features.values())[0]\n",
    "    raw_throughputs = list(dataset.throughputs.values())[0]\n",
    "\n",
    "    \n",
    "    train_set, test_set = dataset.random_split_within_task(train_set_ratio=0, \n",
    "                                                        train_idxs=train_indices.tolist(), \n",
    "                                                        test_idxs=test_indices.tolist())\n",
    "    return train_set, test_set, raw_throughputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ac08a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vae_reg_model(vae, config, input_dim, latent_dim, hidden_dim, y_std, verbose=True):\n",
    "    \n",
    "    print(f\"lambda_reg={config['lambda_reg']}, lambda_pair={config['lambda_pair']}, margin_scale={config['margin_scale']}, epochs={config['epochs']}, gamma={config['gamma']}, beta={config['beta']}, noise_std={config['noise_std']}\",\n",
    "            f\"\\nscratch={config['scratch']}, encoder_freeze={config['encoder_freeze']}, encoder_lr={config['encoder_lr']}, cost_predictor_lr={config['cost_predictor_lr']}\")\n",
    "\n",
    "    vae_cost_model = VAECostPredictor(input_dim=input_dim, \n",
    "                                latent_dim=latent_dim, \n",
    "                                hidden_dim=hidden_dim, \n",
    "                                predictor_layers=2,\n",
    "                                dropout=0.1, use_feature=False).to(device)\n",
    "    if not config['scratch']:\n",
    "        vae_cost_model.load_pretrained_encoder(vae.state_dict())\n",
    "    \n",
    "    if config['encoder_freeze']:\n",
    "        for param in vae_cost_model.get_encoder_params():\n",
    "            param.requires_grad = False\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': vae_cost_model.get_cost_predictor_params(), 'lr': config['cost_predictor_lr']},\n",
    "        ], weight_decay=1e-5)\n",
    "    else:\n",
    "        for param in vae_cost_model.get_encoder_params():\n",
    "            param.requires_grad = True\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': vae_cost_model.get_encoder_params(), 'lr': config['encoder_lr']},\n",
    "            {'params': vae_cost_model.get_cost_predictor_params(), 'lr': config['cost_predictor_lr']},\n",
    "        ], weight_decay=1e-5)\n",
    "        \n",
    "    return vae_cost_model, optimizer, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "df4c5e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_pair_warmup(epoch: int, warmup_epochs: int, lambda_pair_max: float) -> float:\n",
    "    if warmup_epochs <= 0:\n",
    "        return lambda_pair_max\n",
    "    t = min(max(epoch, 0), warmup_epochs) / warmup_epochs  # 0~1\n",
    "    return lambda_pair_max * t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "47e0fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regression(vae_cost_model, optimizer, train_loader, val_loader, input_data_scaled, costs, config, top_k=10, use_rank=True, warmup_epochs=200):\n",
    "\n",
    "    # print(\"Train size :\", len(train_loader.dataset))\n",
    "\n",
    "    # all_reg_results = []\n",
    "\n",
    "    lambda_pair_max = config['lambda_pair']\n",
    "\n",
    "    for epoch in range(1, config['epochs']+1):\n",
    "        vae_cost_model.train()\n",
    "        for x_batch, labels in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            labels = labels.to(device).squeeze(-1)\n",
    "            \n",
    "        \n",
    "            cost_pred, mean, logvar, z = vae_cost_model(x_batch, use_mean=True)\n",
    "\n",
    "            config['lambda_pair'] = lambda_pair_warmup(epoch, warmup_epochs, lambda_pair_max)\n",
    "\n",
    "            \n",
    "            train_loss, train_components = compute_total_loss(vae_cost_model, \n",
    "                                                    cost_pred, mean, logvar, z, labels, None, config)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(vae_cost_model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "        \n",
    "\n",
    "        if epoch % config['epochs'] == 0:\n",
    "            vae_cost_model.eval()\n",
    "            with torch.no_grad():\n",
    "                all_preds = []\n",
    "                all_labels = []\n",
    "                for x_batch, labels in val_loader:\n",
    "                    x_batch = x_batch.to(device)\n",
    "                    labels = labels.to(device).squeeze(-1)\n",
    "\n",
    "                    cost_pred, mean, logvar, z = vae_cost_model(x_batch, use_mean=True)\n",
    "                    all_preds.append(cost_pred)\n",
    "                    all_labels.append(labels)\n",
    "\n",
    "                    val_loss, val_components = compute_total_loss(vae_cost_model, cost_pred, mean, logvar, z, labels, None, config)\n",
    "                val_reg_r2 = r2_score(torch.cat(all_labels).detach().cpu().numpy(), torch.cat(all_preds).detach().cpu().numpy())\n",
    "                val_reg_r2 = round(val_reg_r2, 4)\n",
    "                \n",
    "                print(f\"Train loss epoch {epoch} : reg={train_components['reg_loss']: .4f} rank={train_components['pair_loss']: .4f} kl={train_components['kld_loss']: .4f}\")\n",
    "                print(f\"Val loss epoch {epoch}: reg={val_components['reg_loss']: .4f} rank={val_components['pair_loss']: .4f} kl={val_components['kld_loss']: .4f}\")\n",
    "                \n",
    "                print(f\"Regression R2 : {val_reg_r2:.4f}, \")\n",
    "        \n",
    "        # rank r2 계산\n",
    "        vae_cost_model.eval()\n",
    "        with torch.no_grad():\n",
    "            if epoch % config['epochs'] == 0:\n",
    "                input_data_tensor = torch.from_numpy(input_data_scaled).float().to(device)\n",
    "                all_preds = vae_cost_model(input_data_tensor, use_mean=True)[0].detach().cpu().numpy()\n",
    "                if use_rank:\n",
    "                    val_rank_r2 = pair_accuracy(all_preds, costs)\n",
    "                    val_rank_r2 = round(val_rank_r2, 4)\n",
    "                    print(f\"Rank R2 : {val_rank_r2:.4f}\")\n",
    "                else:\n",
    "                    val_rank_r2 = None\n",
    "                recall_top_k = recall_at_k(torch.tensor(all_preds), torch.from_numpy(costs), k=top_k)\n",
    "                \n",
    "                print(f\"Recall@{top_k} : {recall_top_k}\")\n",
    "\n",
    "    return vae_cost_model, recall_top_k, val_reg_r2, val_rank_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fbf8d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weight_grid(step=0.1):\n",
    "    m = int(round(1.0 / step))  # step=0.1 -> 10\n",
    "    weights = []\n",
    "    for i in range(m + 1):\n",
    "        for j in range(m + 1):\n",
    "            k = m - i - j\n",
    "            if k < 0:\n",
    "                continue\n",
    "            weights.append((i/m, j/m, k/m))\n",
    "    return weights\n",
    "weights = generate_weight_grid(step=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "18ffb652",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_weights = []\n",
    "for w in weights:\n",
    "    w_cost, w_unc, w_div = w\n",
    "    if w_cost < 0.3:\n",
    "        continue\n",
    "    # if w_unc == 0.0 and w_cost > 0.0 and w_div > 0.0:\n",
    "    #     f_weights.append(w)\n",
    "    #     continue\n",
    "    # if w_div == 0.0 and w_cost > 0.0 and w_unc > 0.0:\n",
    "    #     f_weights.append(w)\n",
    "        # continue\n",
    "    f_weights.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c7144bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_already_measured(total_csv_path, sampling_hyper):\n",
    "\n",
    "    if total_csv_path is not None:\n",
    "        total_csv = pd.read_csv(total_csv_path)\n",
    "\n",
    "        measured_keys = {\n",
    "            (\n",
    "                row.measure_size,\n",
    "\n",
    "                # row.scratch,\n",
    "                # row.encoder_freeze,\n",
    "                \n",
    "                row.encoder_lr,\n",
    "                row.cost_predictor_lr,\n",
    "                row.weights,\n",
    "                row.sampling_seed,\n",
    "                row.rank_warmup_epochs,\n",
    "\n",
    "                # row.uncertainty_topk,\n",
    "                row.grad_num,\n",
    "                row.rand_num,\n",
    "            )\n",
    "            for row in total_csv.itertuples(index=False)\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        measured_keys = set()\n",
    "    to_measure_configs = []\n",
    "\n",
    "    for params in itertools.product(*sampling_hyper.values()):\n",
    "        hyper_config = dict(zip(sampling_hyper.keys(), params))\n",
    "\n",
    "        key = (\n",
    "            hyper_config[\"measure_size\"],\n",
    "            # hyper_config[\"scratch\"],\n",
    "            # hyper_config[\"encoder_freeze\"],\n",
    "\n",
    "            hyper_config[\"encoder_lr\"],\n",
    "            hyper_config[\"cost_predictor_lr\"],\n",
    "            str(hyper_config[\"weight\"]),\n",
    "            hyper_config[\"sampling_seed\"],\n",
    "            hyper_config[\"warmup_epochs\"],\n",
    "            \n",
    "            # hyper_config[\"uncertainty_topk\"],\n",
    "            hyper_config[\"grad_num\"],\n",
    "            hyper_config[\"rand_num\"],\n",
    "        )\n",
    "\n",
    "        if key in measured_keys:\n",
    "            continue\n",
    "\n",
    "        to_measure_configs.append(hyper_config)\n",
    "    \n",
    "    return to_measure_configs\n",
    "\n",
    "def save_avg_csv(df_results, filename, top_k):\n",
    "    group_cols = [\n",
    "        \"measure_size\",\n",
    "        # \"scratch\",\n",
    "        # \"encoder_freeze\",\n",
    "        \"encoder_lr\",\n",
    "        \"cost_predictor_lr\",\n",
    "        \"rank_warmup_epochs\",\n",
    "        \"weights\",\n",
    "        \"uncertainty_topk\",\n",
    "        \"grad_num\",\n",
    "        \"rand_num\",\n",
    "    ]\n",
    "\n",
    "    df_avg = (\n",
    "        df_results\n",
    "        .groupby(group_cols, as_index=False)\n",
    "        .agg(\n",
    "            phase=(\"phase\", \"mean\"),\n",
    "            train_size=(\"train_size\", \"mean\"),\n",
    "            used_time=(\"used_time\", \"mean\"),\n",
    "            **{f\"top-{top_k}\": (f\"top-{top_k}\", \"mean\")},\n",
    "            val_reg_r2=(\"val_reg_r2\", \"first\"),\n",
    "            val_rank_r2=(\"val_rank_r2\", \"first\"),\n",
    "            seed_n=(\"sampling_seed\", \"nunique\"),\n",
    "            sampling_seed=(\"sampling_seed\", list),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df_avg.to_csv(filename.replace(\".csv\", \"_avg.csv\"), index=False)\n",
    "    df_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "26db8d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -> 1개의 실험 남음\n",
      "########## 실험 1/1 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2000\n",
      "초기 랜덤 선택 샘플 인덱스: [   8   40   96  105  221  228  245  376  402  422  546  609  619  633\n",
      "  687  720  763  836  872  917  931  941  959  961 1086 1179 1200 1317\n",
      " 1464 1532 1613 1643 1665 1673 1772 1781 1784 1793 1804 1889 1912 1936\n",
      " 1944 2045 2058 2064 2154 2172 2186 2307 2336 2394 2505 2659 2796 3125\n",
      " 3147 3198 3216 3327 3431 3442 3540 3588]\n",
      "=============== 측정 Phase 1 (64개) ================\n",
      "y_train mean: 5.516025543212891, std: 1.7373192410333251\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.2446 rank= 0.0075 kl= 0.5061\n",
      "Val loss epoch 1000: reg= 1.2377 rank= 0.1993 kl= 0.5170\n",
      "Regression R2 : 0.5424, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 (128개) ================\n",
      "y_train mean: 6.616763114929199, std: 1.7019381623132324\n",
      "lambda_reg=0.01, lambda_pair=3.0, margin_scale=0.3, epochs=1000, gamma=0.01, beta=0.01, noise_std=0.001 \n",
      "scratch=False, encoder_freeze=False, encoder_lr=1e-05, cost_predictor_lr=0.0001\n",
      "Train loss epoch 1000 : reg= 0.3130 rank= 0.0125 kl= 0.5161\n",
      "Val loss epoch 1000: reg= 1.1736 rank= 0.1789 kl= 0.5043\n",
      "Regression R2 : 0.5798, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 128\n",
      "총 측정 시간: 7.50 초\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "# 데이터셋 길이만큼의 인덱스 numpy 배열 생성\n",
    "all_indices = np.arange(len(input_data_scaled))\n",
    "costs = np.array(records[\"costs\"], dtype=np.float32)\n",
    "\n",
    "real_optimum_index = np.argmax(costs)\n",
    "\n",
    "top_k = 1\n",
    "\n",
    "train_seed = 2023\n",
    "\n",
    "\n",
    "sampling_hyper = {\n",
    "    \"measure_size\": [64],\n",
    "    \"weight\" : [\n",
    "            # (1.0, 0.0, 0.0),\n",
    "            # (0.7, 0.0, 0.3),\n",
    "            # (0.7, 0.3, 0.0),\n",
    "            # (0.6, 0.1, 0.3),\n",
    "            # (0.3, 0.4, 0.3),\n",
    "            (0.4, 0.3, 0.3),\n",
    "            # (0.3, 0.3, 0.4),\n",
    "            # (0.5, 0.2, 0.3),\n",
    "            ],\n",
    "    \n",
    "    \"uncertainty_topk\": [0],    # 몇 개부터 불확실성 기반 선택을 할지\n",
    "    # \"weight\" : f_weights,\n",
    "    \"grad_num\": [4],\n",
    "    \"rand_num\": [0],\n",
    "    \n",
    "    \"T_mc\": [20],\n",
    "    # \"encoder_freeze\": [False, True],\n",
    "    \"encoder_freeze\": [False],\n",
    "    \"scratch\": [False],\n",
    "    \"encoder_lr\": [1e-5],\n",
    "    \"cost_predictor_lr\": [1e-4],\n",
    "    \"warmup_epochs\" : [100],\n",
    "    # \"cost_predictor_lr\": [1e-4],\n",
    "\n",
    "    # \"sampling_seed\" : range(2000, 2020),\n",
    "    \"sampling_seed\" : [2000],\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "       \n",
    "now = datetime.datetime.now().strftime(\"%m%d_%H%M\")\n",
    "filename = f\"result_dags/{os.path.basename(json_file)}/vae_extent_{now}.csv\"\n",
    "\n",
    "# total_csv_path = f\"result/{os.path.basename(json_file)}/vae_extent_total.csv\"\n",
    "# to_measure_configs = filter_already_measured(total_csv_path, sampling_hyper)\n",
    "\n",
    "\n",
    "to_measure_configs = filter_already_measured(None, sampling_hyper)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"{len(list(itertools.product(*sampling_hyper.values())))} -> {len(to_measure_configs)}개의 실험 남음\")\n",
    "\n",
    "random_indices_list = []\n",
    "all_results = []\n",
    "\n",
    "cnt = 0\n",
    "for hyper_config in to_measure_configs:\n",
    "\n",
    "    # hyper_config = dict(zip(sampling_hyper.keys(), params))\n",
    "\n",
    "    cnt += 1\n",
    "    print(f\"########## 실험 {cnt}/{len(to_measure_configs)} ##########\")\n",
    "\n",
    "    tic = time.time()\n",
    "    # used_indices : 이미 측정된 인덱스 집합. train_indices와 동일\n",
    "    # remaining_indices : 아직 측정되지 않은 인덱스 집합. val_indices와 동일\n",
    "    used_indices = set()\n",
    "    remaining_indices = set(all_indices)\n",
    "    \n",
    "    measure_size = hyper_config[\"measure_size\"]\n",
    "    sampling_seed = hyper_config[\"sampling_seed\"]\n",
    "    w_cost, w_unc, w_div = hyper_config[\"weight\"]\n",
    "    print(f\"weights: {hyper_config['weight']}\")\n",
    "    print(f\"measure_size: {hyper_config['measure_size']}, T_mc: {hyper_config['T_mc']}, sampling_seed: {hyper_config['sampling_seed']}\")\n",
    "\n",
    "    sampling_rng = np.random.default_rng(sampling_seed)\n",
    "\n",
    "    hyperparameter = {\n",
    "\n",
    "        'lambda_reg' : 0.01,\n",
    "        'lambda_pair': 3.0,\n",
    "        'margin_scale': 0.3,\n",
    "        'gamma': 0.01,\n",
    "        'beta': 0.01,\n",
    "        'noise_std': 0.001,\n",
    "\n",
    "        'encoder_lr': hyper_config[\"encoder_lr\"],\n",
    "        'encoder_freeze' : hyper_config[\"encoder_freeze\"],\n",
    "        'scratch': hyper_config[\"scratch\"],\n",
    "        'feature_predictor_lr': 0,\n",
    "        'cost_predictor_lr': hyper_config[\"cost_predictor_lr\"],\n",
    "        'epochs': 1000,\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    seed_everything(sampling_seed)\n",
    "\n",
    "    init_indices, remaining_indices = random_select_indices(remaining_indices, select_size=sampling_hyper[\"measure_size\"][0], rng=sampling_rng)\n",
    "    random_indices_list.append(init_indices)\n",
    "\n",
    "    # random_num = int(sampling_hyper[\"measure_size\"][0] * (3/4))\n",
    "    # diverse_num = int(sampling_hyper[\"measure_size\"][0] - random_num)\n",
    "    # random_indices, remaining_indices = random_select_indices(remaining_indices, select_size=random_num, rng=sampling_rng)\n",
    "    # diverse_indices, remaining_indices = select_init_latent_diversity(vae, input_data_scaled, remaining_indices, diverse_num, device)\n",
    "    # init_indices = np.concatenate([random_indices, diverse_indices])\n",
    "\n",
    "    # init_indices, remaining_indices = select_representative_kmeans(vae, input_data_scaled, remaining_indices, sampling_hyper[\"measure_size\"][0], device, iters=5)\n",
    "\n",
    "\n",
    "    print(f\"초기 랜덤 선택 샘플 인덱스: {np.sort(init_indices)}\")\n",
    "    used_indices.update(init_indices)\n",
    "    \n",
    "\n",
    "    reg_history = []\n",
    "    rank_history = []\n",
    "\n",
    "    for phase in range(1, len(input_data_scaled) // measure_size + 1):\n",
    "\n",
    "        print(f\"=============== 측정 Phase {phase} ({len(used_indices)}개) ================\")\n",
    "\n",
    "\n",
    "        # DataLoader 갱신\n",
    "        seed_everything(train_seed)\n",
    "        train_loader, val_loader, y_mean, y_std = make_vae_reg_dataloaders(input_data_scaled, costs, used_indices, remaining_indices)\n",
    "\n",
    "        \n",
    "        vae_cost_model, optimizer, config = make_vae_reg_model(vae, hyperparameter, input_dim, latent_dim, hidden_dim, y_std, verbose=False)\n",
    "        \n",
    "        seed_everything(train_seed)\n",
    "        vae_cost_model, topk_recall_signal, val_reg_r2, val_rank_r2 = train_regression(vae_cost_model, optimizer, train_loader, val_loader, \n",
    "                                                                                       input_data_scaled, costs, config, top_k=top_k, use_rank=False, warmup_epochs=hyper_config[\"warmup_epochs\"])\n",
    "\n",
    "        reg_history.append(val_reg_r2)\n",
    "        rank_history.append(val_rank_r2)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # 다음 측정할 샘플 선택\n",
    "        selected_indices, remaining_indices = select_programs(\n",
    "            model=vae_cost_model,\n",
    "            input_data_scaled=input_data_scaled,\n",
    "            remaining_indices=remaining_indices,\n",
    "            used_indices=used_indices,\n",
    "            num_select=measure_size,\n",
    "            T_mc=hyper_config[\"T_mc\"],\n",
    "            w_cost=w_cost,\n",
    "            w_unc=w_unc,\n",
    "            w_div=w_div,\n",
    "            # w_cost=0.3,\n",
    "            # w_unc=0.35,\n",
    "            # w_div=0.35,\n",
    "            uncertainty_topk=hyper_config[\"uncertainty_topk\"],\n",
    "            grad_num=hyper_config[\"grad_num\"],\n",
    "            rand_num=hyper_config[\"rand_num\"],\n",
    "            device=device,\n",
    "            rng=sampling_rng,\n",
    "            \n",
    "            topk_factor=5\n",
    "        )\n",
    "        # w_cost += 0.03\n",
    "        # w_unc -= 0.02\n",
    "        # w_div -= 0.01\n",
    "\n",
    "        # selected_indices: numpy 배열\n",
    "        used_indices.update(selected_indices.tolist())\n",
    "\n",
    "        measured_optimum = True if real_optimum_index in used_indices else False\n",
    "\n",
    "\n",
    "        use_topk = False\n",
    "        \n",
    "\n",
    "        break_signal = False\n",
    "        if not use_topk and measured_optimum:\n",
    "            break_signal = True\n",
    "        elif use_topk and topk_recall_signal:\n",
    "            break_signal = True\n",
    "            filename= filename.replace(\"result/\", \"result_topk/\")\n",
    "\n",
    "\n",
    "        if break_signal:\n",
    "            print(\"최적화 종료\")\n",
    "            print(\"학습한 데이터 수 :\", len(used_indices)-measure_size)\n",
    "            used_time = time.time() - tic\n",
    "            print(f\"총 측정 시간: {used_time:.2f} 초\")\n",
    "            print(\"=============================================\")\n",
    "            all_results.append({\n",
    "                \"measure_size\": measure_size,\n",
    "                # \"scratch\": hyper_config[\"scratch\"],\n",
    "                # \"encoder_freeze\": hyper_config[\"encoder_freeze\"],\n",
    "                \"encoder_lr\": hyper_config[\"encoder_lr\"],\n",
    "                \"cost_predictor_lr\": hyper_config[\"cost_predictor_lr\"],\n",
    "                \"rank_warmup_epochs\": hyper_config[\"warmup_epochs\"],\n",
    "                \n",
    "                \"weights\": hyper_config[\"weight\"],\n",
    "                \"uncertainty_topk\": hyper_config[\"uncertainty_topk\"],\n",
    "                \"grad_num\": hyper_config[\"grad_num\"],\n",
    "                \"rand_num\": hyper_config[\"rand_num\"],\n",
    "                \"phase\" : phase,\n",
    "                \"used_time\": round(used_time, 2),\n",
    "                \"train_size\" : len(used_indices)-measure_size,\n",
    "                f\"top-{top_k}\" : topk_recall_signal,\n",
    "                \"val_reg_r2\": reg_history,\n",
    "                \"val_rank_r2\": rank_history,\n",
    "                \"sampling_seed\": sampling_seed,\n",
    "                \n",
    "                \n",
    "            })\n",
    "            if use_topk:\n",
    "                all_results[-1][\"top_k\"] = top_k\n",
    "\n",
    "            df_results = pd.DataFrame(all_results)\n",
    "            \n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            df_results.to_csv(filename, index=False)\n",
    "            \n",
    "            break\n",
    "\n",
    "if len(all_results) > 0:\n",
    "    save_avg_csv(df_results, filename, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "e84b5dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure_size</th>\n",
       "      <th>encoder_lr</th>\n",
       "      <th>cost_predictor_lr</th>\n",
       "      <th>weights</th>\n",
       "      <th>uncertainty_topk</th>\n",
       "      <th>grad_num</th>\n",
       "      <th>rand_num</th>\n",
       "      <th>phase</th>\n",
       "      <th>used_time</th>\n",
       "      <th>train_size</th>\n",
       "      <th>val_reg_r2</th>\n",
       "      <th>sampling_seed</th>\n",
       "      <th>rank_warmup_epochs</th>\n",
       "      <th>top-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.44</td>\n",
       "      <td>64</td>\n",
       "      <td>[0.8153]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.44</td>\n",
       "      <td>64</td>\n",
       "      <td>[0.713]</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.41</td>\n",
       "      <td>64</td>\n",
       "      <td>[0.7737]</td>\n",
       "      <td>2002</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11.19</td>\n",
       "      <td>192</td>\n",
       "      <td>[0.7897, 0.7438, 0.8066]</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10.98</td>\n",
       "      <td>192</td>\n",
       "      <td>[0.7887, 0.8399, 0.8845]</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11.01</td>\n",
       "      <td>192</td>\n",
       "      <td>[0.6095, 0.4503, 0.42]</td>\n",
       "      <td>2015</td>\n",
       "      <td>200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7.07</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.5551, 0.3795]</td>\n",
       "      <td>2016</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.40</td>\n",
       "      <td>64</td>\n",
       "      <td>[0.6697]</td>\n",
       "      <td>2017</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7.33</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.5625, 0.4166]</td>\n",
       "      <td>2018</td>\n",
       "      <td>200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.40</td>\n",
       "      <td>64</td>\n",
       "      <td>[0.5933]</td>\n",
       "      <td>2019</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>810 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     measure_size  encoder_lr  cost_predictor_lr          weights  \\\n",
       "0              64      0.0001            0.01000  (0.4, 0.3, 0.3)   \n",
       "1              64      0.0001            0.01000  (0.4, 0.3, 0.3)   \n",
       "2              64      0.0001            0.01000  (0.4, 0.3, 0.3)   \n",
       "3              64      0.0001            0.01000  (0.4, 0.3, 0.3)   \n",
       "4              64      0.0001            0.01000  (0.4, 0.3, 0.3)   \n",
       "..            ...         ...                ...              ...   \n",
       "805            64      0.0001            0.00001  (0.4, 0.3, 0.3)   \n",
       "806            64      0.0001            0.00001  (0.4, 0.3, 0.3)   \n",
       "807            64      0.0001            0.00001  (0.4, 0.3, 0.3)   \n",
       "808            64      0.0001            0.00001  (0.4, 0.3, 0.3)   \n",
       "809            64      0.0001            0.00001  (0.4, 0.3, 0.3)   \n",
       "\n",
       "     uncertainty_topk  grad_num  rand_num  phase  used_time  train_size  \\\n",
       "0                  64         4         0      1       3.44          64   \n",
       "1                  64         4         0      1       3.44          64   \n",
       "2                  64         4         0      1       3.41          64   \n",
       "3                  64         4         0      3      11.19         192   \n",
       "4                  64         4         0      3      10.98         192   \n",
       "..                ...       ...       ...    ...        ...         ...   \n",
       "805                64         4         0      3      11.01         192   \n",
       "806                64         4         0      2       7.07         128   \n",
       "807                64         4         0      1       3.40          64   \n",
       "808                64         4         0      2       7.33         128   \n",
       "809                64         4         0      1       3.40          64   \n",
       "\n",
       "                   val_reg_r2  sampling_seed  rank_warmup_epochs  top-1  \n",
       "0                    [0.8153]           2000                   0    NaN  \n",
       "1                     [0.713]           2001                   0    NaN  \n",
       "2                    [0.7737]           2002                   0    NaN  \n",
       "3    [0.7897, 0.7438, 0.8066]           2003                   0    NaN  \n",
       "4    [0.7887, 0.8399, 0.8845]           2004                   0    NaN  \n",
       "..                        ...            ...                 ...    ...  \n",
       "805    [0.6095, 0.4503, 0.42]           2015                 200    1.0  \n",
       "806          [0.5551, 0.3795]           2016                 200    0.0  \n",
       "807                  [0.6697]           2017                 200    0.0  \n",
       "808          [0.5625, 0.4166]           2018                 200    1.0  \n",
       "809                  [0.5933]           2019                 200    0.0  \n",
       "\n",
       "[810 rows x 14 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_dir = [\n",
    "    \"/root/work/tenset/scripts/pre_experiments/model_myself/result/([0c9a5ba46ffc5e1a9e5641018527117f,4,7,7,160,1,1,160,960,1,1,1,960,4,7,7,960],cuda).json/vae_extent_1220_1701.csv\",\n",
    "    \"/root/work/tenset/scripts/pre_experiments/model_myself/result/([0c9a5ba46ffc5e1a9e5641018527117f,4,7,7,160,1,1,160,960,1,1,1,960,4,7,7,960],cuda).json/vae_extent_1220_1732.csv\",\n",
    "    \"/root/work/tenset/scripts/pre_experiments/model_myself/result/([0c9a5ba46ffc5e1a9e5641018527117f,4,7,7,160,1,1,160,960,1,1,1,960,4,7,7,960],cuda).json/vae_extent_1220_1802.csv\",\n",
    "    \"/root/work/tenset/scripts/pre_experiments/model_myself/result/([0c9a5ba46ffc5e1a9e5641018527117f,4,7,7,160,1,1,160,960,1,1,1,960,4,7,7,960],cuda).json/vae_extent_1220_1814.csv\",\n",
    "    \"/root/work/tenset/scripts/pre_experiments/model_myself/result/([0c9a5ba46ffc5e1a9e5641018527117f,4,7,7,160,1,1,160,960,1,1,1,960,4,7,7,960],cuda).json/vae_extent_1220_1829.csv\",\n",
    "    \"/root/work/tenset/scripts/pre_experiments/model_myself/result/([0c9a5ba46ffc5e1a9e5641018527117f,4,7,7,160,1,1,160,960,1,1,1,960,4,7,7,960],cuda).json/vae_extent_1220_1925.csv\",\n",
    "    \"/root/work/tenset/scripts/pre_experiments/model_myself/result/([0c9a5ba46ffc5e1a9e5641018527117f,4,7,7,160,1,1,160,960,1,1,1,960,4,7,7,960],cuda).json/vae_extent_1220_1942.csv\",\n",
    "    \"/root/work/tenset/scripts/pre_experiments/model_myself/result/([0c9a5ba46ffc5e1a9e5641018527117f,4,7,7,160,1,1,160,960,1,1,1,960,4,7,7,960],cuda).json/vae_extent_1220_2008.csv\",\n",
    "]\n",
    "dfs = []\n",
    "for p in csv_dir:\n",
    "    sub_df = pd.read_csv(p)\n",
    "\n",
    "    if \"rank_warmup_epochs\" not in sub_df.columns:\n",
    "        sub_df[\"rank_warmup_epochs\"] = 0\n",
    "    if \"measure_size\" not in sub_df.columns:\n",
    "        sub_df[\"measure_size\"] = 64\n",
    "    \n",
    "    dfs.append(sub_df)\n",
    "\n",
    "    \n",
    "df_total = pd.concat(dfs, ignore_index=True)\n",
    "df_total.drop(columns=[\"scratch\", \"encoder_freeze\", \"val_rank_r2\"], inplace=True)\n",
    "\n",
    "# measure_size 컬럼을 맨 앞으로 이동\n",
    "cols = df_total.columns.tolist()\n",
    "df_total = df_total[[\"measure_size\"] + [c for c in cols if c != \"measure_size\"]]\n",
    "\n",
    "df_total.to_csv(os.path.dirname(filename)+\"/vae_extent_total.csv\", index=False)\n",
    "df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "7582a685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure_size</th>\n",
       "      <th>encoder_lr</th>\n",
       "      <th>cost_predictor_lr</th>\n",
       "      <th>rank_warmup_epochs</th>\n",
       "      <th>weights</th>\n",
       "      <th>uncertainty_topk</th>\n",
       "      <th>grad_num</th>\n",
       "      <th>rand_num</th>\n",
       "      <th>phase</th>\n",
       "      <th>train_size</th>\n",
       "      <th>used_time</th>\n",
       "      <th>val_reg_r2</th>\n",
       "      <th>seed_n</th>\n",
       "      <th>sampling_seed</th>\n",
       "      <th>top-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>163.200000</td>\n",
       "      <td>9.323500</td>\n",
       "      <td>[-10.9241, -10.8547, -10.8065]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>124.800000</td>\n",
       "      <td>6.986500</td>\n",
       "      <td>[-8.103, -8.1111]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>108.800000</td>\n",
       "      <td>6.079500</td>\n",
       "      <td>[0.7073]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>7.318500</td>\n",
       "      <td>[0.7496, 0.7937]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.450000</td>\n",
       "      <td>156.800000</td>\n",
       "      <td>9.345500</td>\n",
       "      <td>[0.8061, 0.794]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>102.400000</td>\n",
       "      <td>5.899500</td>\n",
       "      <td>[-7.0339, -7.3333]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.433333</td>\n",
       "      <td>91.733333</td>\n",
       "      <td>5.152333</td>\n",
       "      <td>[0.6151]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>100</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>4.396500</td>\n",
       "      <td>[0.6717]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>200</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>83.200000</td>\n",
       "      <td>4.565500</td>\n",
       "      <td>[0.6877]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>300</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>86.400000</td>\n",
       "      <td>4.747000</td>\n",
       "      <td>[0.6945]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>400</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>4.433500</td>\n",
       "      <td>[0.697]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>500</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>83.200000</td>\n",
       "      <td>4.647500</td>\n",
       "      <td>[0.7001]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>89.600000</td>\n",
       "      <td>5.066800</td>\n",
       "      <td>[0.6977]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>100</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>86.400000</td>\n",
       "      <td>4.804000</td>\n",
       "      <td>[0.6966]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>200</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>83.200000</td>\n",
       "      <td>4.555000</td>\n",
       "      <td>[0.7004]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>300</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>4.337500</td>\n",
       "      <td>[0.6978]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>400</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>83.200000</td>\n",
       "      <td>4.848000</td>\n",
       "      <td>[0.6965]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>500</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>4.420000</td>\n",
       "      <td>[0.6931]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>121.600000</td>\n",
       "      <td>7.096500</td>\n",
       "      <td>[0.7552, 0.7861]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.650000</td>\n",
       "      <td>169.600000</td>\n",
       "      <td>10.196500</td>\n",
       "      <td>[0.7916, 0.8094]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>6.441500</td>\n",
       "      <td>[-0.0166, -0.6613]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>5.296500</td>\n",
       "      <td>[0.621]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>100</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>89.600000</td>\n",
       "      <td>4.979000</td>\n",
       "      <td>[0.6387]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>200</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.466667</td>\n",
       "      <td>93.866667</td>\n",
       "      <td>5.210667</td>\n",
       "      <td>[0.6293]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>300</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>92.800000</td>\n",
       "      <td>5.122000</td>\n",
       "      <td>[0.6235]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>400</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>5.374000</td>\n",
       "      <td>[0.6248]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>500</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>102.400000</td>\n",
       "      <td>5.825500</td>\n",
       "      <td>[0.6256]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>87.040000</td>\n",
       "      <td>4.826600</td>\n",
       "      <td>[0.7052]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>100</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>89.600000</td>\n",
       "      <td>4.948500</td>\n",
       "      <td>[0.7117]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>200</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.550000</td>\n",
       "      <td>99.200000</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>[0.7106]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>300</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>89.600000</td>\n",
       "      <td>5.079500</td>\n",
       "      <td>[0.71]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>400</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>92.800000</td>\n",
       "      <td>5.442500</td>\n",
       "      <td>[0.7098]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>500</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.550000</td>\n",
       "      <td>99.200000</td>\n",
       "      <td>5.723000</td>\n",
       "      <td>[0.7091]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>102.400000</td>\n",
       "      <td>5.653000</td>\n",
       "      <td>[0.7838]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.050000</td>\n",
       "      <td>131.200000</td>\n",
       "      <td>7.462500</td>\n",
       "      <td>[0.8153]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    measure_size  encoder_lr  cost_predictor_lr  rank_warmup_epochs  \\\n",
       "0             64    0.000001           0.000001                   0   \n",
       "1             64    0.000001           0.000010                   0   \n",
       "2             64    0.000001           0.000100                   0   \n",
       "3             64    0.000001           0.001000                   0   \n",
       "4             64    0.000001           0.010000                   0   \n",
       "5             64    0.000010           0.000001                   0   \n",
       "6             64    0.000010           0.000010                   0   \n",
       "7             64    0.000010           0.000010                 100   \n",
       "8             64    0.000010           0.000010                 200   \n",
       "9             64    0.000010           0.000010                 300   \n",
       "10            64    0.000010           0.000010                 400   \n",
       "11            64    0.000010           0.000010                 500   \n",
       "12            64    0.000010           0.000100                   0   \n",
       "13            64    0.000010           0.000100                 100   \n",
       "14            64    0.000010           0.000100                 200   \n",
       "15            64    0.000010           0.000100                 300   \n",
       "16            64    0.000010           0.000100                 400   \n",
       "17            64    0.000010           0.000100                 500   \n",
       "18            64    0.000010           0.001000                   0   \n",
       "19            64    0.000010           0.010000                   0   \n",
       "20            64    0.000100           0.000001                   0   \n",
       "21            64    0.000100           0.000010                   0   \n",
       "22            64    0.000100           0.000010                 100   \n",
       "23            64    0.000100           0.000010                 200   \n",
       "24            64    0.000100           0.000010                 300   \n",
       "25            64    0.000100           0.000010                 400   \n",
       "26            64    0.000100           0.000010                 500   \n",
       "27            64    0.000100           0.000100                   0   \n",
       "28            64    0.000100           0.000100                 100   \n",
       "29            64    0.000100           0.000100                 200   \n",
       "30            64    0.000100           0.000100                 300   \n",
       "31            64    0.000100           0.000100                 400   \n",
       "32            64    0.000100           0.000100                 500   \n",
       "33            64    0.000100           0.001000                   0   \n",
       "34            64    0.000100           0.010000                   0   \n",
       "\n",
       "            weights  uncertainty_topk  grad_num  rand_num     phase  \\\n",
       "0   (0.4, 0.3, 0.3)                64         4         0  2.550000   \n",
       "1   (0.4, 0.3, 0.3)                64         4         0  1.950000   \n",
       "2   (0.4, 0.3, 0.3)                64         4         0  1.700000   \n",
       "3   (0.4, 0.3, 0.3)                64         4         0  2.000000   \n",
       "4   (0.4, 0.3, 0.3)                64         4         0  2.450000   \n",
       "5   (0.4, 0.3, 0.3)                64         4         0  1.600000   \n",
       "6   (0.4, 0.3, 0.3)                64         4         0  1.433333   \n",
       "7   (0.4, 0.3, 0.3)                64         4         0  1.250000   \n",
       "8   (0.4, 0.3, 0.3)                64         4         0  1.300000   \n",
       "9   (0.4, 0.3, 0.3)                64         4         0  1.350000   \n",
       "10  (0.4, 0.3, 0.3)                64         4         0  1.250000   \n",
       "11  (0.4, 0.3, 0.3)                64         4         0  1.300000   \n",
       "12  (0.4, 0.3, 0.3)                64         4         0  1.400000   \n",
       "13  (0.4, 0.3, 0.3)                64         4         0  1.350000   \n",
       "14  (0.4, 0.3, 0.3)                64         4         0  1.300000   \n",
       "15  (0.4, 0.3, 0.3)                64         4         0  1.250000   \n",
       "16  (0.4, 0.3, 0.3)                64         4         0  1.300000   \n",
       "17  (0.4, 0.3, 0.3)                64         4         0  1.250000   \n",
       "18  (0.4, 0.3, 0.3)                64         4         0  1.900000   \n",
       "19  (0.4, 0.3, 0.3)                64         4         0  2.650000   \n",
       "20  (0.4, 0.3, 0.3)                64         4         0  1.750000   \n",
       "21  (0.4, 0.3, 0.3)                64         4         0  1.500000   \n",
       "22  (0.4, 0.3, 0.3)                64         4         0  1.400000   \n",
       "23  (0.4, 0.3, 0.3)                64         4         0  1.466667   \n",
       "24  (0.4, 0.3, 0.3)                64         4         0  1.450000   \n",
       "25  (0.4, 0.3, 0.3)                64         4         0  1.500000   \n",
       "26  (0.4, 0.3, 0.3)                64         4         0  1.600000   \n",
       "27  (0.4, 0.3, 0.3)                64         4         0  1.360000   \n",
       "28  (0.4, 0.3, 0.3)                64         4         0  1.400000   \n",
       "29  (0.4, 0.3, 0.3)                64         4         0  1.550000   \n",
       "30  (0.4, 0.3, 0.3)                64         4         0  1.400000   \n",
       "31  (0.4, 0.3, 0.3)                64         4         0  1.450000   \n",
       "32  (0.4, 0.3, 0.3)                64         4         0  1.550000   \n",
       "33  (0.4, 0.3, 0.3)                64         4         0  1.600000   \n",
       "34  (0.4, 0.3, 0.3)                64         4         0  2.050000   \n",
       "\n",
       "    train_size  used_time                      val_reg_r2  seed_n  \\\n",
       "0   163.200000   9.323500  [-10.9241, -10.8547, -10.8065]      20   \n",
       "1   124.800000   6.986500               [-8.103, -8.1111]      20   \n",
       "2   108.800000   6.079500                        [0.7073]      20   \n",
       "3   128.000000   7.318500                [0.7496, 0.7937]      20   \n",
       "4   156.800000   9.345500                 [0.8061, 0.794]      20   \n",
       "5   102.400000   5.899500              [-7.0339, -7.3333]      20   \n",
       "6    91.733333   5.152333                        [0.6151]      20   \n",
       "7    80.000000   4.396500                        [0.6717]      20   \n",
       "8    83.200000   4.565500                        [0.6877]      20   \n",
       "9    86.400000   4.747000                        [0.6945]      20   \n",
       "10   80.000000   4.433500                         [0.697]      20   \n",
       "11   83.200000   4.647500                        [0.7001]      20   \n",
       "12   89.600000   5.066800                        [0.6977]      20   \n",
       "13   86.400000   4.804000                        [0.6966]      20   \n",
       "14   83.200000   4.555000                        [0.7004]      20   \n",
       "15   80.000000   4.337500                        [0.6978]      20   \n",
       "16   83.200000   4.848000                        [0.6965]      20   \n",
       "17   80.000000   4.420000                        [0.6931]      20   \n",
       "18  121.600000   7.096500                [0.7552, 0.7861]      20   \n",
       "19  169.600000  10.196500                [0.7916, 0.8094]      20   \n",
       "20  112.000000   6.441500              [-0.0166, -0.6613]      20   \n",
       "21   96.000000   5.296500                         [0.621]      20   \n",
       "22   89.600000   4.979000                        [0.6387]      20   \n",
       "23   93.866667   5.210667                        [0.6293]      20   \n",
       "24   92.800000   5.122000                        [0.6235]      20   \n",
       "25   96.000000   5.374000                        [0.6248]      20   \n",
       "26  102.400000   5.825500                        [0.6256]      20   \n",
       "27   87.040000   4.826600                        [0.7052]      20   \n",
       "28   89.600000   4.948500                        [0.7117]      20   \n",
       "29   99.200000   5.900000                        [0.7106]      20   \n",
       "30   89.600000   5.079500                          [0.71]      20   \n",
       "31   92.800000   5.442500                        [0.7098]      20   \n",
       "32   99.200000   5.723000                        [0.7091]      20   \n",
       "33  102.400000   5.653000                        [0.7838]      20   \n",
       "34  131.200000   7.462500                        [0.8153]      20   \n",
       "\n",
       "                                        sampling_seed     top-1  \n",
       "0   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...       NaN  \n",
       "1   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...       NaN  \n",
       "2   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...       NaN  \n",
       "3   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...       NaN  \n",
       "4   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...       NaN  \n",
       "5   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...       NaN  \n",
       "6   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.400000  \n",
       "7   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.300000  \n",
       "8   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.350000  \n",
       "9   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.350000  \n",
       "10  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.250000  \n",
       "11  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.300000  \n",
       "12  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.300000  \n",
       "13  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.250000  \n",
       "14  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.150000  \n",
       "15  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.200000  \n",
       "16  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.200000  \n",
       "17  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.250000  \n",
       "18  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...       NaN  \n",
       "19  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...       NaN  \n",
       "20  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...       NaN  \n",
       "21  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...       NaN  \n",
       "22  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.166667  \n",
       "23  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.266667  \n",
       "24  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.200000  \n",
       "25  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.300000  \n",
       "26  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.250000  \n",
       "27  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.100000  \n",
       "28  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.250000  \n",
       "29  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.300000  \n",
       "30  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.250000  \n",
       "31  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.250000  \n",
       "32  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...  0.300000  \n",
       "33  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...       NaN  \n",
       "34  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...       NaN  "
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_kwargs = {\n",
    "    \"phase\": (\"phase\", \"mean\"),\n",
    "    \"train_size\": (\"train_size\", \"mean\"),\n",
    "    \"used_time\": (\"used_time\", \"mean\"),\n",
    "    \"val_reg_r2\": (\"val_reg_r2\", \"first\"),\n",
    "    \"seed_n\": (\"sampling_seed\", \"nunique\"),\n",
    "    \"sampling_seed\": (\"sampling_seed\", list),\n",
    "}\n",
    "\n",
    "topk_col = f\"top-{top_k}\"\n",
    "if topk_col in df_total.columns:\n",
    "    agg_kwargs[topk_col] = (topk_col, \"mean\")\n",
    "\n",
    "group_cols = [\n",
    "    \"measure_size\",\n",
    "    # \"scratch\",\n",
    "    # \"encoder_freeze\",\n",
    "    \"encoder_lr\",\n",
    "    \"cost_predictor_lr\",\n",
    "    \"rank_warmup_epochs\",\n",
    "    \"weights\",\n",
    "    \"uncertainty_topk\",\n",
    "    \"grad_num\",\n",
    "    \"rand_num\",\n",
    "]\n",
    "\n",
    "df_total_avg = (\n",
    "    df_total\n",
    "    .groupby(group_cols, as_index=False)\n",
    "    .agg(**agg_kwargs)\n",
    ")\n",
    "df_total_avg.to_csv(os.path.dirname(filename)+\"/vae_extent_total_avg.csv\", index=False)\n",
    "df_total_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "83ab0ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoder_lr</th>\n",
       "      <th>cost_predictor_lr</th>\n",
       "      <th>rank_warmup_epochs</th>\n",
       "      <th>weights</th>\n",
       "      <th>uncertainty_topk</th>\n",
       "      <th>grad_num</th>\n",
       "      <th>rand_num</th>\n",
       "      <th>phase</th>\n",
       "      <th>train_size</th>\n",
       "      <th>used_time</th>\n",
       "      <th>val_reg_r2</th>\n",
       "      <th>seed_n</th>\n",
       "      <th>sampling_seed</th>\n",
       "      <th>top-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>163.200000</td>\n",
       "      <td>9.323500</td>\n",
       "      <td>[-10.9241, -10.8547, -10.8065]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>124.800000</td>\n",
       "      <td>6.986500</td>\n",
       "      <td>[-8.103, -8.1111]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>108.800000</td>\n",
       "      <td>6.026500</td>\n",
       "      <td>[0.7073]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>102.400000</td>\n",
       "      <td>5.899500</td>\n",
       "      <td>[-7.0339, -7.3333]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.433333</td>\n",
       "      <td>91.733333</td>\n",
       "      <td>5.152333</td>\n",
       "      <td>[0.6151]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>100</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>4.396500</td>\n",
       "      <td>[0.6717]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>200</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>83.200000</td>\n",
       "      <td>4.565500</td>\n",
       "      <td>[0.6877]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>300</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>86.400000</td>\n",
       "      <td>4.747000</td>\n",
       "      <td>[0.6945]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.433333</td>\n",
       "      <td>91.733333</td>\n",
       "      <td>5.202667</td>\n",
       "      <td>[0.6977]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>100</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>86.400000</td>\n",
       "      <td>4.804000</td>\n",
       "      <td>[0.6966]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>200</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>83.200000</td>\n",
       "      <td>4.555000</td>\n",
       "      <td>[0.7004]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>300</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>4.337500</td>\n",
       "      <td>[0.6978]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>6.441500</td>\n",
       "      <td>[-0.0166, -0.6613]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>5.296500</td>\n",
       "      <td>[0.621]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>100</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>76.800000</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>[0.6387]</td>\n",
       "      <td>10</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>200</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>76.800000</td>\n",
       "      <td>4.099000</td>\n",
       "      <td>[0.6293]</td>\n",
       "      <td>10</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>300</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>89.600000</td>\n",
       "      <td>4.870000</td>\n",
       "      <td>[0.6235]</td>\n",
       "      <td>10</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>89.600000</td>\n",
       "      <td>5.005000</td>\n",
       "      <td>[0.7052]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>100</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>89.600000</td>\n",
       "      <td>4.948500</td>\n",
       "      <td>[0.7117]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>200</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.550000</td>\n",
       "      <td>99.200000</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>[0.7106]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>300</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>89.600000</td>\n",
       "      <td>5.079500</td>\n",
       "      <td>[0.71]</td>\n",
       "      <td>20</td>\n",
       "      <td>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    encoder_lr  cost_predictor_lr  rank_warmup_epochs          weights  \\\n",
       "0     0.000001           0.000001                   0  (0.4, 0.3, 0.3)   \n",
       "1     0.000001           0.000010                   0  (0.4, 0.3, 0.3)   \n",
       "2     0.000001           0.000100                   0  (0.4, 0.3, 0.3)   \n",
       "3     0.000010           0.000001                   0  (0.4, 0.3, 0.3)   \n",
       "4     0.000010           0.000010                   0  (0.4, 0.3, 0.3)   \n",
       "5     0.000010           0.000010                 100  (0.4, 0.3, 0.3)   \n",
       "6     0.000010           0.000010                 200  (0.4, 0.3, 0.3)   \n",
       "7     0.000010           0.000010                 300  (0.4, 0.3, 0.3)   \n",
       "8     0.000010           0.000100                   0  (0.4, 0.3, 0.3)   \n",
       "9     0.000010           0.000100                 100  (0.4, 0.3, 0.3)   \n",
       "10    0.000010           0.000100                 200  (0.4, 0.3, 0.3)   \n",
       "11    0.000010           0.000100                 300  (0.4, 0.3, 0.3)   \n",
       "12    0.000100           0.000001                   0  (0.4, 0.3, 0.3)   \n",
       "13    0.000100           0.000010                   0  (0.4, 0.3, 0.3)   \n",
       "14    0.000100           0.000010                 100  (0.4, 0.3, 0.3)   \n",
       "15    0.000100           0.000010                 200  (0.4, 0.3, 0.3)   \n",
       "16    0.000100           0.000010                 300  (0.4, 0.3, 0.3)   \n",
       "17    0.000100           0.000100                   0  (0.4, 0.3, 0.3)   \n",
       "18    0.000100           0.000100                 100  (0.4, 0.3, 0.3)   \n",
       "19    0.000100           0.000100                 200  (0.4, 0.3, 0.3)   \n",
       "20    0.000100           0.000100                 300  (0.4, 0.3, 0.3)   \n",
       "\n",
       "    uncertainty_topk  grad_num  rand_num     phase  train_size  used_time  \\\n",
       "0                 64         4         0  2.550000  163.200000   9.323500   \n",
       "1                 64         4         0  1.950000  124.800000   6.986500   \n",
       "2                 64         4         0  1.700000  108.800000   6.026500   \n",
       "3                 64         4         0  1.600000  102.400000   5.899500   \n",
       "4                 64         4         0  1.433333   91.733333   5.152333   \n",
       "5                 64         4         0  1.250000   80.000000   4.396500   \n",
       "6                 64         4         0  1.300000   83.200000   4.565500   \n",
       "7                 64         4         0  1.350000   86.400000   4.747000   \n",
       "8                 64         4         0  1.433333   91.733333   5.202667   \n",
       "9                 64         4         0  1.350000   86.400000   4.804000   \n",
       "10                64         4         0  1.300000   83.200000   4.555000   \n",
       "11                64         4         0  1.250000   80.000000   4.337500   \n",
       "12                64         4         0  1.750000  112.000000   6.441500   \n",
       "13                64         4         0  1.500000   96.000000   5.296500   \n",
       "14                64         4         0  1.200000   76.800000   4.125000   \n",
       "15                64         4         0  1.200000   76.800000   4.099000   \n",
       "16                64         4         0  1.400000   89.600000   4.870000   \n",
       "17                64         4         0  1.400000   89.600000   5.005000   \n",
       "18                64         4         0  1.400000   89.600000   4.948500   \n",
       "19                64         4         0  1.550000   99.200000   5.900000   \n",
       "20                64         4         0  1.400000   89.600000   5.079500   \n",
       "\n",
       "                        val_reg_r2  seed_n  \\\n",
       "0   [-10.9241, -10.8547, -10.8065]      20   \n",
       "1                [-8.103, -8.1111]      20   \n",
       "2                         [0.7073]      20   \n",
       "3               [-7.0339, -7.3333]      20   \n",
       "4                         [0.6151]      20   \n",
       "5                         [0.6717]      20   \n",
       "6                         [0.6877]      20   \n",
       "7                         [0.6945]      20   \n",
       "8                         [0.6977]      20   \n",
       "9                         [0.6966]      20   \n",
       "10                        [0.7004]      20   \n",
       "11                        [0.6978]      20   \n",
       "12              [-0.0166, -0.6613]      20   \n",
       "13                         [0.621]      20   \n",
       "14                        [0.6387]      10   \n",
       "15                        [0.6293]      10   \n",
       "16                        [0.6235]      10   \n",
       "17                        [0.7052]      20   \n",
       "18                        [0.7117]      20   \n",
       "19                        [0.7106]      20   \n",
       "20                          [0.71]      20   \n",
       "\n",
       "                                        sampling_seed  top-1  \n",
       "0   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...    NaN  \n",
       "1   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...    NaN  \n",
       "2   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...    NaN  \n",
       "3   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...    NaN  \n",
       "4   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.40  \n",
       "5   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.30  \n",
       "6   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.35  \n",
       "7   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.35  \n",
       "8   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.30  \n",
       "9   [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.25  \n",
       "10  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.15  \n",
       "11  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.20  \n",
       "12  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...    NaN  \n",
       "13  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...    NaN  \n",
       "14  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.30  \n",
       "15  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.20  \n",
       "16  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.20  \n",
       "17  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.10  \n",
       "18  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.25  \n",
       "19  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.30  \n",
       "20  [2000, 2001, 2002, 2003, 2004, 2005, 2006, 200...   0.25  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# group_cols = [\n",
    "#     # \"scratch\",\n",
    "#     # \"encoder_freeze\",\n",
    "#     \"encoder_lr\",\n",
    "#     \"cost_predictor_lr\",\n",
    "#     \"rank_warmup_epochs\",\n",
    "#     \"weights\",\n",
    "#     \"uncertainty_topk\",\n",
    "#     \"grad_num\",\n",
    "#     \"rand_num\",\n",
    "# ]\n",
    "\n",
    "# df_total_avg = (\n",
    "#     df_total\n",
    "#     .groupby(group_cols, as_index=False)\n",
    "#     .agg(\n",
    "#         phase=(\"phase\", \"mean\"),\n",
    "#         train_size=(\"train_size\", \"mean\"),\n",
    "#         used_time=(\"used_time\", \"mean\"),\n",
    "#         **{f\"top-{top_k}\": (f\"top-{top_k}\", \"mean\")},\n",
    "#         val_reg_r2=(\"val_reg_r2\", \"first\"),\n",
    "#         # val_rank_r2=(\"val_rank_r2\", \"first\"),\n",
    "#         seed_n=(\"sampling_seed\", \"nunique\"),\n",
    "#         sampling_seed=(\"sampling_seed\", list),\n",
    "#     )\n",
    "# )\n",
    "df_total_avg.to_csv(os.path.dirname(filename)+\"/vae_extent_avg_total.csv\", index=False)\n",
    "df_total_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074df23e",
   "metadata": {},
   "source": [
    "## XGB test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f06d5436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=UserWarning,\n",
    "    message=\".*Old style callback is deprecated.*\"\n",
    ")\n",
    "\n",
    "from tvm.auto_scheduler.cost_model.xgb_model import XGBModelInternal\n",
    "\n",
    "\n",
    "inputs, results = auto_scheduler.RecordReader(json_file).read_lines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfa4fafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   8   41   99  108  228  236  253  389  415  435  563  629  639  654\n",
      "  709  743  788  864  900  947  961  971  990  991 1121 1217 1239 1359\n",
      " 1511 1581 1665 1696 1719 1727 1828 1838 1841 1851 1863 1949 1974 1998\n",
      " 2006 2111 2124 2129 2223 2241 2256 2381 2411 2471 2585 2745 2885 3225\n",
      " 3248 3300 3320 3434 3542 3552 3654 3703]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.4698\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.5222\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "Fit a xgb booster. Train size: 192\n",
      "XGB Reg R2 : 0.5323\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 4.03 초\n",
      "=============================================\n",
      "[  26   74   81  121  217  371  395  412  420  440  579  602  697  714\n",
      "  745  748  809  811  817  892  945 1024 1104 1206 1210 1242 1474 1493\n",
      " 1555 1562 1589 1603 1620 1637 1667 1746 1752 1764 1811 1827 1901 1974\n",
      " 2066 2069 2082 2114 2119 2189 2210 2331 2527 2656 2713 2820 2886 3122\n",
      " 3130 3151 3168 3222 3462 3538 3651 3678]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.5438\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.5810\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "Fit a xgb booster. Train size: 192\n",
      "XGB Reg R2 : 0.6565\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 4 ================\n",
      "Fit a xgb booster. Train size: 256\n",
      "XGB Reg R2 : 0.6160\n",
      "XGB Recall@1 : 1\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 4.85 초\n",
      "=============================================\n",
      "[  15  135  166  294  337  392  399  406  465  490  507  528  556  560\n",
      "  632  711  728  730  831  860  877  915  921  928 1171 1198 1309 1385\n",
      " 1512 1535 1643 1675 1685 1768 1793 1881 1902 1918 2029 2124 2136 2178\n",
      " 2235 2276 2419 2538 2548 2842 3066 3102 3110 3263 3271 3306 3314 3354\n",
      " 3395 3396 3447 3465 3520 3529 3553 3595]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.6159\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.6019\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "Fit a xgb booster. Train size: 192\n",
      "XGB Reg R2 : 0.6477\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 4 ================\n",
      "Fit a xgb booster. Train size: 256\n",
      "XGB Reg R2 : 0.6020\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 5 ================\n",
      "Fit a xgb booster. Train size: 320\n",
      "XGB Reg R2 : 0.6259\n",
      "XGB Recall@1 : 1\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 5.82 초\n",
      "=============================================\n",
      "[  19   57   69  146  187  276  304  396  470  522  627  714  750  878\n",
      "  961  979  997 1048 1081 1115 1168 1181 1269 1297 1336 1432 1436 1558\n",
      " 1619 1712 1852 1933 2043 2063 2094 2132 2202 2223 2394 2402 2428 2449\n",
      " 2456 2467 2473 2622 2680 2920 2924 2934 2988 3171 3190 3230 3234 3237\n",
      " 3398 3409 3417 3418 3479 3614 3672 3711]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.5736\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.5157\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "Fit a xgb booster. Train size: 192\n",
      "XGB Reg R2 : 0.6200\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 4 ================\n",
      "Fit a xgb booster. Train size: 256\n",
      "XGB Reg R2 : 0.6610\n",
      "XGB Recall@1 : 1\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 4.81 초\n",
      "=============================================\n",
      "[  10  111  113  116  194  252  340  497  502  543  618  684 1068 1084\n",
      " 1095 1133 1147 1177 1291 1297 1305 1394 1404 1427 1474 1553 1563 1580\n",
      " 1685 1815 1971 2019 2020 2063 2073 2137 2172 2248 2280 2286 2293 2350\n",
      " 2437 2492 2547 2597 2603 2607 2613 2625 2639 2784 2794 2954 3011 3055\n",
      " 3082 3111 3236 3368 3372 3420 3514 3538]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.4630\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.5990\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.28 초\n",
      "=============================================\n",
      "[  12   33   61  108  110  360  420  485  567  675  701  715  720  737\n",
      "  770  786  838  963  982 1141 1173 1274 1307 1309 1389 1420 1445 1473\n",
      " 1722 1777 1866 1868 2044 2083 2103 2156 2178 2203 2266 2298 2345 2399\n",
      " 2457 2481 2517 2531 2635 2648 2669 2684 2737 2739 2788 2944 2959 2981\n",
      " 3143 3300 3320 3350 3466 3503 3517 3567]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.3881\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.5376\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.34 초\n",
      "=============================================\n",
      "[  18   46   77  203  211  213  276  350  462  535  541  560  700  730\n",
      "  809 1086 1112 1113 1139 1284 1317 1324 1420 1509 1538 1571 1671 1675\n",
      " 1747 1840 2050 2054 2157 2185 2259 2336 2433 2445 2453 2489 2509 2555\n",
      " 2689 2829 2830 2899 3009 3037 3047 3152 3160 3183 3193 3201 3239 3305\n",
      " 3339 3342 3425 3491 3533 3563 3658 3689]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.5962\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.5675\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "Fit a xgb booster. Train size: 192\n",
      "XGB Reg R2 : 0.4604\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 4 ================\n",
      "Fit a xgb booster. Train size: 256\n",
      "XGB Reg R2 : 0.5493\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 4.93 초\n",
      "=============================================\n",
      "[  45   95  113  192  215  372  382  510  516  633  670  702  735  796\n",
      "  802  827  902 1005 1006 1038 1054 1057 1074 1082 1125 1140 1271 1322\n",
      " 1375 1378 1440 1442 1464 1674 1744 1771 1776 1913 2034 2084 2145 2186\n",
      " 2230 2240 2428 2475 2479 2482 2646 2673 2784 2790 2818 2894 2971 3032\n",
      " 3177 3225 3255 3260 3370 3383 3408 3663]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.4937\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.4923\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.33 초\n",
      "=============================================\n",
      "[  17   52  111  159  172  182  185  193  211  229  482  664  930  937\n",
      " 1046 1125 1183 1222 1256 1258 1276 1298 1350 1360 1433 1475 1519 1589\n",
      " 1685 1805 1842 1996 2102 2127 2174 2274 2419 2472 2520 2618 2664 2689\n",
      " 2718 2779 2806 2833 2877 2878 2971 2990 3039 3068 3069 3112 3126 3164\n",
      " 3218 3260 3287 3331 3412 3441 3455 3694]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.5632\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.6486\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "Fit a xgb booster. Train size: 192\n",
      "XGB Reg R2 : 0.6080\n",
      "XGB Recall@1 : 1\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 4.06 초\n",
      "=============================================\n",
      "[   0  110  270  332  342  396  482  516  672  744  778  873  901  952\n",
      "  970  972 1088 1173 1233 1324 1343 1382 1422 1428 1566 1572 1605 1698\n",
      " 1747 1804 1862 1928 1946 2023 2102 2209 2230 2423 2444 2460 2498 2641\n",
      " 2668 2676 2708 2722 2747 2831 2867 2947 3042 3071 3076 3182 3188 3242\n",
      " 3285 3320 3514 3523 3623 3658 3659 3720]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.3922\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.5490\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.37 초\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "topk_size = int(measure_size * 0.95)\n",
    "eps_greedy_size = measure_size - topk_size\n",
    "\n",
    "\n",
    "seeds = sampling_hyper[\"seed\"]\n",
    "random_indices = random_indices_list[:len(seeds)]\n",
    "\n",
    "xgb_results = []\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%m%d_%H%M\")\n",
    "xgb_filename = f\"result_xgb/{os.path.basename(json_file)}/xgb_search_{now}.csv\"\n",
    "\n",
    "for i, seed in enumerate(seeds):\n",
    "\n",
    "    tic = time.time()\n",
    "    sample_rng = np.random.default_rng(seed)\n",
    "\n",
    "    \n",
    "    \n",
    "    tenset_model = XGBModelInternal(use_workload_embedding=False, seed=train_seed)\n",
    "\n",
    "    seed_everything(train_seed)\n",
    "    dataset = make_xgb_datasets(inputs, results)\n",
    "\n",
    "    \n",
    "    used_indices = set(random_indices[i])\n",
    "    remaining_indices = set(all_indices)\n",
    "    remaining_indices.difference_update(used_indices)\n",
    "\n",
    "    train_indices = np.array(sorted(used_indices), dtype=np.int64)\n",
    "    test_indices = np.array(sorted(remaining_indices), dtype=np.int64)\n",
    "    print(train_indices)\n",
    "\n",
    "    reg_history = []\n",
    "    rank_history = []\n",
    "\n",
    "    for phase in range(1,  len(input_data_scaled) // measure_size + 1):\n",
    "\n",
    "        print(f\"=============== 측정 Phase {phase} ================\")\n",
    "\n",
    "        seed_everything(train_seed)\n",
    "        train_set, test_set, dataset_costs = split_xgb_datasets(dataset, train_indices, test_indices)\n",
    "        real_optimum_idx = np.argmax(dataset_costs)\n",
    "        seed_everything(train_seed)\n",
    "        tenset_model.fit_base(train_set=train_set)\n",
    "        xgb_all_preds = tenset_model.predict(dataset)\n",
    "        xgb_all_preds = np.array(list(xgb_all_preds.values())[0], dtype=np.float32)\n",
    "        \n",
    "        \n",
    "        xgb_reg_r2 = r2_score(dataset_costs, xgb_all_preds)\n",
    "        reg_history.append(round(xgb_reg_r2, 4))\n",
    "        print(f\"XGB Reg R2 : {xgb_reg_r2:.4f}\")\n",
    "\n",
    "        # xgb_rank_r2 = pair_accuracy(xgb_all_preds, dataset_costs)\n",
    "        # rank_history.append(round(xgb_rank_r2, 4))\n",
    "        # print(f\"XGB Rank R2 : {xgb_rank_r2:.4f}\")\n",
    "\n",
    "        recall_score = recall_at_k(torch.tensor(xgb_all_preds), torch.tensor(dataset_costs), k=10)        \n",
    "        print(f\"XGB Recall@{top_k} : {recall_score}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # 다음 측정할 샘플 선택\n",
    "        train_indices, test_indices = xgb_select_indices(xgb_all_preds, \n",
    "                            train_indices, test_indices, topk_size=topk_size, eps_greedy_size=eps_greedy_size, rng=sample_rng)\n",
    "        measured_optimum = True if real_optimum_idx in train_indices else False\n",
    "\n",
    "        use_topk = False\n",
    "        \n",
    "\n",
    "        break_signal = False\n",
    "        if not use_topk and measured_optimum:\n",
    "            break_signal = True\n",
    "            \n",
    "        elif use_topk and recall_score:\n",
    "            break_signal = True\n",
    "            xgb_filename= xgb_filename.replace(\"result_xgb/\", \"result_xgb_topk/\")\n",
    "\n",
    "\n",
    "        if break_signal:\n",
    "        # if recall_score:\n",
    "            print(\"XGB 최적화 종료 신호 감지\")\n",
    "            print(f\"총 측정 시간: {time.time() - tic:.2f} 초\")\n",
    "            print(\"=============================================\")\n",
    "            xgb_results.append({\n",
    "                \"measure_size\": measure_size,\n",
    "                \"phase\" : phase,\n",
    "                \"used_time\": round(time.time() - tic, 2),\n",
    "                \"train_size\" : len(train_indices) - measure_size,\n",
    "                \"val_reg_r2\": reg_history,\n",
    "                \"val_rank_r2\": rank_history,\n",
    "                \"sampling_seed\": seed,\n",
    "                \n",
    "            })\n",
    "            df_xgb_results = pd.DataFrame(xgb_results)\n",
    "            os.makedirs(os.path.dirname(xgb_filename), exist_ok=True)\n",
    "            df_xgb_results.to_csv(xgb_filename, index=False)\n",
    "            # raise KeyboardInterrupt\n",
    "            break\n",
    "        \n",
    "        if test_indices.shape[0] < measure_size:\n",
    "            print(\"측정할 샘플이 더 이상 남아있지 않음\")\n",
    "            xgb_results.append({\n",
    "                \"measure_size\": measure_size,\n",
    "                \"phase\" : \"all but not found\",\n",
    "                \"used_time\": round(time.time() - tic, 2),\n",
    "                \"train_size\" : len(train_indices) - measure_size,\n",
    "                \"val_reg_r2\": reg_history,\n",
    "                \"val_rank_r2\": rank_history,\n",
    "                \"sampling_seed\": seed,\n",
    "                \n",
    "            })\n",
    "            df_xgb_results = pd.DataFrame(xgb_results)\n",
    "            os.makedirs(os.path.dirname(xgb_filename), exist_ok=True)\n",
    "            df_xgb_results.to_csv(xgb_filename, index=False)\n",
    "            break\n",
    "            # raise KeyboardInterrupt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d6f6988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure_size</th>\n",
       "      <th>train_size</th>\n",
       "      <th>used_time</th>\n",
       "      <th>val_reg_r2</th>\n",
       "      <th>val_rank_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>198.4</td>\n",
       "      <td>4.182</td>\n",
       "      <td>[0.4698, 0.5222, 0.5323]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   measure_size  train_size  used_time                val_reg_r2 val_rank_r2\n",
       "0            64       198.4      4.182  [0.4698, 0.5222, 0.5323]          []"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_cols = [\n",
    "    \"measure_size\",\n",
    "]\n",
    "\n",
    "agg_dict = {\n",
    "    # \"phase\": \"mean\",\n",
    "    \"train_size\": \"mean\",\n",
    "    \"used_time\": \"mean\",\n",
    "    \"val_reg_r2\": \"first\",\n",
    "    \"val_rank_r2\": \"first\",\n",
    "}\n",
    "\n",
    "df_avg = (\n",
    "    df_xgb_results\n",
    "    .groupby(group_cols, as_index=False)\n",
    "    .agg(agg_dict)\n",
    ")\n",
    "df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf23a3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   8   41   99  108  228  236  253  389  415  435  563  629  639  654\n",
      "  709  743  788  864  900  947  961  971  990  991 1121 1217 1239 1359\n",
      " 1511 1581 1665 1696 1719 1727 1828 1838 1841 1851 1863 1949 1974 1998\n",
      " 2006 2111 2124 2129 2223 2241 2256 2381 2411 2471 2585 2745 2885 3225\n",
      " 3248 3300 3320 3434 3542 3552 3654 3703]\n",
      "=============== 측정 Phase 1 ================\n",
      "XGB Reg R2 : 0.6295\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.15 초\n",
      "=============================================\n",
      "[  26   74   81  121  217  371  395  412  420  440  579  602  697  714\n",
      "  745  748  809  811  817  892  945 1024 1104 1206 1210 1242 1474 1493\n",
      " 1555 1562 1589 1603 1620 1637 1667 1746 1752 1764 1811 1827 1901 1974\n",
      " 2066 2069 2082 2114 2119 2189 2210 2331 2527 2656 2713 2820 2886 3122\n",
      " 3130 3151 3168 3222 3462 3538 3651 3678]\n",
      "=============== 측정 Phase 1 ================\n",
      "XGB Reg R2 : 0.5064\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "XGB Reg R2 : 0.6101\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "XGB Reg R2 : 0.6527\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 4 ================\n",
      "XGB Reg R2 : 0.6519\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 5 ================\n",
      "XGB Reg R2 : 0.6354\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 6.34 초\n",
      "=============================================\n",
      "[  15  135  166  294  337  392  399  406  465  490  507  528  556  560\n",
      "  632  711  728  730  831  860  877  915  921  928 1171 1198 1309 1385\n",
      " 1512 1535 1643 1675 1685 1768 1793 1881 1902 1918 2029 2124 2136 2178\n",
      " 2235 2276 2419 2538 2548 2842 3066 3102 3110 3263 3271 3306 3314 3354\n",
      " 3395 3396 3447 3465 3520 3529 3553 3595]\n",
      "=============== 측정 Phase 1 ================\n",
      "XGB Reg R2 : 0.5497\n",
      "XGB Recall@1 : 1\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.13 초\n",
      "=============================================\n",
      "[  19   57   69  146  187  276  304  396  470  522  627  714  750  878\n",
      "  961  979  997 1048 1081 1115 1168 1181 1269 1297 1336 1432 1436 1558\n",
      " 1619 1712 1852 1933 2043 2063 2094 2132 2202 2223 2394 2402 2428 2449\n",
      " 2456 2467 2473 2622 2680 2920 2924 2934 2988 3171 3190 3230 3234 3237\n",
      " 3398 3409 3417 3418 3479 3614 3672 3711]\n",
      "=============== 측정 Phase 1 ================\n",
      "XGB Reg R2 : 0.4898\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "XGB Reg R2 : 0.6130\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.89 초\n",
      "=============================================\n",
      "[  10  111  113  116  194  252  340  497  502  543  618  684 1068 1084\n",
      " 1095 1133 1147 1177 1291 1297 1305 1394 1404 1427 1474 1553 1563 1580\n",
      " 1685 1815 1971 2019 2020 2063 2073 2137 2172 2248 2280 2286 2293 2350\n",
      " 2437 2492 2547 2597 2603 2607 2613 2625 2639 2784 2794 2954 3011 3055\n",
      " 3082 3111 3236 3368 3372 3420 3514 3538]\n",
      "=============== 측정 Phase 1 ================\n",
      "XGB Reg R2 : 0.5044\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "XGB Reg R2 : 0.3218\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "XGB Reg R2 : 0.7177\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 4 ================\n",
      "XGB Reg R2 : 0.7214\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 5 ================\n",
      "XGB Reg R2 : 0.7610\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 6 ================\n",
      "XGB Reg R2 : 0.7626\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 7 ================\n",
      "XGB Reg R2 : 0.7528\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 7.88 초\n",
      "=============================================\n",
      "[  12   33   61  108  110  360  420  485  567  675  701  715  720  737\n",
      "  770  786  838  963  982 1141 1173 1274 1307 1309 1389 1420 1445 1473\n",
      " 1722 1777 1866 1868 2044 2083 2103 2156 2178 2203 2266 2298 2345 2399\n",
      " 2457 2481 2517 2531 2635 2648 2669 2684 2737 2739 2788 2944 2959 2981\n",
      " 3143 3300 3320 3350 3466 3503 3517 3567]\n",
      "=============== 측정 Phase 1 ================\n",
      "XGB Reg R2 : 0.5193\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.12 초\n",
      "=============================================\n",
      "[  18   46   77  203  211  213  276  350  462  535  541  560  700  730\n",
      "  809 1086 1112 1113 1139 1284 1317 1324 1420 1509 1538 1571 1671 1675\n",
      " 1747 1840 2050 2054 2157 2185 2259 2336 2433 2445 2453 2489 2509 2555\n",
      " 2689 2829 2830 2899 3009 3037 3047 3152 3160 3183 3193 3201 3239 3305\n",
      " 3339 3342 3425 3491 3533 3563 3658 3689]\n",
      "=============== 측정 Phase 1 ================\n",
      "XGB Reg R2 : 0.5668\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "XGB Reg R2 : 0.6046\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "XGB Reg R2 : 0.6341\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 4.65 초\n",
      "=============================================\n",
      "[  45   95  113  192  215  372  382  510  516  633  670  702  735  796\n",
      "  802  827  902 1005 1006 1038 1054 1057 1074 1082 1125 1140 1271 1322\n",
      " 1375 1378 1440 1442 1464 1674 1744 1771 1776 1913 2034 2084 2145 2186\n",
      " 2230 2240 2428 2475 2479 2482 2646 2673 2784 2790 2818 2894 2971 3032\n",
      " 3177 3225 3255 3260 3370 3383 3408 3663]\n",
      "=============== 측정 Phase 1 ================\n",
      "XGB Reg R2 : 0.3925\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "XGB Reg R2 : 0.5128\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.88 초\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import multiprocessing\n",
    "\n",
    "topk_size = int(measure_size * 0.95)\n",
    "eps_greedy_size = measure_size - topk_size\n",
    "\n",
    "\n",
    "seeds = sampling_hyper[\"seed\"]\n",
    "random_indices = random_indices_list[:len(seeds)]\n",
    "\n",
    "xgb_results = []\n",
    "\n",
    "# XGBModelInternal과 동일한 xgb_params 설정\n",
    "xgb_params = {\n",
    "    \"max_depth\": 6,\n",
    "    \"gamma\": 0.003,\n",
    "    \"min_child_weight\": 2,\n",
    "    \"eta\": 0.2,\n",
    "    \"n_gpus\": 0,\n",
    "    \"nthread\": multiprocessing.cpu_count() // 2,\n",
    "    \"verbosity\": 0,\n",
    "    \"seed\": train_seed or 43,\n",
    "    \"disable_default_eval_metric\": 1,\n",
    "}\n",
    "\n",
    "for i, seed in enumerate(seeds):\n",
    "\n",
    "    tic = time.time()\n",
    "    sample_rng = np.random.default_rng(seed)\n",
    "\n",
    "    dataset = make_xgb_datasets(inputs, results)\n",
    "    \n",
    "    used_indices = set(random_indices[i])\n",
    "    remaining_indices = set(all_indices)\n",
    "    remaining_indices.difference_update(used_indices)\n",
    "\n",
    "    train_indices = np.array(sorted(used_indices), dtype=np.int64)\n",
    "    test_indices = np.array(sorted(remaining_indices), dtype=np.int64)\n",
    "    print(train_indices)\n",
    "\n",
    "    reg_history = []\n",
    "    rank_history = []\n",
    "\n",
    "    for phase in range(1,  len(input_data_scaled) // measure_size + 1):\n",
    "\n",
    "        print(f\"=============== 측정 Phase {phase} ================\")\n",
    "\n",
    "        seed_everything(train_seed)\n",
    "        _, _, dataset_costs = split_xgb_datasets(dataset, train_indices, test_indices)\n",
    "        input_train = input_data_scaled[train_indices]\n",
    "        label_train = dataset_costs[train_indices]\n",
    "        input_test = input_data_scaled[test_indices]\n",
    "        label_test = dataset_costs[test_indices]\n",
    "        \n",
    "        real_optimum_idx = np.argmax(dataset_costs)\n",
    "        \n",
    "        # XGB 모델 학습 - input_train, label_train 사용\n",
    "        seed_everything(train_seed)\n",
    "        dtrain = xgb.DMatrix(input_train, label=label_train)\n",
    "        dtest = xgb.DMatrix(input_test, label=label_test)\n",
    "        \n",
    "        # 학습 (XGBModelInternal과 유사하게 num_boost_round=300, early stopping 없이 단순화)\n",
    "        bst = xgb.train(\n",
    "            params=xgb_params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=300,\n",
    "            evals=[(dtrain, \"train\"), (dtest, \"test\")],\n",
    "            verbose_eval=50,\n",
    "        )\n",
    "        \n",
    "        # input_data_scaled 전체로 predict\n",
    "        dmatrix_all = xgb.DMatrix(input_data_scaled)\n",
    "        xgb_all_preds = bst.predict(dmatrix_all)\n",
    "        xgb_all_preds = np.array(xgb_all_preds, dtype=np.float32)\n",
    "        \n",
    "        \n",
    "        xgb_reg_r2 = r2_score(dataset_costs, xgb_all_preds)\n",
    "        reg_history.append(round(xgb_reg_r2, 4))\n",
    "        print(f\"XGB Reg R2 : {xgb_reg_r2:.4f}\")\n",
    "\n",
    "        # xgb_rank_r2 = pair_accuracy(xgb_all_preds, dataset_costs)\n",
    "        # rank_history.append(round(xgb_rank_r2, 4))\n",
    "        # print(f\"XGB Rank R2 : {xgb_rank_r2:.4f}\")\n",
    "\n",
    "        recall_score = recall_at_k(torch.tensor(xgb_all_preds), torch.tensor(dataset_costs), k=10)        \n",
    "        print(f\"XGB Recall@{top_k} : {recall_score}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # 다음 측정할 샘플 선택\n",
    "        train_indices, test_indices = xgb_select_indices(xgb_all_preds, \n",
    "                            train_indices, test_indices, topk_size=topk_size, eps_greedy_size=eps_greedy_size, rng=sample_rng)\n",
    "        measured_optimum = True if real_optimum_idx in train_indices else False\n",
    "\n",
    "        use_topk = False\n",
    "        \n",
    "\n",
    "        break_signal = False\n",
    "        if not use_topk and measured_optimum:\n",
    "            break_signal = True\n",
    "            \n",
    "        elif use_topk and recall_score:\n",
    "            break_signal = True\n",
    "            xgb_filename= xgb_filename.replace(\"result_xgb/\", \"result_xgb_topk/topk_\")\n",
    "\n",
    "\n",
    "        if break_signal:\n",
    "            print(\"XGB 최적화 종료 신호 감지\")\n",
    "            print(f\"총 측정 시간: {time.time() - tic:.2f} 초\")\n",
    "            print(\"=============================================\")\n",
    "            xgb_results.append({\n",
    "                \"measure_size\": measure_size,\n",
    "                \"phase\" : phase,\n",
    "                \"used_time\": round(time.time() - tic, 2),\n",
    "                \"train_size\" : len(train_indices) - measure_size,\n",
    "                \"val_reg_r2\": reg_history,\n",
    "                \"val_rank_r2\": rank_history,\n",
    "                \"sampling_seed\": seed,\n",
    "                \n",
    "            })\n",
    "            df_xgb_results = pd.DataFrame(xgb_results)\n",
    "            # df_xgb_results.to_csv(xgb_filename, index=False)\n",
    "            break\n",
    "        \n",
    "        if test_indices.shape[0] < measure_size:\n",
    "            print(\"측정할 샘플이 더 이상 남아있지 않음\")\n",
    "            xgb_results.append({\n",
    "                \"measure_size\": measure_size,\n",
    "                \"phase\" : \"all but not found\",\n",
    "                \"used_time\": round(time.time() - tic, 2),\n",
    "                \"train_size\" : len(train_indices) - measure_size,\n",
    "                \"val_reg_r2\": reg_history,\n",
    "                \"val_rank_r2\": rank_history,\n",
    "                \"sampling_seed\": seed,\n",
    "                \n",
    "            })\n",
    "            df_xgb_results = pd.DataFrame(xgb_results)\n",
    "            # df_xgb_results.to_csv(xgb_filename, index=False)\n",
    "            break\n",
    "            # raise KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "83a274fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure_size</th>\n",
       "      <th>phase</th>\n",
       "      <th>used_time</th>\n",
       "      <th>train_size</th>\n",
       "      <th>val_reg_r2</th>\n",
       "      <th>val_rank_r2</th>\n",
       "      <th>sampling_seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>3.16</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.455, 0.4711]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>4.44</td>\n",
       "      <td>320</td>\n",
       "      <td>[0.5689, 0.5475, 0.5938, 0.5836, 0.6436]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>4.01</td>\n",
       "      <td>256</td>\n",
       "      <td>[0.588, 0.5441, 0.5812, 0.6114]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>3.09</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.5876, 0.5477]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>3.24</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.4325, 0.608]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>3.09</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.4016, 0.5381]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>3.09</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.5755, 0.333]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>5.60</td>\n",
       "      <td>448</td>\n",
       "      <td>[0.4894, 0.5619, 0.478, 0.5676, 0.5699, 0.5365...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>3.11</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.5457, 0.6573]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3.92</td>\n",
       "      <td>256</td>\n",
       "      <td>[0.3686, 0.5665, 0.5754, 0.6717]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   measure_size  phase  used_time  train_size  \\\n",
       "0            64      2       3.16         128   \n",
       "1            64      5       4.44         320   \n",
       "2            64      4       4.01         256   \n",
       "3            64      2       3.09         128   \n",
       "4            64      2       3.24         128   \n",
       "5            64      2       3.09         128   \n",
       "6            64      2       3.09         128   \n",
       "7            64      7       5.60         448   \n",
       "8            64      2       3.11         128   \n",
       "9            64      4       3.92         256   \n",
       "\n",
       "                                          val_reg_r2 val_rank_r2  \\\n",
       "0                                    [0.455, 0.4711]          []   \n",
       "1           [0.5689, 0.5475, 0.5938, 0.5836, 0.6436]          []   \n",
       "2                    [0.588, 0.5441, 0.5812, 0.6114]          []   \n",
       "3                                   [0.5876, 0.5477]          []   \n",
       "4                                    [0.4325, 0.608]          []   \n",
       "5                                   [0.4016, 0.5381]          []   \n",
       "6                                    [0.5755, 0.333]          []   \n",
       "7  [0.4894, 0.5619, 0.478, 0.5676, 0.5699, 0.5365...          []   \n",
       "8                                   [0.5457, 0.6573]          []   \n",
       "9                   [0.3686, 0.5665, 0.5754, 0.6717]          []   \n",
       "\n",
       "   sampling_seed  \n",
       "0           2000  \n",
       "1           2001  \n",
       "2           2002  \n",
       "3           2003  \n",
       "4           2004  \n",
       "5           2005  \n",
       "6           2006  \n",
       "7           2007  \n",
       "8           2008  \n",
       "9           2009  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xgb_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d57c684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure_size</th>\n",
       "      <th>train_size</th>\n",
       "      <th>used_time</th>\n",
       "      <th>val_reg_r2</th>\n",
       "      <th>val_rank_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>176.0</td>\n",
       "      <td>4.505</td>\n",
       "      <td>[0.6295]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   measure_size  train_size  used_time val_reg_r2 val_rank_r2\n",
       "0            64       176.0      4.505   [0.6295]          []"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_cols = [\n",
    "    \"measure_size\",\n",
    "]\n",
    "\n",
    "agg_dict = {\n",
    "    # \"phase\": \"mean\",\n",
    "    \"train_size\": \"mean\",\n",
    "    \"used_time\": \"mean\",\n",
    "    \"val_reg_r2\": \"first\",\n",
    "    \"val_rank_r2\": \"first\",\n",
    "}\n",
    "\n",
    "df_avg = (\n",
    "    df_xgb_results\n",
    "    .groupby(group_cols, as_index=False)\n",
    "    .agg(agg_dict)\n",
    ")\n",
    "df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336e266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit a xgb booster. Train size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenset 모델 Rank Accuracy: 0.8091\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tvm.auto_scheduler.cost_model.xgb_model import XGBModelInternal\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    tenset_model = XGBModelInternal()\n",
    "    tenset_model.fit_base(train_set, valid_set=test_set)\n",
    "    throughputs = np.array(list(test_set.throughputs.values()))\n",
    "\n",
    "    pred = tenset_model.predict(test_set)\n",
    "\n",
    "    true_biggest_index = np.argsort(throughputs[0])[-1]\n",
    "    biggest_indices_64 = np.argsort(list(pred.values())[0])[-64:]\n",
    "\n",
    "    # list(pred.values())[0]\n",
    "    if true_biggest_index in biggest_indices_64:\n",
    "        print(\"✓ Tenset 모델이 실제 가장 높은 throughput 정확히 예측했습니다!\")\n",
    "        break\n",
    "    break\n",
    "\n",
    "\n",
    "# pred, throughputs rank accuracy\n",
    "correct_pairs = 0\n",
    "total_pairs = 0\n",
    "n_samples = min(2000, throughputs.shape[-1])\n",
    "sample_indices = np.random.choice(throughputs.shape[-1], n_samples, replace=False)\n",
    "pred_values = list(pred.values())[0]\n",
    "throughput_values = throughputs.squeeze()\n",
    "rank_accuracy = pair_accuracy(pred_values, throughput_values)\n",
    "print(f\"Tenset 모델 Rank Accuracy: {rank_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
