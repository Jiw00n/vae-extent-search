{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d665df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "\n",
    "project_root = \"/root/work/tenset\"\n",
    "os.environ[\"TVM_HOME\"] = f\"{project_root}\"\n",
    "os.environ[\"TVM_LIBRARY_PATH\"] = f\"{project_root}/build\"\n",
    "if f\"{project_root}/python\" not in sys.path:\n",
    "    sys.path.insert(0, f\"{project_root}/python\")\n",
    "    \n",
    "\n",
    "sys.path = [p for p in sys.path if not p.startswith(f\"{project_root}/build\")]\n",
    "sys.path.append(f\"{project_root}/build\")\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = f\"{project_root}/build:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf5b5652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class NpzRegressionDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            self.X = torch.from_numpy(X).float()\n",
    "        else:\n",
    "            self.X = X\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        # y shape이 (N,)이면 (N,1)로 바꿔주는 게 편할 때가 많음\n",
    "        if self.y.ndim == 1:\n",
    "            self.y = self.y.unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c9695ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tvm import auto_scheduler\n",
    "\n",
    "\n",
    "# record_index\n",
    "# vector_index\n",
    "# diff_indices\n",
    "# diff_values\n",
    "# cost\n",
    "\n",
    "json_diffs = np.load(\"../i_vectors_diffs.npz\")\n",
    "raw_input = json_diffs[\"diff_values\"]\n",
    "\n",
    "# input_data = json_diffs[\"diff_values\"]\n",
    "costs = -np.log(json_diffs[\"cost\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbf2b63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def transform_schedule(x_int, mean=None, std=None, eps=1e-6):\n",
    "    \"\"\"\n",
    "    스케줄 파라미터 int 텐서를 log-scale + zero-flag로 변환\n",
    "\n",
    "    Args:\n",
    "        x_int: (B, D) int 텐서. 예: 0, 1, 2, 4, 8, ..., 1024\n",
    "        mean, std: (1, D) 형태의 텐서. None이면 입력에서 계산해서 반환.\n",
    "        eps: 분산 0 방지용 작은 값.\n",
    "\n",
    "    Returns:\n",
    "        x_cont: (B, 2*D) float 텐서. [v_norm, is_zero] concat\n",
    "        mean, std: (1, D) float 텐서. 나중에 validation/test에도 reuse\n",
    "    \"\"\"\n",
    "    # int → float\n",
    "    x_int = torch.tensor(x_int)\n",
    "    x = x_int.to(torch.float32)\n",
    "\n",
    "    # zero flag\n",
    "    is_zero = (x_int == 0).to(torch.float32)  # (B, D)\n",
    "\n",
    "    # log2 변환 (0은 일단 0으로 두고 mask)\n",
    "    v = torch.zeros_like(x, dtype=torch.float32)  # (B, D)\n",
    "    mask = (x_int > 0)\n",
    "    v[mask] = torch.log2(x[mask])\n",
    "\n",
    "    # mean / std 없으면 전체 batch 기준으로 계산 (보통은 train 전체로 미리 계산)\n",
    "    if mean is None or std is None:\n",
    "        mean = v.mean(dim=0, keepdim=True)           # (1, D)\n",
    "        std = v.std(dim=0, keepdim=True) + eps       # (1, D)\n",
    "\n",
    "    # 정규화\n",
    "    v_norm = (v - mean) / std   # (B, D)\n",
    "\n",
    "    # v_norm과 is_zero concat → (B, 2D)\n",
    "    x_cont = torch.cat([v_norm, is_zero], dim=-1)\n",
    "\n",
    "    return x_cont, mean, std\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8623adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=16, hidden_dim=128):\n",
    "        \"\"\"\n",
    "        input_dim: 2 * D (v_norm + is_zero concat한 차원)\n",
    "        latent_dim: latent space 차원\n",
    "        hidden_dim: MLP hidden 크기\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            \n",
    "            # 출력은 연속값이니까 activation 없이 그대로\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar, z\n",
    "\n",
    "def vae_loss(x_recon, x, mu, logvar, beta=1.0):\n",
    "    \"\"\"\n",
    "    x, x_recon: (B, input_dim)\n",
    "    mu, logvar: (B, latent_dim)\n",
    "\n",
    "    beta: KL 가중치 (β-VAE 스타일로 조절)\n",
    "    \"\"\"\n",
    "    # reconstruction loss: MSE\n",
    "    recon_loss = F.mse_loss(x_recon, x, reduction=\"mean\")\n",
    "\n",
    "    # KL divergence: D_KL(q(z|x) || N(0, I))\n",
    "    kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    loss = recon_loss + beta * kl\n",
    "    return loss, recon_loss, kl\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa3ee161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "input_data = np.log(json_diffs[\"diff_values\"]+1e-8)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "input_data_scaled = scaler.fit_transform(input_data)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    input_data_scaled, costs, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a87e2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss=0.9582, recon=0.9580, kl=0.0040\n",
      "epoch 0: val loss=1.0142, val recon=1.0139, val kl=0.0052\n",
      "-0.0026533503296284344\n",
      "epoch 20: loss=0.2727, recon=0.2264, kl=0.9275\n",
      "epoch 20: val loss=0.2838, val recon=0.2339, val kl=0.9979\n",
      "0.7673657920635055\n",
      "epoch 40: loss=0.1521, recon=0.0979, kl=1.0838\n",
      "epoch 40: val loss=0.1686, val recon=0.1143, val kl=1.0868\n",
      "0.8867139025535845\n",
      "epoch 60: loss=0.1230, recon=0.0689, kl=1.0828\n",
      "epoch 60: val loss=0.1418, val recon=0.0874, val kl=1.0893\n",
      "0.9131667091352129\n",
      "epoch 80: loss=0.1074, recon=0.0560, kl=1.0288\n",
      "epoch 80: val loss=0.1207, val recon=0.0689, val kl=1.0362\n",
      "0.9313943473540682\n",
      "epoch 100: loss=0.0919, recon=0.0426, kl=0.9841\n",
      "epoch 100: val loss=0.1102, val recon=0.0605, val kl=0.9941\n",
      "0.9395658845274228\n",
      "epoch 120: loss=0.0838, recon=0.0352, kl=0.9704\n",
      "epoch 120: val loss=0.1045, val recon=0.0568, val kl=0.9531\n",
      "0.9435181589507591\n",
      "epoch 140: loss=0.0783, recon=0.0328, kl=0.9101\n",
      "epoch 140: val loss=0.0970, val recon=0.0512, val kl=0.9157\n",
      "0.9487748037507997\n",
      "epoch 160: loss=0.0731, recon=0.0279, kl=0.9032\n",
      "epoch 160: val loss=0.0919, val recon=0.0476, val kl=0.8857\n",
      "0.9523255957997976\n",
      "epoch 180: loss=0.0699, recon=0.0269, kl=0.8602\n",
      "epoch 180: val loss=0.0870, val recon=0.0444, val kl=0.8526\n",
      "0.9555414424396788\n",
      "epoch 200: loss=0.0637, recon=0.0235, kl=0.8051\n",
      "epoch 200: val loss=0.0838, val recon=0.0426, val kl=0.8236\n",
      "0.9574359503145881\n",
      "epoch 220: loss=0.0621, recon=0.0216, kl=0.8111\n",
      "epoch 220: val loss=0.0822, val recon=0.0418, val kl=0.8073\n",
      "0.9582587543573862\n",
      "epoch 240: loss=0.0604, recon=0.0216, kl=0.7765\n",
      "epoch 240: val loss=0.0799, val recon=0.0406, val kl=0.7873\n",
      "0.959447504436639\n",
      "epoch 260: loss=0.0572, recon=0.0189, kl=0.7652\n",
      "epoch 260: val loss=0.0769, val recon=0.0386, val kl=0.7677\n",
      "0.9615809432028396\n",
      "epoch 280: loss=0.0558, recon=0.0186, kl=0.7439\n",
      "epoch 280: val loss=0.0757, val recon=0.0381, val kl=0.7516\n",
      "0.9620098630317082\n",
      "epoch 300: loss=0.0545, recon=0.0174, kl=0.7421\n",
      "epoch 300: val loss=0.0738, val recon=0.0367, val kl=0.7431\n",
      "0.9634240045021414\n",
      "epoch 320: loss=0.0541, recon=0.0179, kl=0.7243\n",
      "epoch 320: val loss=0.0718, val recon=0.0358, val kl=0.7197\n",
      "0.9643748611332215\n",
      "epoch 340: loss=0.0527, recon=0.0167, kl=0.7199\n",
      "epoch 340: val loss=0.0708, val recon=0.0348, val kl=0.7191\n",
      "0.9652473861342564\n",
      "epoch 360: loss=0.0519, recon=0.0170, kl=0.6980\n",
      "epoch 360: val loss=0.0703, val recon=0.0348, val kl=0.7112\n",
      "0.9652913083771903\n",
      "epoch 380: loss=0.0512, recon=0.0158, kl=0.7084\n",
      "epoch 380: val loss=0.0703, val recon=0.0352, val kl=0.7009\n",
      "0.9647857969408067\n",
      "epoch 400: loss=0.0499, recon=0.0152, kl=0.6928\n",
      "epoch 400: val loss=0.0676, val recon=0.0322, val kl=0.7072\n",
      "0.967874755327564\n",
      "epoch 420: loss=0.0488, recon=0.0150, kl=0.6760\n",
      "epoch 420: val loss=0.0684, val recon=0.0344, val kl=0.6812\n",
      "0.9657331486351779\n",
      "epoch 440: loss=0.0480, recon=0.0138, kl=0.6833\n",
      "epoch 440: val loss=0.0673, val recon=0.0330, val kl=0.6857\n",
      "0.9670444220865866\n",
      "epoch 460: loss=0.0487, recon=0.0147, kl=0.6799\n",
      "epoch 460: val loss=0.0664, val recon=0.0322, val kl=0.6834\n",
      "0.9679261515693031\n",
      "epoch 480: loss=0.0489, recon=0.0147, kl=0.6826\n",
      "epoch 480: val loss=0.0670, val recon=0.0332, val kl=0.6754\n",
      "0.9668513164470554\n",
      "epoch 500: loss=0.0478, recon=0.0146, kl=0.6638\n",
      "epoch 500: val loss=0.0662, val recon=0.0329, val kl=0.6669\n",
      "0.9671956409662756\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import torch\n",
    "# 예시 세팅\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "train_dataset = NpzRegressionDataset(X_train, y_train)\n",
    "val_dataset   = NpzRegressionDataset(X_val,   y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=512, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[-1]\n",
    "latent_dim = 64\n",
    "hidden_dim = 256\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vae = VAE(input_dim=input_dim, latent_dim=latent_dim, hidden_dim=hidden_dim).to(device)\n",
    "\n",
    "\n",
    "# train_data: (N, D) int 텐서라고 가정\n",
    "# train_data = torch.randint(low=0, high=1025, size=(1024, D))  # 예시용 dummy\n",
    "\n",
    "# 전처리용 mean/std 미리 계산\n",
    "# with torch.no_grad():\n",
    "#     _x_cont, mean, std = transform_schedule(X_train.to(device))\n",
    "\n",
    "beta = 0.05\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "# 간단한 에폭 루프 예시\n",
    "\n",
    "for epoch in range(501):\n",
    "    # 여기서는 그냥 전체를 한 번에 돌린다고 가정 (실제로는 DataLoader로 배치 쪼개기)\n",
    "    vae.train()\n",
    "    for x_batch in train_loader:\n",
    "        x_batch = x_batch[0].to(device)  # (N, D)\n",
    "    \n",
    "        # x_cont, _, _ = transform_schedule(x_batch, mean=mean, std=std)  # (N, 2D)\n",
    "\n",
    "        x_recon, mu, logvar, z = vae(x_batch)\n",
    "\n",
    "        loss, recon_loss, kl = vae_loss(x_recon, x_batch, mu, logvar, beta=beta)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    vae.eval()\n",
    "    for x_batch in val_loader:\n",
    "        x_batch = x_batch[0].to(device)\n",
    "        x_recon, mu, logvar, z = vae(x_batch)\n",
    "        val_loss, val_recon_loss, val_kl = vae_loss(x_recon, x_batch, mu, logvar, beta=beta)\n",
    "        val_r2 = r2_score(x_batch.detach().cpu().numpy(), x_recon.detach().cpu().numpy())\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"epoch {epoch}: loss={loss.item():.4f}, recon={recon_loss.item():.4f}, kl={kl.item():.4f}\")\n",
    "        print(f\"epoch {epoch}: val loss={val_loss.item():.4f}, val recon={val_recon_loss.item():.4f}, val kl={val_kl.item():.4f}\")\n",
    "        \n",
    "        print(val_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a08123a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6310053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAECostPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    VAE 기반 Cost Regression 모델\n",
    "    \n",
    "    구조:\n",
    "    - input → segment_encoder → segment_sum → VAE encoder → z → cost_predictor → cost\n",
    "    \n",
    "    특징:\n",
    "    - Pretrained VAE encoder를 finetune (작은 learning rate)\n",
    "    - Cost predictor는 더 큰 learning rate로 학습\n",
    "    - 전체 forward 경로가 완전히 미분 가능 (detach, stop_grad 없음)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, feature_dim=None, hidden_dim=256, latent_dim=64, \n",
    "                 predictor_hidden=256, predictor_layers=2, dropout=0.1, use_feature=False):\n",
    "        super(VAECostPredictor, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # ========== Cost Predictor (새로 학습) ==========\n",
    "        predictor_modules = []\n",
    "        current_dim = latent_dim\n",
    "        for i in range(predictor_layers):\n",
    "            predictor_modules.extend([\n",
    "                nn.Linear(current_dim, predictor_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout) if i < predictor_layers - 1 else nn.Identity(),\n",
    "            ])\n",
    "            current_dim = predictor_hidden\n",
    "        predictor_modules.append(nn.Linear(predictor_hidden, 1))\n",
    "        \n",
    "        self.cost_predictor = nn.Sequential(*predictor_modules)\n",
    "\n",
    "        self.use_feature = use_feature\n",
    "        if self.use_feature:\n",
    "            pass\n",
    "            self.feature_predictor = nn.Sequential(\n",
    "                nn.Linear(latent_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, feature_dim),  # feature_dim는 feature 차원\n",
    "            )\n",
    "        \n",
    "    \n",
    "    def encode(self, input_data):\n",
    "        \"\"\"\n",
    "        Full encoding path: features → z\n",
    "        완전히 미분 가능\n",
    "        \"\"\"\n",
    "                \n",
    "        # VAE Encoder\n",
    "        h = self.encoder(input_data)\n",
    "        \n",
    "        mean = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        \n",
    "        return mean, logvar\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        \"\"\"Reparameterization trick - 미분 가능\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "    \n",
    "    def predict_cost(self, z):\n",
    "        \"\"\"z → cost prediction - 완전히 미분 가능\"\"\"\n",
    "        return self.cost_predictor(z).squeeze(-1)\n",
    "    \n",
    "    def predict_feature(self, z):\n",
    "        return self.feature_predictor(z)\n",
    "    \n",
    "    def forward(self, input_data, use_mean=True):\n",
    "        \"\"\"\n",
    "        Forward pass: input → z → cost\n",
    "        \n",
    "        Args:\n",
    "            use_mean: True면 reparameterize 대신 mean 사용 (inference용)\n",
    "        \n",
    "        Returns:\n",
    "            cost_pred: 예측된 cost\n",
    "            mean: latent mean\n",
    "            logvar: latent log-variance\n",
    "            z: sampled/mean latent vector\n",
    "        \"\"\"\n",
    "        mean, logvar = self.encode(input_data)\n",
    "        \n",
    "        if use_mean:\n",
    "            z = mean  # Inference시 deterministic\n",
    "        else:\n",
    "            z = self.reparameterize(mean, logvar)  # Training시 stochastic\n",
    "        \n",
    "        cost_pred = self.predict_cost(z)\n",
    "        \n",
    "        return cost_pred, mean, logvar, z\n",
    "    \n",
    "    def get_encoder_params(self):\n",
    "        \"\"\"Encoder 파라미터 (작은 lr)\"\"\"\n",
    "        encoder_params = []\n",
    "        encoder_params.extend(self.encoder.parameters())\n",
    "        encoder_params.extend(self.fc_mu.parameters())\n",
    "        encoder_params.extend(self.fc_logvar.parameters())\n",
    "        return encoder_params\n",
    "    \n",
    "    def get_cost_predictor_params(self):\n",
    "        \"\"\"Predictor 파라미터 (큰 lr)\"\"\"\n",
    "        return self.cost_predictor.parameters()\n",
    "    \n",
    "    def get_feature_predictor_params(self):\n",
    "        \"\"\"Feature Predictor 파라미터\"\"\"\n",
    "        return self.feature_predictor.parameters()\n",
    "\n",
    "    def load_pretrained_encoder(self, checkpoint):\n",
    "        \"\"\"Pretrained VAE encoder 가중치 로드\"\"\"\n",
    "        \n",
    "\n",
    "        vae_state = checkpoint\n",
    "        \n",
    "        # 매칭되는 키만 로드\n",
    "        encoder_keys = ['encoder', 'fc_mu', 'fc_logvar']\n",
    "        own_state = self.state_dict()\n",
    "        \n",
    "        loaded_keys = []\n",
    "        for name, param in vae_state.items():\n",
    "            if any(name.startswith(k) for k in encoder_keys):\n",
    "                if name in own_state and own_state[name].shape == param.shape:\n",
    "                    own_state[name].copy_(param)\n",
    "                    loaded_keys.append(name)\n",
    "        \n",
    "        # print(f\"Loaded {len(loaded_keys)} parameters from pretrained VAE\")\n",
    "        # return loaded_keys\n",
    "\n",
    "    def _enable_dropout(self):\n",
    "        \"\"\"모든 Dropout 모듈을 train 모드로 강제 활성화\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.train()\n",
    "\n",
    "    def mc_predict(self, input_tensor, T=20):\n",
    "        \"\"\"\n",
    "        MC Dropout 기반 불확실성 추정\n",
    "        \n",
    "        Args:\n",
    "            input_tensor: 입력 텐서 (shape [N, input_dim])\n",
    "            T: MC 샘플 수\n",
    "        \n",
    "        Returns:\n",
    "            mean: epistemic 평균 cost (shape [N])\n",
    "            var: epistemic 분산 (shape [N])\n",
    "        \"\"\"\n",
    "\n",
    "        self.eval()  # 전체 모델을 eval 모드로\n",
    "        self._enable_dropout()  # Dropout만 train 모드로 활성화\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            \n",
    "            for _ in range(T):\n",
    "                # Encode\n",
    "                z, logvar = self.encode(input_tensor)\n",
    "                cost_pred = self.predict_cost(z)\n",
    "                predictions.append(cost_pred)\n",
    "            \n",
    "            predictions = torch.stack(predictions, dim=0)\n",
    "            \n",
    "            # epistemic mean & variance\n",
    "            mc_mean = predictions.mean(dim=0)\n",
    "            mc_var = predictions.var(dim=0)\n",
    "\n",
    "        return mc_mean, mc_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b584ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_loss_fn(cost_pred, cost_true, loss_type='mse'):\n",
    "    \"\"\"\n",
    "    기본 회귀 손실 (MSE 또는 MAE)\n",
    "    \"\"\"\n",
    "    if loss_type == 'mse':\n",
    "        return F.mse_loss(cost_pred, cost_true)\n",
    "    else:  # mae\n",
    "        return F.l1_loss(cost_pred, cost_true)\n",
    "\n",
    "\n",
    "def pair_loss_fn(cost_pred, cost_true, margin=0.1):\n",
    "    \"\"\"\n",
    "    Pairwise ranking loss: 실제 cost 순서를 예측이 유지하도록.\n",
    "    cost_true[i] < cost_true[j] 이면 cost_pred[i] < cost_pred[j] + margin\n",
    "    \"\"\"\n",
    "    batch_size = cost_pred.size(0)\n",
    "    if batch_size < 2:\n",
    "        return torch.tensor(0.0, device=cost_pred.device)\n",
    "    \n",
    "    # 모든 쌍에 대해 ranking loss 계산\n",
    "    idx = torch.arange(batch_size, device=cost_pred.device)\n",
    "    i_idx, j_idx = torch.meshgrid(idx, idx, indexing='ij')\n",
    "    mask = i_idx < j_idx  # upper triangular only\n",
    "    \n",
    "    pred_i = cost_pred[i_idx[mask]]\n",
    "    pred_j = cost_pred[j_idx[mask]]\n",
    "    true_i = cost_true[i_idx[mask]]\n",
    "    true_j = cost_true[j_idx[mask]]\n",
    "    \n",
    "    # label: 1 if true_i < true_j, -1 otherwise\n",
    "    labels = torch.sign(true_j - true_i).float()\n",
    "    \n",
    "    # Margin ranking loss\n",
    "    loss = F.margin_ranking_loss(pred_j.view(-1), pred_i.view(-1), labels.view(-1), margin=margin)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def smooth_loss_fn(model, z, noise_std=0.1):\n",
    "    \"\"\"\n",
    "    Smoothness loss: z에 작은 노이즈를 더했을 때 예측이 크게 변하지 않도록.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z_noisy = z + noise_std * torch.randn_like(z)\n",
    "    \n",
    "    cost_original = model.predict_cost(z)\n",
    "    cost_noisy = model.predict_cost(z_noisy)\n",
    "    \n",
    "    smooth_loss = F.mse_loss(cost_original, cost_noisy)\n",
    "    return smooth_loss\n",
    "\n",
    "\n",
    "def kld_loss_fn(mean, logvar):\n",
    "    \"\"\"\n",
    "    KL Divergence: q(z|x) || N(0, I)\n",
    "    \"\"\"\n",
    "    kld = -0.5 * torch.mean(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "    return kld\n",
    "\n",
    "def feature_loss_fn(use_feature, feature_pred, feature_true, coef=0.1):\n",
    "    \"\"\"\n",
    "    Feature 예측 손실 (MSE)\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if not use_feature:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "    return F.mse_loss(feature_pred, feature_true) * coef\n",
    "\n",
    "\n",
    "def compute_total_loss(model, cost_pred, mean, logvar, z, labels, feature, config, return_components=True):\n",
    "    \"\"\"\n",
    "    Total loss 계산 (Segment 기반 데이터용).\n",
    "    total_loss = reg_loss + λ_pair * pair_loss + γ * smooth_loss + β * kld_loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # Individual losses\n",
    "    reg = reg_loss_fn(cost_pred, labels, loss_type=config.get('loss_type', 'mse'))\n",
    "    pair = pair_loss_fn(cost_pred.view(-1), labels.view(-1), margin=config.get('margin', 0.1))\n",
    "    smooth = smooth_loss_fn(model, z, noise_std=config.get('noise_std', 0.1))\n",
    "    kld = kld_loss_fn(mean, logvar)\n",
    "    feature_loss = feature_loss_fn(model.use_feature, None, feature, coef=0)\n",
    "    \n",
    "    # Weighted sum\n",
    "    total = config['lambda_reg'] * reg + config['lambda_pair'] * pair + config['gamma'] * smooth + config['beta'] * kld + feature_loss\n",
    "    \n",
    "    if return_components:\n",
    "        return total, {\n",
    "            'reg_loss': reg.item(),\n",
    "            'pair_loss': pair.item(),\n",
    "            'smooth_loss': smooth.item(),\n",
    "            'kld_loss': kld.item(),\n",
    "            'feature_loss': feature_loss.item(),\n",
    "        }\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ef7144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_accuracy(cost_pred, labels, rng=np.random.default_rng(42)):\n",
    "    \"\"\"\n",
    "    cost_pred, labels: (B,) 텐서\n",
    "    \"\"\"\n",
    "    n_samples = min(2000, len(cost_pred))\n",
    "    sample_indices = rng.choice(len(cost_pred), n_samples, replace=False)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i + 1, n_samples):\n",
    "            idx_i = sample_indices[i]\n",
    "            idx_j = sample_indices[j]\n",
    "            pred_diff = cost_pred[idx_i] - cost_pred[idx_j]\n",
    "            true_diff = labels[idx_i] - labels[idx_j]\n",
    "            if (pred_diff * true_diff) > 0:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy\n",
    "\n",
    "def recall_at_k(pred, labels, k=1):\n",
    "    true_best_idx = torch.argmax(labels)\n",
    "    topk_pred_idx = torch.topk(pred, k=k, largest=True).indices\n",
    "\n",
    "    return int((topk_pred_idx == true_best_idx).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a03fd72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_select_indices(xgb_all_preds, train_indices, test_indices, topk_size, eps_greedy_size, rng):\n",
    "    \"\"\"\n",
    "    랜덤으로 2개, xgb 모델로 상위 62개 선택\n",
    "    \"\"\"\n",
    "    # 남은 인덱스 중에서 무작위로 random_select_size개 선택\n",
    "\n",
    "    remaining_indices = set(test_indices)\n",
    "\n",
    "    if topk_size + eps_greedy_size > test_indices.shape[0]:\n",
    "        remaining_indices.update(train_indices.tolist())\n",
    "        train_indices = np.array(list(remaining_indices), dtype=np.int64)\n",
    "        return train_indices, np.array([], dtype=np.int64)\n",
    "\n",
    "\n",
    "    top_indices, remaining_indices = select_topk_cost(xgb_all_preds, remaining_indices, topk_size)\n",
    "    random_indices, remaining_indices = random_select_indices(remaining_indices, eps_greedy_size, rng=rng)\n",
    "    test_indices = np.array(list(remaining_indices), dtype=np.int64)\n",
    "\n",
    "    selected_indices = np.concatenate([top_indices, random_indices])\n",
    "\n",
    "    train_indices = np.concatenate([train_indices, selected_indices])\n",
    "\n",
    "    return train_indices, test_indices\n",
    "\n",
    "\n",
    "\n",
    "def random_select_indices(remaining_indices, select_size, rng=np.random.default_rng(42)):\n",
    "    if select_size == 0:\n",
    "        return np.array([], dtype=np.int64), remaining_indices\n",
    "    \n",
    "    random_indices = rng.choice(list(remaining_indices), size=select_size, replace=False)\n",
    "\n",
    "    remaining_indices = util_update_remaining_indices(remaining_indices, random_indices)\n",
    "\n",
    "    return random_indices, remaining_indices\n",
    "\n",
    "\n",
    "\n",
    "def util_update_remaining_indices(remaining_indices, selected_indices):\n",
    "    \"\"\"\n",
    "    남은 인덱스 집합 업데이트\n",
    "    util_update_remaining_indices에서 selected_indices 제거\n",
    "    \"\"\"\n",
    "    selected_indices = set(selected_indices)\n",
    "    remaining_indices.difference_update(selected_indices)\n",
    "\n",
    "    return remaining_indices\n",
    "\n",
    "\n",
    "\n",
    "def util_select_topk(predictions, remaining_indices, num_select):\n",
    "    \"\"\"\n",
    "    예측값 기반 다음 측정할 샘플 선택\n",
    "    \n",
    "    Args:\n",
    "        predictions: 전체 예측값 리스트 ([N, ] 형태)\n",
    "        remaining_indices: 아직 측정되지 않은 인덱스 집합 (set)\n",
    "        num_select: 선택할 샘플 수\n",
    "    \n",
    "    Returns:\n",
    "        selected_indices: 선택된 샘플의 인덱스 numpy 배열\n",
    "        remaining_indices: 업데이트된 남은 인덱스 집합 (set)\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction = np.asarray(predictions)  # [N]\n",
    "\n",
    "    remaining_np = np.array(list(remaining_indices), dtype=np.int64)\n",
    "    remaining_pred = prediction[remaining_np]\n",
    "\n",
    "    k = min(num_select, len(remaining_np))\n",
    "\n",
    "    topk_local = np.argsort(remaining_pred)[-k:]\n",
    "    selected_indices = remaining_np[topk_local]\n",
    "\n",
    "    # remaining 업데이트\n",
    "    remaining_indices.difference_update(selected_indices.tolist())\n",
    "\n",
    "    return selected_indices, remaining_indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_topk_cost(cost_pred, remaining_indices, num_select):\n",
    "    \"\"\"\n",
    "    예측된 cost 기반 다음 측정할 샘플 선택\n",
    "    \n",
    "    Args:\n",
    "        model: VAECostPredictor 모델\n",
    "        input_data_scaled: 전체 input 리스트 ([N, input_dim] 형태)\n",
    "        remaining_indices: 아직 측정되지 않은 인덱스 집합 (set)\n",
    "        num_select: 선택할 샘플 수\n",
    "    \n",
    "    \"\"\"\n",
    "    if num_select == 0:\n",
    "        return np.array([], dtype=np.int64), remaining_indices\n",
    "\n",
    "    if isinstance(cost_pred, torch.Tensor):\n",
    "        cost_pred = cost_pred.detach().cpu().numpy()  # [N]\n",
    "\n",
    "    topk_cost_indices, remaining_indices = util_select_topk(cost_pred, remaining_indices, num_select)\n",
    "    \n",
    "\n",
    "    return topk_cost_indices, remaining_indices\n",
    "\n",
    "\n",
    "def select_topk_z_grad(z, cost_pred, remaining_indices, num_select):\n",
    "    \"\"\"\n",
    "    z에 대한 cost gradient 기반 다음 측정할 샘플 선택\n",
    "    \n",
    "    Args:\n",
    "        model: VAECostPredictor 모델\n",
    "        input_tensor: 전체 input numpy 배열 ([N, input_dim] 형태)\n",
    "        remaining_indices: 아직 측정되지 않은 인덱스 집합 (set)\n",
    "        num_select: 선택할 샘플 수\n",
    "    \n",
    "    \"\"\"\n",
    "    if num_select == 0:\n",
    "        return np.array([], dtype=np.int64), remaining_indices\n",
    "\n",
    "    candidate_indices = np.array(list(remaining_indices), dtype=np.int64)\n",
    "\n",
    "    # z-gradient 계산\n",
    "    z_grad = torch.autograd.grad(\n",
    "        outputs=cost_pred.sum(),\n",
    "        inputs=z,\n",
    "        retain_graph=False,\n",
    "        create_graph=False\n",
    "    )[0]  # [N, latent_dim]\n",
    "\n",
    "    z_grad_norm = torch.norm(z_grad, dim=1).detach().cpu().numpy()  # [N]\n",
    "\n",
    "    # 후보 중 grad-norm top-k\n",
    "    candidate_grad = z_grad_norm[candidate_indices]\n",
    "    k = min(num_select, len(candidate_indices))\n",
    "\n",
    "    topk_local = np.argsort(candidate_grad)[-k:]\n",
    "    selected_indices = candidate_indices[topk_local]\n",
    "\n",
    "    # remaining 업데이트\n",
    "    remaining_indices = set(remaining_indices)\n",
    "    remaining_indices.difference_update(selected_indices.tolist())\n",
    "\n",
    "    return selected_indices, remaining_indices\n",
    "\n",
    "\n",
    "def select_topk_uncertainty(model, input_tensor, remaining_indices, num_select, T_mc=10):\n",
    "    \"\"\"\n",
    "    MC Dropout 기반 불확실성 추정으로 다음 측정할 샘플 선택\n",
    "    \n",
    "    Args:\n",
    "        model: VAECostPredictor 모델\n",
    "        input_data_scaled: 전체 input 리스트 ([N, input_dim] 형태)\n",
    "        remaining_indices: 아직 측정되지 않은 인덱스 집합 (set)\n",
    "        num_select: 선택할 샘플 수\n",
    "        T_mc: MC Dropout 샘플 수\n",
    "    \n",
    "    Returns:\n",
    "        selected_indices: 선택된 샘플의 인덱스 리스트\n",
    "    \"\"\"\n",
    "    if num_select == 0:\n",
    "        return np.array([], dtype=np.int64), remaining_indices\n",
    "\n",
    "\n",
    "    was_training = model.training\n",
    "    model.train()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, mc_var = model.mc_predict(input_tensor, T=T_mc)\n",
    "\n",
    "    if not was_training:\n",
    "        model.eval()  # 원복\n",
    "\n",
    "    var_np = mc_var.detach().cpu().numpy()  # [N]\n",
    "\n",
    "    topk_uncertainty_indices, remaining_indices = util_select_topk(var_np, remaining_indices, num_select)\n",
    "\n",
    "    return topk_uncertainty_indices, remaining_indices\n",
    "\n",
    "\n",
    "def select_topk_latent_diversity(z, candidate_indices, used_indices, select_n_div, chunk_size=1024, eps=1e-12):\n",
    "    \"\"\"\n",
    "    먼저 candidates 320개를 뽑았다고 치자.\n",
    "    이후 앞에서 topk_cost, topk_z_grad로 40개 정도를 뽑았다고 치자.\n",
    "    latent diversity는 40개 + used_indices로부터 가장 멀리 떨어진 24개를 280개에서 뽑는다.\n",
    "\n",
    "    z를 L2 정규화한 뒤, k-center greedy(farthest-first)로 diversity 선택.\n",
    "    초기 센터는 used_indices (이미 측정된 점들).\n",
    "    매 스텝마다 \"센터 집합까지의 최소거리\"가 최대인 candidate를 하나씩 추가.\n",
    "    \n",
    "    Args:\n",
    "        z: torch.Tensor [N, latent_dim]\n",
    "        candidate_indices: set(int)\n",
    "        used_indices: set(int)\n",
    "        select_n_div: int\n",
    "        chunk_size: int\n",
    "    Returns:\n",
    "        diverse_indices: np.ndarray (int64)\n",
    "        candidate_indices: set (선택된 인덱스 제거된 상태)\n",
    "    \"\"\"\n",
    "    if select_n_div == 0 or len(candidate_indices) == 0:\n",
    "        return np.array([], dtype=np.int64), candidate_indices\n",
    "\n",
    "\n",
    "    device = z.device\n",
    "\n",
    "    # 1) L2 normalize z  (각 벡터를 단위벡터로)\n",
    "    with torch.no_grad():\n",
    "        z_norm = z / (z.norm(dim=1, keepdim=True) + eps)\n",
    "\n",
    "    cand = np.array(list(candidate_indices), dtype=np.int64)\n",
    "    k = min(select_n_div, len(cand))\n",
    "\n",
    "    cand_t = torch.from_numpy(cand).to(device=device)\n",
    "    z_cand = z_norm[cand_t]  # [M, D], M=len(cand)\n",
    "\n",
    "    # 초기 센터: used_indices (비어있을 수도 있음)\n",
    "    used = np.array(list(used_indices), dtype=np.int64)\n",
    "    selected = []\n",
    "\n",
    "    # 2) 각 candidate의 \"현재 센터 집합까지 최소거리\" 벡터 init\n",
    "    #    used가 비어있으면 +inf로 시작해서 임의 첫 점을 뽑게(가장 큰 값) 만들기\n",
    "    if len(used) > 0:\n",
    "        used_t = torch.from_numpy(used).to(device=device)\n",
    "        z_used = z_norm[used_t]  # [U, D]\n",
    "\n",
    "        # min_dists[j] = min_{u in used} ||z_cand[j] - z_used[u]||\n",
    "        min_dists = torch.empty(len(cand), device=device, dtype=torch.float32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for s in range(0, len(cand), chunk_size):\n",
    "                e = min(s + chunk_size, len(cand))\n",
    "                d = torch.cdist(z_cand[s:e], z_used, p=2)  # [B, U]\n",
    "                min_dists[s:e] = d.min(dim=1).values\n",
    "    else:\n",
    "        # 센터가 없으면 모두 동일하게 시작 → 첫 선택은 아래 argmax가 0번째로 갈 수 있음\n",
    "        # 다양성 목적이면 랜덤/최대 norm 등으로 첫 점을 정할 수도 있지만,\n",
    "        # 여기서는 \"가장 큰 min_dists\"를 위해 +inf로 둔다.\n",
    "        min_dists = torch.full((len(cand),), float(\"inf\"), device=device, dtype=torch.float32)\n",
    "\n",
    "    # 3) k-center greedy 반복\n",
    "    #    매번 argmax(min_dists) 하나 선택 -> 그 점을 센터에 추가 -> min_dists 갱신\n",
    "    with torch.no_grad():\n",
    "        for _ in range(k):\n",
    "            j = torch.argmax(min_dists).item()     # cand 내부 위치\n",
    "            sel_idx = cand[j]                      # 원본 인덱스\n",
    "            selected.append(sel_idx)\n",
    "\n",
    "            # 선택된 점을 \"센터\"로 추가: 모든 candidate에 대해 dist_to_new_center 계산 후 min 갱신\n",
    "            new_center = z_cand[j:j+1]  # [1, D]\n",
    "\n",
    "            # 방금 뽑은 점은 다시 뽑히지 않게 min_dists를 -inf로\n",
    "            min_dists[j] = -float(\"inf\")\n",
    "\n",
    "            # 나머지 후보들의 min 거리 업데이트\n",
    "            for s in range(0, len(cand), chunk_size):\n",
    "                e = min(s + chunk_size, len(cand))\n",
    "                d_new = torch.cdist(z_cand[s:e], new_center, p=2).squeeze(1)  # [B]\n",
    "                min_dists[s:e] = torch.minimum(min_dists[s:e], d_new)\n",
    "\n",
    "    diverse_indices = np.array(selected, dtype=np.int64)\n",
    "\n",
    "    candidate_indices = set(candidate_indices)\n",
    "    candidate_indices.difference_update(diverse_indices.tolist())\n",
    "\n",
    "    return diverse_indices, candidate_indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_programs(model, input_data_scaled, used_indices, remaining_indices, num_select=64, T_mc=10, uncertainty_topk=128,\n",
    "                    w_cost=0.5, w_unc=0.3, w_div=0.2, grad_num=2, rand_num=0, rng=np.random.default_rng(42), device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), topk_factor=5):\n",
    "    \"\"\"\n",
    "    Active learning 기반 다음 측정할 샘플 선택\n",
    "    \n",
    "    Args:\n",
    "        model: VAECostPredictor 모델\n",
    "        input_data_scaled: 전체 input 리스트 ([N, input_dim] 형태)\n",
    "        used_indices: 이미 측정된 인덱스 집합(set)\n",
    "        remaining_indices: 아직 측정되지 않은 인덱스 집합 (set)\n",
    "        num_select: 선택할 샘플 수\n",
    "        T_mc: MC Dropout 샘플 수\n",
    "        w_cost: 예측값이 큰 샘플 가중치\n",
    "        w_unc: epistemic 불확실성이 높은 샘플 가중치\n",
    "        w_div: latent 다양성이 높은 샘플 가중치\n",
    "        grad_num: z에 대한 cost의 gradient가 큰 샘플 수\n",
    "        rand_num: 무작위로 선택할 샘플 수\n",
    "    \n",
    "    Returns:\n",
    "        selected_indices: 선택된 샘플의 인덱스 리스트\n",
    "    \"\"\"\n",
    "\n",
    "    # 합쳐서 64개 선택\n",
    "    total = num_select\n",
    "    budget = total - grad_num - rand_num\n",
    "\n",
    "    # 랜덤 선택만 할 경우\n",
    "    if num_select == 0 and rand_num > 0:\n",
    "        rand_indices, remaining_indices = random_select_indices(remaining_indices, rand_num, rng=rng)\n",
    "        return rand_indices, remaining_indices\n",
    "    \n",
    "\n",
    "    select_n_cost = int(budget * w_cost)\n",
    "    select_n_unc  = int(budget * w_unc)\n",
    "    select_n_div  = int(budget * w_div)\n",
    "    select_n_grad = grad_num\n",
    "    s = select_n_cost + select_n_unc + select_n_div\n",
    "    if s < budget:\n",
    "        select_n_cost += budget - s\n",
    "\n",
    "    input_tensor = torch.tensor(input_data_scaled, dtype=torch.float32, device=device)\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z, _ = model.encode(input_tensor)\n",
    "    z = z.detach().requires_grad_(True)\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    cost_pred = model.predict_cost(z)\n",
    "    cost_pred = cost_pred.view(-1)\n",
    "    cost_np = cost_pred.detach().cpu().numpy()\n",
    "\n",
    "    remaining_np = np.array(list(remaining_indices), dtype=np.int64)\n",
    "    remaining_cost = cost_np[remaining_np]\n",
    "\n",
    "    k_pref = min(len(remaining_np), total * topk_factor)\n",
    "    top_local = np.argsort(remaining_cost)[-k_pref:]\n",
    "    candidate_indices = set(remaining_np[top_local].tolist())  # 작업용 remaining\n",
    "\n",
    "    # print(f\"Candidate pool size: {len(candidate_indices)}\")\n",
    "\n",
    "\n",
    "    # 중복 방지용\n",
    "    currently_used = set()\n",
    "    topk_cost_indices, candidate_indices = select_topk_cost(cost_pred, candidate_indices, select_n_cost)\n",
    "    currently_used.update(topk_cost_indices.tolist())\n",
    "    z_grad_indices, candidate_indices = select_topk_z_grad(z, cost_pred, candidate_indices, select_n_grad)\n",
    "    currently_used.update(z_grad_indices.tolist())\n",
    "\n",
    "    # if len(used_indices) / len(input_data_scaled) >= 0.1:\n",
    "    if len(used_indices) >= uncertainty_topk:\n",
    "        uncertainty_indices, candidate_indices = select_topk_uncertainty(model, input_tensor, candidate_indices, select_n_unc, T_mc=T_mc)\n",
    "    else:\n",
    "        pool_for_uncertainty = set(remaining_indices)\n",
    "        pool_for_uncertainty.difference_update(currently_used)\n",
    "        uncertainty_indices, _ = select_topk_uncertainty(model, input_tensor, pool_for_uncertainty, select_n_unc, T_mc=T_mc)\n",
    "        candidate_indices.difference_update(uncertainty_indices.tolist())\n",
    "\n",
    "\n",
    "    currently_used.update(uncertainty_indices.tolist())\n",
    "    used_local = set(used_indices)\n",
    "    used_local.update(currently_used)\n",
    "\n",
    "    diverse_indices, _ = select_topk_latent_diversity(z, candidate_indices, used_local, select_n_div)\n",
    "    currently_used.update(diverse_indices.tolist())\n",
    "\n",
    "\n",
    "    remaining_indices.difference_update(currently_used)\n",
    "\n",
    "\n",
    "    rand_indices, remaining_indices = random_select_indices(remaining_indices, rand_num, rng=rng)\n",
    "    currently_used.update(rand_indices.tolist())\n",
    "\n",
    "    \n",
    "\n",
    "    all_selected_indices = np.array(sorted(currently_used), dtype=np.int64)\n",
    "\n",
    "\n",
    "\n",
    "    return all_selected_indices, remaining_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3fe4fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vae_reg_dataloaders(input_data_scaled, costs, used_indices, remaining_indices):\n",
    "\n",
    "    \n",
    "    train_indices = np.array(list(used_indices), dtype=np.int64)\n",
    "    val_indices = np.array(list(remaining_indices), dtype=np.int64)\n",
    "\n",
    "    X_train = input_data_scaled[train_indices]\n",
    "    X_val = input_data_scaled[val_indices]\n",
    "    y_train = costs[train_indices]\n",
    "    y_val = costs[val_indices]\n",
    "\n",
    "    train_dataset = NpzRegressionDataset(X_train, y_train)\n",
    "    val_dataset   = NpzRegressionDataset(X_val,   y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=False)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=512, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "    y_mean = y_train.mean()\n",
    "    y_std = y_train.std() + 1e-8  # 0 나누기 방지용 작은 값 추가\n",
    "    print(f\"y_train mean: {y_mean}, std: {y_std}\")\n",
    "\n",
    "    \n",
    "    return train_loader, val_loader, y_mean, y_std\n",
    "\n",
    "\n",
    "def make_xgb_datasets(inputs, results):\n",
    "    f_inputs = []\n",
    "    f_results = []\n",
    "    r_costs = []\n",
    "    for inp, res in zip(inputs, results):\n",
    "        cost = np.mean([c.value for c in res.costs])\n",
    "        if cost < 1e10:\n",
    "            f_inputs.append(inp)\n",
    "            f_results.append(res)\n",
    "            r_costs.append(cost)\n",
    "    r_costs = np.array(r_costs, dtype=np.float32)\n",
    "    \n",
    "    dataset = auto_scheduler.dataset.Dataset()\n",
    "    dataset.update_from_measure_pairs(f_inputs, f_results)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def split_xgb_datasets(dataset, train_indices, test_indices):\n",
    "\n",
    "    raw_features = list(dataset.features.values())[0]\n",
    "    raw_throughputs = list(dataset.throughputs.values())[0]\n",
    "\n",
    "    \n",
    "    train_set, test_set = dataset.random_split_within_task(train_set_ratio=0, \n",
    "                                                        train_idxs=train_indices.tolist(), \n",
    "                                                        test_idxs=test_indices.tolist())\n",
    "    return train_set, test_set, raw_throughputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46087ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train mean: 6.446558870745085, std: 1.4566711846140028\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = NpzRegressionDataset(X_train, y_train)\n",
    "val_dataset   = NpzRegressionDataset(X_val,   y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "input_data = np.log(json_diffs[\"diff_values\"]+1e-8)\n",
    "costs = -np.log(json_diffs[\"cost\"])\n",
    "\n",
    "train_size = 64\n",
    "\n",
    "np.random.seed(3000)\n",
    "random_indices = np.random.permutation(len(input_data))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "input_data_scaled = scaler.fit_transform(input_data)\n",
    "costs_scaled = (costs - costs.mean()) / (costs.std() + 1e-8)\n",
    "\n",
    "X_train = input_data_scaled[random_indices[:train_size]]\n",
    "X_val = input_data_scaled[random_indices[train_size:]]\n",
    "y_train = costs[random_indices[:train_size]]\n",
    "y_val = costs[random_indices[train_size:]]\n",
    "\n",
    "train_dataset = NpzRegressionDataset(X_train, y_train)\n",
    "val_dataset   = NpzRegressionDataset(X_val,   y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=False)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=512, shuffle=False)\n",
    "\n",
    "y_mean = y_train.mean()\n",
    "y_std = y_train.std() + 1e-8  # 0 나누기 방지용 작은 값 추가\n",
    "print(f\"y_train mean: {y_mean}, std: {y_std}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac08a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def make_vae_reg_model(vae, hyperparameter, input_dim, latent_dim, hidden_dim, y_std, verbose=True):\n",
    "\n",
    "\n",
    "    cnt = 0\n",
    "    for vals in itertools.product(*hyperparameter.values()):\n",
    "        (lambda_reg, lambda_pair, margin_scale, gamma, beta, noise_std, \n",
    "        encoder_lr, feature_predictor_lr, cost_predictor_lr,  epochs) = vals\n",
    "        cnt += 1\n",
    "        if verbose:\n",
    "            print(f\"Experiment {cnt}/{len(list(itertools.product(*hyperparameter.values())))}\")\n",
    "            print(f\"lambda_reg={lambda_reg}, lambda_pair={lambda_pair}, margin_scale={margin_scale}, \\\n",
    "              gamma={gamma}, beta={beta}, noise_std={noise_std}\\nencoder_lr={encoder_lr}, cost_predictor_lr={cost_predictor_lr}, epochs={epochs}\")\n",
    "        config = {\n",
    "                    'encoder_lr': encoder_lr,\n",
    "                    'feature_predictor_lr': feature_predictor_lr,\n",
    "                    'cost_predictor_lr': cost_predictor_lr,\n",
    "                    'lambda_reg' : lambda_reg,\n",
    "                    'lambda_pair': lambda_pair,\n",
    "                    'gamma': gamma,\n",
    "                    'beta': beta,\n",
    "                    'margin': margin_scale * y_std,\n",
    "                    'noise_std': noise_std,\n",
    "                    'loss_type': 'mse',\n",
    "                    'epochs': epochs,\n",
    "                }\n",
    "\n",
    "        vae_cost_model = VAECostPredictor(input_dim=input_dim, \n",
    "                                    latent_dim=latent_dim, \n",
    "                                    hidden_dim=hidden_dim, \n",
    "                                    predictor_layers=2,\n",
    "                                    dropout=0.1, use_feature=False).to(device)\n",
    "        vae_cost_model.load_pretrained_encoder(vae.state_dict())\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': vae_cost_model.get_encoder_params(), 'lr': config['encoder_lr']},\n",
    "            {'params': vae_cost_model.get_cost_predictor_params(), 'lr': config['cost_predictor_lr']}\n",
    "        ], weight_decay=1e-5)\n",
    "    return vae_cost_model, optimizer, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47e0fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regression(vae_cost_model, optimizer, train_loader, val_loader, input_data_scaled, costs, config, top_k=10, use_rank=True):\n",
    "\n",
    "    print(\"Train size :\", len(train_loader.dataset))\n",
    "\n",
    "    # all_reg_results = []\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    for epoch in range(1, config['epochs']+1):\n",
    "        vae_cost_model.train()\n",
    "        for x_batch, labels in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            labels = labels.to(device).squeeze(-1)\n",
    "            \n",
    "        \n",
    "            cost_pred, mean, logvar, z = vae_cost_model(x_batch, use_mean=True)\n",
    "\n",
    "            train_loss, train_components = compute_total_loss(vae_cost_model, \n",
    "                                                    cost_pred, mean, logvar, z, labels, None, config)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(vae_cost_model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "        \n",
    "\n",
    "        if epoch % config['epochs'] == 0:\n",
    "            vae_cost_model.eval()\n",
    "            with torch.no_grad():\n",
    "                all_preds = []\n",
    "                all_labels = []\n",
    "                for x_batch, labels in val_loader:\n",
    "                    x_batch = x_batch.to(device)\n",
    "                    labels = labels.to(device).squeeze(-1)\n",
    "\n",
    "                    cost_pred, mean, logvar, z = vae_cost_model(x_batch, use_mean=True)\n",
    "                    all_preds.append(cost_pred)\n",
    "                    all_labels.append(labels)\n",
    "\n",
    "                    val_loss, val_components = compute_total_loss(vae_cost_model, cost_pred, mean, logvar, z, labels, None, config)\n",
    "                val_reg_r2 = r2_score(torch.cat(all_labels).detach().cpu().numpy(), torch.cat(all_preds).detach().cpu().numpy())\n",
    "                val_reg_r2 = round(val_reg_r2, 4)\n",
    "                \n",
    "                print(f\"Train loss epoch {epoch} : reg={train_components['reg_loss']: .4f} rank={train_components['pair_loss']: .4f} kl={train_components['kld_loss']: .4f}\")\n",
    "                print(f\"Val loss epoch {epoch}: reg={val_components['reg_loss']: .4f} rank={val_components['pair_loss']: .4f} kl={val_components['kld_loss']: .4f}\")\n",
    "                \n",
    "                print(f\"Regression R2 : {val_reg_r2:.4f}, \")\n",
    "        \n",
    "        # rank r2 계산\n",
    "        vae_cost_model.eval()\n",
    "        with torch.no_grad():\n",
    "            if epoch % config['epochs'] == 0:\n",
    "                input_data_tensor = torch.from_numpy(input_data_scaled).float().to(device)\n",
    "                all_preds = vae_cost_model(input_data_tensor, use_mean=True)[0].detach().cpu().numpy()\n",
    "                if use_rank:\n",
    "                    val_rank_r2 = pair_accuracy(all_preds, costs)\n",
    "                    val_rank_r2 = round(val_rank_r2, 4)\n",
    "                    print(f\"Rank R2 : {val_rank_r2:.4f}\")\n",
    "                else:\n",
    "                    val_rank_r2 = None\n",
    "                recall_top_k = recall_at_k(torch.tensor(all_preds), torch.from_numpy(costs), k=top_k)\n",
    "                \n",
    "                print(f\"Recall@{top_k} : {recall_top_k}\")\n",
    "                if recall_top_k:\n",
    "                    break_signal = True\n",
    "                else:\n",
    "                    break_signal = False\n",
    "\n",
    "    # print(\"=============================================\")\n",
    "    # all_reg_results.append({\n",
    "    #     \"lambda_reg\": lambda_reg,\n",
    "    #     \"lambda_pair\": lambda_pair,\n",
    "    #     \"margin_scale\": margin_scale,\n",
    "    #     \"gamma\": gamma,\n",
    "    #     \"beta\": beta,\n",
    "    #     \"noise_std\": noise_std,\n",
    "    #     \"encoder_lr\": encoder_lr,\n",
    "    #     \"feature_predictor_lr\": feature_predictor_lr,\n",
    "    #     \"cost_predictor_lr\": cost_predictor_lr,\n",
    "    #     \"seed\": seed,\n",
    "    #     \"reg_r2\": val_reg_r2,\n",
    "    #     \"rank_r2\": val_rank_r2,\n",
    "    #     \"recall@64\": recall_top_k\n",
    "    # })\n",
    "    return vae_cost_model, break_signal, val_reg_r2, val_rank_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbf8d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weight_grid(step=0.1):\n",
    "    m = int(round(1.0 / step))  # step=0.1 -> 10\n",
    "    weights = []\n",
    "    for i in range(m + 1):\n",
    "        for j in range(m + 1):\n",
    "            k = m - i - j\n",
    "            if k < 0:\n",
    "                continue\n",
    "            weights.append((i/m, j/m, k/m))\n",
    "    return weights\n",
    "weights = generate_weight_grid(step=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18ffb652",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_weights = []\n",
    "for w in weights:\n",
    "    w_cost, w_unc, w_div = w\n",
    "    if w_cost < 0.3:\n",
    "        continue\n",
    "    # if w_unc == 0.0 and w_cost > 0.0 and w_div > 0.0:\n",
    "    #     f_weights.append(w)\n",
    "    #     continue\n",
    "    # if w_div == 0.0 and w_cost > 0.0 and w_unc > 0.0:\n",
    "    #     f_weights.append(w)\n",
    "        # continue\n",
    "    f_weights.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26db8d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## 실험 1/10 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2000\n",
      "초기 랜덤 선택 샘플 인덱스: [   7   38   92  100  212  218  234  360  385  404  522  583  593  606\n",
      "  657  689  730  800  834  878  891  900  918  919 1039 1128 1149 1260\n",
      " 1400 1466 1543 1573 1593 1601 1695 1704 1707 1716 1726 1808 1829 1851\n",
      " 1859 1956 1969 1975 2061 2078 2093 2206 2234 2291 2397 2544 2675 2991\n",
      " 3011 3060 3076 3183 3282 3294 3387 3433]\n",
      "=============== 측정 Phase 1 ================\n",
      "y_train mean: 6.2098153523581425, std: 1.4098141953314174\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.1155 rank= 0.0159 kl= 0.1053\n",
      "Val loss epoch 1000: reg= 2.4471 rank= 0.4958 kl= 0.1300\n",
      "Regression R2 : 0.0257, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "y_train mean: 6.997604687688236, std: 1.3824993344972187\n",
      "Train size : 128\n",
      "Train loss epoch 1000 : reg= 1.4346 rank= 0.0165 kl= 0.1101\n",
      "Val loss epoch 1000: reg= 1.9596 rank= 0.3386 kl= 0.1332\n",
      "Regression R2 : 0.0527, \n",
      "Recall@1 : 1\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 128\n",
      "총 측정 시간: 6.89 초\n",
      "=============================================\n",
      "########## 실험 2/10 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2001\n",
      "초기 랜덤 선택 샘플 인덱스: [  24   69   75  112  201  344  366  382  390  407  537  558  646  662\n",
      "  691  693  751  752  757  827  877  949 1024 1118 1122 1151 1367 1384\n",
      " 1441 1449 1473 1485 1503 1518 1546 1618 1624 1634 1679 1694 1762 1830\n",
      " 1915 1918 1931 1959 1964 2030 2048 2161 2342 2464 2516 2614 2674 2895\n",
      " 2904 2922 2938 2988 3210 3279 3384 3408]\n",
      "=============== 측정 Phase 1 ================\n",
      "y_train mean: 6.563847660988456, std: 1.1661686580184756\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.0617 rank= 0.0131 kl= 0.1041\n",
      "Val loss epoch 1000: reg= 2.3121 rank= 0.3536 kl= 0.1284\n",
      "Regression R2 : -0.0138, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "y_train mean: 7.108189332096317, std: 1.0755812131649702\n",
      "Train size : 128\n",
      "Train loss epoch 1000 : reg= 1.0333 rank= 0.0131 kl= 0.1111\n",
      "Val loss epoch 1000: reg= 1.5281 rank= 0.2891 kl= 0.1313\n",
      "Regression R2 : 0.2743, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "y_train mean: 7.252180395772484, std: 1.0735632264887045\n",
      "Train size : 192\n",
      "Train loss epoch 1000 : reg= 1.0906 rank= 0.0131 kl= 0.1152\n",
      "Val loss epoch 1000: reg= 1.9244 rank= 0.3410 kl= 0.1389\n",
      "Regression R2 : 0.2653, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 192\n",
      "총 측정 시간: 10.43 초\n",
      "=============================================\n",
      "########## 실험 3/10 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2002\n",
      "초기 랜덤 선택 샘플 인덱스: [  13  125  154  273  313  363  369  376  432  455  470  489  515  519\n",
      "  586  659  676  677  771  798  813  848  854  861 1085 1110 1213 1284\n",
      " 1402 1424 1524 1553 1562 1639 1664 1743 1763 1778 1882 1970 1982 2019\n",
      " 2071 2111 2241 2355 2361 2636 2843 2874 2884 3023 3032 3064 3071 3110\n",
      " 3146 3148 3195 3212 3264 3270 3295 3333]\n",
      "=============== 측정 Phase 1 ================\n",
      "y_train mean: 6.232170016915225, std: 1.5124953147499975\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.4493 rank= 0.0161 kl= 0.1121\n",
      "Val loss epoch 1000: reg= 2.2230 rank= 0.4722 kl= 0.1267\n",
      "Regression R2 : 0.0228, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 64\n",
      "총 측정 시간: 3.32 초\n",
      "=============================================\n",
      "########## 실험 4/10 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2003\n",
      "초기 랜덤 선택 샘플 인덱스: [  18   52   64  135  173  255  282  367  435  484  581  662  695  813\n",
      "  891  907  924  972 1002 1034 1083 1095 1177 1203 1238 1328 1332 1444\n",
      " 1501 1588 1717 1792 1895 1911 1940 1976 2042 2061 2218 2228 2252 2270\n",
      " 2276 2287 2294 2430 2485 2708 2709 2721 2770 2940 2956 2993 2998 2999\n",
      " 3150 3162 3168 3170 3223 3349 3351 3402]\n",
      "=============== 측정 Phase 1 ================\n",
      "y_train mean: 6.474529812728601, std: 1.4394890619546665\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.2964 rank= 0.0152 kl= 0.1017\n",
      "Val loss epoch 1000: reg= 2.3193 rank= 0.4590 kl= 0.1265\n",
      "Regression R2 : -0.2380, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "y_train mean: 7.064414918736851, std: 1.2865661865311284\n",
      "Train size : 128\n",
      "Train loss epoch 1000 : reg= 1.3786 rank= 0.0159 kl= 0.1075\n",
      "Val loss epoch 1000: reg= 1.7882 rank= 0.3205 kl= 0.1369\n",
      "Regression R2 : 0.1076, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 128\n",
      "총 측정 시간: 7.00 초\n",
      "=============================================\n",
      "########## 실험 5/10 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2004\n",
      "초기 랜덤 선택 샘플 인덱스: [  10  103  105  107  179  233  315  461  465  503  572  634  990 1006\n",
      " 1015 1051 1063 1092 1197 1203 1210 1292 1302 1323 1367 1440 1450 1465\n",
      " 1562 1682 1828 1871 1872 1913 1923 1981 2015 2084 2114 2119 2126 2177\n",
      " 2258 2311 2360 2408 2413 2415 2422 2433 2446 2582 2588 2741 2790 2833\n",
      " 2857 2886 3000 3120 3126 3171 3260 3278]\n",
      "=============== 측정 Phase 1 ================\n",
      "y_train mean: 6.546234324828204, std: 1.2692278471547163\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.0344 rank= 0.0130 kl= 0.0993\n",
      "Val loss epoch 1000: reg= 2.3232 rank= 0.4953 kl= 0.1223\n",
      "Regression R2 : 0.0759, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "y_train mean: 7.085575155199846, std: 1.2063701979340682\n",
      "Train size : 128\n",
      "Train loss epoch 1000 : reg= 1.1533 rank= 0.0143 kl= 0.1136\n",
      "Val loss epoch 1000: reg= 1.5114 rank= 0.2779 kl= 0.1348\n",
      "Regression R2 : 0.1949, \n",
      "Recall@1 : 1\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 128\n",
      "총 측정 시간: 7.13 초\n",
      "=============================================\n",
      "########## 실험 6/10 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2005\n",
      "초기 랜덤 선택 샘플 인덱스: [  11   30   57  100  102  334  390  449  526  626  649  663  667  683\n",
      "  714  729  777  892  910 1057 1088 1181 1212 1213 1288 1317 1340 1366\n",
      " 1596 1647 1729 1731 1894 1931 1950 2000 2018 2042 2101 2132 2176 2224\n",
      " 2277 2300 2333 2347 2441 2456 2474 2490 2538 2539 2586 2730 2742 2764\n",
      " 2913 3057 3079 3105 3212 3249 3260 3305]\n",
      "=============== 측정 Phase 1 ================\n",
      "y_train mean: 6.154060347025649, std: 1.3585720333086648\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.2549 rank= 0.0144 kl= 0.1021\n",
      "Val loss epoch 1000: reg= 2.4575 rank= 0.5172 kl= 0.1273\n",
      "Regression R2 : -0.2091, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "y_train mean: 6.754638053015155, std: 1.340165413028323\n",
      "Train size : 128\n",
      "Train loss epoch 1000 : reg= 1.3063 rank= 0.0156 kl= 0.1022\n",
      "Val loss epoch 1000: reg= 1.9719 rank= 0.3312 kl= 0.1331\n",
      "Regression R2 : -0.0694, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "y_train mean: 7.0772254578897345, std: 1.2580627179648811\n",
      "Train size : 192\n",
      "Train loss epoch 1000 : reg= 1.3256 rank= 0.0160 kl= 0.1055\n",
      "Val loss epoch 1000: reg= 1.9785 rank= 0.2856 kl= 0.1428\n",
      "Regression R2 : -0.1084, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 192\n",
      "총 측정 시간: 10.56 초\n",
      "=============================================\n",
      "########## 실험 7/10 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2006\n",
      "초기 랜덤 선택 샘플 인덱스: [  16   43   72  188  196  197  256  324  428  496  501  519  649  677\n",
      "  750 1006 1030 1032 1056 1190 1220 1228 1316 1398 1426 1457 1549 1554\n",
      " 1619 1707 1902 1903 2000 2027 2094 2164 2257 2266 2275 2306 2325 2369\n",
      " 2494 2624 2626 2686 2789 2817 2824 2924 2932 2951 2962 2966 2967 3003\n",
      " 3062 3095 3099 3176 3235 3274 3305 3392]\n",
      "=============== 측정 Phase 1 ================\n",
      "y_train mean: 6.2701311644768545, std: 1.401357074662268\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.3952 rank= 0.0137 kl= 0.1020\n",
      "Val loss epoch 1000: reg= 2.1253 rank= 0.4570 kl= 0.1227\n",
      "Regression R2 : 0.0731, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "y_train mean: 7.047402291516256, std: 1.346623610610329\n",
      "Train size : 128\n",
      "Train loss epoch 1000 : reg= 1.4082 rank= 0.0170 kl= 0.1091\n",
      "Val loss epoch 1000: reg= 2.2719 rank= 0.3785 kl= 0.1311\n",
      "Regression R2 : -0.0215, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 128\n",
      "총 측정 시간: 6.85 초\n",
      "=============================================\n",
      "########## 실험 8/10 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2007\n",
      "초기 랜덤 선택 샘플 인덱스: [  41   88  105  178  199  344  354  473  479  587  621  651  681  738\n",
      "  743  767  836  932  962  977  980  995 1003 1044 1057 1179 1225 1275\n",
      " 1278 1335 1336 1357 1552 1616 1641 1647 1774 1885 1932 1989 2028 2068\n",
      " 2075 2250 2294 2299 2301 2452 2477 2583 2585 2613 2684 2755 2809 2945\n",
      " 2991 3017 3023 3124 3137 3159 3397 3458]\n",
      "=============== 측정 Phase 1 ================\n",
      "y_train mean: 6.3179106950042545, std: 1.3985202326761403\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.3367 rank= 0.0153 kl= 0.1056\n",
      "Val loss epoch 1000: reg= 2.2393 rank= 0.4857 kl= 0.1297\n",
      "Regression R2 : -0.0178, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "y_train mean: 6.857847928245932, std: 1.337271380387718\n",
      "Train size : 128\n",
      "Train loss epoch 1000 : reg= 1.5389 rank= 0.0164 kl= 0.1075\n",
      "Val loss epoch 1000: reg= 1.3310 rank= 0.3176 kl= 0.1363\n",
      "Regression R2 : 0.2696, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 128\n",
      "총 측정 시간: 6.82 초\n",
      "=============================================\n",
      "########## 실험 9/10 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2008\n",
      "초기 랜덤 선택 샘플 인덱스: [  16   49  102  147  160  169  171  179  195  212  447  616  863  868\n",
      "  970 1043 1096 1132 1165 1166 1183 1204 1251 1262 1329 1366 1408 1474\n",
      " 1563 1675 1708 1850 1950 1973 2015 2108 2243 2290 2334 2426 2471 2492\n",
      " 2519 2574 2601 2625 2667 2669 2755 2773 2819 2845 2884 2900 2934 2983\n",
      " 3021 3045 3088 3163 3190 3202 3426 3451]\n",
      "=============== 측정 Phase 1 ================\n",
      "y_train mean: 6.084587986439838, std: 1.3154878106876569\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.0668 rank= 0.0146 kl= 0.0996\n",
      "Val loss epoch 1000: reg= 1.8487 rank= 0.3913 kl= 0.1315\n",
      "Regression R2 : 0.1881, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "y_train mean: 6.888764827698112, std: 1.358915251018972\n",
      "Train size : 128\n",
      "Train loss epoch 1000 : reg= 1.3436 rank= 0.0166 kl= 0.1076\n",
      "Val loss epoch 1000: reg= 1.9903 rank= 0.3370 kl= 0.1367\n",
      "Regression R2 : 0.0539, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 128\n",
      "총 측정 시간: 6.73 초\n",
      "=============================================\n",
      "########## 실험 10/10 ##########\n",
      "weights: (0.4, 0.3, 0.3)\n",
      "measure_size: 64, T_mc: 20, sampling_seed: 2009\n",
      "초기 랜덤 선택 샘플 인덱스: [   0  102  250  308  317  367  447  478  623  689  721  809  835  883\n",
      "  900  902 1008 1088 1142 1228 1246 1281 1317 1323 1452 1456 1488 1574\n",
      " 1620 1673 1726 1788 1804 1877 1949 2047 2067 2246 2266 2282 2315 2449\n",
      " 2472 2482 2511 2523 2547 2624 2656 2732 2822 2846 2852 2951 2956 3007\n",
      " 3045 3076 3258 3266 3357 3391 3394 3450]\n",
      "=============== 측정 Phase 1 ================\n",
      "y_train mean: 6.146279351949605, std: 1.3078588549555408\n",
      "Train size : 64\n",
      "Train loss epoch 1000 : reg= 1.0446 rank= 0.0136 kl= 0.1020\n",
      "Val loss epoch 1000: reg= 1.6156 rank= 0.3908 kl= 0.1259\n",
      "Regression R2 : 0.2024, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "y_train mean: 6.872136351783883, std: 1.2894724332671987\n",
      "Train size : 128\n",
      "Train loss epoch 1000 : reg= 1.2373 rank= 0.0155 kl= 0.1113\n",
      "Val loss epoch 1000: reg= 1.6496 rank= 0.3210 kl= 0.1344\n",
      "Regression R2 : 0.1428, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "y_train mean: 7.19229550513485, std: 1.2103070997314311\n",
      "Train size : 192\n",
      "Train loss epoch 1000 : reg= 1.2946 rank= 0.0157 kl= 0.1104\n",
      "Val loss epoch 1000: reg= 1.7029 rank= 0.2690 kl= 0.1430\n",
      "Regression R2 : 0.1657, \n",
      "Recall@1 : 0\n",
      "=============== 측정 Phase 4 ================\n",
      "y_train mean: 7.353371603566577, std: 1.1845318142759875\n",
      "Train size : 256\n",
      "Train loss epoch 1000 : reg= 1.3523 rank= 0.0155 kl= 0.1094\n",
      "Val loss epoch 1000: reg= 1.3918 rank= 0.2280 kl= 0.1431\n",
      "Regression R2 : 0.1294, \n",
      "Recall@1 : 0\n",
      "최적화 종료\n",
      "학습한 데이터 수 : 256\n",
      "총 측정 시간: 14.66 초\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# 데이터셋 길이만큼의 인덱스 numpy 배열 생성\n",
    "all_indices = np.arange(len(input_data_scaled))\n",
    "costs = -np.log(json_diffs[\"cost\"])\n",
    "\n",
    "real_optimum_index = np.argmax(costs)\n",
    "\n",
    "top_k = 1\n",
    "\n",
    "train_seed = 2023\n",
    "\n",
    "\n",
    "sampling_hyper = {\n",
    "    \"measure_size\": [64],\n",
    "    \"weight\" : [\n",
    "            # (1.0, 0.0, 0.0),\n",
    "            # (0.7, 0.0, 0.3),\n",
    "            # (0.7, 0.3, 0.0),\n",
    "            # (0.6, 0.1, 0.3),\n",
    "            # (0.3, 0.4, 0.3),\n",
    "            (0.4, 0.3, 0.3),\n",
    "            # (0.3, 0.3, 0.4),\n",
    "            # (0.5, 0.2, 0.3),\n",
    "            ],\n",
    "    \"uncertainty_topk\": [64],\n",
    "    # \"weight\" : f_weights,\n",
    "    \"grad_num\": [4],\n",
    "    \"rand_num\": [0],\n",
    "    \n",
    "    \"T_mc\": [20],\n",
    "    \"seed\" : range(2000, 2010),\n",
    "    # \"seed\" : [2023,2025],\n",
    "}\n",
    "\n",
    "random_indices_list = []\n",
    "all_results = []\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%m%d_%H%M\")\n",
    "filename = f\"sch_result/dataset_pkl/vae_sch_{now}.csv\"\n",
    "\n",
    "for params in itertools.product(*sampling_hyper.values()):\n",
    "\n",
    "    cnt += 1\n",
    "    print(f\"########## 실험 {cnt}/{len(list(itertools.product(*sampling_hyper.values())))} ##########\")\n",
    "\n",
    "    tic = time.time()\n",
    "    # used_indices : 이미 측정된 인덱스 집합. train_indices와 동일\n",
    "    # remaining_indices : 아직 측정되지 않은 인덱스 집합. val_indices와 동일\n",
    "    used_indices = set()\n",
    "    remaining_indices = set(all_indices)\n",
    "    \n",
    "    measure_size, weight, uncertainty_topk, grad_num, rand_num, T_mc, sampling_seed = params\n",
    "    w_cost, w_unc, w_div = weight\n",
    "    print(f\"weights: {weight}\")\n",
    "    print(f\"measure_size: {measure_size}, T_mc: {T_mc}, sampling_seed: {sampling_seed}\")\n",
    "\n",
    "    sampling_rng = np.random.default_rng(sampling_seed)\n",
    "\n",
    "    hyperparameter = {\n",
    "\n",
    "        'lambda_reg' : [0.01],\n",
    "        'lambda_pair': [3.0],\n",
    "        'margin_scale': [0.3],\n",
    "        'gamma': [0.01],\n",
    "        'beta': [0.01],\n",
    "        'noise_std': [0.001],\n",
    "\n",
    "        'encoder_lr': [1e-4],\n",
    "        'feature_predictor_lr': [0],\n",
    "        'cost_predictor_lr': [1e-2],\n",
    "        'epochs': [1000],\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    random_indices, remaining_indices = random_select_indices(remaining_indices, select_size=sampling_hyper[\"measure_size\"][0], rng=sampling_rng)\n",
    "    print(f\"초기 랜덤 선택 샘플 인덱스: {np.sort(random_indices)}\")\n",
    "    used_indices.update(random_indices)\n",
    "    random_indices_list.append(random_indices)\n",
    "\n",
    "    reg_history = []\n",
    "    rank_history = []\n",
    "\n",
    "    for phase in range(1, len(input_data_scaled) // measure_size + 1):\n",
    "\n",
    "        print(f\"=============== 측정 Phase {phase} ================\")\n",
    "\n",
    "\n",
    "        # DataLoader 갱신\n",
    "        seed_everything(train_seed)\n",
    "        train_loader, val_loader, y_mean, y_std = make_vae_reg_dataloaders(input_data_scaled, costs, used_indices, remaining_indices)\n",
    "\n",
    "        \n",
    "        vae_cost_model, optimizer, config = make_vae_reg_model(vae, hyperparameter, input_dim, latent_dim, hidden_dim, y_std, verbose=False)\n",
    "        \n",
    "        seed_everything(train_seed)\n",
    "        vae_cost_model, topk_recall_signal, val_reg_r2, val_rank_r2 = train_regression(vae_cost_model, optimizer, train_loader, val_loader, input_data_scaled, costs, config, top_k=top_k, use_rank=False)\n",
    "\n",
    "        reg_history.append(val_reg_r2)\n",
    "        rank_history.append(val_rank_r2)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # 다음 측정할 샘플 선택\n",
    "        selected_indices, remaining_indices = select_programs(\n",
    "            model=vae_cost_model,\n",
    "            input_data_scaled=input_data_scaled,\n",
    "            remaining_indices=remaining_indices,\n",
    "            used_indices=used_indices,\n",
    "            num_select=measure_size,\n",
    "            T_mc=T_mc,\n",
    "            w_cost=weight[0],\n",
    "            w_unc=weight[1],\n",
    "            w_div=weight[2],\n",
    "            # w_cost=0.3,\n",
    "            # w_unc=0.35,\n",
    "            # w_div=0.35,\n",
    "            uncertainty_topk=uncertainty_topk,\n",
    "            grad_num=grad_num,\n",
    "            rand_num=rand_num,\n",
    "            device=device,\n",
    "            rng=sampling_rng,\n",
    "            \n",
    "            topk_factor=5\n",
    "        )\n",
    "        # w_cost += 0.03\n",
    "        # w_unc -= 0.02\n",
    "        # w_div -= 0.01\n",
    "\n",
    "        # selected_indices: numpy 배열\n",
    "        used_indices.update(selected_indices.tolist())\n",
    "\n",
    "        measured_optimum = True if real_optimum_index in used_indices else False\n",
    "\n",
    "\n",
    "        use_topk = False\n",
    "        \n",
    "\n",
    "        break_signal = False\n",
    "        if not use_topk and measured_optimum:\n",
    "            break_signal = True\n",
    "        elif use_topk and topk_recall_signal:\n",
    "            break_signal = True\n",
    "            filename= filename.replace(\"sch_result/\", \"sch_result_topk/\")\n",
    "\n",
    "\n",
    "        if break_signal:\n",
    "            print(\"최적화 종료\")\n",
    "            print(\"학습한 데이터 수 :\", len(used_indices)-measure_size)\n",
    "            used_time = time.time() - tic\n",
    "            print(f\"총 측정 시간: {used_time:.2f} 초\")\n",
    "            print(\"=============================================\")\n",
    "            all_results.append({\n",
    "                \"measure_size\": measure_size,\n",
    "                \"weights\": weight,\n",
    "                \"uncertainty_topk\": uncertainty_topk,\n",
    "                \"grad_num\": grad_num,\n",
    "                \"rand_num\": rand_num,\n",
    "                \"phase\" : phase,\n",
    "                \"used_time\": round(used_time, 2),\n",
    "                \"train_size\" : len(used_indices)-measure_size,\n",
    "                \"val_reg_r2\": reg_history,\n",
    "                \"val_rank_r2\": rank_history,\n",
    "                \"sampling_seed\": sampling_seed,\n",
    "                \n",
    "            })\n",
    "            if use_topk:\n",
    "                all_results[-1][\"top_k\"] = top_k\n",
    "\n",
    "            df_results = pd.DataFrame(all_results)\n",
    "            \n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            df_results.to_csv(filename, index=False)\n",
    "            \n",
    "            break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "feb68db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure_size</th>\n",
       "      <th>weights</th>\n",
       "      <th>uncertainty_topk</th>\n",
       "      <th>grad_num</th>\n",
       "      <th>rand_num</th>\n",
       "      <th>phase</th>\n",
       "      <th>train_size</th>\n",
       "      <th>used_time</th>\n",
       "      <th>val_reg_r2</th>\n",
       "      <th>val_rank_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>(0.4, 0.3, 0.3)</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>147.2</td>\n",
       "      <td>8.039</td>\n",
       "      <td>[0.0257, 0.0527]</td>\n",
       "      <td>[None, None]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   measure_size          weights  uncertainty_topk  grad_num  rand_num  phase  \\\n",
       "0            64  (0.4, 0.3, 0.3)                64         4         0    2.3   \n",
       "\n",
       "   train_size  used_time        val_reg_r2   val_rank_r2  \n",
       "0       147.2      8.039  [0.0257, 0.0527]  [None, None]  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_cols = [\n",
    "    \"measure_size\",\n",
    "    \"weights\",\n",
    "    \"uncertainty_topk\",\n",
    "    \"grad_num\",\n",
    "    \"rand_num\",\n",
    "]\n",
    "\n",
    "agg_dict = {\n",
    "    \"phase\": \"mean\",\n",
    "    \"train_size\": \"mean\",\n",
    "    \"used_time\": \"mean\",\n",
    "    \"val_reg_r2\": \"first\",\n",
    "    \"val_rank_r2\": \"first\",\n",
    "}\n",
    "\n",
    "df_avg = (\n",
    "    df_results\n",
    "    .groupby(group_cols, as_index=False)\n",
    "    .agg(agg_dict)\n",
    ")\n",
    "df_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074df23e",
   "metadata": {},
   "source": [
    "## XGB test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d5436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=UserWarning,\n",
    "    message=\".*Old style callback is deprecated.*\"\n",
    ")\n",
    "\n",
    "from tvm.auto_scheduler.cost_model.xgb_model import XGBModelInternal\n",
    "\n",
    "json_file=\n",
    "inputs, results = auto_scheduler.RecordReader(json_file).read_lines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfa4fafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   8   41   99  108  228  236  253  389  415  435  563  629  639  654\n",
      "  709  743  788  864  900  947  961  971  990  991 1121 1217 1239 1359\n",
      " 1511 1581 1665 1696 1719 1727 1828 1838 1841 1851 1863 1949 1974 1998\n",
      " 2006 2111 2124 2129 2223 2241 2256 2381 2411 2471 2585 2745 2885 3225\n",
      " 3248 3300 3320 3434 3542 3552 3654 3703]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.4698\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.5222\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "Fit a xgb booster. Train size: 192\n",
      "XGB Reg R2 : 0.5323\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 4.03 초\n",
      "=============================================\n",
      "[  26   74   81  121  217  371  395  412  420  440  579  602  697  714\n",
      "  745  748  809  811  817  892  945 1024 1104 1206 1210 1242 1474 1493\n",
      " 1555 1562 1589 1603 1620 1637 1667 1746 1752 1764 1811 1827 1901 1974\n",
      " 2066 2069 2082 2114 2119 2189 2210 2331 2527 2656 2713 2820 2886 3122\n",
      " 3130 3151 3168 3222 3462 3538 3651 3678]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.5438\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.5810\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "Fit a xgb booster. Train size: 192\n",
      "XGB Reg R2 : 0.6565\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 4 ================\n",
      "Fit a xgb booster. Train size: 256\n",
      "XGB Reg R2 : 0.6160\n",
      "XGB Recall@1 : 1\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 4.85 초\n",
      "=============================================\n",
      "[  15  135  166  294  337  392  399  406  465  490  507  528  556  560\n",
      "  632  711  728  730  831  860  877  915  921  928 1171 1198 1309 1385\n",
      " 1512 1535 1643 1675 1685 1768 1793 1881 1902 1918 2029 2124 2136 2178\n",
      " 2235 2276 2419 2538 2548 2842 3066 3102 3110 3263 3271 3306 3314 3354\n",
      " 3395 3396 3447 3465 3520 3529 3553 3595]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.6159\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.6019\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "Fit a xgb booster. Train size: 192\n",
      "XGB Reg R2 : 0.6477\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 4 ================\n",
      "Fit a xgb booster. Train size: 256\n",
      "XGB Reg R2 : 0.6020\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 5 ================\n",
      "Fit a xgb booster. Train size: 320\n",
      "XGB Reg R2 : 0.6259\n",
      "XGB Recall@1 : 1\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 5.82 초\n",
      "=============================================\n",
      "[  19   57   69  146  187  276  304  396  470  522  627  714  750  878\n",
      "  961  979  997 1048 1081 1115 1168 1181 1269 1297 1336 1432 1436 1558\n",
      " 1619 1712 1852 1933 2043 2063 2094 2132 2202 2223 2394 2402 2428 2449\n",
      " 2456 2467 2473 2622 2680 2920 2924 2934 2988 3171 3190 3230 3234 3237\n",
      " 3398 3409 3417 3418 3479 3614 3672 3711]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.5736\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.5157\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "Fit a xgb booster. Train size: 192\n",
      "XGB Reg R2 : 0.6200\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 4 ================\n",
      "Fit a xgb booster. Train size: 256\n",
      "XGB Reg R2 : 0.6610\n",
      "XGB Recall@1 : 1\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 4.81 초\n",
      "=============================================\n",
      "[  10  111  113  116  194  252  340  497  502  543  618  684 1068 1084\n",
      " 1095 1133 1147 1177 1291 1297 1305 1394 1404 1427 1474 1553 1563 1580\n",
      " 1685 1815 1971 2019 2020 2063 2073 2137 2172 2248 2280 2286 2293 2350\n",
      " 2437 2492 2547 2597 2603 2607 2613 2625 2639 2784 2794 2954 3011 3055\n",
      " 3082 3111 3236 3368 3372 3420 3514 3538]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.4630\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.5990\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.28 초\n",
      "=============================================\n",
      "[  12   33   61  108  110  360  420  485  567  675  701  715  720  737\n",
      "  770  786  838  963  982 1141 1173 1274 1307 1309 1389 1420 1445 1473\n",
      " 1722 1777 1866 1868 2044 2083 2103 2156 2178 2203 2266 2298 2345 2399\n",
      " 2457 2481 2517 2531 2635 2648 2669 2684 2737 2739 2788 2944 2959 2981\n",
      " 3143 3300 3320 3350 3466 3503 3517 3567]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.3881\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.5376\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.34 초\n",
      "=============================================\n",
      "[  18   46   77  203  211  213  276  350  462  535  541  560  700  730\n",
      "  809 1086 1112 1113 1139 1284 1317 1324 1420 1509 1538 1571 1671 1675\n",
      " 1747 1840 2050 2054 2157 2185 2259 2336 2433 2445 2453 2489 2509 2555\n",
      " 2689 2829 2830 2899 3009 3037 3047 3152 3160 3183 3193 3201 3239 3305\n",
      " 3339 3342 3425 3491 3533 3563 3658 3689]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.5962\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.5675\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "Fit a xgb booster. Train size: 192\n",
      "XGB Reg R2 : 0.4604\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 4 ================\n",
      "Fit a xgb booster. Train size: 256\n",
      "XGB Reg R2 : 0.5493\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 4.93 초\n",
      "=============================================\n",
      "[  45   95  113  192  215  372  382  510  516  633  670  702  735  796\n",
      "  802  827  902 1005 1006 1038 1054 1057 1074 1082 1125 1140 1271 1322\n",
      " 1375 1378 1440 1442 1464 1674 1744 1771 1776 1913 2034 2084 2145 2186\n",
      " 2230 2240 2428 2475 2479 2482 2646 2673 2784 2790 2818 2894 2971 3032\n",
      " 3177 3225 3255 3260 3370 3383 3408 3663]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.4937\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.4923\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.33 초\n",
      "=============================================\n",
      "[  17   52  111  159  172  182  185  193  211  229  482  664  930  937\n",
      " 1046 1125 1183 1222 1256 1258 1276 1298 1350 1360 1433 1475 1519 1589\n",
      " 1685 1805 1842 1996 2102 2127 2174 2274 2419 2472 2520 2618 2664 2689\n",
      " 2718 2779 2806 2833 2877 2878 2971 2990 3039 3068 3069 3112 3126 3164\n",
      " 3218 3260 3287 3331 3412 3441 3455 3694]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.5632\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.6486\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "Fit a xgb booster. Train size: 192\n",
      "XGB Reg R2 : 0.6080\n",
      "XGB Recall@1 : 1\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 4.06 초\n",
      "=============================================\n",
      "[   0  110  270  332  342  396  482  516  672  744  778  873  901  952\n",
      "  970  972 1088 1173 1233 1324 1343 1382 1422 1428 1566 1572 1605 1698\n",
      " 1747 1804 1862 1928 1946 2023 2102 2209 2230 2423 2444 2460 2498 2641\n",
      " 2668 2676 2708 2722 2747 2831 2867 2947 3042 3071 3076 3182 3188 3242\n",
      " 3285 3320 3514 3523 3623 3658 3659 3720]\n",
      "=============== 측정 Phase 1 ================\n",
      "Fit a xgb booster. Train size: 64\n",
      "XGB Reg R2 : 0.3922\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "Fit a xgb booster. Train size: 128\n",
      "XGB Reg R2 : 0.5490\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.37 초\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "topk_size = int(measure_size * 0.95)\n",
    "eps_greedy_size = measure_size - topk_size\n",
    "\n",
    "\n",
    "seeds = sampling_hyper[\"seed\"]\n",
    "random_indices = random_indices_list[:len(seeds)]\n",
    "\n",
    "xgb_results = []\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%m%d_%H%M\")\n",
    "xgb_filename = f\"result_xgb/{os.path.basename(json_file)}/xgb_search_{now}.csv\"\n",
    "\n",
    "for i, seed in enumerate(seeds):\n",
    "\n",
    "    tic = time.time()\n",
    "    sample_rng = np.random.default_rng(seed)\n",
    "\n",
    "    \n",
    "    \n",
    "    tenset_model = XGBModelInternal(use_workload_embedding=False, seed=train_seed)\n",
    "\n",
    "    seed_everything(train_seed)\n",
    "    dataset = make_xgb_datasets(inputs, results)\n",
    "\n",
    "    \n",
    "    used_indices = set(random_indices[i])\n",
    "    remaining_indices = set(all_indices)\n",
    "    remaining_indices.difference_update(used_indices)\n",
    "\n",
    "    train_indices = np.array(sorted(used_indices), dtype=np.int64)\n",
    "    test_indices = np.array(sorted(remaining_indices), dtype=np.int64)\n",
    "    print(train_indices)\n",
    "\n",
    "    reg_history = []\n",
    "    rank_history = []\n",
    "\n",
    "    for phase in range(1,  len(input_data_scaled) // measure_size + 1):\n",
    "\n",
    "        print(f\"=============== 측정 Phase {phase} ================\")\n",
    "\n",
    "        seed_everything(train_seed)\n",
    "        train_set, test_set, dataset_costs = split_xgb_datasets(dataset, train_indices, test_indices)\n",
    "        real_optimum_idx = np.argmax(dataset_costs)\n",
    "        seed_everything(train_seed)\n",
    "        tenset_model.fit_base(train_set=train_set)\n",
    "        xgb_all_preds = tenset_model.predict(dataset)\n",
    "        xgb_all_preds = np.array(list(xgb_all_preds.values())[0], dtype=np.float32)\n",
    "        \n",
    "        \n",
    "        xgb_reg_r2 = r2_score(dataset_costs, xgb_all_preds)\n",
    "        reg_history.append(round(xgb_reg_r2, 4))\n",
    "        print(f\"XGB Reg R2 : {xgb_reg_r2:.4f}\")\n",
    "\n",
    "        # xgb_rank_r2 = pair_accuracy(xgb_all_preds, dataset_costs)\n",
    "        # rank_history.append(round(xgb_rank_r2, 4))\n",
    "        # print(f\"XGB Rank R2 : {xgb_rank_r2:.4f}\")\n",
    "\n",
    "        recall_score = recall_at_k(torch.tensor(xgb_all_preds), torch.tensor(dataset_costs), k=10)        \n",
    "        print(f\"XGB Recall@{top_k} : {recall_score}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # 다음 측정할 샘플 선택\n",
    "        train_indices, test_indices = xgb_select_indices(xgb_all_preds, \n",
    "                            train_indices, test_indices, topk_size=topk_size, eps_greedy_size=eps_greedy_size, rng=sample_rng)\n",
    "        measured_optimum = True if real_optimum_idx in train_indices else False\n",
    "\n",
    "        use_topk = False\n",
    "        \n",
    "\n",
    "        break_signal = False\n",
    "        if not use_topk and measured_optimum:\n",
    "            break_signal = True\n",
    "            \n",
    "        elif use_topk and recall_score:\n",
    "            break_signal = True\n",
    "            xgb_filename= xgb_filename.replace(\"result_xgb/\", \"result_xgb_topk/\")\n",
    "\n",
    "\n",
    "        if break_signal:\n",
    "        # if recall_score:\n",
    "            print(\"XGB 최적화 종료 신호 감지\")\n",
    "            print(f\"총 측정 시간: {time.time() - tic:.2f} 초\")\n",
    "            print(\"=============================================\")\n",
    "            xgb_results.append({\n",
    "                \"measure_size\": measure_size,\n",
    "                \"phase\" : phase,\n",
    "                \"used_time\": round(time.time() - tic, 2),\n",
    "                \"train_size\" : len(train_indices) - measure_size,\n",
    "                \"val_reg_r2\": reg_history,\n",
    "                \"val_rank_r2\": rank_history,\n",
    "                \"sampling_seed\": seed,\n",
    "                \n",
    "            })\n",
    "            df_xgb_results = pd.DataFrame(xgb_results)\n",
    "            os.makedirs(os.path.dirname(xgb_filename), exist_ok=True)\n",
    "            df_xgb_results.to_csv(xgb_filename, index=False)\n",
    "            # raise KeyboardInterrupt\n",
    "            break\n",
    "        \n",
    "        if test_indices.shape[0] < measure_size:\n",
    "            print(\"측정할 샘플이 더 이상 남아있지 않음\")\n",
    "            xgb_results.append({\n",
    "                \"measure_size\": measure_size,\n",
    "                \"phase\" : \"all but not found\",\n",
    "                \"used_time\": round(time.time() - tic, 2),\n",
    "                \"train_size\" : len(train_indices) - measure_size,\n",
    "                \"val_reg_r2\": reg_history,\n",
    "                \"val_rank_r2\": rank_history,\n",
    "                \"sampling_seed\": seed,\n",
    "                \n",
    "            })\n",
    "            df_xgb_results = pd.DataFrame(xgb_results)\n",
    "            os.makedirs(os.path.dirname(xgb_filename), exist_ok=True)\n",
    "            df_xgb_results.to_csv(xgb_filename, index=False)\n",
    "            break\n",
    "            # raise KeyboardInterrupt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d6f6988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure_size</th>\n",
       "      <th>train_size</th>\n",
       "      <th>used_time</th>\n",
       "      <th>val_reg_r2</th>\n",
       "      <th>val_rank_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>198.4</td>\n",
       "      <td>4.182</td>\n",
       "      <td>[0.4698, 0.5222, 0.5323]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   measure_size  train_size  used_time                val_reg_r2 val_rank_r2\n",
       "0            64       198.4      4.182  [0.4698, 0.5222, 0.5323]          []"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_cols = [\n",
    "    \"measure_size\",\n",
    "]\n",
    "\n",
    "agg_dict = {\n",
    "    # \"phase\": \"mean\",\n",
    "    \"train_size\": \"mean\",\n",
    "    \"used_time\": \"mean\",\n",
    "    \"val_reg_r2\": \"first\",\n",
    "    \"val_rank_r2\": \"first\",\n",
    "}\n",
    "\n",
    "df_avg = (\n",
    "    df_xgb_results\n",
    "    .groupby(group_cols, as_index=False)\n",
    "    .agg(agg_dict)\n",
    ")\n",
    "df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf23a3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   8   41   99  108  228  236  253  389  415  435  563  629  639  654\n",
      "  709  743  788  864  900  947  961  971  990  991 1121 1217 1239 1359\n",
      " 1511 1581 1665 1696 1719 1727 1828 1838 1841 1851 1863 1949 1974 1998\n",
      " 2006 2111 2124 2129 2223 2241 2256 2381 2411 2471 2585 2745 2885 3225\n",
      " 3248 3300 3320 3434 3542 3552 3654 3703]\n",
      "=============== 측정 Phase 1 ================\n",
      "XGB Reg R2 : 0.6295\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.15 초\n",
      "=============================================\n",
      "[  26   74   81  121  217  371  395  412  420  440  579  602  697  714\n",
      "  745  748  809  811  817  892  945 1024 1104 1206 1210 1242 1474 1493\n",
      " 1555 1562 1589 1603 1620 1637 1667 1746 1752 1764 1811 1827 1901 1974\n",
      " 2066 2069 2082 2114 2119 2189 2210 2331 2527 2656 2713 2820 2886 3122\n",
      " 3130 3151 3168 3222 3462 3538 3651 3678]\n",
      "=============== 측정 Phase 1 ================\n",
      "XGB Reg R2 : 0.5064\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "XGB Reg R2 : 0.6101\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "XGB Reg R2 : 0.6527\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 4 ================\n",
      "XGB Reg R2 : 0.6519\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 5 ================\n",
      "XGB Reg R2 : 0.6354\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 6.34 초\n",
      "=============================================\n",
      "[  15  135  166  294  337  392  399  406  465  490  507  528  556  560\n",
      "  632  711  728  730  831  860  877  915  921  928 1171 1198 1309 1385\n",
      " 1512 1535 1643 1675 1685 1768 1793 1881 1902 1918 2029 2124 2136 2178\n",
      " 2235 2276 2419 2538 2548 2842 3066 3102 3110 3263 3271 3306 3314 3354\n",
      " 3395 3396 3447 3465 3520 3529 3553 3595]\n",
      "=============== 측정 Phase 1 ================\n",
      "XGB Reg R2 : 0.5497\n",
      "XGB Recall@1 : 1\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.13 초\n",
      "=============================================\n",
      "[  19   57   69  146  187  276  304  396  470  522  627  714  750  878\n",
      "  961  979  997 1048 1081 1115 1168 1181 1269 1297 1336 1432 1436 1558\n",
      " 1619 1712 1852 1933 2043 2063 2094 2132 2202 2223 2394 2402 2428 2449\n",
      " 2456 2467 2473 2622 2680 2920 2924 2934 2988 3171 3190 3230 3234 3237\n",
      " 3398 3409 3417 3418 3479 3614 3672 3711]\n",
      "=============== 측정 Phase 1 ================\n",
      "XGB Reg R2 : 0.4898\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "XGB Reg R2 : 0.6130\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.89 초\n",
      "=============================================\n",
      "[  10  111  113  116  194  252  340  497  502  543  618  684 1068 1084\n",
      " 1095 1133 1147 1177 1291 1297 1305 1394 1404 1427 1474 1553 1563 1580\n",
      " 1685 1815 1971 2019 2020 2063 2073 2137 2172 2248 2280 2286 2293 2350\n",
      " 2437 2492 2547 2597 2603 2607 2613 2625 2639 2784 2794 2954 3011 3055\n",
      " 3082 3111 3236 3368 3372 3420 3514 3538]\n",
      "=============== 측정 Phase 1 ================\n",
      "XGB Reg R2 : 0.5044\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "XGB Reg R2 : 0.3218\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "XGB Reg R2 : 0.7177\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 4 ================\n",
      "XGB Reg R2 : 0.7214\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 5 ================\n",
      "XGB Reg R2 : 0.7610\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 6 ================\n",
      "XGB Reg R2 : 0.7626\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 7 ================\n",
      "XGB Reg R2 : 0.7528\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 7.88 초\n",
      "=============================================\n",
      "[  12   33   61  108  110  360  420  485  567  675  701  715  720  737\n",
      "  770  786  838  963  982 1141 1173 1274 1307 1309 1389 1420 1445 1473\n",
      " 1722 1777 1866 1868 2044 2083 2103 2156 2178 2203 2266 2298 2345 2399\n",
      " 2457 2481 2517 2531 2635 2648 2669 2684 2737 2739 2788 2944 2959 2981\n",
      " 3143 3300 3320 3350 3466 3503 3517 3567]\n",
      "=============== 측정 Phase 1 ================\n",
      "XGB Reg R2 : 0.5193\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.12 초\n",
      "=============================================\n",
      "[  18   46   77  203  211  213  276  350  462  535  541  560  700  730\n",
      "  809 1086 1112 1113 1139 1284 1317 1324 1420 1509 1538 1571 1671 1675\n",
      " 1747 1840 2050 2054 2157 2185 2259 2336 2433 2445 2453 2489 2509 2555\n",
      " 2689 2829 2830 2899 3009 3037 3047 3152 3160 3183 3193 3201 3239 3305\n",
      " 3339 3342 3425 3491 3533 3563 3658 3689]\n",
      "=============== 측정 Phase 1 ================\n",
      "XGB Reg R2 : 0.5668\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "XGB Reg R2 : 0.6046\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 3 ================\n",
      "XGB Reg R2 : 0.6341\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 4.65 초\n",
      "=============================================\n",
      "[  45   95  113  192  215  372  382  510  516  633  670  702  735  796\n",
      "  802  827  902 1005 1006 1038 1054 1057 1074 1082 1125 1140 1271 1322\n",
      " 1375 1378 1440 1442 1464 1674 1744 1771 1776 1913 2034 2084 2145 2186\n",
      " 2230 2240 2428 2475 2479 2482 2646 2673 2784 2790 2818 2894 2971 3032\n",
      " 3177 3225 3255 3260 3370 3383 3408 3663]\n",
      "=============== 측정 Phase 1 ================\n",
      "XGB Reg R2 : 0.3925\n",
      "XGB Recall@1 : 0\n",
      "=============== 측정 Phase 2 ================\n",
      "XGB Reg R2 : 0.5128\n",
      "XGB Recall@1 : 0\n",
      "XGB 최적화 종료 신호 감지\n",
      "총 측정 시간: 3.88 초\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import multiprocessing\n",
    "\n",
    "topk_size = int(measure_size * 0.95)\n",
    "eps_greedy_size = measure_size - topk_size\n",
    "\n",
    "\n",
    "seeds = sampling_hyper[\"seed\"]\n",
    "random_indices = random_indices_list[:len(seeds)]\n",
    "\n",
    "xgb_results = []\n",
    "\n",
    "# XGBModelInternal과 동일한 xgb_params 설정\n",
    "xgb_params = {\n",
    "    \"max_depth\": 6,\n",
    "    \"gamma\": 0.003,\n",
    "    \"min_child_weight\": 2,\n",
    "    \"eta\": 0.2,\n",
    "    \"n_gpus\": 0,\n",
    "    \"nthread\": multiprocessing.cpu_count() // 2,\n",
    "    \"verbosity\": 0,\n",
    "    \"seed\": train_seed or 43,\n",
    "    \"disable_default_eval_metric\": 1,\n",
    "}\n",
    "\n",
    "for i, seed in enumerate(seeds):\n",
    "\n",
    "    tic = time.time()\n",
    "    sample_rng = np.random.default_rng(seed)\n",
    "\n",
    "    dataset = make_xgb_datasets(inputs, results)\n",
    "    \n",
    "    used_indices = set(random_indices[i])\n",
    "    remaining_indices = set(all_indices)\n",
    "    remaining_indices.difference_update(used_indices)\n",
    "\n",
    "    train_indices = np.array(sorted(used_indices), dtype=np.int64)\n",
    "    test_indices = np.array(sorted(remaining_indices), dtype=np.int64)\n",
    "    print(train_indices)\n",
    "\n",
    "    reg_history = []\n",
    "    rank_history = []\n",
    "\n",
    "    for phase in range(1,  len(input_data_scaled) // measure_size + 1):\n",
    "\n",
    "        print(f\"=============== 측정 Phase {phase} ================\")\n",
    "\n",
    "        seed_everything(train_seed)\n",
    "        _, _, dataset_costs = split_xgb_datasets(dataset, train_indices, test_indices)\n",
    "        input_train = input_data_scaled[train_indices]\n",
    "        label_train = dataset_costs[train_indices]\n",
    "        input_test = input_data_scaled[test_indices]\n",
    "        label_test = dataset_costs[test_indices]\n",
    "        \n",
    "        real_optimum_idx = np.argmax(dataset_costs)\n",
    "        \n",
    "        # XGB 모델 학습 - input_train, label_train 사용\n",
    "        seed_everything(train_seed)\n",
    "        dtrain = xgb.DMatrix(input_train, label=label_train)\n",
    "        dtest = xgb.DMatrix(input_test, label=label_test)\n",
    "        \n",
    "        # 학습 (XGBModelInternal과 유사하게 num_boost_round=300, early stopping 없이 단순화)\n",
    "        bst = xgb.train(\n",
    "            params=xgb_params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=300,\n",
    "            evals=[(dtrain, \"train\"), (dtest, \"test\")],\n",
    "            verbose_eval=50,\n",
    "        )\n",
    "        \n",
    "        # input_data_scaled 전체로 predict\n",
    "        dmatrix_all = xgb.DMatrix(input_data_scaled)\n",
    "        xgb_all_preds = bst.predict(dmatrix_all)\n",
    "        xgb_all_preds = np.array(xgb_all_preds, dtype=np.float32)\n",
    "        \n",
    "        \n",
    "        xgb_reg_r2 = r2_score(dataset_costs, xgb_all_preds)\n",
    "        reg_history.append(round(xgb_reg_r2, 4))\n",
    "        print(f\"XGB Reg R2 : {xgb_reg_r2:.4f}\")\n",
    "\n",
    "        # xgb_rank_r2 = pair_accuracy(xgb_all_preds, dataset_costs)\n",
    "        # rank_history.append(round(xgb_rank_r2, 4))\n",
    "        # print(f\"XGB Rank R2 : {xgb_rank_r2:.4f}\")\n",
    "\n",
    "        recall_score = recall_at_k(torch.tensor(xgb_all_preds), torch.tensor(dataset_costs), k=10)        \n",
    "        print(f\"XGB Recall@{top_k} : {recall_score}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # 다음 측정할 샘플 선택\n",
    "        train_indices, test_indices = xgb_select_indices(xgb_all_preds, \n",
    "                            train_indices, test_indices, topk_size=topk_size, eps_greedy_size=eps_greedy_size, rng=sample_rng)\n",
    "        measured_optimum = True if real_optimum_idx in train_indices else False\n",
    "\n",
    "        use_topk = False\n",
    "        \n",
    "\n",
    "        break_signal = False\n",
    "        if not use_topk and measured_optimum:\n",
    "            break_signal = True\n",
    "            \n",
    "        elif use_topk and recall_score:\n",
    "            break_signal = True\n",
    "            xgb_filename= xgb_filename.replace(\"result_xgb/\", \"result_xgb_topk/topk_\")\n",
    "\n",
    "\n",
    "        if break_signal:\n",
    "            print(\"XGB 최적화 종료 신호 감지\")\n",
    "            print(f\"총 측정 시간: {time.time() - tic:.2f} 초\")\n",
    "            print(\"=============================================\")\n",
    "            xgb_results.append({\n",
    "                \"measure_size\": measure_size,\n",
    "                \"phase\" : phase,\n",
    "                \"used_time\": round(time.time() - tic, 2),\n",
    "                \"train_size\" : len(train_indices) - measure_size,\n",
    "                \"val_reg_r2\": reg_history,\n",
    "                \"val_rank_r2\": rank_history,\n",
    "                \"sampling_seed\": seed,\n",
    "                \n",
    "            })\n",
    "            df_xgb_results = pd.DataFrame(xgb_results)\n",
    "            # df_xgb_results.to_csv(xgb_filename, index=False)\n",
    "            break\n",
    "        \n",
    "        if test_indices.shape[0] < measure_size:\n",
    "            print(\"측정할 샘플이 더 이상 남아있지 않음\")\n",
    "            xgb_results.append({\n",
    "                \"measure_size\": measure_size,\n",
    "                \"phase\" : \"all but not found\",\n",
    "                \"used_time\": round(time.time() - tic, 2),\n",
    "                \"train_size\" : len(train_indices) - measure_size,\n",
    "                \"val_reg_r2\": reg_history,\n",
    "                \"val_rank_r2\": rank_history,\n",
    "                \"sampling_seed\": seed,\n",
    "                \n",
    "            })\n",
    "            df_xgb_results = pd.DataFrame(xgb_results)\n",
    "            # df_xgb_results.to_csv(xgb_filename, index=False)\n",
    "            break\n",
    "            # raise KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "83a274fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure_size</th>\n",
       "      <th>phase</th>\n",
       "      <th>used_time</th>\n",
       "      <th>train_size</th>\n",
       "      <th>val_reg_r2</th>\n",
       "      <th>val_rank_r2</th>\n",
       "      <th>sampling_seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>3.16</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.455, 0.4711]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>4.44</td>\n",
       "      <td>320</td>\n",
       "      <td>[0.5689, 0.5475, 0.5938, 0.5836, 0.6436]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>4.01</td>\n",
       "      <td>256</td>\n",
       "      <td>[0.588, 0.5441, 0.5812, 0.6114]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>3.09</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.5876, 0.5477]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>3.24</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.4325, 0.608]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>3.09</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.4016, 0.5381]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>3.09</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.5755, 0.333]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>5.60</td>\n",
       "      <td>448</td>\n",
       "      <td>[0.4894, 0.5619, 0.478, 0.5676, 0.5699, 0.5365...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>3.11</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.5457, 0.6573]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>3.92</td>\n",
       "      <td>256</td>\n",
       "      <td>[0.3686, 0.5665, 0.5754, 0.6717]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   measure_size  phase  used_time  train_size  \\\n",
       "0            64      2       3.16         128   \n",
       "1            64      5       4.44         320   \n",
       "2            64      4       4.01         256   \n",
       "3            64      2       3.09         128   \n",
       "4            64      2       3.24         128   \n",
       "5            64      2       3.09         128   \n",
       "6            64      2       3.09         128   \n",
       "7            64      7       5.60         448   \n",
       "8            64      2       3.11         128   \n",
       "9            64      4       3.92         256   \n",
       "\n",
       "                                          val_reg_r2 val_rank_r2  \\\n",
       "0                                    [0.455, 0.4711]          []   \n",
       "1           [0.5689, 0.5475, 0.5938, 0.5836, 0.6436]          []   \n",
       "2                    [0.588, 0.5441, 0.5812, 0.6114]          []   \n",
       "3                                   [0.5876, 0.5477]          []   \n",
       "4                                    [0.4325, 0.608]          []   \n",
       "5                                   [0.4016, 0.5381]          []   \n",
       "6                                    [0.5755, 0.333]          []   \n",
       "7  [0.4894, 0.5619, 0.478, 0.5676, 0.5699, 0.5365...          []   \n",
       "8                                   [0.5457, 0.6573]          []   \n",
       "9                   [0.3686, 0.5665, 0.5754, 0.6717]          []   \n",
       "\n",
       "   sampling_seed  \n",
       "0           2000  \n",
       "1           2001  \n",
       "2           2002  \n",
       "3           2003  \n",
       "4           2004  \n",
       "5           2005  \n",
       "6           2006  \n",
       "7           2007  \n",
       "8           2008  \n",
       "9           2009  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xgb_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d57c684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure_size</th>\n",
       "      <th>train_size</th>\n",
       "      <th>used_time</th>\n",
       "      <th>val_reg_r2</th>\n",
       "      <th>val_rank_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>176.0</td>\n",
       "      <td>4.505</td>\n",
       "      <td>[0.6295]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   measure_size  train_size  used_time val_reg_r2 val_rank_r2\n",
       "0            64       176.0      4.505   [0.6295]          []"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_cols = [\n",
    "    \"measure_size\",\n",
    "]\n",
    "\n",
    "agg_dict = {\n",
    "    # \"phase\": \"mean\",\n",
    "    \"train_size\": \"mean\",\n",
    "    \"used_time\": \"mean\",\n",
    "    \"val_reg_r2\": \"first\",\n",
    "    \"val_rank_r2\": \"first\",\n",
    "}\n",
    "\n",
    "df_avg = (\n",
    "    df_xgb_results\n",
    "    .groupby(group_cols, as_index=False)\n",
    "    .agg(agg_dict)\n",
    ")\n",
    "df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336e266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit a xgb booster. Train size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/work/tenset/.venv/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenset 모델 Rank Accuracy: 0.8091\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tvm.auto_scheduler.cost_model.xgb_model import XGBModelInternal\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    tenset_model = XGBModelInternal()\n",
    "    tenset_model.fit_base(train_set, valid_set=test_set)\n",
    "    throughputs = np.array(list(test_set.throughputs.values()))\n",
    "\n",
    "    pred = tenset_model.predict(test_set)\n",
    "\n",
    "    true_biggest_index = np.argsort(throughputs[0])[-1]\n",
    "    biggest_indices_64 = np.argsort(list(pred.values())[0])[-64:]\n",
    "\n",
    "    # list(pred.values())[0]\n",
    "    if true_biggest_index in biggest_indices_64:\n",
    "        print(\"✓ Tenset 모델이 실제 가장 높은 throughput 정확히 예측했습니다!\")\n",
    "        break\n",
    "    break\n",
    "\n",
    "\n",
    "# pred, throughputs rank accuracy\n",
    "correct_pairs = 0\n",
    "total_pairs = 0\n",
    "n_samples = min(2000, throughputs.shape[-1])\n",
    "sample_indices = np.random.choice(throughputs.shape[-1], n_samples, replace=False)\n",
    "pred_values = list(pred.values())[0]\n",
    "throughput_values = throughputs.squeeze()\n",
    "rank_accuracy = pair_accuracy(pred_values, throughput_values)\n",
    "print(f\"Tenset 모델 Rank Accuracy: {rank_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
