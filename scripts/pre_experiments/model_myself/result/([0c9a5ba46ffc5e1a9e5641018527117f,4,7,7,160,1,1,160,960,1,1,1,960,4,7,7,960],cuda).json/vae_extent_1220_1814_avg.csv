encoder_lr,cost_predictor_lr,rank_warmup_epochs,weights,uncertainty_topk,grad_num,rand_num,phase,train_size,used_time,top-1,val_reg_r2,val_rank_r2,sampling_seed
1e-05,1e-05,100,"(0.4, 0.3, 0.3)",64,4,0,1.4,89.6,4.997999999999999,0.3,[0.6203],[None],"[2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]"
1e-05,1e-05,200,"(0.4, 0.3, 0.3)",64,4,0,1.5,96.0,5.297,0.4,[0.6518],[None],"[2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]"
1e-05,1e-05,300,"(0.4, 0.3, 0.3)",64,4,0,1.6,102.4,5.7299999999999995,0.4,[0.6725],[None],"[2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]"
1e-05,0.0001,100,"(0.4, 0.3, 0.3)",64,4,0,1.6,102.4,5.808,0.2,[0.6498],[None],"[2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]"
1e-05,0.0001,200,"(0.4, 0.3, 0.3)",64,4,0,1.5,96.0,5.332,0.0,[0.6379],[None],"[2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]"
1e-05,0.0001,300,"(0.4, 0.3, 0.3)",64,4,0,1.4,89.6,4.931,0.1,[0.6373],[None],"[2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]"
0.0001,0.0001,100,"(0.4, 0.3, 0.3)",64,4,0,1.7,108.8,6.1339999999999995,0.2,[0.7003],[None],"[2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]"
0.0001,0.0001,200,"(0.4, 0.3, 0.3)",64,4,0,2.0,128.0,8.003,0.3,[0.7047],[None],"[2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]"
0.0001,0.0001,300,"(0.4, 0.3, 0.3)",64,4,0,1.7,108.8,6.359,0.2,[0.7035],[None],"[2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]"
