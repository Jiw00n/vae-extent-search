measure_size,encoder_lr,cost_predictor_lr,rank_warmup_epochs,weights,uncertainty_topk,grad_num,rand_num,phase,train_size,used_time,top-1,val_reg_r2,val_rank_r2,seed_n,sampling_seed
64,1e-05,1e-05,500,"(0.4, 0.3, 0.3)",64,4,0,1.3,83.2,4.6475,0.3,[0.7001],[None],20,"[2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]"
64,1e-05,0.0001,500,"(0.4, 0.3, 0.3)",64,4,0,1.25,80.0,4.42,0.25,[0.6931],[None],20,"[2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]"
64,0.0001,1e-05,100,"(0.4, 0.3, 0.3)",64,4,0,1.5,96.0,5.417,0.1,[0.6047],[None],10,"[2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]"
64,0.0001,1e-05,200,"(0.4, 0.3, 0.3)",64,4,0,1.6,102.4,5.859,0.3,[0.6092],[None],10,"[2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]"
64,0.0001,1e-05,300,"(0.4, 0.3, 0.3)",64,4,0,1.5,96.0,5.3740000000000006,0.2,[0.6194],[None],10,"[2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]"
64,0.0001,1e-05,400,"(0.4, 0.3, 0.3)",64,4,0,1.5,96.0,5.3740000000000006,0.3,[0.6248],[None],20,"[2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]"
64,0.0001,1e-05,500,"(0.4, 0.3, 0.3)",64,4,0,1.6,102.4,5.8255,0.25,[0.6256],[None],20,"[2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]"
64,0.0001,0.0001,400,"(0.4, 0.3, 0.3)",64,4,0,1.45,92.8,5.442500000000001,0.25,[0.7098],[None],20,"[2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]"
64,0.0001,0.0001,500,"(0.4, 0.3, 0.3)",64,4,0,1.55,99.2,5.723000000000001,0.3,[0.7091],[None],20,"[2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]"
